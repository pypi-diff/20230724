# Comparing `tmp/functime-0.2.4-py3-none-any.whl.zip` & `tmp/functime-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,45 +1,44 @@
-Zip file size: 57973 bytes, number of entries: 43
--rw-r--r--  2.0 unx       66 b- defN 23-Jul-18 15:20 functime/__init__.py
--rw-r--r--  2.0 unx      117 b- defN 23-Jul-18 15:20 functime/__main__.py
--rw-r--r--  2.0 unx     5893 b- defN 23-Jul-18 15:20 functime/backtesting.py
--rw-r--r--  2.0 unx     1838 b- defN 23-Jul-18 15:20 functime/conformal.py
--rw-r--r--  2.0 unx     1379 b- defN 23-Jul-18 15:20 functime/conversion.py
--rw-r--r--  2.0 unx     5549 b- defN 23-Jul-18 15:20 functime/cross_validation.py
--rw-r--r--  2.0 unx      123 b- defN 23-Jul-18 15:20 functime/embeddings.py
--rw-r--r--  2.0 unx     2241 b- defN 23-Jul-18 15:20 functime/offsets.py
--rw-r--r--  2.0 unx      393 b- defN 23-Jul-18 15:20 functime/plotting.py
--rw-r--r--  2.0 unx    18763 b- defN 23-Jul-18 15:20 functime/preprocessing.py
--rw-r--r--  2.0 unx     1976 b- defN 23-Jul-18 15:20 functime/ranges.py
--rw-r--r--  2.0 unx     1481 b- defN 23-Jul-18 15:20 functime/stats.py
--rw-r--r--  2.0 unx      610 b- defN 23-Jul-18 15:20 functime/umap.py
--rw-r--r--  2.0 unx      217 b- defN 23-Jul-18 15:20 functime/base/__init__.py
--rw-r--r--  2.0 unx     7424 b- defN 23-Jul-18 15:20 functime/base/forecaster.py
--rw-r--r--  2.0 unx     1630 b- defN 23-Jul-18 15:20 functime/base/metric.py
--rw-r--r--  2.0 unx     2771 b- defN 23-Jul-18 15:20 functime/base/model.py
--rw-r--r--  2.0 unx     2237 b- defN 23-Jul-18 15:20 functime/base/transformer.py
--rw-r--r--  2.0 unx      285 b- defN 23-Jul-18 15:20 functime/feature_extraction/__init__.py
--rw-r--r--  2.0 unx     4064 b- defN 23-Jul-18 15:20 functime/feature_extraction/calendar.py
--rw-r--r--  2.0 unx      745 b- defN 23-Jul-18 15:20 functime/forecasting/__init__.py
--rw-r--r--  2.0 unx    12183 b- defN 23-Jul-18 15:20 functime/forecasting/_ar.py
--rw-r--r--  2.0 unx     4798 b- defN 23-Jul-18 15:20 functime/forecasting/_evaluate.py
--rw-r--r--  2.0 unx     2231 b- defN 23-Jul-18 15:20 functime/forecasting/_reduction.py
--rw-r--r--  2.0 unx     8481 b- defN 23-Jul-18 15:20 functime/forecasting/_regressors.py
--rw-r--r--  2.0 unx     8215 b- defN 23-Jul-18 15:20 functime/forecasting/automl.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Jul-18 15:20 functime/forecasting/catboost.py
--rw-r--r--  2.0 unx     3714 b- defN 23-Jul-18 15:20 functime/forecasting/censored.py
--rw-r--r--  2.0 unx     1034 b- defN 23-Jul-18 15:20 functime/forecasting/knn.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-18 15:20 functime/forecasting/lance.py
--rw-r--r--  2.0 unx     4215 b- defN 23-Jul-18 15:20 functime/forecasting/lightgbm.py
--rw-r--r--  2.0 unx     4018 b- defN 23-Jul-18 15:20 functime/forecasting/linear.py
--rw-r--r--  2.0 unx     2412 b- defN 23-Jul-18 15:20 functime/forecasting/xgboost.py
--rw-r--r--  2.0 unx      229 b- defN 23-Jul-18 15:20 functime/metrics/__init__.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Jul-18 15:20 functime/metrics/multi_objective.py
--rw-r--r--  2.0 unx     6810 b- defN 23-Jul-18 15:20 functime/metrics/point.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-18 15:20 functime/metrics/probabilistic.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     6373 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       52 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3584 b- defN 23-Jul-18 15:21 functime-0.2.4.dist-info/RECORD
-43 files, 172479 bytes uncompressed, 52275 bytes compressed:  69.7%
+Zip file size: 57432 bytes, number of entries: 42
+-rw-r--r--  2.0 unx       66 b- defN 23-Jul-24 08:42 functime/__init__.py
+-rw-r--r--  2.0 unx      117 b- defN 23-Jul-24 08:42 functime/__main__.py
+-rw-r--r--  2.0 unx     5893 b- defN 23-Jul-24 08:42 functime/backtesting.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Jul-24 08:42 functime/conformal.py
+-rw-r--r--  2.0 unx     1379 b- defN 23-Jul-24 08:42 functime/conversion.py
+-rw-r--r--  2.0 unx     5549 b- defN 23-Jul-24 08:42 functime/cross_validation.py
+-rw-r--r--  2.0 unx      123 b- defN 23-Jul-24 08:42 functime/embeddings.py
+-rw-r--r--  2.0 unx     1495 b- defN 23-Jul-24 08:42 functime/offsets.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 08:42 functime/plotting.py
+-rw-r--r--  2.0 unx    20397 b- defN 23-Jul-24 08:42 functime/preprocessing.py
+-rw-r--r--  2.0 unx     1976 b- defN 23-Jul-24 08:42 functime/ranges.py
+-rw-r--r--  2.0 unx     1481 b- defN 23-Jul-24 08:42 functime/stats.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-24 08:42 functime/base/__init__.py
+-rw-r--r--  2.0 unx     8253 b- defN 23-Jul-24 08:42 functime/base/forecaster.py
+-rw-r--r--  2.0 unx     1630 b- defN 23-Jul-24 08:42 functime/base/metric.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Jul-24 08:42 functime/base/model.py
+-rw-r--r--  2.0 unx     2237 b- defN 23-Jul-24 08:42 functime/base/transformer.py
+-rw-r--r--  2.0 unx      285 b- defN 23-Jul-24 08:42 functime/feature_extraction/__init__.py
+-rw-r--r--  2.0 unx     4064 b- defN 23-Jul-24 08:42 functime/feature_extraction/calendar.py
+-rw-r--r--  2.0 unx      745 b- defN 23-Jul-24 08:42 functime/forecasting/__init__.py
+-rw-r--r--  2.0 unx    12624 b- defN 23-Jul-24 08:42 functime/forecasting/_ar.py
+-rw-r--r--  2.0 unx     4909 b- defN 23-Jul-24 08:42 functime/forecasting/_evaluate.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Jul-24 08:42 functime/forecasting/_reduction.py
+-rw-r--r--  2.0 unx     7569 b- defN 23-Jul-24 08:42 functime/forecasting/_regressors.py
+-rw-r--r--  2.0 unx     8515 b- defN 23-Jul-24 08:42 functime/forecasting/automl.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Jul-24 08:42 functime/forecasting/catboost.py
+-rw-r--r--  2.0 unx     3714 b- defN 23-Jul-24 08:42 functime/forecasting/censored.py
+-rw-r--r--  2.0 unx     1010 b- defN 23-Jul-24 08:42 functime/forecasting/knn.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-24 08:42 functime/forecasting/lance.py
+-rw-r--r--  2.0 unx     4215 b- defN 23-Jul-24 08:42 functime/forecasting/lightgbm.py
+-rw-r--r--  2.0 unx     3958 b- defN 23-Jul-24 08:42 functime/forecasting/linear.py
+-rw-r--r--  2.0 unx     2412 b- defN 23-Jul-24 08:42 functime/forecasting/xgboost.py
+-rw-r--r--  2.0 unx      229 b- defN 23-Jul-24 08:42 functime/metrics/__init__.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Jul-24 08:42 functime/metrics/multi_objective.py
+-rw-r--r--  2.0 unx     6810 b- defN 23-Jul-24 08:42 functime/metrics/point.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 08:42 functime/metrics/probabilistic.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6366 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3510 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/RECORD
+42 files, 172968 bytes uncompressed, 51842 bytes compressed:  70.0%
```

## zipnote {}

```diff
@@ -30,17 +30,14 @@
 
 Filename: functime/ranges.py
 Comment: 
 
 Filename: functime/stats.py
 Comment: 
 
-Filename: functime/umap.py
-Comment: 
-
 Filename: functime/base/__init__.py
 Comment: 
 
 Filename: functime/base/forecaster.py
 Comment: 
 
 Filename: functime/base/metric.py
@@ -105,26 +102,26 @@
 
 Filename: functime/metrics/point.py
 Comment: 
 
 Filename: functime/metrics/probabilistic.py
 Comment: 
 
-Filename: functime-0.2.4.dist-info/LICENSE
+Filename: functime-0.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: functime-0.2.4.dist-info/METADATA
+Filename: functime-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: functime-0.2.4.dist-info/WHEEL
+Filename: functime-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: functime-0.2.4.dist-info/entry_points.txt
+Filename: functime-0.3.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: functime-0.2.4.dist-info/top_level.txt
+Filename: functime-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: functime-0.2.4.dist-info/RECORD
+Filename: functime-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## functime/offsets.py

```diff
@@ -12,70 +12,49 @@
     for x in OFFSET_ALIASES:
         if freq.endswith(x):
             offset_n = int(freq.rstrip(x))
             offset_alias = x
             return offset_n, offset_alias
 
 
-def freq_to_sp(freq: str, include_dec: bool = False) -> Union[List[int], List[float]]:
+def freq_to_sp(freq: str) -> Union[List[int], List[float]]:
     """Return seasonal periods given offset alias.
 
+    Reference: https://robjhyndman.com/hyndsight/seasonal-periods/
+
     Parameters
     ----------
     freq : str
-        Offset alias supported by Polars.
-
-        The offset is dictated by the following string language:\n
-        - 1ns (1 nanosecond)
-        - 1us (1 microsecond)
-        - 1ms (1 millisecond)
+        Supported offset aliases:\n
         - 1s (1 second)
         - 1m (1 minute)
+        - 30m (30 minute)
         - 1h (1 hour)
         - 1d (1 day)
         - 1w (1 week)
         - 1mo (1 calendar month)
-        - 1q (1 calendar quarter)
+        - 3mo (1 calendar quarter)
         - 1y (1 calendar year)
-        - 1i (1 index count)
-    include_dec : bool
-        If True, return floating point seasonal periods.
-        Otherwise, all seasonal periods are rounded down
-        to the nearest integer.
 
     Returns
     -------
-    sp : list of int, list of float
+    sp : list of int
     """
 
-    # Seasonal periods for given frequency as suggested by:
-    # https://robjhyndman.com/hyndsight/seasonal-periods/
-
-    alias_to_sp = {
-        "s": [7.0, 365.25],
-        "m": [48.0, 336.0, 17532.0],
-        "30m": [60.0, 1440.0, 10080.0, 525960.0],
-        "h": [24.0, 168.0, 8766.0],
-        "d": [7.0, 365.25],
-        # 365.25/7 = 52.18 on average, allowing
-        # for a leap year every fourth year
-        "w": [52.18],
-        "y": [1.0],
-        "mo": [12.0],
-        "3mo": [4.0],
+    seasonal_periods = {
+        "1s": [60, 3_600, 86_400, 604_800, 31_557_600],
+        "1m": [60, 1_440, 10_080, 525_960],
+        "30m": [48, 336, 17_532],
+        "1h": [24, 168, 8_766],
+        "1d": [7, 365],
+        "1w": [52],
+        "1mo": [12],
+        "3mo": [4],
+        "1y": [1],
     }
-    n, alias = _strip_freq_alias(freq)
-    alias = (
-        f"{n}{alias}"
-        if ((alias.endswith("m") and n == 30) or (alias.endswith("mo") and n == 3))
-        else alias
-    )
 
     try:
-        sp = alias_to_sp[alias]
+        sp = seasonal_periods[freq]
     except KeyError as exc:
         raise ValueError(f"Offset {freq!r} not supported") from exc
 
-    if not include_dec:
-        sp = list(map(int, sp))
-
     return sp
```

## functime/plotting.py

```diff
@@ -1,25 +0,0 @@
-00000000: 696d 706f 7274 206e 756d 7079 2061 7320  import numpy as 
-00000010: 6e70 0a69 6d70 6f72 7420 706c 6f74 6c79  np.import plotly
-00000020: 2e65 7870 7265 7373 2061 7320 7078 0a0a  .express as px..
-00000030: 0a64 6566 2070 6c6f 745f 7363 6174 7465  .def plot_scatte
-00000040: 7228 583a 206e 702e 6e64 6172 7261 792c  r(X: np.ndarray,
-00000050: 2079 3a20 6e70 2e6e 6461 7272 6179 2c20   y: np.ndarray, 
-00000060: 6e61 6d65 733d 4e6f 6e65 2c20 6474 7970  names=None, dtyp
-00000070: 653d 4e6f 6e65 293a 0a20 2020 2064 7479  e=None):.    dty
-00000080: 7065 203d 2064 7479 7065 206f 7220 7374  pe = dtype or st
-00000090: 720a 2020 2020 6669 6720 3d20 7078 2e73  r.    fig = px.s
-000000a0: 6361 7474 6572 5f33 6428 0a20 2020 2020  catter_3d(.     
-000000b0: 2020 2078 3d58 5b3a 2c20 305d 2c0a 2020     x=X[:, 0],.  
-000000c0: 2020 2020 2020 793d 585b 3a2c 2031 5d2c        y=X[:, 1],
-000000d0: 0a20 2020 2020 2020 207a 3d58 5b3a 2c20  .        z=X[:, 
-000000e0: 325d 2c0a 2020 2020 2020 2020 636f 6c6f  2],.        colo
-000000f0: 723d 792e 7371 7565 657a 6528 292e 6173  r=y.squeeze().as
-00000100: 7479 7065 2864 7479 7065 292c 2020 2320  type(dtype),  # 
-00000110: 4469 7363 7265 7465 2063 6f6c 6f72 730a  Discrete colors.
-00000120: 2020 2020 2020 2020 6f70 6163 6974 793d          opacity=
-00000130: 302e 352c 0a20 2020 2020 2020 2068 6f76  0.5,.        hov
-00000140: 6572 5f6e 616d 653d 6e61 6d65 732c 0a20  er_name=names,. 
-00000150: 2020 2029 0a20 2020 2066 6967 2e75 7064     ).    fig.upd
-00000160: 6174 655f 7472 6163 6573 286d 6172 6b65  ate_traces(marke
-00000170: 725f 7369 7a65 3d33 290a 2020 2020 7265  r_size=3).    re
-00000180: 7475 726e 2066 6967 0a                   turn fig.
```

## functime/preprocessing.py

```diff
@@ -1,27 +1,22 @@
 from itertools import product
 from typing import List, Mapping, Union
 
 import polars as pl
+import polars.selectors as cs
 from scipy.stats import boxcox_normmax
 from typing_extensions import Literal
 
 from functime.base import transformer
 from functime.base.model import ModelState
 from functime.offsets import _strip_freq_alias
 
-PL_FLOAT_DTYPES = [pl.Float32, pl.Float64]
-PL_INT_DTYPES = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]
-PL_NUMERIC_DTYPES = [*PL_INT_DTYPES, *PL_FLOAT_DTYPES]
-PL_FLOAT_COLS = pl.col(PL_FLOAT_DTYPES)
-PL_INT_COLS = pl.col(PL_INT_DTYPES)
-
 
 def PL_NUMERIC_COLS(*exclude):
-    return pl.col(PL_NUMERIC_DTYPES).exclude(exclude)
+    return cs.numeric() - cs.by_name(exclude)
 
 
 def reindex(X: pl.DataFrame) -> pl.DataFrame:
     entity_col, time_col = X.columns[:2]
     dtypes = X.dtypes[:2]
     entities = sorted(set(X.get_column(entity_col)))
     timestamps = sorted(set(X.get_column(time_col)))
@@ -109,15 +104,15 @@
             X.lazy()
             .groupby_dynamic(time_col, every=freq, by=entity_col)
             .agg(agg_exprs[agg_method])
             # Must defensive sort columns otherwise time_col and target_col
             # positions are incorrectly swapped in lazy
             .select([entity_col, time_col, target_col])
             # Impute gaps after reindex
-            .pipe(impute(impute_method))
+            .pipe(experimental_impute(impute_method))
             # Defensive fill null with 0 for impute method `ffill`
             .fill_null(0)
         )
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
@@ -162,14 +157,58 @@
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
+def one_hot_encode(drop_first: bool = False):
+    """Encode categorical features as a one-hot numeric array.
+
+    Parameters
+    ----------
+    drop_first : bool
+        Drop the first one hot feature.
+
+    Raises
+    ------
+    ValueError
+        if X passed into `transform_new` contains unknown categories.
+    """
+
+    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        # NOTE: You can't do lazy one hot encoding because
+        # polars needs to know the unique values in the selected columns
+        cat_cols = X.select(pl.col(pl.Categorical)).columns
+        X_new = X.collect().to_dummies(
+            columns=cat_cols, drop_first=drop_first, separator="__"
+        )
+        artifacts = {
+            "X_new": X_new,
+            "dummy_cols": X_new.columns,
+        }
+        return artifacts
+
+    def invert(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
+        return NotImplemented
+
+    def transform_new(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
+        cat_cols = X.select(pl.col(pl.Categorical)).columns
+        dummy_cols = state.artifacts["dummy_cols"]
+        X_new = X.collect().to_dummies(columns=cat_cols, separator="__")
+        if len(set(dummy_cols) & set(X_new.columns)) < len(dummy_cols):
+            raise ValueError(
+                f"Missing categories: {set(dummy_cols) & set(X_new.columns)}"
+            )
+        return X_new
+
+    return transform, invert, transform_new
+
+
+@transformer
 def roll(
     window_sizes: List[int],
     stats: List[Literal["mean", "min", "max", "mlm", "sum", "std", "cv"]],
     freq: str,
 ):
     """
     Performs rolling window calculations on specified columns of a DataFrame.
@@ -233,110 +272,122 @@
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
-def scale(use_mean: bool = True, use_std: bool = True, rescale_bool: bool = True):
+def scale(use_mean: bool = True, use_std: bool = True, rescale_bool: bool = False):
     """
     Performs scaling and rescaling operations on the numeric columns of a DataFrame.
 
     Parameters
     ----------
     use_mean : bool
         Whether to subtract the mean from the numeric columns. Defaults to True.
     use_std : bool
         Whether to divide the numeric columns by the standard deviation. Defaults to True.
     rescale_bool : bool
-        Whether to rescale boolean columns to the range [-1, 1]. Defaults to True.
+        Whether to rescale boolean columns to the range [-1, 1]. Defaults to False.
     """
 
     if not (use_mean or use_std):
         raise ValueError("At least one of `use_mean` or `use_std` must be set to True")
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
         idx_cols = X.columns[:2]
         entity_col, time_col = idx_cols
-        cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
+        numeric_cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
         boolean_cols = None
         _mean = None
         _std = None
         if use_mean:
             X = X.with_columns(
                 PL_NUMERIC_COLS(entity_col, time_col)
                 .mean()
                 .over(entity_col)
                 .suffix("_mean")
             )
             mean_cols = [col for col in X.columns if col.endswith("_mean")]
             _mean = X.select([*idx_cols, *mean_cols])
             X = X.select(
-                idx_cols + [pl.col(col) - pl.col(f"{col}_mean") for col in cols]
+                idx_cols + [pl.col(col) - pl.col(f"{col}_mean") for col in numeric_cols]
             )
         if use_std:
             X = X.with_columns(
                 PL_NUMERIC_COLS(entity_col, time_col)
                 .std()
                 .over(entity_col)
                 .suffix("_std")
             )
             std_cols = [col for col in X.columns if col.endswith("_std")]
             _std = X.select([*idx_cols, *std_cols])
             X = X.select(
-                idx_cols + [pl.col(col) / pl.col(f"{col}_std") for col in cols]
+                idx_cols + [pl.col(col) / pl.col(f"{col}_std") for col in numeric_cols]
             )
-        expr = pl.all()
         if rescale_bool:
-            # Original boolean column names
             boolean_cols = X.select(pl.col(pl.Boolean)).columns
-            # Minmax rescale boolean cols [-1, 1]
-            expr = [expr, pl.col(pl.Boolean).cast(pl.Int8) * 2 - 1]
-        X_new = X.select(expr)
+            X = X.with_columns(pl.col(pl.Boolean).cast(pl.Int8) * 2 - 1)
         artifacts = {
-            "X_new": X_new,
+            "X_new": X,
+            "numeric_cols": numeric_cols,
             "boolean_cols": boolean_cols,
             "_mean": _mean,
             "_std": _std,
         }
         return artifacts
 
     def invert(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
         idx_cols = X.columns[:2]
-        cols = X.select(PL_NUMERIC_COLS(state.time)).columns
+        artifacts = state.artifacts
+        numeric_cols = artifacts["numeric_cols"]
         if use_std:
-            _std = state.artifacts["_std"]
+            _std = artifacts["_std"]
             X = X.join(_std, on=idx_cols, how="left").select(
-                idx_cols + [pl.col(col) * pl.col(f"{col}_std") for col in cols]
+                idx_cols + [pl.col(col) * pl.col(f"{col}_std") for col in numeric_cols]
             )
         if use_mean:
-            _mean = state.artifacts["_mean"]
+            _mean = artifacts["_mean"]
             X = X.join(_mean, on=idx_cols, how="left").select(
-                idx_cols + [pl.col(col) + pl.col(f"{col}_mean") for col in cols]
+                idx_cols + [pl.col(col) + pl.col(f"{col}_mean") for col in numeric_cols]
             )
-        expr = pl.all()
         if rescale_bool:
-            # Minmax rescale boolean cols [-1, 1]
-            boolean_cols = pl.col(state.artifacts["boolean_cols"])
-            expr = [expr, (boolean_cols + 1).cast(pl.Int8)]
-        X_new = X.select(expr)
-        return X_new
+            X = X.with_columns(pl.col(artifacts["boolean_cols"]).cast(pl.Int8))
+        return X
+
+    def transform_new(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
+        artifacts = state.artifacts
+        idx_cols = X.columns[:2]
+        numeric_cols = state.artifacts["numeric_cols"]
+        _mean = artifacts["_mean"]
+        _std = artifacts["_std"]
+        if use_mean:
+            X = X.join(_mean, on=idx_cols, how="left").select(
+                idx_cols + [pl.col(col) - pl.col(f"{col}_mean") for col in numeric_cols]
+            )
+        if use_std:
+            X = X.join(_std, on=idx_cols, how="left").select(
+                idx_cols + [pl.col(col) / pl.col(f"{col}_std") for col in numeric_cols]
+            )
+        if rescale_bool:
+            X = X.with_columns(pl.col(pl.Boolean).cast(pl.Int8) * 2 - 1)
+        return X
 
-    return transform, invert
+    return transform, invert, transform_new
 
 
 @transformer
-def impute(
+def experimental_impute(
     method: Union[
         Literal["mean", "median", "fill", "ffill", "bfill", "interpolate"],
         Union[int, float],
     ]
 ):
     """
-    Performs missing value imputation on numeric columns of a DataFrame.
+    [EXPERIMENTAL] Performs missing value imputation on numeric columns of a DataFrame grouped by entity.
 
     Parameters
     ----------
     method : Union[str, int, float]
         The imputation method to use.
 
         Supported methods are:\n
@@ -348,25 +399,23 @@
         - 'interpolate': Interpolate missing values using linear interpolation.
         - int or float: Replace missing values with the specified constant.
     """
 
     def method_to_expr(entity_col, time_col):
         """Fill-in methods."""
         return {
-            "mean": PL_NUMERIC_COLS(entity_col, time_col).fill_null(strategy="mean"),
+            "mean": PL_NUMERIC_COLS(entity_col, time_col).fill_null(
+                PL_NUMERIC_COLS(entity_col, time_col).mean().over(entity_col)
+            ),
             "median": PL_NUMERIC_COLS(entity_col, time_col).fill_null(
                 PL_NUMERIC_COLS(entity_col, time_col).median().over(entity_col)
             ),
-            # "mode": PL_NUMERIC_COLS(entity_col, time_col).fill_null(PL_NUMERIC_COLS(entity_col, time_col).mode().over(entity_col)),
             "fill": [
-                PL_FLOAT_COLS.fill_null(PL_FLOAT_COLS.mean().over(entity_col)),
-                PL_INT_COLS.fill_null(PL_INT_COLS.median().over(entity_col)),
-                # pl.col([pl.Categorical, pl.Boolean]).fill_null(
-                #     PL_FLOAT_COLS.mode().over(entity_col)
-                # ),
+                cs.float().fill_null(cs.float().mean().over(entity_col)),
+                cs.integer().fill_null(cs.integer().median().over(entity_col)),
             ],
             "ffill": PL_NUMERIC_COLS(entity_col, time_col)
             .fill_null(strategy="forward")
             .over(entity_col),
             "bfill": PL_NUMERIC_COLS(entity_col, time_col)
             .fill_null(strategy="backward")
             .over(entity_col),
@@ -399,15 +448,15 @@
         Seasonal periodicity.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
         def _diff(X):
             X_new = (
                 X.groupby(entity_col, maintain_order=True)
-                .agg([pl.col(time_col), PL_FLOAT_COLS - PL_FLOAT_COLS.shift(sp)])
+                .agg([pl.col(time_col), cs.float() - cs.float().shift(sp)])
                 .explode(pl.all().exclude(entity_col))
             )
             return X_new
 
         idx_cols = X.columns[:2]
         entity_col = idx_cols[0]
         time_col = idx_cols[1]
@@ -432,15 +481,15 @@
 
     def invert(
         state: ModelState, X: pl.LazyFrame, from_last: bool = False
     ) -> pl.LazyFrame:
         def _inverse_diff(X: pl.LazyFrame):
             X_new = (
                 X.groupby(entity_col, maintain_order=True)
-                .agg([pl.col(time_col), PL_FLOAT_COLS.cumsum()])
+                .agg([pl.col(time_col), cs.float().cumsum()])
                 .explode(pl.all().exclude(entity_col))
             )
             return X_new
 
         artifacts = state.artifacts
         entity_col = X.columns[0]
         time_col = X.columns[1]
```

## functime/base/forecaster.py

```diff
@@ -1,28 +1,30 @@
 from dataclasses import dataclass
-from typing import Callable, List, Optional, Tuple, TypeVar, Union
+from typing import Callable, List, Mapping, Optional, Tuple, TypeVar, Union
 
 import polars as pl
 from typing_extensions import Literal, ParamSpec
 
 from functime.base.model import Model, ModelState
+from functime.base.transformer import Transformer
 from functime.ranges import make_future_ranges
 
 # The parameters of the Model
 P = ParamSpec("P")
 # The return type of the esimator's curried function
 R = Tuple[TypeVar("fit", bound=Callable), TypeVar("predict", bound=Callable)]
 
 FORECAST_STRATEGIES = Optional[Literal["direct", "recursive", "naive"]]
 DF_TYPE = Union[pl.LazyFrame, pl.DataFrame]
 
 
 @dataclass(frozen=True)
 class ForecastState(ModelState):
     target: str
+    target_schema: Mapping[str, pl.DataType]
     strategy: Optional[str] = "naive"
     features: Optional[List[str]] = None
 
 
 class Forecaster(Model):
     """Autoregressive forecaster.
 
@@ -34,30 +36,35 @@
         Number of lagged target variables.
     max_horizons: Optional[int]
         Maximum number of horizons to predict directly.
         Only applied if `strategy` equals "direct" or "ensemble".
     strategy : Optional[str]
         Forecasting strategy. Currently supports "recursive", "direct",
         and "ensemble" of both recursive and direct strategies.
+    target_transform : Optional[Transformer]
+        functime transformer to apply to `y` before fit. The transform is inverted at predict time.
     **kwargs : Mapping[str, Any]
         Additional keyword arguments passed into underlying sklearn-compatible estimator.
     """
 
     def __init__(
         self,
         freq: Union[str, None],
         lags: int,
         max_horizons: Optional[int] = None,
         strategy: FORECAST_STRATEGIES = None,
+        target_transform: Optional[Transformer] = None,
         **kwargs,
     ):
         self.freq = freq
         self.lags = lags
         self.max_horizons = max_horizons
         self.strategy = strategy
+        self.target_transform = target_transform
+        self._time_col_dtype = None
         self.kwargs = kwargs
         super().__init__()
 
     def __call__(
         self,
         y: DF_TYPE,
         fh: int,
@@ -68,31 +75,40 @@
         return self.predict(fh=fh, X=X_future)
 
     @property
     def name(self):
         return f"{self.__class__.__name__}(strategy={self.strategy})"
 
     def fit(self, y: DF_TYPE, X: Optional[DF_TYPE] = None):
+        # Prepare y
+        target_transform = self.target_transform
         y: pl.LazyFrame = self._set_string_cache(y.lazy().collect()).lazy()
+        if target_transform is not None:
+            y = y.pipe(self.target_transform).collect(streaming=True).lazy()
+        # Prepare X
         if X is not None:
             if X.columns[0] == y.columns[0]:
                 X = self._enforce_string_cache(X.lazy().collect())
             X = X.lazy()
+        # Fit AR forecaster
         artifacts = self._fit(y=y, X=X)
+        # Prepare artifacts
         cutoffs = y.groupby(y.columns[0]).agg(pl.col(y.columns[1]).max().alias("low"))
         artifacts["__cutoffs"] = cutoffs.collect(streaming=True)
         state = ForecastState(
             entity=y.columns[0],
             time=y.columns[1],
             artifacts=artifacts,
             target=y.columns[-1],
+            target_schema=y.schema,
             strategy=self.strategy or "recursive",
             features=X.columns[2:] if X is not None else None,
         )
         self.state = state
+        self.target_transform = target_transform
         return self
 
     def predict(self, fh: int, X: Optional[DF_TYPE] = None) -> pl.DataFrame:
 
         from functime.forecasting._ar import predict_autoreg
 
         state = self.state
@@ -125,27 +141,30 @@
             # NOTE: Unlike `y_lag` we DO NOT reshape exogenous features
             # into list columns. This is because .arr[i] with List[cat] does
             # not seem to support null values
             # Raises: ComputeError: cannot construct Categorical
             # from these categories, at least on of them is out of bounds
             X = X.select(pl.all().exclude(time)).lazy()
         y_pred_vals = predict_autoreg(self.state, fh=fh, X=X)
-        y_pred_vals = y_pred_vals.rename(
-            {x: y for x, y in zip(y_pred_vals.columns, [entity, target])}
-        )
-        y_pred = (
-            future_ranges.lazy()
-            .join(y_pred_vals.lazy(), on=entity)
-            # Explode from wide arrs to long form
-            .explode(pl.all().exclude(entity))
-            .pipe(self._reset_string_cache)
-            # NOTE: Cannot use streaming here...
-            # Causes change error "cannot append series, data types don't match
-            .collect()
+        y_pred_vals = y_pred_vals.sort(by=entity).select(
+            pl.col(y_pred_vals.columns[-1]).alias(target)
         )
+        y_pred = pl.concat(
+            [future_ranges.sort(by=entity), y_pred_vals], how="horizontal"
+        ).explode(pl.all().exclude(entity))
+
+        if self.target_transform is not None:
+            schema = self.state.target_schema
+            y_pred = (
+                y_pred.with_columns(pl.col(time).cast(schema[time]))
+                .pipe(self.target_transform.invert)
+                .collect(streaming=True)
+            )
+
+        y_pred = y_pred.pipe(self._reset_string_cache)
         return y_pred
 
     def backtest(
         self,
         y: DF_TYPE,
         X: Optional[pl.DataFrame],
         test_size: int = 1,
```

## functime/forecasting/_ar.py

```diff
@@ -1,13 +1,13 @@
 import logging
 from typing import Any, Callable, List, Mapping, Optional, Union
 
 import numpy as np
 import polars as pl
-from tqdm import trange
+from tqdm import tqdm, trange
 from typing_extensions import Literal
 
 from functime.cross_validation import expanding_window_split
 from functime.forecasting._evaluate import evaluate
 from functime.forecasting._reduction import (
     make_direct_reduction,
     make_reduction,
@@ -128,31 +128,40 @@
     num_samples: int = -1,
     cv: Optional[
         Callable[[pl.LazyFrame, bool, bool], Union[pl.LazyFrame, pl.DataFrame]]
     ] = None,
     X: Optional[pl.LazyFrame] = None,
     **kwargs,
 ) -> Mapping[str, Any]:
+
+    # TODO: Consolidate logging
+    logging.basicConfig(level=logging.INFO)
+
     # Set defaults
     strategy = strategy or "recursive"
     # Prepare CV splits query plan i.e. LazyFrames
     cv = cv or expanding_window_split(
         test_size=test_size, n_splits=n_splits, step_size=step_size, eager=True
     )
     y_splits = cv(y)
     X_splits = X if X is None else cv(X)
 
     # Test each lag
     best_lags = None
+    best_score = np.inf
     best_params = None
     scores_path = []
     lags_path = list(range(min_lags, max_lags + 1))
     scores_path = []
-    for lags in lags_path:
-        score = evaluate(
+    for lags in (
+        pbar := tqdm(
+            lags_path, desc=f"🚀 Evaluating models with n={min(lags_path)} lags"
+        )
+    ):
+        score, params = evaluate(
             **{
                 "lags": lags,
                 "n_splits": n_splits,
                 "time_budget": time_budget,
                 "points_to_evaluate": points_to_evaluate,
                 "num_samples": num_samples,
                 "low_cost_partial_config": low_cost_partial_config,
@@ -160,20 +169,25 @@
                 "test_size": test_size,
                 "max_horizons": max_horizons,
                 "strategy": strategy,
                 "freq": freq,
                 "forecaster_cls": forecaster_cls,
                 "y_splits": y_splits,
                 "X_splits": X_splits,
+                "include_best_params": True,
             },
         )
         scores_path.append(score)
-    best_idx = np.argmin(scores_path)
-    best_score = scores_path[best_idx]
-    best_lags = lags_path[best_idx]
+        if score < best_score:
+            best_score = score
+            best_lags = lags
+            best_params = params
+        pbar.set_description(
+            f"🚀 [Best round: lags={best_lags}, score={best_score:.2f}] Evaluating models with n={lags + 1} lags"
+        )
 
     # Refit
     best_params = best_params or {}
     best_params = {
         "freq": freq,
         **best_params,
         "max_horizons": max_horizons,
```

## functime/forecasting/_evaluate.py

```diff
@@ -112,14 +112,15 @@
     max_horizons: int,
     strategy: str,
     freq: str,
     forecaster_cls: Callable,
     y_splits: Mapping[int, Tuple[pl.DataFrame, pl.DataFrame]],
     X_splits: Optional[Mapping[int, Tuple[pl.DataFrame, pl.DataFrame]]],
     search_space: Optional[Mapping[str, Domain]] = None,
+    include_best_params: bool = False,
 ):
     params = None
     if search_space is None:
         result = evaluate_windows(
             config=params,
             lags=lags,
             n_splits=n_splits,
@@ -152,8 +153,12 @@
             time_budget_s=time_budget,
             points_to_evaluate=points_to_evaluate,
             num_samples=num_samples,
             search_alg=CFO(low_cost_partial_config=low_cost_partial_config),
         )
         score = tuner.best_result["mae"]
         params = tuner.best_config
-    return score
+
+    if include_best_params:
+        return score, params
+    else:
+        return score
```

## functime/forecasting/_regressors.py

```diff
@@ -11,28 +11,47 @@
 
 from functime.conversion import df_to_ndarray
 from functime.preprocessing import PL_NUMERIC_COLS
 
 
 def _X_to_numpy(X: pl.DataFrame) -> np.ndarray:
     X_arr = (
-        X.select(pl.col(X.columns[2:]).cast(pl.Float32))
-        .fill_null(strategy="mean")
+        X.lazy()
+        .select(pl.col(X.columns[2:]).cast(pl.Float32))
+        .select(
+            pl.when(pl.all().is_infinite() | pl.all().is_nan())
+            .then(None)
+            .otherwise(pl.all())
+            .keep_name()
+        )
+        # TODO: Support custom groupby imputation
+        .fill_null(strategy="mean")  # Do not fill backward (data leak)
+        .collect(streaming=True)
         .pipe(df_to_ndarray)
     )
     return X_arr
 
 
 def _y_to_numpy(y: pl.DataFrame) -> np.ndarray:
-    return (
-        y.get_column(y.columns[-1])
-        .cast(pl.Float32)
-        .fill_null(strategy="mean")
+    y_arr = (
+        y.lazy()
+        .select(pl.col(y.columns[-1]).cast(pl.Float32))
+        .select(
+            pl.when(pl.all().is_infinite() | pl.all().is_nan())
+            .then(None)
+            .otherwise(pl.all())
+            .keep_name()
+        )
+        # TODO: Support custom groupby imputation
+        .fill_null(strategy="mean")  # Do not fill backward (data leak)
+        .collect(streaming=True)
+        .get_column(y.columns[-1])
         .to_numpy(zero_copy_only=True)
     )
+    return y_arr
 
 
 class GradientBoostedTreeRegressor:
     def __init__(
         self,
         regress,
         weight_transform: Optional[Callable] = None,
@@ -84,18 +103,17 @@
             X_coerced = X.to_arrow
         elif isinstance(self.predict_dtype, Callable):
             X_coerced = self.predict_dtype(X)
         y_pred = self.regressor.predict(X_coerced)
         return y_pred
 
 
-class StandardizedSklearnRegressor:
+class SklearnRegressor:
     def __init__(self, estimator):
         self.estimator = estimator
-        self.pipeline = None
 
     def _preproc_X(self, X: pl.DataFrame):
         entity_col, time_col = X.columns[:2]
         X_new = X.select(
             [
                 entity_col,
                 time_col,
@@ -103,61 +121,26 @@
                 pl.col(pl.Categorical).exclude(entity_col).to_physical(),
                 pl.col(pl.Boolean).cast(pl.Int8),
             ]
         )
         return X_new
 
     def fit(self, X: pl.DataFrame, y: pl.DataFrame):
-        from sklearn.compose import ColumnTransformer
-        from sklearn.pipeline import Pipeline
-        from sklearn.preprocessing import MaxAbsScaler, OneHotEncoder
-
-        # Get column names
-        entity_col, time_col = X.columns[:2]
-        numeric_cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
-        categorical_cols = X.select(pl.col(pl.Categorical).exclude(entity_col)).columns
-        boolean_cols = X.select(pl.col(pl.Boolean)).columns
-        # Feature sizes
-        n_numeric_cols = len(numeric_cols)
-        n_categorical_cols = len(categorical_cols)
-        n_boolean_cols = len(boolean_cols)
-        # Feature slices
-        numeric_idx = slice(0, n_numeric_cols)
-        categorical_idx = slice(n_numeric_cols, n_numeric_cols + n_categorical_cols)
-        boolean_idx = slice(
-            n_numeric_cols + n_categorical_cols,
-            n_numeric_cols + n_categorical_cols + n_boolean_cols,
-        )
-        # Defensive reordering X and cast boolean to 0, 1
-        X = self._preproc_X(X)
-        scaler = MaxAbsScaler()
-        ohe = OneHotEncoder(
-            drop=None, dtype=np.int8, sparse_output=False, handle_unknown="ignore"
-        )
-
-        transformers = [("numeric", scaler, numeric_idx)]
-        if len(categorical_cols) > 1:
-            transformers += [("categorical", ohe, categorical_idx)]
-        if len(boolean_cols) > 1:
-            transformers += [("boolean", "passthrough", boolean_idx)]
-
-        transformer = ColumnTransformer(transformers=transformers, n_jobs=-1)
-        steps = [("transformer", transformer), ("regressor", self.estimator)]
-
-        # Fit pipeline
-        pipeline = Pipeline(steps=steps)
+        X_new = self._preproc_X(X).lazy()
+        # Regress
         with sklearn.config_context(assume_finite=True):
-            self.pipeline = pipeline.fit(X=_X_to_numpy(X), y=_y_to_numpy(y))
+            # NOTE: We can assume finite due to preproc
+            self.estimator = self.estimator.fit(X=_X_to_numpy(X_new), y=_y_to_numpy(y))
         return self
 
     def predict(self, X: pl.DataFrame) -> np.ndarray:
-        # Defensive reordering X and cast boolean to 0, 1
-        X = self._preproc_X(X)
+        X_new = self._preproc_X(X).lazy()
         with sklearn.config_context(assume_finite=True):
-            y_pred = self.pipeline.predict(_X_to_numpy(X))
+            # NOTE: We can assume finite due to preproc
+            y_pred = self.estimator.predict(_X_to_numpy(X_new))
         return y_pred
 
 
 class CensoredRegressor:
     def __init__(
         self,
         threshold: Union[int, float],
```

## functime/forecasting/automl.py

```diff
@@ -3,14 +3,15 @@
 from typing import Any, Mapping, Optional, Union
 
 import polars as pl
 from flaml import tune
 from typing_extensions import Literal
 
 from functime.base.forecaster import FORECAST_STRATEGIES, Forecaster
+from functime.base.transformer import Transformer
 from functime.forecasting.knn import knn
 from functime.forecasting.lightgbm import lightgbm
 from functime.forecasting.linear import elastic_net, lasso, linear_model, ridge
 
 
 class AutoForecaster(Forecaster):
     """AutoML forecaster with automated hyperparameter tuning and lags selection.
@@ -39,14 +40,16 @@
         Maximum time budgeted to train each forecaster per window and set of hyperparameters.
     search_space : Optional[dict]
         Equivalent to `config` in [FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#search-space)
     points_to_evaluate : Optional[dict]
         Equivalent to `points_to_evaluate` in [FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#warm-start)
     num_samples : int
         Number of hyper-parameter sets to test. -1 means unlimited (until `time_budget` is exhausted.)
+    target_transform : Optional[Transformer]
+        functime transformer to apply to `y` before fit. The transform is inverted at predict time.
     **kwargs : Mapping[str, Any]
         Additional keyword arguments passed into underlying sklearn-compatible estimator.
     """
 
     def __init__(
         self,
         # NOTE: MUST EXPLICITLY SPECIFIC FREQ IN ORDER FOR
@@ -59,28 +62,30 @@
         test_size: int = 1,
         step_size: int = 1,
         n_splits: int = 5,
         time_budget: int = 5,
         search_space: Optional[Mapping[str, Any]] = None,
         points_to_evaluate: Optional[Mapping[str, Any]] = None,
         num_samples: int = -1,
+        target_transform: Optional[Transformer] = None,
         **kwargs,
     ):
         self.freq = freq
         self.min_lags = min_lags
         self.max_lags = max_lags
         self.max_horizons = max_horizons
         self.strategy = strategy
         self.test_size = test_size
         self.step_size = step_size
         self.n_splits = n_splits
         self.time_budget = time_budget
         self.search_space = search_space
         self.points_to_evaluate = points_to_evaluate
         self.num_samples = num_samples
+        self.target_transform = target_transform
         self.kwargs = kwargs
 
     @property
     @abstractmethod
     def forecaster(self):
         pass
```

## functime/forecasting/knn.py

```diff
@@ -1,21 +1,21 @@
 from typing import Optional
 
 import polars as pl
 
 from functime.base import Forecaster
 from functime.forecasting._ar import fit_autoreg
-from functime.forecasting._regressors import StandardizedSklearnRegressor
+from functime.forecasting._regressors import SklearnRegressor
 
 
 def _knn(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.neighbors import KNeighborsRegressor
 
-        regressor = StandardizedSklearnRegressor(
+        regressor = SklearnRegressor(
             estimator=KNeighborsRegressor(**kwargs, n_jobs=-1),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
```

## functime/forecasting/linear.py

```diff
@@ -1,57 +1,57 @@
 from typing import Optional
 
 import polars as pl
 
 from functime.base import Forecaster
 from functime.forecasting._ar import fit_autoreg
-from functime.forecasting._regressors import StandardizedSklearnRegressor
+from functime.forecasting._regressors import SklearnRegressor
 
 
 def _linear_model(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import LinearRegression
 
-        regressor = StandardizedSklearnRegressor(
+        regressor = SklearnRegressor(
             estimator=LinearRegression(**kwargs, copy_X=False),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _lasso(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import Lasso
 
-        regressor = StandardizedSklearnRegressor(
+        regressor = SklearnRegressor(
             estimator=Lasso(**kwargs, tol=0.001, copy_X=False, max_iter=10000),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _ridge(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import Ridge
 
-        regressor = StandardizedSklearnRegressor(
+        regressor = SklearnRegressor(
             estimator=Ridge(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _elastic_net(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import ElasticNet
 
-        regressor = StandardizedSklearnRegressor(
+        regressor = SklearnRegressor(
             estimator=ElasticNet(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
         )
         return regressor.fit(X=X, y=y)
 
     return regress
```

## Comparing `functime-0.2.4.dist-info/LICENSE` & `functime-0.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `functime-0.2.4.dist-info/METADATA` & `functime-0.3.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: functime
-Version: 0.2.4
+Version: 0.3.0
 Summary: The easiest way to run and scale time-series machine learning in the Cloud.
 Author-email: functime Team <team@functime.ai>, Chris Lo <chris@functime.ai>, Daryl Lim <daryl@functime.ai>
 Project-URL: Homepage, https://github.com/descendant-ai/functime
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
@@ -18,46 +18,44 @@
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: catboost
 Requires-Dist: dask
 Requires-Dist: flaml[automl] (==1.2.4)
 Requires-Dist: holidays
 Requires-Dist: joblib
-Requires-Dist: kaleido
+Requires-Dist: kaleido (==0.2.1)
 Requires-Dist: lightgbm
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: plotly
 Requires-Dist: polars (==0.18.7)
 Requires-Dist: pyarrow
 Requires-Dist: pylance
 Requires-Dist: pynndescent (==0.5.8)
 Requires-Dist: rich (>=12.0.0)
 Requires-Dist: scikit-learn (==1.2.2)
 Requires-Dist: scipy
+Requires-Dist: tqdm
 Requires-Dist: typing-extensions
-Requires-Dist: umap-learn
 Requires-Dist: xgboost
 Requires-Dist: zarr
 Provides-Extra: doc
 Requires-Dist: mkdocs ; extra == 'doc'
 Requires-Dist: mkdocs-material ; extra == 'doc'
 Requires-Dist: mkdocstrings-python ; extra == 'doc'
 Provides-Extra: performance
 Requires-Dist: scikit-learn-intelex ; extra == 'performance'
 Provides-Extra: test
 Requires-Dist: coverage[toml] ; extra == 'test'
 Requires-Dist: fastapi ; extra == 'test'
-Requires-Dist: mlforecast ; extra == 'test'
+Requires-Dist: mlforecast (==0.8.1) ; extra == 'test'
 Requires-Dist: pytest-benchmark ; extra == 'test'
 Requires-Dist: pytest-memray ; extra == 'test'
 Requires-Dist: pytest-timeout ; extra == 'test'
 Requires-Dist: pytest ; extra == 'test'
-Requires-Dist: skforecast ; extra == 'test'
-Requires-Dist: u8darts (==0.24.0) ; extra == 'test'
 
 <div align="center">
     <h1>Time-series machine learning and embeddings at scale</h1>
 <br />
 
 ![functime](https://github.com/descendant-ai/functime/raw/main/static/images/functime_banner.png)
 [![Python](https://img.shields.io/pypi/pyversions/functime)](https://pypi.org/project/functime/)
@@ -119,15 +117,16 @@
 
 # Score forecasts in parallel
 scores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)
 ```
 
 ## Serverless Deployment
 
-Only available on `functime` Enterprise.
+Currently in closed-beta for `functime` Teams.
+Contact us for a demo via [Calendly](https://calendly.com/functime).
 
 Deploy and train forecasters the moment you call any `.fit` method.
 Run the `functime list` CLI command to list all deployed models.
 Finally, track data and forecasts usage using `functime usage` CLI command.
 
 ![Example CLI usage](static/gifs/functime_cli_usage.gif)
```

## Comparing `functime-0.2.4.dist-info/RECORD` & `functime-0.3.0.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,43 +1,42 @@
 functime/__init__.py,sha256=IpTQKBIY-7DJ1VnnWAQcziICkAKKoLMM-rJkzlO_x2o,66
 functime/__main__.py,sha256=HpYJUAIeN5HXQhaM5_kDwo441jDPe9GJ8dLjF9HE9VQ,117
 functime/backtesting.py,sha256=zuFuaqM_qWyf-3-xpnlGgo7s24ngV9akp1UNPpmgo9s,5893
 functime/conformal.py,sha256=SLLPOEEzOdzVdFzCpNfmETmr4fHKqIhtJC-Xce6mHyo,1838
 functime/conversion.py,sha256=ulS0QWDUWgA3KgJ_2Gn0R-eMRWADN9sVfSOghtzjUIE,1379
 functime/cross_validation.py,sha256=uTkcotiLATM_qA0zfgWAIPaD4fJW9LSqiBABfAbeqvY,5549
 functime/embeddings.py,sha256=rupMM8Fs6-nExoWYHS-PWFHiEHXsrNBvP4o-PPzACos,123
-functime/offsets.py,sha256=MO-St46MdhutFtkPJyr5XskYr_0yj_l1nnxh-1fC7GQ,2241
-functime/plotting.py,sha256=r-CaXqOhRzWKTdRDbp5l4fTyerGq5qj2Re91mQ9W15U,393
-functime/preprocessing.py,sha256=9WNOyz3lXOs7FLEfOF9dS2pOXeYje-dogI4pp4vja74,18763
+functime/offsets.py,sha256=GUCVLFs0xBSj_gsSaBz19ibBr_nquJVUxtuosObBucU,1495
+functime/plotting.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+functime/preprocessing.py,sha256=l7j_cELjhMMK9zupXNGfNpUrVD4Uer9krLGDqnB9enI,20397
 functime/ranges.py,sha256=b3biGP7s2OpoYopEGExELes79D_LdloDW4QH9j1slxI,1976
 functime/stats.py,sha256=fDy8OpsQl0TCYyD98r9D9Y5IVo8m2u9DoY8aOsYA7CA,1481
-functime/umap.py,sha256=9-kKBvDYK7rmijQxUmIsuy-5VHkIQhCEYQDYzTya8iI,610
 functime/base/__init__.py,sha256=YKzO_7RyxYbn3-jTa4gipQxrwnzUQN2_d2edjDehLmw,217
-functime/base/forecaster.py,sha256=wryuI4OGMGwWVcizx0lKlKHB-ROMw35YSpE1gb3ggeQ,7424
+functime/base/forecaster.py,sha256=Ve7Ba5a0sJwkUtGrx7Mwqv3P_-xug4TrbVsRRqBLqCc,8253
 functime/base/metric.py,sha256=Ous83EZ4ADpcyvHnTGNaxekqKEkwQB7qbYWuzKiGAAI,1630
 functime/base/model.py,sha256=mL8GFL7qNNb4rufm3ZbLzxyDPoDuoHgQhpZ0t20AHv4,2771
 functime/base/transformer.py,sha256=mL0ljSUtdJ6ZX-nEck553pbHk6odZdkqxnGkfYDqWJs,2237
 functime/feature_extraction/__init__.py,sha256=GS4FtQUUuzJZAJJ4g7fgamtbZhAecz8iwaQsejPTVhs,285
 functime/feature_extraction/calendar.py,sha256=yQwbo_27M2lyCqr9FxUKwEHC5_Kp75XB003HhEsnAmE,4064
 functime/forecasting/__init__.py,sha256=XWBiKZpNADvDUe-aaMtdppZt_8LvEYth7xzELJuy6Do,745
-functime/forecasting/_ar.py,sha256=dcqb7JxcNEdnB6VpoKNV8s_9Q1PR-yCMdLo1Clhbhnw,12183
-functime/forecasting/_evaluate.py,sha256=vwAntAvolOza_-U9e1UZlpHnUwldl8dFQTHgC1Vn1CQ,4798
+functime/forecasting/_ar.py,sha256=n612ZuK-cnCmEN3rn7wkINw64Lq-naZgidmRjs9Bnl0,12624
+functime/forecasting/_evaluate.py,sha256=mVXNCX-JBXq2FNJFAGn_rF22zVidlQFG6cIA0dtyxaQ,4909
 functime/forecasting/_reduction.py,sha256=FLYt6FXJCIcg5j60BiJNX8HvE8LHnkBdoc0BvYyDEtA,2231
-functime/forecasting/_regressors.py,sha256=sH7T8Ieafoj1Q4TVPKp6lIlbvo4mfc_lm9mezkB_Dn8,8481
-functime/forecasting/automl.py,sha256=4I3qNEjS35f7XlymJYZOjCS6rcjjIpTbNYBMrxSgDxE,8215
+functime/forecasting/_regressors.py,sha256=9z5yWKAWP2aHpk5q6__yWevPUc-9VtDpT1cgHnnAuH8,7569
+functime/forecasting/automl.py,sha256=LmCXrETtn39LbX_H3jVg7-Xs8TB6BfNCVf8qvRrNXwI,8515
 functime/forecasting/catboost.py,sha256=nF8I4AxJH7qDIu2BUGE_l7w-guNt8iYuNKH9oy-N3tU,2175
 functime/forecasting/censored.py,sha256=rZjxId4AZlz3bkf0YnKp5fqR5S3kI9Q42Swl0gGFtaU,3714
-functime/forecasting/knn.py,sha256=Uai51RhAOPfluG8US5KmmyRAPMqU2HV2hcSW3smGS-Q,1034
+functime/forecasting/knn.py,sha256=nu4XqAiJfMbNdCH0g9dzERSKvmrbGIuk9U3ypMqkX3E,1010
 functime/forecasting/lance.py,sha256=LiEhbLrd6DpMoFhYjbjyPXWhHxNEdM_v_XdpPcgpwiU,3703
 functime/forecasting/lightgbm.py,sha256=b0nDwFQLFtyAB-0vbeL8JRehO_sszjJFm_PAOT5MFJY,4215
-functime/forecasting/linear.py,sha256=42vzUSPukIfXiyyW3krq8QQorvw--cHexjUaAmw7WP0,4018
+functime/forecasting/linear.py,sha256=u7uwt4n_sCP3SfZClP0m_CGx1U0ZhLQq8XWi_V0HXrM,3958
 functime/forecasting/xgboost.py,sha256=kguM3IvqtJKSD3tysxgCiOe-NwApBew5tOO9_zxt4-I,2412
 functime/metrics/__init__.py,sha256=Bx-vU-jxkAg-zy2mVWYDtNOkluvgKJnL9uf0IhKIapw,229
 functime/metrics/multi_objective.py,sha256=EP9sGQ4HSCRN1SQazoJfntcovMP6RAVf7hZMXgcA2gE,3826
 functime/metrics/point.py,sha256=HlkcYvGAtFPr8kiTHQnoXZujysg2DK-BuUJVmLwiNKM,6810
 functime/metrics/probabilistic.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime-0.2.4.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-functime-0.2.4.dist-info/METADATA,sha256=tMBhmNkR8v-EOOWs8a8TSCcMjSg9o_V98NmGGdrTwZw,6373
-functime-0.2.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-functime-0.2.4.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
-functime-0.2.4.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
-functime-0.2.4.dist-info/RECORD,,
+functime-0.3.0.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+functime-0.3.0.dist-info/METADATA,sha256=NijULCgMeRLG_PeeYnTqyy1wbcXVsN447LUx0TJAaDY,6366
+functime-0.3.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+functime-0.3.0.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
+functime-0.3.0.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
+functime-0.3.0.dist-info/RECORD,,
```

