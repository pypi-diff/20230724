# Comparing `tmp/nucliadb_node_binding-0.7.9.tar.gz` & `tmp/nucliadb_node_binding-0.8.0.tar.gz`

## Comparing `nucliadb_node_binding-0.7.9.tar` & `nucliadb_node_binding-0.8.0.tar`

### file list

```diff
@@ -1,229 +1,235 @@
--rw-r--r--   0        0        0      696 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/Cargo.toml
--rw-r--r--   0      501       20       80 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/MANIFEST.in
--rw-r--r--   0      501       20     1148 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/Makefile
--rw-r--r--   0      501       20     5596 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/README.md
--rw-r--r--   0      501       20        7 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/VERSION
--rw-r--r--   0      501       20      899 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py
--rw-r--r--   0      501       20    12027 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py
--rw-r--r--   0      501       20     1254 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py
--rw-r--r--   0      501       20     2450 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/context.py
--rw-r--r--   0      501       20     2761 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py
--rw-r--r--   0      501       20     3476 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py
--rw-r--r--   0      501       20     1665 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/context.py
--rw-r--r--   0      501       20     3980 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/metrics.py
--rw-r--r--   0      501       20    14430 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/tracing.py
--rw-r--r--   0      501       20     2364 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/utils.py
--rw-r--r--   0      501       20    14966 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py
--rw-r--r--   0      501       20     5218 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc_metrics.py
--rw-r--r--   0      501       20     7231 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py
--rw-r--r--   0      501       20     9402 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py
--rw-r--r--   0      501       20     6825 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/logs.py
--rw-r--r--   0      501       20     7737 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/metrics.py
--rw-r--r--   0      501       20        0 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/py.typed
--rw-r--r--   0      501       20     1521 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py
--rw-r--r--   0      501       20      833 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py
--rw-r--r--   0      501       20      960 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py
--rw-r--r--   0      501       20      292 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/Makefile
--rw-r--r--   0      501       20      833 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py
--rw-r--r--   0      501       20     1111 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
--rw-r--r--   0      501       20     2340 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py
--rw-r--r--   0      501       20     1411 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi
--rw-r--r--   0      501       20     2822 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py
--rw-r--r--   0      501       20     1084 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi
--rw-r--r--   0      501       20     1126 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto
--rw-r--r--   0      501       20     2290 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py
--rw-r--r--   0      501       20     1190 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi
--rw-r--r--   0      501       20     2677 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py
--rw-r--r--   0      501       20      957 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi
--rw-r--r--   0      501       20      833 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/__init__.py
--rw-r--r--   0      501       20     6815 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_fastapi.py
--rw-r--r--   0      501       20     3907 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py
--rw-r--r--   0      501       20    12682 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py
--rw-r--r--   0      501       20      833 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/__init__.py
--rw-r--r--   0      501       20      833 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/__init__.py
--rw-r--r--   0      501       20     1608 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_context.py
--rw-r--r--   0      501       20     1381 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_fastapi.py
--rw-r--r--   0      501       20     2290 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_tracing.py
--rw-r--r--   0      501       20     1369 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_utils.py
--rw-r--r--   0      501       20     2183 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_context.py
--rw-r--r--   0      501       20     3537 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py
--rw-r--r--   0      501       20     5731 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_logs.py
--rw-r--r--   0      501       20     6982 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py
--rw-r--r--   0      501       20     5175 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_tikv_instrumentation.py
--rw-r--r--   0      501       20     5385 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tikv.py
--rw-r--r--   0      501       20     3883 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py
--rw-r--r--   0      501       20     4955 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py
--rw-r--r--   0      501       20      658 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/requirements.txt
--rw-r--r--   0      501       20      562 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/setup.cfg
--rw-r--r--   0      501       20     1620 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/setup.py
--rw-r--r--   0      501       20     2497 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/blocking.rs
--rw-r--r--   0      501       20     1219 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/lib.rs
--rw-r--r--   0      501       20     4928 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/payload.rs
--rw-r--r--   0      501       20    12829 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/sender.rs
--rw-r--r--   0      501       20     3166 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/sink.rs
--rw-r--r--   0      501       20     1867 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/sync.rs
--rw-r--r--   0        0        0     3053 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/Cargo.toml
--rw-r--r--   0      501       20     1045 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/.rustc_info.json
--rw-r--r--   0      501       20      788 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/Makefile
--rw-r--r--   0      501       20     1503 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/README.md
--rw-r--r--   0      501       20     1757 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/__init__.py
--rw-r--r--   0      501       20     2963 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/app.py
--rw-r--r--   0      501       20    12624 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/pull.py
--rw-r--r--   0      501       20     2338 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/reader.py
--rw-r--r--   0      501       20     1911 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/service.py
--rw-r--r--   0      501       20     2496 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/servicer.py
--rw-r--r--   0      501       20     1454 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/settings.py
--rw-r--r--   0      501       20      835 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py
--rw-r--r--   0      501       20     1055 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py
--rw-r--r--   0      501       20     7951 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/fixtures.py
--rw-r--r--   0      501       20     6058 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_indexing.py
--rw-r--r--   0      501       20     1815 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_app.py
--rw-r--r--   0      501       20     4647 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_pull.py
--rw-r--r--   0      501       20     2119 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/writer.py
--rw-r--r--   0      501       20       96 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/requirements-sources.txt
--rw-r--r--   0      501       20      208 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/requirements.txt
--rw-r--r--   0      501       20      249 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/setup.cfg
--rw-r--r--   0      501       20     1449 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/setup.py
--rw-r--r--   0      501       20     2193 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/bin/payload_test.rs
--rw-r--r--   0      501       20     4016 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/bin/reader.rs
--rw-r--r--   0      501       20    10293 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/bin/writer.rs
--rw-r--r--   0      501       20     9283 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/env.rs
--rw-r--r--   0      501       20     1173 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs
--rw-r--r--   0      501       20     1516 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/http_server/mod.rs
--rw-r--r--   0      501       20     1320 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/lib.rs
--rw-r--r--   0      501       20     3058 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/debug.rs
--rw-r--r--   0      501       20      956 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/mod.rs
--rw-r--r--   0      501       20     4090 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/telemetry.rs
--rw-r--r--   0      501       20     3223 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/node_metadata.rs
--rw-r--r--   0      501       20    14476 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs
--rw-r--r--   0      501       20     9052 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/reader/mod.rs
--rw-r--r--   0      501       20     1286 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/mod.rs
--rw-r--r--   0      501       20    24609 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/reader.rs
--rw-r--r--   0      501       20     8831 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/versions.rs
--rw-r--r--   0      501       20    20637 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/writer.rs
--rw-r--r--   0      501       20     3988 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shard_metadata.rs
--rw-r--r--   0      501       20     1221 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/mod.rs
--rw-r--r--   0      501       20     1913 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/shards_provider.rs
--rw-r--r--   0      501       20     4111 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_reader.rs
--rw-r--r--   0      501       20      851 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_writer.rs
--rw-r--r--   0      501       20     1313 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/mod.rs
--rw-r--r--   0      501       20     3270 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_reader.rs
--rw-r--r--   0      501       20      851 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_writer.rs
--rw-r--r--   0      501       20     4125 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/telemetry.rs
--rw-r--r--   0      501       20     2122 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/utils.rs
--rw-r--r--   0      501       20    14842 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs
--rw-r--r--   0      501       20     9505 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/writer/mod.rs
--rw-r--r--   0      501       20       42 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/test.sh
--rw-r--r--   0      501       20     1420 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/common/constants.rs
--rw-r--r--   0      501       20     1133 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/common/mod.rs
--rw-r--r--   0      501       20     5512 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/common/node_services.rs
--rw-r--r--   0      501       20    22179 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_search_relations.rs
--rw-r--r--   0      501       20     6763 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_search_sorting.rs
--rw-r--r--   0      501       20     7207 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_shards.rs
--rw-r--r--   0        0        0      604 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/Cargo.toml
--rw-r--r--   0      501       20       69 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/README.md
--rw-r--r--   0      501       20     9788 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs
--rw-r--r--   0      501       20    15679 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/mod.rs
--rw-r--r--   0      501       20    11727 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/node.rs
--rw-r--r--   0      501       20    11738 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/ops_hnsw.rs
--rw-r--r--   0      501       20     4063 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs
--rw-r--r--   0      501       20     7332 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/tests.rs
--rw-r--r--   0      501       20     4295 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs
--rw-r--r--   0      501       20     2911 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs
--rw-r--r--   0      501       20     9987 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs
--rw-r--r--   0      501       20    12092 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs
--rw-r--r--   0      501       20     1439 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs
--rw-r--r--   0      501       20     6957 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs
--rw-r--r--   0      501       20    13487 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs
--rw-r--r--   0      501       20     2049 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/mod.rs
--rw-r--r--   0      501       20     6202 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/trie.rs
--rw-r--r--   0      501       20     2986 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/trie_ram.rs
--rw-r--r--   0      501       20     4636 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/vector.rs
--rw-r--r--   0      501       20     6556 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/formula/mod.rs
--rw-r--r--   0      501       20     3882 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/indexset/mod.rs
--rw-r--r--   0      501       20     4483 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/indexset/state.rs
--rw-r--r--   0      501       20     1653 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/lib.rs
--rw-r--r--   0      501       20     1264 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/mod.rs
--rw-r--r--   0      501       20    13386 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/reader.rs
--rw-r--r--   0      501       20    22649 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/writer.rs
--rw-r--r--   0        0        0     1263 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/Cargo.toml
--rw-r--r--   0      501       20      869 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/LICENSE-AGPL
--rw-r--r--   0      501       20      188 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/README.md
--rw-r--r--   0      501       20     4099 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/bin/manager.rs
--rw-r--r--   0      501       20      267 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/error.rs
--rw-r--r--   0      501       20      908 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/key.rs
--rw-r--r--   0      501       20      166 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/lib.rs
--rw-r--r--   0      501       20    15097 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/node.rs
--rw-r--r--   0      501       20     1186 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/register.rs
--rwxr-xr-x   0      501       20     1333 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/tests/cluster_reader.py
--rw-r--r--   0      501       20     7072 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/tests/integration.rs
--rw-r--r--   0        0        0      676 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/Cargo.toml
--rw-r--r--   0      501       20     5774 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/fs_state.rs
--rw-r--r--   0      501       20     4087 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/lib.rs
--rw-r--r--   0      501       20     1473 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/console.rs
--rw-r--r--   0      501       20     1412 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/mod.rs
--rw-r--r--   0      501       20     1227 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/noop.rs
--rw-r--r--   0      501       20     2840 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/prometheus.rs
--rw-r--r--   0      501       20     1522 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/metric/mod.rs
--rw-r--r--   0      501       20     2631 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/metric/request_time.rs
--rw-r--r--   0      501       20    13385 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/metric/tokio_tasks.rs
--rw-r--r--   0      501       20     2926 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/middleware.rs
--rw-r--r--   0      501       20     1507 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/mod.rs
--rw-r--r--   0      501       20     2335 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/task_monitor.rs
--rw-r--r--   0      501       20     1917 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/paragraphs.rs
--rw-r--r--   0      501       20     1598 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/relations.rs
--rw-r--r--   0      501       20     1795 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/texts.rs
--rw-r--r--   0      501       20     1691 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/vectors.rs
--rw-r--r--   0        0        0      491 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/Cargo.toml
--rw-r--r--   0      501       20       18 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/.gitignore
--rw-r--r--   0      501       20     2098 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/build.rs
--rw-r--r--   0      501       20     8117 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs
--rw-r--r--   0      501       20      998 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/lib.rs
--rw-r--r--   0      501       20    38297 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/reader.rs
--rw-r--r--   0      501       20     4339 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/schema.rs
--rw-r--r--   0      501       20    18196 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/search_query.rs
--rw-r--r--   0      501       20    11761 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/search_response.rs
--rw-r--r--   0      501       20    18193 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/writer.rs
--rw-r--r--   0      501       20     1495 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/ca.json
--rw-r--r--   0      501       20     4724 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/en.json
--rw-r--r--   0      501       20     4516 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/es.json
--rw-r--r--   0      501       20     5233 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/fr.json
--rw-r--r--   0        0        0      687 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/Cargo.toml
--rw-r--r--   0      501       20     9337 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/bfs_engine.rs
--rw-r--r--   0      501       20     1761 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/errors.rs
--rw-r--r--   0      501       20    21702 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/graph_db.rs
--rw-r--r--   0      501       20     1831 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/graph_test_utils.rs
--rw-r--r--   0      501       20    10747 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/index.rs
--rw-r--r--   0      501       20     1003 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/lib.rs
--rw-r--r--   0      501       20     5752 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/node_dictionary.rs
--rw-r--r--   0      501       20     5251 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/relations_io.rs
--rw-r--r--   0      501       20     2692 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/bfs.rs
--rw-r--r--   0      501       20      970 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/mod.rs
--rw-r--r--   0      501       20    14066 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/reader.rs
--rw-r--r--   0      501       20    14393 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/tests.rs
--rw-r--r--   0      501       20     3071 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/utils.rs
--rw-r--r--   0      501       20    11329 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/writer.rs
--rw-r--r--   0        0        0      284 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/Cargo.toml
--rw-r--r--   0      501       20      917 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/lib.rs
--rw-r--r--   0      501       20    27858 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/reader.rs
--rw-r--r--   0      501       20     2917 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/schema.rs
--rw-r--r--   0      501       20     6946 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/search_query.rs
--rw-r--r--   0      501       20    12465 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/writer.rs
--rw-r--r--   0        0        0      501 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/Cargo.toml
--rw-r--r--   0      501       20     2155 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/build.rs
--rw-r--r--   0      501       20    25632 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/fdbwriter.rs
--rw-r--r--   0      501       20        0 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/google.protobuf.rs
--rw-r--r--   0      501       20     7526 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/knowledgebox.rs
--rw-r--r--   0      501       20     1197 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/lib.rs
--rw-r--r--   0      501       20    72102 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/nodereader.rs
--rw-r--r--   0      501       20     9630 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/noderesources.rs
--rw-r--r--   0      501       20    48704 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/nodewriter.rs
--rw-r--r--   0      501       20    28862 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/resources.rs
--rw-r--r--   0      501       20     5879 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/utils.rs
--rw-r--r--   0        0        0      868 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/Cargo.toml
--rw-r--r--   0      501       20     1505 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/CHANGELOG.md
--rw-r--r--   0      501       20       52 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/README.md
--rw-r--r--   0      501       20     1162 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/pyproject.toml
--rw-r--r--   0      501       20    22067 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/src/lib.rs
--rw-r--r--   0      501       20     2041 2023-06-28 14:30:12.000000 nucliadb_node_binding-0.7.9/test.py
--rw-r--r--   0        0        0      392 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.9/PKG-INFO
+-rw-r--r--   0        0        0      719 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/Cargo.toml
+-rw-r--r--   0        0        0       84 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/MANIFEST.in
+-rw-r--r--   0        0        0     1182 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/Makefile
+-rw-r--r--   0        0        0     5804 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/README.md
+-rw-r--r--   0        0        0        8 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/VERSION
+-rw-r--r--   0        0        0      921 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py
+-rw-r--r--   0        0        0    12339 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py
+-rw-r--r--   0        0        0     1289 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py
+-rw-r--r--   0        0        0     2530 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/context.py
+-rw-r--r--   0        0        0     2855 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py
+-rw-r--r--   0        0        0     3576 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py
+-rw-r--r--   0        0        0     1708 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/context.py
+-rw-r--r--   0        0        0     4087 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/metrics.py
+-rw-r--r--   0        0        0    14885 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/tracing.py
+-rw-r--r--   0        0        0     2426 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/utils.py
+-rw-r--r--   0        0        0    15377 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py
+-rw-r--r--   0        0        0     5384 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc_metrics.py
+-rw-r--r--   0        0        0     7420 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py
+-rw-r--r--   0        0        0     9674 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py
+-rw-r--r--   0        0        0     7040 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/logs.py
+-rw-r--r--   0        0        0     7981 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/metrics.py
+-rw-r--r--   0        0        0        0 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/py.typed
+-rw-r--r--   0        0        0     1570 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py
+-rw-r--r--   0        0        0      851 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py
+-rw-r--r--   0        0        0      984 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py
+-rw-r--r--   0        0        0      294 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/Makefile
+-rw-r--r--   0        0        0      851 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py
+-rw-r--r--   0        0        0     1149 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
+-rw-r--r--   0        0        0     2395 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py
+-rw-r--r--   0        0        0     1464 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi
+-rw-r--r--   0        0        0     2903 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py
+-rw-r--r--   0        0        0     1120 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi
+-rw-r--r--   0        0        0     1163 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto
+-rw-r--r--   0        0        0     2347 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py
+-rw-r--r--   0        0        0     1235 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi
+-rw-r--r--   0        0        0     2758 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py
+-rw-r--r--   0        0        0      990 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi
+-rw-r--r--   0        0        0      851 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/__init__.py
+-rw-r--r--   0        0        0     7046 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_fastapi.py
+-rw-r--r--   0        0        0     4022 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py
+-rw-r--r--   0        0        0    13036 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py
+-rw-r--r--   0        0        0      851 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/__init__.py
+-rw-r--r--   0        0        0      851 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/__init__.py
+-rw-r--r--   0        0        0     1664 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_context.py
+-rw-r--r--   0        0        0     1414 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_fastapi.py
+-rw-r--r--   0        0        0     2354 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_tracing.py
+-rw-r--r--   0        0        0     1411 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_utils.py
+-rw-r--r--   0        0        0     2254 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_context.py
+-rw-r--r--   0        0        0     3634 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py
+-rw-r--r--   0        0        0     5927 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_logs.py
+-rw-r--r--   0        0        0     7199 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py
+-rw-r--r--   0        0        0     5324 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_tikv_instrumentation.py
+-rw-r--r--   0        0        0     5523 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tikv.py
+-rw-r--r--   0        0        0     3984 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py
+-rw-r--r--   0        0        0     5088 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py
+-rw-r--r--   0        0        0      689 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/requirements.txt
+-rw-r--r--   0        0        0      583 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/setup.cfg
+-rw-r--r--   0        0        0     1670 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/setup.py
+-rw-r--r--   0        0        0     2590 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/blocking.rs
+-rw-r--r--   0        0        0     1249 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/lib.rs
+-rw-r--r--   0        0        0     5074 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/payload.rs
+-rw-r--r--   0        0        0    13230 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/sender.rs
+-rw-r--r--   0        0        0     3272 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/sink.rs
+-rw-r--r--   0        0        0     1915 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/sync.rs
+-rw-r--r--   0        0        0      522 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/Cargo.toml
+-rw-r--r--   0        0        0     2215 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/build.rs
+-rw-r--r--   0        0        0    26377 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/fdbwriter.rs
+-rw-r--r--   0        0        0        0 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/google.protobuf.rs
+-rw-r--r--   0        0        0     7779 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/knowledgebox.rs
+-rw-r--r--   0        0        0     1233 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/lib.rs
+-rw-r--r--   0        0        0    73873 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/nodereader.rs
+-rw-r--r--   0        0        0     9973 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/noderesources.rs
+-rw-r--r--   0        0        0    49814 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/nodewriter.rs
+-rw-r--r--   0        0        0    30488 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/resources.rs
+-rw-r--r--   0        0        0     6040 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/utils.rs
+-rw-r--r--   0        0        0      676 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/Cargo.toml
+-rw-r--r--   0        0        0     5986 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/fs_state.rs
+-rw-r--r--   0        0        0     4112 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/lib.rs
+-rw-r--r--   0        0        0     1519 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/console.rs
+-rw-r--r--   0        0        0     1456 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/mod.rs
+-rw-r--r--   0        0        0     1262 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/noop.rs
+-rw-r--r--   0        0        0     2928 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/prometheus.rs
+-rw-r--r--   0        0        0     1551 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/metric/mod.rs
+-rw-r--r--   0        0        0     2708 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/metric/request_time.rs
+-rw-r--r--   0        0        0    13774 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/metric/tokio_tasks.rs
+-rw-r--r--   0        0        0     3010 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/middleware.rs
+-rw-r--r--   0        0        0     1561 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/mod.rs
+-rw-r--r--   0        0        0     2407 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/task_monitor.rs
+-rw-r--r--   0        0        0     1972 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/paragraphs.rs
+-rw-r--r--   0        0        0     1643 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/relations.rs
+-rw-r--r--   0        0        0     1848 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/texts.rs
+-rw-r--r--   0        0        0     1741 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/vectors.rs
+-rw-r--r--   0        0        0     1308 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/Cargo.toml
+-rw-r--r--   0        0        0      888 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/LICENSE-AGPL
+-rw-r--r--   0        0        0      190 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/README.md
+-rw-r--r--   0        0        0     4336 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/bin/manager.rs
+-rw-r--r--   0        0        0      277 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/error.rs
+-rw-r--r--   0        0        0      951 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/key.rs
+-rw-r--r--   0        0        0      174 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/lib.rs
+-rw-r--r--   0        0        0    15579 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/node.rs
+-rw-r--r--   0        0        0     1227 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/register.rs
+-rw-r--r--   0        0        0     1368 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/tests/cluster_reader.py
+-rw-r--r--   0        0        0     7291 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/tests/integration.rs
+-rw-r--r--   0        0        0      284 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/Cargo.toml
+-rw-r--r--   0        0        0      941 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/lib.rs
+-rw-r--r--   0        0        0    26732 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/reader.rs
+-rw-r--r--   0        0        0     3013 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/schema.rs
+-rw-r--r--   0        0        0     7166 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/search_query.rs
+-rw-r--r--   0        0        0    12674 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/writer.rs
+-rw-r--r--   0        0        0      687 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/Cargo.toml
+-rw-r--r--   0        0        0     9600 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/bfs_engine.rs
+-rw-r--r--   0        0        0     1812 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/errors.rs
+-rw-r--r--   0        0        0    22269 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/graph_db.rs
+-rw-r--r--   0        0        0     1895 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/graph_test_utils.rs
+-rw-r--r--   0        0        0    11054 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/index.rs
+-rw-r--r--   0        0        0     1032 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/lib.rs
+-rw-r--r--   0        0        0     5908 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/node_dictionary.rs
+-rw-r--r--   0        0        0     5413 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/relations_io.rs
+-rw-r--r--   0        0        0     2770 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/bfs.rs
+-rw-r--r--   0        0        0      999 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/mod.rs
+-rw-r--r--   0        0        0    14289 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/reader.rs
+-rw-r--r--   0        0        0    14932 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/tests.rs
+-rw-r--r--   0        0        0     3158 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/utils.rs
+-rw-r--r--   0        0        0    11459 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/writer.rs
+-rw-r--r--   0        0        0      491 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/Cargo.toml
+-rw-r--r--   0        0        0       19 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/.gitignore
+-rw-r--r--   0        0        0     2160 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/build.rs
+-rw-r--r--   0        0        0     8365 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs
+-rw-r--r--   0        0        0     1025 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/lib.rs
+-rw-r--r--   0        0        0    37246 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/reader.rs
+-rw-r--r--   0        0        0     4464 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/schema.rs
+-rw-r--r--   0        0        0    18720 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/search_query.rs
+-rw-r--r--   0        0        0    12074 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/search_response.rs
+-rw-r--r--   0        0        0    18576 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/writer.rs
+-rw-r--r--   0        0        0     1496 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/ca.json
+-rw-r--r--   0        0        0     4724 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/en.json
+-rw-r--r--   0        0        0     4517 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/es.json
+-rw-r--r--   0        0        0     5233 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/fr.json
+-rw-r--r--   0        0        0     3119 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/Cargo.toml
+-rw-r--r--   0        0        0     1045 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/.rustc_info.json
+-rw-r--r--   0        0        0      863 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/Makefile
+-rw-r--r--   0        0        0     1534 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/README.md
+-rw-r--r--   0        0        0     1800 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/__init__.py
+-rw-r--r--   0        0        0     3059 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/app.py
+-rw-r--r--   0        0        0    12975 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/pull.py
+-rw-r--r--   0        0        0     2402 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/reader.py
+-rw-r--r--   0        0        0     1961 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/service.py
+-rw-r--r--   0        0        0     1701 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/servicer.py
+-rw-r--r--   0        0        0     1502 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/settings.py
+-rw-r--r--   0        0        0      854 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py
+-rw-r--r--   0        0        0     1082 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py
+-rw-r--r--   0        0        0     8528 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/fixtures.py
+-rw-r--r--   0        0        0     6438 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_indexing.py
+-rw-r--r--   0        0        0     1866 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_app.py
+-rw-r--r--   0        0        0     4795 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_pull.py
+-rw-r--r--   0        0        0     2181 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/writer.py
+-rw-r--r--   0        0        0       99 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/requirements-sources.txt
+-rw-r--r--   0        0        0      217 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/requirements.txt
+-rw-r--r--   0        0        0      268 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/setup.cfg
+-rw-r--r--   0        0        0     1501 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/setup.py
+-rw-r--r--   0        0        0     2627 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/bin/payload_test.rs
+-rw-r--r--   0        0        0     4232 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/bin/reader.rs
+-rw-r--r--   0        0        0     9348 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/bin/writer.rs
+-rw-r--r--   0        0        0     1660 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/disk_structure.rs
+-rw-r--r--   0        0        0     2661 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/env.rs
+-rw-r--r--   0        0        0     1205 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs
+-rw-r--r--   0        0        0     1559 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/http_server/mod.rs
+-rw-r--r--   0        0        0     1420 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/lib.rs
+-rw-r--r--   0        0        0     3145 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/debug.rs
+-rw-r--r--   0        0        0      980 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/mod.rs
+-rw-r--r--   0        0        0     4209 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/telemetry.rs
+-rw-r--r--   0        0        0     3459 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/node_metadata.rs
+-rw-r--r--   0        0        0    14707 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs
+-rw-r--r--   0        0        0     9223 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/reader/mod.rs
+-rw-r--r--   0        0        0      942 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/mod.rs
+-rw-r--r--   0        0        0    21502 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/reader.rs
+-rw-r--r--   0        0        0     9043 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/versions.rs
+-rw-r--r--   0        0        0    18590 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/writer.rs
+-rw-r--r--   0        0        0    10331 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/settings/mod.rs
+-rw-r--r--   0        0        0     5386 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/settings/providers.rs
+-rw-r--r--   0        0        0     4114 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shard_metadata.rs
+-rw-r--r--   0        0        0     1404 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/mod.rs
+-rw-r--r--   0        0        0     2685 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/shards_provider.rs
+-rw-r--r--   0        0        0     4360 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_reader.rs
+-rw-r--r--   0        0        0     6386 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_writer.rs
+-rw-r--r--   0        0        0     1462 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/mod.rs
+-rw-r--r--   0        0        0     3506 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_reader.rs
+-rw-r--r--   0        0        0     5279 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_writer.rs
+-rw-r--r--   0        0        0     4257 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/telemetry.rs
+-rw-r--r--   0        0        0     3986 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/utils.rs
+-rw-r--r--   0        0        0    16095 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs
+-rw-r--r--   0        0        0     9756 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/writer/mod.rs
+-rw-r--r--   0        0        0       43 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/test.sh
+-rw-r--r--   0        0        0     1454 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/common/constants.rs
+-rw-r--r--   0        0        0     1162 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/common/mod.rs
+-rw-r--r--   0        0        0     6936 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/common/node_services.rs
+-rw-r--r--   0        0        0    22798 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_search_relations.rs
+-rw-r--r--   0        0        0     6849 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_search_sorting.rs
+-rw-r--r--   0        0        0     7436 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_shards.rs
+-rw-r--r--   0        0        0      604 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/Cargo.toml
+-rw-r--r--   0        0        0       74 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/README.md
+-rw-r--r--   0        0        0    10081 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs
+-rw-r--r--   0        0        0    16238 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/mod.rs
+-rw-r--r--   0        0        0    12011 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/node.rs
+-rw-r--r--   0        0        0    12067 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/ops_hnsw.rs
+-rw-r--r--   0        0        0     4199 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs
+-rw-r--r--   0        0        0     7580 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/tests.rs
+-rw-r--r--   0        0        0     4171 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs
+-rw-r--r--   0        0        0     2995 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs
+-rw-r--r--   0        0        0    10722 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs
+-rw-r--r--   0        0        0    12487 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs
+-rw-r--r--   0        0        0     1477 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs
+-rw-r--r--   0        0        0     7189 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs
+-rw-r--r--   0        0        0    13871 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs
+-rw-r--r--   0        0        0     2115 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/mod.rs
+-rw-r--r--   0        0        0     6382 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/trie.rs
+-rw-r--r--   0        0        0     3083 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/trie_ram.rs
+-rw-r--r--   0        0        0     4782 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/vector.rs
+-rw-r--r--   0        0        0     6760 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/formula/mod.rs
+-rw-r--r--   0        0        0     3996 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/indexset/mod.rs
+-rw-r--r--   0        0        0     4602 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/indexset/state.rs
+-rw-r--r--   0        0        0     1702 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/lib.rs
+-rw-r--r--   0        0        0     1301 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/service/mod.rs
+-rw-r--r--   0        0        0    13481 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/service/reader.rs
+-rw-r--r--   0        0        0    23042 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/service/writer.rs
+-rw-r--r--   0        0        0      872 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/Cargo.toml
+-rw-r--r--   0        0        0     1629 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/CHANGELOG.md
+-rw-r--r--   0        0        0       53 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/README.md
+-rw-r--r--   0        0        0     1192 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/pyproject.toml
+-rw-r--r--   0        0        0     1239 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/src/errors.rs
+-rw-r--r--   0        0        0     2312 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/src/lib.rs
+-rw-r--r--   0        0        0     9904 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/src/reader.rs
+-rw-r--r--   0        0        0    14096 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/src/writer.rs
+-rw-r--r--   0        0        0     2097 2023-07-24 13:13:53.000000 nucliadb_node_binding-0.8.0/test.py
+-rw-r--r--   0        0        0      393 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.8.0/PKG-INFO
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/Cargo.toml`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-[package]
-name = "nucliadb_telemetry"
-version = "0.1.0"
-authors = ["nucliadb <nucliadb@nuclia.com>"]
-edition = "2021"
-license = "AGPL-3.0-or-later"
-description = "nucliadb telemetry"
-repository = "https://github.com/nuclia/nucliadb"
-homepage = "https://nucliadb.io/"
-documentation = "https://nucliadb.io/docs/"
-
-
-[dependencies]
-once_cell = "1.8.0"
-reqwest = { version = "0.11", default-features=false, features = ["json", "rustls-tls", "blocking"] }
-tokio = {version = "1", features = ["full"]}
-serde = {version="1", features = ["derive"]}
-uuid = { version= "1.1", features = ["v4", "serde"]}
-tracing = "0.1"
-async-trait = "0.1"
-hostname = "0.3"
-username = "0.2"
-md5 = "0.7"
+[package]
+name = "nucliadb_telemetry"
+version = "0.1.0"
+authors = ["nucliadb <nucliadb@nuclia.com>"]
+edition = "2021"
+license = "AGPL-3.0-or-later"
+description = "nucliadb telemetry"
+repository = "https://github.com/nuclia/nucliadb"
+homepage = "https://nucliadb.io/"
+documentation = "https://nucliadb.io/docs/"
+
+
+[dependencies]
+once_cell = "1.8.0"
+reqwest = { version = "0.11", default-features=false, features = ["json", "rustls-tls", "blocking"] }
+tokio = {version = "1", features = ["full"]}
+serde = {version="1", features = ["derive"]}
+uuid = { version= "1.1", features = ["v4", "serde"]}
+tracing = "0.1"
+async-trait = "0.1"
+hostname = "0.3"
+username = "0.2"
+md5 = "0.7"
 lazy_static = "1.4.0"
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/Makefile` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/Makefile`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-
-protos:
-	python -m grpc_tools.protoc ./nucliadb_telemetry/tests/grpc/helloworld.proto           -I ./ --python_out=./ --mypy_out=./ --grpc_python_out=./ --mypy_grpc_out=./
-	python -m grpc_tools.protoc ./nucliadb_telemetry/tests/grpc/hellostreamingworld.proto  -I ./ --python_out=./ --mypy_out=./ --grpc_python_out=./ --mypy_grpc_out=./
-
-.PHONY: install-dev
-install-dev:
-	pip install --upgrade pip wheel
-	pip install -r ../test-requirements.txt -r ../code-requirements.txt -r requirements.txt
-# utils dep required for test fixtures for nats
-# NEEDS TO BE DECOUPLED EVENTUALLY
-	pip install -e ../nucliadb_protos/python
-	pip install -e ../nucliadb_utils
-	pip install -e .
-
-.PHONY: format
-format:
-	isort --profile black .
-	black .
-
-.PHONY: lint
-lint:
-	flake8 nucliadb_telemetry --config=setup.cfg
-	isort -c --profile black .
-	black --check .
-	MYPYPATH=../mypy_stubs mypy --config-file=../mypy.ini .
-
-.PHONY: test
-test:
-	pytest -s --tb=native -v nucliadb_telemetry
-
-.PHONY: test-cov
-test-cov:
-	pytest -rfE --cov=nucliadb_telemetry --cov-config=../.coveragerc -s --tb=native -v --cov-report term-missing:skip-covered --cov-report xml nucliadb_telemetry
+
+protos:
+	python -m grpc_tools.protoc ./nucliadb_telemetry/tests/grpc/helloworld.proto           -I ./ --python_out=./ --mypy_out=./ --grpc_python_out=./ --mypy_grpc_out=./
+	python -m grpc_tools.protoc ./nucliadb_telemetry/tests/grpc/hellostreamingworld.proto  -I ./ --python_out=./ --mypy_out=./ --grpc_python_out=./ --mypy_grpc_out=./
+
+.PHONY: install-dev
+install-dev:
+	pip install --upgrade pip wheel
+	pip install -r ../test-requirements.txt -r ../code-requirements.txt -r requirements.txt
+# utils dep required for test fixtures for nats
+# NEEDS TO BE DECOUPLED EVENTUALLY
+	pip install -e ../nucliadb_protos/python
+	pip install -e ../nucliadb_utils
+	pip install -e .
+
+.PHONY: format
+format:
+	isort --profile black .
+	black .
+
+.PHONY: lint
+lint:
+	flake8 nucliadb_telemetry --config=setup.cfg
+	isort -c --profile black .
+	black --check .
+	MYPYPATH=../mypy_stubs mypy --config-file=../mypy.ini .
+
+.PHONY: test
+test:
+	pytest -s --tb=native -v nucliadb_telemetry
+
+.PHONY: test-cov
+test-cov:
+	pytest -rfE --cov=nucliadb_telemetry --cov-config=../.coveragerc -s --tb=native -v --cov-report term-missing:skip-covered --cov-report xml nucliadb_telemetry
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/README.md` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/README.md`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,208 +1,208 @@
-# NucliaDB Telemetry
-
-Open telemetry compatible plugin to propagate traceid on FastAPI, Nats and GRPC with Asyncio.
-
-ENV vars:
-
-```
-    JAEGER_ENABLED = True
-    JAEGER_HOST = "127.0.0.1"
-    JAEGER_PORT = server.port
-```
-
-On FastAPI you should add:
-
-```python
-    tracer_provider = get_telemetry("HTTP_SERVICE")
-    app = FastAPI(title="Test API")  # type: ignore
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    FastAPIInstrumentor.instrument_app(app, tracer_provider=tracer_provider)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-
-```
-
-On GRPC Server you should add:
-
-```python
-    tracer_provider = get_telemetry("GRPC_SERVER_SERVICE")
-    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    server = telemetry_grpc.init_server()
-    helloworld_pb2_grpc.add_GreeterServicer_to_server(SERVICER, server)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-```
-
-On GRPC Client you should add:
-
-```python
-    tracer_provider = get_telemetry("GRPC_CLIENT_SERVICE")
-    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-    stub = helloworld_pb2_grpc.GreeterStub(channel)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-
-```
-
-On Nats jetstream push subscriber you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    subscription = await jsotel.subscribe(
-        subject="testing.telemetry",
-        stream="testing",
-        cb=handler,
-    )
-
-```
-
-On Nats publisher you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-     await jsotel.publish("testing.telemetry", request.name.encode())
-
-```
-
-
-On Nats jetstream pull subscription you can use different patterns if you want to
-just get one message and exit or pull several ones. For just one message
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    # You can use either pull_subscribe or pull_subscribe_bind
-    subscription = await jsotel.pull_subscribe(
-        subject="testing.telemetry",
-        durable="consumer_name"
-        stream="testing",
-    )
-
-    async def callback(message):
-        # Do something with your message
-        # and optionally return something
-        return True
-
-    try:
-        result = await jsotel.pull_one(subscription, callback)
-    except errors.TimeoutError
-        pass
-
-```
-For multiple messages just wrap it in a loop:
-
-```python
-    while True:
-        try:
-            result = await jsotel.pull_one(subscription, callback)
-        except errors.TimeoutError
-            pass
-
-```
-
-
-On Nats client (NO Jestream! ) publisher you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientTelemetry(
-        nc, "NATS_SERVICE", tracer_provider
-    )
-
-     await ncotel.publish("testing.telemetry", request.name.encode())
-
-```
-
-On Nats client (NO Jestream! ) subscriber you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    subscription = await ncotel.subscribe(
-        subject="testing.telemetry",
-        queue="queue_nname",
-        cb=handler,
-    )
-
-```
-
-
-On Nats client (NO Jestream! ) request you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientTelemetry(
-        nc, "NATS_SERVICE", tracer_provider
-    )
-
-    response = await ncotel.request("testing.telemetry", request.name.encode())
-
-```
-
-And to handle responses on the other side, you can use the same pattern as in plain Nats client
-subscriber, just adding the `msg.respond()` on the handler when done
+# NucliaDB Telemetry
+
+Open telemetry compatible plugin to propagate traceid on FastAPI, Nats and GRPC with Asyncio.
+
+ENV vars:
+
+```
+    JAEGER_ENABLED = True
+    JAEGER_HOST = "127.0.0.1"
+    JAEGER_PORT = server.port
+```
+
+On FastAPI you should add:
+
+```python
+    tracer_provider = get_telemetry("HTTP_SERVICE")
+    app = FastAPI(title="Test API")  # type: ignore
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    FastAPIInstrumentor.instrument_app(app, tracer_provider=tracer_provider)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+
+```
+
+On GRPC Server you should add:
+
+```python
+    tracer_provider = get_telemetry("GRPC_SERVER_SERVICE")
+    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    server = telemetry_grpc.init_server()
+    helloworld_pb2_grpc.add_GreeterServicer_to_server(SERVICER, server)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+```
+
+On GRPC Client you should add:
+
+```python
+    tracer_provider = get_telemetry("GRPC_CLIENT_SERVICE")
+    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+    stub = helloworld_pb2_grpc.GreeterStub(channel)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+
+```
+
+On Nats jetstream push subscriber you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    subscription = await jsotel.subscribe(
+        subject="testing.telemetry",
+        stream="testing",
+        cb=handler,
+    )
+
+```
+
+On Nats publisher you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+     await jsotel.publish("testing.telemetry", request.name.encode())
+
+```
+
+
+On Nats jetstream pull subscription you can use different patterns if you want to
+just get one message and exit or pull several ones. For just one message
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    # You can use either pull_subscribe or pull_subscribe_bind
+    subscription = await jsotel.pull_subscribe(
+        subject="testing.telemetry",
+        durable="consumer_name"
+        stream="testing",
+    )
+
+    async def callback(message):
+        # Do something with your message
+        # and optionally return something
+        return True
+
+    try:
+        result = await jsotel.pull_one(subscription, callback)
+    except errors.TimeoutError
+        pass
+
+```
+For multiple messages just wrap it in a loop:
+
+```python
+    while True:
+        try:
+            result = await jsotel.pull_one(subscription, callback)
+        except errors.TimeoutError
+            pass
+
+```
+
+
+On Nats client (NO Jestream! ) publisher you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientTelemetry(
+        nc, "NATS_SERVICE", tracer_provider
+    )
+
+     await ncotel.publish("testing.telemetry", request.name.encode())
+
+```
+
+On Nats client (NO Jestream! ) subscriber you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    subscription = await ncotel.subscribe(
+        subject="testing.telemetry",
+        queue="queue_nname",
+        cb=handler,
+    )
+
+```
+
+
+On Nats client (NO Jestream! ) request you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientTelemetry(
+        nc, "NATS_SERVICE", tracer_provider
+    )
+
+    response = await ncotel.request("testing.telemetry", request.name.encode())
+
+```
+
+And to handle responses on the other side, you can use the same pattern as in plain Nats client
+subscriber, just adding the `msg.respond()` on the handler when done
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import logging
-
-logger = logging.getLogger("nucliadb_telemetry")
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,312 +1,312 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import List, Optional
-
-from opentelemetry.context import (  # type: ignore
-    _SUPPRESS_INSTRUMENTATION_KEY,
-    Context,
-    attach,
-    detach,
-    set_value,
-)
-from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
-from opentelemetry.sdk.trace.export import SpanExporter  # type: ignore
-from opentelemetry.util._time import _time_ns  # type: ignore
-
-from nucliadb_telemetry import logger
-
-
-class _FlushRequest:
-    """Represents a request for the BatchSpanProcessor to flush spans."""
-
-    __slots__ = ["event", "num_spans"]
-
-    def __init__(self):
-        self.event = asyncio.Event()
-        self.num_spans = 0
-
-
-class BatchSpanProcessor(SpanProcessor):
-    """Batch span processor implementation.
-    `BatchSpanProcessor` is an implementation of `SpanProcessor` that
-    batches ended spans and pushes them to the configured `SpanExporter`.
-    `BatchSpanProcessor` is configurable with the following environment
-    variables which correspond to constructor parameters:
-    - :envvar:`OTEL_BSP_SCHEDULE_DELAY`
-    - :envvar:`OTEL_BSP_MAX_QUEUE_SIZE`
-    - :envvar:`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`
-    - :envvar:`OTEL_BSP_EXPORT_TIMEOUT`
-    """
-
-    def __init__(
-        self,
-        span_exporter: SpanExporter,
-        max_queue_size: int = 2048,
-        schedule_delay_millis: int = 5000,
-        max_export_batch_size: int = 512,
-        export_timeout_millis: int = 30000,
-    ):
-        if max_queue_size <= 0:
-            raise ValueError("max_queue_size must be a positive integer.")
-
-        if schedule_delay_millis <= 0:
-            raise ValueError("schedule_delay_millis must be positive.")
-
-        if max_export_batch_size <= 0:
-            raise ValueError("max_export_batch_size must be a positive integer.")
-
-        if max_export_batch_size > max_queue_size:
-            raise ValueError(
-                "max_export_batch_size must be less than or equal to max_queue_size."
-            )
-
-        self.span_exporter = span_exporter
-        self.queue = asyncio.Queue(maxsize=max_queue_size)  # type: asyncio.Queue[Span]
-        self.worker_task: asyncio.Task = asyncio.create_task(
-            self.worker(), name="OtelBatchSpanProcessor"
-        )
-        self.condition = asyncio.Condition()
-        self._flush_request = None  # type: Optional[_FlushRequest]
-        self.schedule_delay_millis = schedule_delay_millis
-        self.max_export_batch_size = max_export_batch_size
-        self.max_queue_size = max_queue_size
-        self.export_timeout_millis = export_timeout_millis
-        self.done = False
-        # flag that indicates that spans are being dropped
-        self._spans_dropped = False
-        # precallocated list to send spans to exporter
-        self.spans_list = [
-            None
-        ] * self.max_export_batch_size  # type: List[Optional[Span]]
-
-    def on_start(self, span: Span, parent_context: Optional[Context] = None) -> None:
-        pass
-
-    def on_end(self, span: ReadableSpan) -> None:
-        if self.done:
-            logger.warning("Already shutdown, dropping span.")
-            return
-        if not span.context.trace_flags.sampled:
-            return
-        if self.queue.full():
-            if not self._spans_dropped:
-                logger.warning("Queue is full, likely spans will be dropped.")
-                self._spans_dropped = True
-
-        try:
-            self.queue.put_nowait(span)  # type: ignore
-        except asyncio.QueueFull:
-            logger.warning(f"Queue is full. Queue size : {self.queue.qsize()}")
-        except Exception as e:
-            logger.exception(e)
-
-        if self.queue.qsize() >= self.max_export_batch_size:
-            asyncio.create_task(self.notify())
-
-    async def notify(self):
-        async with self.condition:
-            self.condition.notify()
-
-    async def notify_all(self):
-        async with self.condition:
-            self.condition.notify_all()
-
-    async def worker(self):
-        try:
-            logger.info("Batch telemetry event loop started")
-            await self._worker()
-        except Exception as e:
-            logger.exception(e)
-            raise e
-
-    async def _worker(self) -> None:
-        timeout = self.schedule_delay_millis / 1e3
-        flush_request = None  # type: Optional[_FlushRequest]
-        while not self.done:
-            logger.debug("Waiting condition")
-            async with self.condition:
-                if self.done:
-                    # done flag may have changed, avoid waiting
-                    break
-                logger.debug(f"{self.queue.qsize()} spans on queue")
-                flush_request = self._get_and_unset_flush_request()
-                if (
-                    self.queue.qsize() < self.max_export_batch_size
-                    and flush_request is None
-                ):
-                    try:
-                        await asyncio.wait_for(self.condition.wait(), timeout)
-                    except asyncio.TimeoutError:
-                        pass
-                    flush_request = self._get_and_unset_flush_request()
-                    if not self.queue:
-                        # spurious notification, let's wait again, reset timeout
-                        timeout = self.schedule_delay_millis / 1e3
-                        self._notify_flush_request_finished(flush_request)
-                        flush_request = None
-                        continue
-                    if self.done:
-                        # missing spans will be sent when calling flush
-                        break
-
-            # subtract the duration of this export call to the next timeout
-            start = _time_ns()
-            try:
-                await asyncio.wait_for(self._export(flush_request), timeout)
-            except asyncio.TimeoutError:
-                logger.exception("Took to much time to export, network problem ahead")
-            end = _time_ns()
-            duration = (end - start) / 1e9
-            timeout = self.schedule_delay_millis / 1e3 - duration
-
-            self._notify_flush_request_finished(flush_request)
-            flush_request = None
-
-        # there might have been a new flush request while export was running
-        # and before the done flag switched to true
-        async with self.condition:
-            shutdown_flush_request = self._get_and_unset_flush_request()
-
-        # be sure that all spans are sent
-        await self._drain_queue()
-        self._notify_flush_request_finished(flush_request)
-        self._notify_flush_request_finished(shutdown_flush_request)
-
-    def _get_and_unset_flush_request(
-        self,
-    ) -> Optional[_FlushRequest]:
-        """Returns the current flush request and makes it invisible to the
-        worker thread for subsequent calls.
-        """
-        flush_request = self._flush_request
-        self._flush_request = None
-        if flush_request is not None:
-            flush_request.num_spans = self.queue.qsize()
-        return flush_request
-
-    @staticmethod
-    def _notify_flush_request_finished(
-        flush_request: Optional[_FlushRequest],
-    ):
-        """Notifies the flush initiator(s) waiting on the given request/event
-        that the flush operation was finished.
-        """
-        if flush_request is not None:
-            flush_request.event.set()
-
-    def _get_or_create_flush_request(self) -> _FlushRequest:
-        """Either returns the current active flush event or creates a new one.
-        The flush event will be visible and read by the worker thread before an
-        export operation starts. Callers of a flush operation may wait on the
-        returned event to be notified when the flush/export operation was
-        finished.
-        This method is not thread-safe, i.e. callers need to take care about
-        synchronization/locking.
-        """
-        if self._flush_request is None:
-            self._flush_request = _FlushRequest()
-        return self._flush_request
-
-    async def _export(self, flush_request: Optional[_FlushRequest]):
-        """Exports spans considering the given flush_request.
-        In case of a given flush_requests spans are exported in batches until
-        the number of exported spans reached or exceeded the number of spans in
-        the flush request.
-        In no flush_request was given at most max_export_batch_size spans are
-        exported.
-        """
-        if not flush_request:
-            await self._export_batch()
-            return
-
-        num_spans = flush_request.num_spans
-        while self.queue.qsize():
-            num_exported = await self._export_batch()
-            num_spans -= num_exported
-
-            if num_spans <= 0:
-                break
-
-    async def _export_batch(self) -> int:
-        """Exports at most max_export_batch_size spans and returns the number of
-        exported spans.
-        """
-        idx = 0
-        # currently only a single thread acts as consumer, so queue.pop() will
-        # not raise an exception
-        while idx < self.max_export_batch_size and self.queue.qsize():
-            self.spans_list[idx] = self.queue.get_nowait()
-            idx += 1
-        token = attach(set_value(_SUPPRESS_INSTRUMENTATION_KEY, True))
-        try:
-            # Ignore type b/c the Optional[None]+slicing is too "clever"
-            # for mypy
-            await self.span_exporter.async_export(self.spans_list[:idx])  # type: ignore
-        except asyncio.CancelledError:
-            logger.exception("Task was canceled while exporting Span batch")
-        except Exception:  # pylint: disable=broad-except
-            logger.exception("Exception while exporting Span batch)")
-        detach(token)
-
-        # clean up list
-        for index in range(idx):
-            self.spans_list[index] = None
-        return idx
-
-    async def _drain_queue(self):
-        """Export all elements until queue is empty.
-        Can only be called from the worker thread context because it invokes
-        `export` that is not thread safe.
-        """
-        while self.queue.qsize():
-            await self._export_batch()
-
-    async def async_force_flush(self, timeout_millis: Optional[int] = None) -> bool:
-        if timeout_millis is None:
-            timeout_millis = self.export_timeout_millis
-
-        if self.done:
-            logger.warning("Already shutdown, ignoring call to async_force_flush().")
-            return True
-
-        async with self.condition:
-            flush_request = self._get_or_create_flush_request()
-            # signal the worker task to flush and wait for it to finish
-            self.condition.notify_all()
-
-        if flush_request.num_spans == 0:
-            return True
-
-        # wait for token to be processed
-        ret = await asyncio.wait_for(flush_request.event.wait(), timeout_millis)
-        if not ret:
-            logger.warning("Timeout was exceeded in async_force_flush().")
-            return False
-        return ret
-
-    def shutdown(self) -> None:
-        # signal the worker thread to finish and then wait for it
-        self.done = True
-        self.span_exporter.shutdown()
-        try:
-            self.worker_task.cancel()
-        except RuntimeError:
-            pass
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import List, Optional
+
+from opentelemetry.context import (  # type: ignore
+    _SUPPRESS_INSTRUMENTATION_KEY,
+    Context,
+    attach,
+    detach,
+    set_value,
+)
+from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
+from opentelemetry.sdk.trace.export import SpanExporter  # type: ignore
+from opentelemetry.util._time import _time_ns  # type: ignore
+
+from nucliadb_telemetry import logger
+
+
+class _FlushRequest:
+    """Represents a request for the BatchSpanProcessor to flush spans."""
+
+    __slots__ = ["event", "num_spans"]
+
+    def __init__(self):
+        self.event = asyncio.Event()
+        self.num_spans = 0
+
+
+class BatchSpanProcessor(SpanProcessor):
+    """Batch span processor implementation.
+    `BatchSpanProcessor` is an implementation of `SpanProcessor` that
+    batches ended spans and pushes them to the configured `SpanExporter`.
+    `BatchSpanProcessor` is configurable with the following environment
+    variables which correspond to constructor parameters:
+    - :envvar:`OTEL_BSP_SCHEDULE_DELAY`
+    - :envvar:`OTEL_BSP_MAX_QUEUE_SIZE`
+    - :envvar:`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`
+    - :envvar:`OTEL_BSP_EXPORT_TIMEOUT`
+    """
+
+    def __init__(
+        self,
+        span_exporter: SpanExporter,
+        max_queue_size: int = 2048,
+        schedule_delay_millis: int = 5000,
+        max_export_batch_size: int = 512,
+        export_timeout_millis: int = 30000,
+    ):
+        if max_queue_size <= 0:
+            raise ValueError("max_queue_size must be a positive integer.")
+
+        if schedule_delay_millis <= 0:
+            raise ValueError("schedule_delay_millis must be positive.")
+
+        if max_export_batch_size <= 0:
+            raise ValueError("max_export_batch_size must be a positive integer.")
+
+        if max_export_batch_size > max_queue_size:
+            raise ValueError(
+                "max_export_batch_size must be less than or equal to max_queue_size."
+            )
+
+        self.span_exporter = span_exporter
+        self.queue = asyncio.Queue(maxsize=max_queue_size)  # type: asyncio.Queue[Span]
+        self.worker_task: asyncio.Task = asyncio.create_task(
+            self.worker(), name="OtelBatchSpanProcessor"
+        )
+        self.condition = asyncio.Condition()
+        self._flush_request = None  # type: Optional[_FlushRequest]
+        self.schedule_delay_millis = schedule_delay_millis
+        self.max_export_batch_size = max_export_batch_size
+        self.max_queue_size = max_queue_size
+        self.export_timeout_millis = export_timeout_millis
+        self.done = False
+        # flag that indicates that spans are being dropped
+        self._spans_dropped = False
+        # precallocated list to send spans to exporter
+        self.spans_list = [
+            None
+        ] * self.max_export_batch_size  # type: List[Optional[Span]]
+
+    def on_start(self, span: Span, parent_context: Optional[Context] = None) -> None:
+        pass
+
+    def on_end(self, span: ReadableSpan) -> None:
+        if self.done:
+            logger.warning("Already shutdown, dropping span.")
+            return
+        if not span.context.trace_flags.sampled:
+            return
+        if self.queue.full():
+            if not self._spans_dropped:
+                logger.warning("Queue is full, likely spans will be dropped.")
+                self._spans_dropped = True
+
+        try:
+            self.queue.put_nowait(span)  # type: ignore
+        except asyncio.QueueFull:
+            logger.warning(f"Queue is full. Queue size : {self.queue.qsize()}")
+        except Exception as e:
+            logger.exception(e)
+
+        if self.queue.qsize() >= self.max_export_batch_size:
+            asyncio.create_task(self.notify())
+
+    async def notify(self):
+        async with self.condition:
+            self.condition.notify()
+
+    async def notify_all(self):
+        async with self.condition:
+            self.condition.notify_all()
+
+    async def worker(self):
+        try:
+            logger.info("Batch telemetry event loop started")
+            await self._worker()
+        except Exception as e:
+            logger.exception(e)
+            raise e
+
+    async def _worker(self) -> None:
+        timeout = self.schedule_delay_millis / 1e3
+        flush_request = None  # type: Optional[_FlushRequest]
+        while not self.done:
+            logger.debug("Waiting condition")
+            async with self.condition:
+                if self.done:
+                    # done flag may have changed, avoid waiting
+                    break
+                logger.debug(f"{self.queue.qsize()} spans on queue")
+                flush_request = self._get_and_unset_flush_request()
+                if (
+                    self.queue.qsize() < self.max_export_batch_size
+                    and flush_request is None
+                ):
+                    try:
+                        await asyncio.wait_for(self.condition.wait(), timeout)
+                    except asyncio.TimeoutError:
+                        pass
+                    flush_request = self._get_and_unset_flush_request()
+                    if not self.queue:
+                        # spurious notification, let's wait again, reset timeout
+                        timeout = self.schedule_delay_millis / 1e3
+                        self._notify_flush_request_finished(flush_request)
+                        flush_request = None
+                        continue
+                    if self.done:
+                        # missing spans will be sent when calling flush
+                        break
+
+            # subtract the duration of this export call to the next timeout
+            start = _time_ns()
+            try:
+                await asyncio.wait_for(self._export(flush_request), timeout)
+            except asyncio.TimeoutError:
+                logger.exception("Took to much time to export, network problem ahead")
+            end = _time_ns()
+            duration = (end - start) / 1e9
+            timeout = self.schedule_delay_millis / 1e3 - duration
+
+            self._notify_flush_request_finished(flush_request)
+            flush_request = None
+
+        # there might have been a new flush request while export was running
+        # and before the done flag switched to true
+        async with self.condition:
+            shutdown_flush_request = self._get_and_unset_flush_request()
+
+        # be sure that all spans are sent
+        await self._drain_queue()
+        self._notify_flush_request_finished(flush_request)
+        self._notify_flush_request_finished(shutdown_flush_request)
+
+    def _get_and_unset_flush_request(
+        self,
+    ) -> Optional[_FlushRequest]:
+        """Returns the current flush request and makes it invisible to the
+        worker thread for subsequent calls.
+        """
+        flush_request = self._flush_request
+        self._flush_request = None
+        if flush_request is not None:
+            flush_request.num_spans = self.queue.qsize()
+        return flush_request
+
+    @staticmethod
+    def _notify_flush_request_finished(
+        flush_request: Optional[_FlushRequest],
+    ):
+        """Notifies the flush initiator(s) waiting on the given request/event
+        that the flush operation was finished.
+        """
+        if flush_request is not None:
+            flush_request.event.set()
+
+    def _get_or_create_flush_request(self) -> _FlushRequest:
+        """Either returns the current active flush event or creates a new one.
+        The flush event will be visible and read by the worker thread before an
+        export operation starts. Callers of a flush operation may wait on the
+        returned event to be notified when the flush/export operation was
+        finished.
+        This method is not thread-safe, i.e. callers need to take care about
+        synchronization/locking.
+        """
+        if self._flush_request is None:
+            self._flush_request = _FlushRequest()
+        return self._flush_request
+
+    async def _export(self, flush_request: Optional[_FlushRequest]):
+        """Exports spans considering the given flush_request.
+        In case of a given flush_requests spans are exported in batches until
+        the number of exported spans reached or exceeded the number of spans in
+        the flush request.
+        In no flush_request was given at most max_export_batch_size spans are
+        exported.
+        """
+        if not flush_request:
+            await self._export_batch()
+            return
+
+        num_spans = flush_request.num_spans
+        while self.queue.qsize():
+            num_exported = await self._export_batch()
+            num_spans -= num_exported
+
+            if num_spans <= 0:
+                break
+
+    async def _export_batch(self) -> int:
+        """Exports at most max_export_batch_size spans and returns the number of
+        exported spans.
+        """
+        idx = 0
+        # currently only a single thread acts as consumer, so queue.pop() will
+        # not raise an exception
+        while idx < self.max_export_batch_size and self.queue.qsize():
+            self.spans_list[idx] = self.queue.get_nowait()
+            idx += 1
+        token = attach(set_value(_SUPPRESS_INSTRUMENTATION_KEY, True))
+        try:
+            # Ignore type b/c the Optional[None]+slicing is too "clever"
+            # for mypy
+            await self.span_exporter.async_export(self.spans_list[:idx])  # type: ignore
+        except asyncio.CancelledError:
+            logger.exception("Task was canceled while exporting Span batch")
+        except Exception:  # pylint: disable=broad-except
+            logger.exception("Exception while exporting Span batch)")
+        detach(token)
+
+        # clean up list
+        for index in range(idx):
+            self.spans_list[index] = None
+        return idx
+
+    async def _drain_queue(self):
+        """Export all elements until queue is empty.
+        Can only be called from the worker thread context because it invokes
+        `export` that is not thread safe.
+        """
+        while self.queue.qsize():
+            await self._export_batch()
+
+    async def async_force_flush(self, timeout_millis: Optional[int] = None) -> bool:
+        if timeout_millis is None:
+            timeout_millis = self.export_timeout_millis
+
+        if self.done:
+            logger.warning("Already shutdown, ignoring call to async_force_flush().")
+            return True
+
+        async with self.condition:
+            flush_request = self._get_or_create_flush_request()
+            # signal the worker task to flush and wait for it to finish
+            self.condition.notify_all()
+
+        if flush_request.num_spans == 0:
+            return True
+
+        # wait for token to be processed
+        ret = await asyncio.wait_for(flush_request.event.wait(), timeout_millis)
+        if not ret:
+            logger.warning("Timeout was exceeded in async_force_flush().")
+            return False
+        return ret
+
+    def shutdown(self) -> None:
+        # signal the worker thread to finish and then wait for it
+        self.done = True
+        self.span_exporter.shutdown()
+        try:
+            self.worker_task.cancel()
+        except RuntimeError:
+            pass
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import traceback
-
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.trace.status import Status, StatusCode  # type: ignore
-
-
-def set_span_exception(span: Span, exception: Exception):
-    description = traceback.format_exc()
-
-    span.set_status(
-        Status(
-            status_code=StatusCode.ERROR,
-            description=description,
-        )
-    )
-    span.record_exception(exception)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import traceback
+
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.trace.status import Status, StatusCode  # type: ignore
+
+
+def set_span_exception(span: Span, exception: Exception):
+    description = traceback.format_exc()
+
+    span.set_status(
+        Status(
+            status_code=StatusCode.ERROR,
+            description=description,
+        )
+    )
+    span.record_exception(exception)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/context.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/context.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,80 +1,80 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# ABOUT
-# The module is meant to manage telemetry oriented context data.
-# This is a little different than the span context because we do
-# not necessarily want to inject all context data into every span,
-# only particular ones like the request handler root span.
-#
-# This allows us to leverage context data for both tracing and logs.
-#
-import contextvars
-from typing import Optional, Sequence, Union
-
-from opentelemetry.trace import get_current_span
-
-from nucliadb_telemetry.settings import telemetry_settings
-
-context_data = contextvars.ContextVar[Optional[dict[str, str]]]("data", default=None)
-
-
-def add_context(new_data: dict[str, str]):
-    """
-    This implementation always merges and sets the context, even if is was already set.
-
-    This is so data is propated forward but not backward.
-    """
-
-    # set the data on the current active span
-    set_info_on_span({f"nuclia.{key}": value for key, value in new_data.items()})
-
-    data = context_data.get()
-    if data is None:
-        data = {}
-    else:
-        data = data.copy()
-
-    data.update(new_data)
-    context_data.set(data)  # always set the context
-
-
-def get_context() -> dict[str, str]:
-    return context_data.get() or {}
-
-
-def set_info_on_span(
-    headers: dict[
-        str,
-        Union[
-            str,
-            bool,
-            int,
-            float,
-            Sequence[str],
-            Sequence[bool],
-            Sequence[int],
-            Sequence[float],
-        ],
-    ]
-):
-    if telemetry_settings.jaeger_enabled:
-        span = get_current_span()
-        if span is not None:
-            span.set_attributes(headers)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# ABOUT
+# The module is meant to manage telemetry oriented context data.
+# This is a little different than the span context because we do
+# not necessarily want to inject all context data into every span,
+# only particular ones like the request handler root span.
+#
+# This allows us to leverage context data for both tracing and logs.
+#
+import contextvars
+from typing import Optional, Sequence, Union
+
+from opentelemetry.trace import get_current_span
+
+from nucliadb_telemetry.settings import telemetry_settings
+
+context_data = contextvars.ContextVar[Optional[dict[str, str]]]("data", default=None)
+
+
+def add_context(new_data: dict[str, str]):
+    """
+    This implementation always merges and sets the context, even if is was already set.
+
+    This is so data is propated forward but not backward.
+    """
+
+    # set the data on the current active span
+    set_info_on_span({f"nuclia.{key}": value for key, value in new_data.items()})
+
+    data = context_data.get()
+    if data is None:
+        data = {}
+    else:
+        data = data.copy()
+
+    data.update(new_data)
+    context_data.set(data)  # always set the context
+
+
+def get_context() -> dict[str, str]:
+    return context_data.get() or {}
+
+
+def set_info_on_span(
+    headers: dict[
+        str,
+        Union[
+            str,
+            bool,
+            int,
+            float,
+            Sequence[str],
+            Sequence[bool],
+            Sequence[int],
+            Sequence[float],
+        ],
+    ]
+):
+    if telemetry_settings.jaeger_enabled:
+        span = get_current_span()
+        if span is not None:
+            span.set_attributes(headers)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# abstract advanced error handling into its own module to prevent
-# code from handling sentry integration everywhere
-from typing import Any, ContextManager, Optional
-
-import pydantic
-
-try:
-    import sentry_sdk
-    from sentry_sdk import Scope
-
-    SENTRY = True
-except ImportError:  # pragma: no cover
-    Scope = sentry_sdk = None  # type: ignore
-    SENTRY = False
-
-
-def capture_exception(error: BaseException) -> Optional[str]:
-    if SENTRY:
-        return sentry_sdk.capture_exception(error)
-    return None
-
-
-def capture_message(
-    error_msg: str,
-    level: Optional[str] = None,
-    scope: Optional[Any] = None,
-    **scope_args: Any
-) -> Optional[str]:
-    if SENTRY:
-        return sentry_sdk.capture_message(error_msg, level, scope, **scope_args)
-    return None
-
-
-class NoopScope:
-    def __enter__(self):
-        return self
-
-    def __exit__(self, *args):
-        ...
-
-    def set_extra(self, key: str, value: Any) -> None:
-        ...
-
-
-def push_scope(**kwargs: Any) -> ContextManager[Scope]:
-    if SENTRY:
-        return sentry_sdk.push_scope(**kwargs)
-    else:
-        return NoopScope()  # type: ignore
-
-
-class ErrorHandlingSettings(pydantic.BaseSettings):
-    sentry_url: Optional[str] = None
-    environment: str = pydantic.Field(
-        "local", env=["environment", "running_environment"]
-    )
-
-
-def setup_error_handling(version: str) -> None:
-    settings = ErrorHandlingSettings()
-
-    if settings.sentry_url:
-        # Disabled everywhere for now. Let's have less knobs to tweak.
-        # Either we use with with sentry or we don't.
-        # enabled_integrations: list[Any] = [
-        #     LoggingIntegration(level=logging.CRITICAL, event_level=logging.CRITICAL)
-        # ]
-
-        sentry_sdk.init(
-            release=version,
-            environment=settings.environment,
-            dsn=settings.sentry_url,
-            integrations=[],
-            default_integrations=False,
-        )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# abstract advanced error handling into its own module to prevent
+# code from handling sentry integration everywhere
+from typing import Any, ContextManager, Optional
+
+import pydantic
+
+try:
+    import sentry_sdk
+    from sentry_sdk import Scope
+
+    SENTRY = True
+except ImportError:  # pragma: no cover
+    Scope = sentry_sdk = None  # type: ignore
+    SENTRY = False
+
+
+def capture_exception(error: BaseException) -> Optional[str]:
+    if SENTRY:
+        return sentry_sdk.capture_exception(error)
+    return None
+
+
+def capture_message(
+    error_msg: str,
+    level: Optional[str] = None,
+    scope: Optional[Any] = None,
+    **scope_args: Any
+) -> Optional[str]:
+    if SENTRY:
+        return sentry_sdk.capture_message(error_msg, level, scope, **scope_args)
+    return None
+
+
+class NoopScope:
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *args):
+        ...
+
+    def set_extra(self, key: str, value: Any) -> None:
+        ...
+
+
+def push_scope(**kwargs: Any) -> ContextManager[Scope]:
+    if SENTRY:
+        return sentry_sdk.push_scope(**kwargs)
+    else:
+        return NoopScope()  # type: ignore
+
+
+class ErrorHandlingSettings(pydantic.BaseSettings):
+    sentry_url: Optional[str] = None
+    environment: str = pydantic.Field(
+        "local", env=["environment", "running_environment"]
+    )
+
+
+def setup_error_handling(version: str) -> None:
+    settings = ErrorHandlingSettings()
+
+    if settings.sentry_url:
+        # Disabled everywhere for now. Let's have less knobs to tweak.
+        # Either we use with with sentry or we don't.
+        # enabled_integrations: list[Any] = [
+        #     LoggingIntegration(level=logging.CRITICAL, event_level=logging.CRITICAL)
+        # ]
+
+        sentry_sdk.init(
+            release=version,
+            environment=settings.environment,
+            dsn=settings.sentry_url,
+            integrations=[],
+            default_integrations=False,
+        )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,100 +1,100 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from typing import Iterable, List
-from urllib.parse import urlparse
-
-import prometheus_client  # type: ignore
-from fastapi import FastAPI
-from opentelemetry.instrumentation.fastapi import _get_route_details  # type: ignore
-from prometheus_client import CONTENT_TYPE_LATEST
-from starlette.responses import PlainTextResponse
-
-from nucliadb_telemetry.fastapi.metrics import PrometheusMiddleware
-from nucliadb_telemetry.fastapi.tracing import (
-    CaptureTraceIdMiddleware,
-    OpenTelemetryMiddleware,
-    ServerRequestHookT,
-)
-
-from .context import ContextInjectorMiddleware
-
-try:
-    from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
-except ImportError:  # pragma: no cover
-    SentryAsgiMiddleware = None  # type: ignore
-
-
-async def metrics_endpoint(request):
-    output = prometheus_client.exposition.generate_latest()
-    return PlainTextResponse(
-        output.decode("utf8"), headers={"Content-Type": CONTENT_TYPE_LATEST}
-    )
-
-
-application_metrics = FastAPI(title="Metrics")  # type: ignore
-application_metrics.add_route("/metrics", metrics_endpoint)
-
-
-class ExcludeList:
-
-    """Class to exclude certain paths (given as a list of regexes) from tracing requests"""
-
-    def __init__(self, excluded_urls: Iterable[str]):
-        self._excluded_urls = excluded_urls
-
-    def url_disabled(self, url: str) -> bool:
-        return bool(self._excluded_urls and urlparse(url).path in self._excluded_urls)
-
-
-def instrument_app(
-    app: FastAPI,
-    excluded_urls: List[str],
-    server_request_hook: ServerRequestHookT = None,
-    tracer_provider=None,
-    metrics=False,
-    trace_id_on_responses: bool = False,
-):
-    """
-    :param trace_id_on_responses: If set to True, trace ids will be returned in the X-NUCLIA-TRACE-ID header for all HTTP responses of this app.
-    """
-    if metrics:
-        # b/w compat
-        app.add_middleware(PrometheusMiddleware)
-
-    excluded_urls_obj = ExcludeList(excluded_urls)
-
-    if trace_id_on_responses:
-        # Trace ids are provided by OpenTelemetryMiddleware,
-        # so the capture middleware needs to be added before.
-        app.add_middleware(CaptureTraceIdMiddleware)
-
-    app.add_middleware(ContextInjectorMiddleware)
-    app.add_middleware(
-        OpenTelemetryMiddleware,
-        excluded_urls=excluded_urls_obj,
-        default_span_details=_get_route_details,
-        server_request_hook=server_request_hook,
-        tracer_provider=tracer_provider,
-    )
-
-    if SentryAsgiMiddleware is not None:
-        # add last to catch all exceptions
-        # `add_middleware` always adds to the beginning of the middleware list
-        app.add_middleware(SentryAsgiMiddleware)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from typing import Iterable, List
+from urllib.parse import urlparse
+
+import prometheus_client  # type: ignore
+from fastapi import FastAPI
+from opentelemetry.instrumentation.fastapi import _get_route_details  # type: ignore
+from prometheus_client import CONTENT_TYPE_LATEST
+from starlette.responses import PlainTextResponse
+
+from nucliadb_telemetry.fastapi.metrics import PrometheusMiddleware
+from nucliadb_telemetry.fastapi.tracing import (
+    CaptureTraceIdMiddleware,
+    OpenTelemetryMiddleware,
+    ServerRequestHookT,
+)
+
+from .context import ContextInjectorMiddleware
+
+try:
+    from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
+except ImportError:  # pragma: no cover
+    SentryAsgiMiddleware = None  # type: ignore
+
+
+async def metrics_endpoint(request):
+    output = prometheus_client.exposition.generate_latest()
+    return PlainTextResponse(
+        output.decode("utf8"), headers={"Content-Type": CONTENT_TYPE_LATEST}
+    )
+
+
+application_metrics = FastAPI(title="Metrics")  # type: ignore
+application_metrics.add_route("/metrics", metrics_endpoint)
+
+
+class ExcludeList:
+
+    """Class to exclude certain paths (given as a list of regexes) from tracing requests"""
+
+    def __init__(self, excluded_urls: Iterable[str]):
+        self._excluded_urls = excluded_urls
+
+    def url_disabled(self, url: str) -> bool:
+        return bool(self._excluded_urls and urlparse(url).path in self._excluded_urls)
+
+
+def instrument_app(
+    app: FastAPI,
+    excluded_urls: List[str],
+    server_request_hook: ServerRequestHookT = None,
+    tracer_provider=None,
+    metrics=False,
+    trace_id_on_responses: bool = False,
+):
+    """
+    :param trace_id_on_responses: If set to True, trace ids will be returned in the X-NUCLIA-TRACE-ID header for all HTTP responses of this app.
+    """
+    if metrics:
+        # b/w compat
+        app.add_middleware(PrometheusMiddleware)
+
+    excluded_urls_obj = ExcludeList(excluded_urls)
+
+    if trace_id_on_responses:
+        # Trace ids are provided by OpenTelemetryMiddleware,
+        # so the capture middleware needs to be added before.
+        app.add_middleware(CaptureTraceIdMiddleware)
+
+    app.add_middleware(ContextInjectorMiddleware)
+    app.add_middleware(
+        OpenTelemetryMiddleware,
+        excluded_urls=excluded_urls_obj,
+        default_span_details=_get_route_details,
+        server_request_hook=server_request_hook,
+        tracer_provider=tracer_provider,
+    )
+
+    if SentryAsgiMiddleware is not None:
+        # add last to catch all exceptions
+        # `add_middleware` always adds to the beginning of the middleware list
+        app.add_middleware(SentryAsgiMiddleware)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/context.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/context.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
-from starlette.requests import Request
-from starlette.responses import Response
-
-from nucliadb_telemetry import context
-
-from .utils import get_path_template
-
-
-class ContextInjectorMiddleware(BaseHTTPMiddleware):
-    """
-    Automatically inject context values for the current request's path parameters
-
-    For example:
-        - `/api/v1/kb/{kbid}` would inject a context value for `kbid`
-    """
-
-    async def dispatch(
-        self, request: Request, call_next: RequestResponseEndpoint
-    ) -> Response:
-        found_path_template = get_path_template(request.scope)
-        if found_path_template.match:
-            context.add_context(found_path_template.scope.get("path_params", {}))  # type: ignore
-
-        return await call_next(request)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
+from starlette.requests import Request
+from starlette.responses import Response
+
+from nucliadb_telemetry import context
+
+from .utils import get_path_template
+
+
+class ContextInjectorMiddleware(BaseHTTPMiddleware):
+    """
+    Automatically inject context values for the current request's path parameters
+
+    For example:
+        - `/api/v1/kb/{kbid}` would inject a context value for `kbid`
+    """
+
+    async def dispatch(
+        self, request: Request, call_next: RequestResponseEndpoint
+    ) -> Response:
+        found_path_template = get_path_template(request.scope)
+        if found_path_template.match:
+            context.add_context(found_path_template.scope.get("path_params", {}))  # type: ignore
+
+        return await call_next(request)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/metrics.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/metrics.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import time
-
-from prometheus_client import Counter, Gauge, Histogram
-from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
-from starlette.requests import Request
-from starlette.responses import Response
-from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR
-
-from .utils import get_path_template
-
-try:
-    from starlette_prometheus.middleware import (
-        EXCEPTIONS,
-        REQUESTS,
-        REQUESTS_IN_PROGRESS,
-        REQUESTS_PROCESSING_TIME,
-        RESPONSES,
-    )
-except ImportError:  # pragma: no cover
-    # prometheus does not allow duplicate metric names, so we need to
-    # conditionally import to avoid conflicts when both are installed
-    REQUESTS = Counter(
-        "starlette_requests_total",
-        "Total count of requests by method and path.",
-        ["method", "path_template"],
-    )
-    RESPONSES = Counter(
-        "starlette_responses_total",
-        "Total count of responses by method, path and status codes.",
-        ["method", "path_template", "status_code"],
-    )
-    REQUESTS_PROCESSING_TIME = Histogram(
-        "starlette_requests_processing_time_seconds",
-        "Histogram of requests processing time by path (in seconds)",
-        ["method", "path_template"],
-    )
-    EXCEPTIONS = Counter(
-        "starlette_exceptions_total",
-        "Total count of exceptions raised by path and exception type",
-        ["method", "path_template", "exception_type"],
-    )
-    REQUESTS_IN_PROGRESS = Gauge(
-        "starlette_requests_in_progress",
-        "Gauge of requests by method and path currently being processed",
-        ["method", "path_template"],
-    )
-
-
-class PrometheusMiddleware(BaseHTTPMiddleware):
-    async def dispatch(
-        self, request: Request, call_next: RequestResponseEndpoint
-    ) -> Response:
-        method = request.method
-        found_path_template = get_path_template(request.scope)
-
-        if not found_path_template.match:
-            return await call_next(request)
-
-        path_template = found_path_template.path
-
-        REQUESTS_IN_PROGRESS.labels(method=method, path_template=path_template).inc()
-        REQUESTS.labels(method=method, path_template=path_template).inc()
-        before_time = time.perf_counter()
-        try:
-            response = await call_next(request)
-        except BaseException as e:
-            status_code = HTTP_500_INTERNAL_SERVER_ERROR
-            EXCEPTIONS.labels(
-                method=method,
-                path_template=path_template,
-                exception_type=type(e).__name__,
-            ).inc()
-            raise e from None
-        else:
-            status_code = response.status_code
-            after_time = time.perf_counter()
-            REQUESTS_PROCESSING_TIME.labels(
-                method=method, path_template=path_template
-            ).observe(after_time - before_time)
-        finally:
-            RESPONSES.labels(
-                method=method, path_template=path_template, status_code=status_code
-            ).inc()
-            REQUESTS_IN_PROGRESS.labels(
-                method=method, path_template=path_template
-            ).dec()
-
-        return response
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import time
+
+from prometheus_client import Counter, Gauge, Histogram
+from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
+from starlette.requests import Request
+from starlette.responses import Response
+from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR
+
+from .utils import get_path_template
+
+try:
+    from starlette_prometheus.middleware import (
+        EXCEPTIONS,
+        REQUESTS,
+        REQUESTS_IN_PROGRESS,
+        REQUESTS_PROCESSING_TIME,
+        RESPONSES,
+    )
+except ImportError:  # pragma: no cover
+    # prometheus does not allow duplicate metric names, so we need to
+    # conditionally import to avoid conflicts when both are installed
+    REQUESTS = Counter(
+        "starlette_requests_total",
+        "Total count of requests by method and path.",
+        ["method", "path_template"],
+    )
+    RESPONSES = Counter(
+        "starlette_responses_total",
+        "Total count of responses by method, path and status codes.",
+        ["method", "path_template", "status_code"],
+    )
+    REQUESTS_PROCESSING_TIME = Histogram(
+        "starlette_requests_processing_time_seconds",
+        "Histogram of requests processing time by path (in seconds)",
+        ["method", "path_template"],
+    )
+    EXCEPTIONS = Counter(
+        "starlette_exceptions_total",
+        "Total count of exceptions raised by path and exception type",
+        ["method", "path_template", "exception_type"],
+    )
+    REQUESTS_IN_PROGRESS = Gauge(
+        "starlette_requests_in_progress",
+        "Gauge of requests by method and path currently being processed",
+        ["method", "path_template"],
+    )
+
+
+class PrometheusMiddleware(BaseHTTPMiddleware):
+    async def dispatch(
+        self, request: Request, call_next: RequestResponseEndpoint
+    ) -> Response:
+        method = request.method
+        found_path_template = get_path_template(request.scope)
+
+        if not found_path_template.match:
+            return await call_next(request)
+
+        path_template = found_path_template.path
+
+        REQUESTS_IN_PROGRESS.labels(method=method, path_template=path_template).inc()
+        REQUESTS.labels(method=method, path_template=path_template).inc()
+        before_time = time.perf_counter()
+        try:
+            response = await call_next(request)
+        except BaseException as e:
+            status_code = HTTP_500_INTERNAL_SERVER_ERROR
+            EXCEPTIONS.labels(
+                method=method,
+                path_template=path_template,
+                exception_type=type(e).__name__,
+            ).inc()
+            raise e from None
+        else:
+            status_code = response.status_code
+            after_time = time.perf_counter()
+            REQUESTS_PROCESSING_TIME.labels(
+                method=method, path_template=path_template
+            ).observe(after_time - before_time)
+        finally:
+            RESPONSES.labels(
+                method=method, path_template=path_template, status_code=status_code
+            ).inc()
+            REQUESTS_IN_PROGRESS.labels(
+                method=method, path_template=path_template
+            ).dec()
+
+        return response
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/tracing.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/tracing.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,380 +1,382 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-import typing
-import urllib
-from functools import wraps
-from typing import Callable, Optional, Tuple
-
-from asgiref.compatibility import guarantee_single_callable
-from fastapi import Request, Response
-from opentelemetry import context, trace
-from opentelemetry.instrumentation.asgi.version import __version__  # noqa
-from opentelemetry.instrumentation.propagators import get_global_response_propagator
-from opentelemetry.instrumentation.utils import (
-    _start_internal_or_server_span,
-    http_status_to_status_code,
-)
-from opentelemetry.propagators.textmap import Getter, Setter
-from opentelemetry.semconv.trace import SpanAttributes
-from opentelemetry.trace import Span  # type: ignore
-from opentelemetry.trace import format_trace_id, set_span_in_context
-from opentelemetry.trace.status import Status, StatusCode
-from opentelemetry.util.http import (
-    OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST,
-    OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE,
-    get_custom_headers,
-    normalise_request_header_name,
-    normalise_response_header_name,
-    remove_url_credentials,
-)
-from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
-
-ServerRequestHookT = Optional[Callable[[Span, dict], None]]
-
-
-NUCLIA_TRACE_ID_HEADER = "X-NUCLIA-TRACE-ID"
-ACCESS_CONTROL_EXPOSE_HEADER = "Access-Control-Expose-Headers"
-
-# ----------------------------
-# Forked from https://raw.githubusercontent.com/open-telemetry/opentelemetry-python-contrib/main/instrumentation/opentelemetry-instrumentation-asgi/src/opentelemetry/instrumentation/asgi/__init__.py
-#
-# Changes:
-#
-# - Remove receive and send hooks
-# - Remove adding spans on receive
-# - Remove adding spans on send, while keeping the header extraction and propagation
-# ----------------------------
-
-
-class ASGIGetter(Getter):
-    def get(self, carrier: dict, key: str) -> typing.Optional[typing.List[str]]:  # type: ignore
-        """Getter implementation to retrieve a HTTP header value from the ASGI
-        scope.
-        Args:
-            carrier: ASGI scope object
-            key: header name in scope
-        Returns:
-            A list with a single string with the header value if it exists,
-                else None.
-        """
-        headers = carrier.get("headers")
-        if not headers:
-            return None
-
-        # asgi header keys are in lower case
-        key = key.lower()
-        decoded = [
-            _value.decode("utf8")
-            for (_key, _value) in headers
-            if _key.decode("utf8") == key
-        ]
-        if not decoded:
-            return None
-        return decoded
-
-    def keys(self, carrier: dict) -> typing.List[str]:  # type: ignore
-        return list(carrier.keys())
-
-
-asgi_getter = ASGIGetter()
-
-
-class ASGISetter(Setter):
-    def set(self, carrier: dict, key: str, value: str) -> None:  # type: ignore
-        """Sets response header values on an ASGI scope according to `the spec <https://asgi.readthedocs.io/en/latest/specs/www.html#response-start-send-event>`_.
-        Args:
-            carrier: ASGI scope object
-            key: response header name to set
-            value: response header value
-        Returns:
-            None
-        """
-        headers = carrier.get("headers")
-        if not headers:
-            headers = []
-            carrier["headers"] = headers
-
-        headers.append([key.lower().encode(), value.encode()])
-
-
-asgi_setter = ASGISetter()
-
-
-def collect_request_attributes(scope):
-    """Collects HTTP request attributes from the ASGI scope and returns a
-    dictionary to be used as span creation attributes."""
-    server_host, port, http_url = get_host_port_url_tuple(scope)
-    query_string = scope.get("query_string")
-    if query_string and http_url:
-        if isinstance(query_string, bytes):
-            query_string = query_string.decode("utf8")
-        http_url += "?" + urllib.parse.unquote(query_string)
-
-    result = {
-        SpanAttributes.HTTP_SCHEME: scope.get("scheme"),
-        SpanAttributes.HTTP_HOST: server_host,
-        SpanAttributes.NET_HOST_PORT: port,
-        SpanAttributes.HTTP_FLAVOR: scope.get("http_version"),
-        SpanAttributes.HTTP_TARGET: scope.get("path"),
-        SpanAttributes.HTTP_URL: remove_url_credentials(http_url),
-    }
-    http_method = scope.get("method")
-    if http_method:
-        result[SpanAttributes.HTTP_METHOD] = http_method
-
-    http_host_value_list = asgi_getter.get(scope, "host")
-    if http_host_value_list:
-        result[SpanAttributes.HTTP_SERVER_NAME] = ",".join(http_host_value_list)
-    http_user_agent = asgi_getter.get(scope, "user-agent")
-    if http_user_agent:
-        result[SpanAttributes.HTTP_USER_AGENT] = http_user_agent[0]
-
-    if "client" in scope and scope["client"] is not None:
-        result[SpanAttributes.NET_PEER_IP] = scope.get("client")[0]
-        result[SpanAttributes.NET_PEER_PORT] = scope.get("client")[1]
-
-    # remove None values
-    result = {k: v for k, v in result.items() if v is not None}
-
-    return result
-
-
-def collect_custom_request_headers_attributes(scope):
-    """returns custom HTTP request headers to be added into SERVER span as span attributes
-    Refer specification https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/http.md#http-request-and-response-headers
-    """
-
-    attributes = {}
-    custom_request_headers = get_custom_headers(
-        OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST
-    )
-
-    for header in custom_request_headers:
-        values = asgi_getter.get(scope, header)
-        if values:
-            key = normalise_request_header_name(header)
-            attributes.setdefault(key, []).extend(values)
-
-    return attributes
-
-
-def collect_custom_response_headers_attributes(message):
-    """returns custom HTTP response headers to be added into SERVER span as span attributes
-    Refer specification https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/http.md#http-request-and-response-headers
-    """
-    attributes = {}
-    custom_response_headers = get_custom_headers(
-        OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE
-    )
-
-    for header in custom_response_headers:
-        values = asgi_getter.get(message, header)
-        if values:
-            key = normalise_response_header_name(header)
-            attributes.setdefault(key, []).extend(values)
-
-    return attributes
-
-
-def get_host_port_url_tuple(scope):
-    """Returns (host, port, full_url) tuple."""
-    server = scope.get("server") or ["0.0.0.0", 80]
-    port = server[1]
-    server_host = server[0] + (":" + str(port) if str(port) != "80" else "")
-    full_path = scope.get("root_path", "") + scope.get("path", "")
-    http_url = scope.get("scheme", "http") + "://" + server_host + full_path
-    return server_host, port, http_url
-
-
-def set_status_code(span, status_code):
-    """Adds HTTP response attributes to span using the status_code argument."""
-    if not span.is_recording():
-        return
-    try:
-        status_code = int(status_code)
-    except ValueError:
-        span.set_status(
-            Status(
-                StatusCode.ERROR,
-                "Non-integer HTTP status: " + repr(status_code),
-            )
-        )
-    else:
-        span.set_attribute(SpanAttributes.HTTP_STATUS_CODE, status_code)
-        span.set_status(
-            Status(http_status_to_status_code(status_code, server_span=True))
-        )
-
-
-def get_default_span_details(scope: dict) -> Tuple[str, dict]:
-    """Default implementation for get_default_span_details
-    Args:
-        scope: the asgi scope dictionary
-    Returns:
-        a tuple of the span name, and any attributes to attach to the span.
-    """
-    span_name = (
-        scope.get("path", "").strip() or f"HTTP {scope.get('method', '').strip()}"
-    )
-
-    return span_name, {}
-
-
-class OpenTelemetryMiddleware:
-    """The ASGI application middleware.
-    This class is an ASGI middleware that starts and annotates spans for any
-    requests it is invoked with.
-    Args:
-        app: The ASGI application callable to forward requests to.
-        default_span_details: Callback which should return a string and a tuple, representing the desired default span name and a
-                      dictionary with any additional span attributes to set.
-                      Optional: Defaults to get_default_span_details.
-        server_request_hook: Optional callback which is called with the server span and ASGI
-                      scope object for every incoming request.
-        tracer_provider: The optional tracer provider to use. If omitted
-            the current globally configured one is used.
-    """
-
-    def __init__(
-        self,
-        app,
-        excluded_urls=None,
-        default_span_details=None,
-        server_request_hook: ServerRequestHookT = None,
-        tracer_provider=None,
-    ):
-        self.app = guarantee_single_callable(app)
-        self.tracer = trace.get_tracer(__name__, __version__, tracer_provider)
-        self.excluded_urls = excluded_urls
-        self.default_span_details = default_span_details or get_default_span_details
-        self.server_request_hook = server_request_hook
-
-    async def __call__(self, scope, receive, send):
-        """The ASGI application
-        Args:
-            scope: A ASGI environment.
-            receive: An awaitable callable yielding dictionaries
-            send: An awaitable callable taking a single dictionary as argument.
-        """
-        if scope["type"] not in ("http", "websocket"):
-            return await self.app(scope, receive, send)
-
-        _, _, url = get_host_port_url_tuple(scope)
-        if self.excluded_urls and self.excluded_urls.url_disabled(url):
-            return await self.app(scope, receive, send)
-
-        span_name, additional_attributes = self.default_span_details(scope)
-
-        span, token = _start_internal_or_server_span(
-            tracer=self.tracer,
-            span_name=span_name,
-            start_time=None,
-            context_carrier=scope,
-            context_getter=asgi_getter,
-        )
-
-        try:
-            with trace.use_span(span, end_on_exit=True) as current_span:
-                if current_span.is_recording():
-                    attributes = collect_request_attributes(scope)
-                    attributes.update(additional_attributes)
-                    for key, value in attributes.items():
-                        current_span.set_attribute(key, value)
-
-                    if current_span.kind == trace.SpanKind.SERVER:
-                        custom_attributes = collect_custom_request_headers_attributes(
-                            scope
-                        )
-                        if len(custom_attributes) > 0:
-                            current_span.set_attributes(custom_attributes)
-
-                if callable(self.server_request_hook):
-                    self.server_request_hook(current_span, scope)
-
-                otel_send = self._get_otel_send(
-                    current_span,
-                    span_name,
-                    scope,
-                    send,
-                )
-                await self.app(scope, receive, otel_send)
-
-        finally:
-            if token:
-                context.detach(token)
-
-    def _get_otel_send(self, server_span, server_span_name, scope, send):
-        @wraps(send)
-        async def otel_send(message):
-            if message["type"] == "http.response.start":
-                status_code = message["status"]
-                set_status_code(server_span, status_code)
-            elif message["type"] == "websocket.send":
-                set_status_code(server_span, 200)
-            if (
-                server_span.is_recording()
-                and server_span.kind == trace.SpanKind.SERVER
-                and "headers" in message
-            ):
-                custom_response_attributes = collect_custom_response_headers_attributes(
-                    message
-                )
-                if len(custom_response_attributes) > 0:
-                    server_span.set_attributes(custom_response_attributes)
-
-            propagator = get_global_response_propagator()
-            if propagator:
-                propagator.inject(
-                    message,
-                    context=set_span_in_context(
-                        server_span, trace.context_api.Context()
-                    ),
-                    setter=asgi_setter,
-                )
-
-            await send(message)
-
-        return otel_send
-
-
-class CaptureTraceIdMiddleware(BaseHTTPMiddleware):
-    def capture_trace_id(self, response):
-        span = trace.get_current_span()
-        if span is None:
-            return
-        trace_id = format_trace_id(span.get_span_context().trace_id)
-        response.headers[NUCLIA_TRACE_ID_HEADER] = trace_id
-
-    def expose_trace_id_header(self, response):
-        exposed_headers = []
-        if ACCESS_CONTROL_EXPOSE_HEADER in response.headers:
-            exposed_headers = response.headers[ACCESS_CONTROL_EXPOSE_HEADER].split(",")
-        if NUCLIA_TRACE_ID_HEADER not in exposed_headers:
-            exposed_headers.append(NUCLIA_TRACE_ID_HEADER)
-            response.headers[ACCESS_CONTROL_EXPOSE_HEADER] = ",".join(exposed_headers)
-
-    async def dispatch(
-        self, request: Request, call_next: RequestResponseEndpoint
-    ) -> Response:
-        try:
-            response = await call_next(request)
-        finally:
-            self.capture_trace_id(response)
-            self.expose_trace_id_header(response)
-            return response
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+import typing
+import urllib
+from functools import wraps
+from typing import Callable, Optional, Tuple
+
+from asgiref.compatibility import guarantee_single_callable
+from fastapi import Request, Response
+from opentelemetry import context, trace
+from opentelemetry.instrumentation.asgi.version import __version__  # noqa
+from opentelemetry.instrumentation.propagators import get_global_response_propagator
+from opentelemetry.instrumentation.utils import (
+    _start_internal_or_server_span,
+    http_status_to_status_code,
+)
+from opentelemetry.propagators.textmap import Getter, Setter
+from opentelemetry.semconv.trace import SpanAttributes
+from opentelemetry.trace import Span  # type: ignore
+from opentelemetry.trace import format_trace_id, set_span_in_context
+from opentelemetry.trace.status import Status, StatusCode
+from opentelemetry.util.http import (
+    OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST,
+    OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE,
+    get_custom_headers,
+    normalise_request_header_name,
+    normalise_response_header_name,
+    remove_url_credentials,
+)
+from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
+
+ServerRequestHookT = Optional[Callable[[Span, dict], None]]
+
+
+NUCLIA_TRACE_ID_HEADER = "X-NUCLIA-TRACE-ID"
+ACCESS_CONTROL_EXPOSE_HEADER = "Access-Control-Expose-Headers"
+
+# ----------------------------
+# Forked from https://raw.githubusercontent.com/open-telemetry/opentelemetry-python-contrib/main/instrumentation/opentelemetry-instrumentation-asgi/src/opentelemetry/instrumentation/asgi/__init__.py
+#
+# Changes:
+#
+# - Remove receive and send hooks
+# - Remove adding spans on receive
+# - Remove adding spans on send, while keeping the header extraction and propagation
+# ----------------------------
+
+
+class ASGIGetter(Getter):
+    def get(self, carrier: dict, key: str) -> typing.Optional[typing.List[str]]:  # type: ignore
+        """Getter implementation to retrieve a HTTP header value from the ASGI
+        scope.
+        Args:
+            carrier: ASGI scope object
+            key: header name in scope
+        Returns:
+            A list with a single string with the header value if it exists,
+                else None.
+        """
+        headers = carrier.get("headers")
+        if not headers:
+            return None
+
+        # asgi header keys are in lower case
+        key = key.lower()
+        decoded = [
+            _value.decode("utf8")
+            for (_key, _value) in headers
+            if _key.decode("utf8") == key
+        ]
+        if not decoded:
+            return None
+        return decoded
+
+    def keys(self, carrier: dict) -> typing.List[str]:  # type: ignore
+        return list(carrier.keys())
+
+
+asgi_getter = ASGIGetter()
+
+
+class ASGISetter(Setter):
+    def set(self, carrier: dict, key: str, value: str) -> None:  # type: ignore
+        """Sets response header values on an ASGI scope according to `the spec <https://asgi.readthedocs.io/en/latest/specs/www.html#response-start-send-event>`_.
+        Args:
+            carrier: ASGI scope object
+            key: response header name to set
+            value: response header value
+        Returns:
+            None
+        """
+        headers = carrier.get("headers")
+        if not headers:
+            headers = []
+            carrier["headers"] = headers
+
+        headers.append([key.lower().encode(), value.encode()])
+
+
+asgi_setter = ASGISetter()
+
+
+def collect_request_attributes(scope):
+    """Collects HTTP request attributes from the ASGI scope and returns a
+    dictionary to be used as span creation attributes."""
+    server_host, port, http_url = get_host_port_url_tuple(scope)
+    query_string = scope.get("query_string")
+    if query_string and http_url:
+        if isinstance(query_string, bytes):
+            query_string = query_string.decode("utf8")
+        http_url += "?" + urllib.parse.unquote(query_string)
+
+    result = {
+        SpanAttributes.HTTP_SCHEME: scope.get("scheme"),
+        SpanAttributes.HTTP_HOST: server_host,
+        SpanAttributes.NET_HOST_PORT: port,
+        SpanAttributes.HTTP_FLAVOR: scope.get("http_version"),
+        SpanAttributes.HTTP_TARGET: scope.get("path"),
+        SpanAttributes.HTTP_URL: remove_url_credentials(http_url),
+    }
+    http_method = scope.get("method")
+    if http_method:
+        result[SpanAttributes.HTTP_METHOD] = http_method
+
+    http_host_value_list = asgi_getter.get(scope, "host")
+    if http_host_value_list:
+        result[SpanAttributes.HTTP_SERVER_NAME] = ",".join(http_host_value_list)
+    http_user_agent = asgi_getter.get(scope, "user-agent")
+    if http_user_agent:
+        result[SpanAttributes.HTTP_USER_AGENT] = http_user_agent[0]
+
+    if "client" in scope and scope["client"] is not None:
+        result[SpanAttributes.NET_PEER_IP] = scope.get("client")[0]
+        result[SpanAttributes.NET_PEER_PORT] = scope.get("client")[1]
+
+    # remove None values
+    result = {k: v for k, v in result.items() if v is not None}
+
+    return result
+
+
+def collect_custom_request_headers_attributes(scope):
+    """returns custom HTTP request headers to be added into SERVER span as span attributes
+    Refer specification https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/http.md#http-request-and-response-headers
+    """
+
+    attributes = {}
+    custom_request_headers = get_custom_headers(
+        OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST
+    )
+
+    for header in custom_request_headers:
+        values = asgi_getter.get(scope, header)
+        if values:
+            key = normalise_request_header_name(header)
+            attributes.setdefault(key, []).extend(values)
+
+    return attributes
+
+
+def collect_custom_response_headers_attributes(message):
+    """returns custom HTTP response headers to be added into SERVER span as span attributes
+    Refer specification https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/http.md#http-request-and-response-headers
+    """
+    attributes = {}
+    custom_response_headers = get_custom_headers(
+        OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE
+    )
+
+    for header in custom_response_headers:
+        values = asgi_getter.get(message, header)
+        if values:
+            key = normalise_response_header_name(header)
+            attributes.setdefault(key, []).extend(values)
+
+    return attributes
+
+
+def get_host_port_url_tuple(scope):
+    """Returns (host, port, full_url) tuple."""
+    server = scope.get("server") or ["0.0.0.0", 80]
+    port = server[1]
+    server_host = server[0] + (":" + str(port) if str(port) != "80" else "")
+    full_path = scope.get("root_path", "") + scope.get("path", "")
+    http_url = scope.get("scheme", "http") + "://" + server_host + full_path
+    return server_host, port, http_url
+
+
+def set_status_code(span, status_code):
+    """Adds HTTP response attributes to span using the status_code argument."""
+    if not span.is_recording():
+        return
+    try:
+        status_code = int(status_code)
+    except ValueError:
+        span.set_status(
+            Status(
+                StatusCode.ERROR,
+                "Non-integer HTTP status: " + repr(status_code),
+            )
+        )
+    else:
+        span.set_attribute(SpanAttributes.HTTP_STATUS_CODE, status_code)
+        span.set_status(
+            Status(http_status_to_status_code(status_code, server_span=True))
+        )
+
+
+def get_default_span_details(scope: dict) -> Tuple[str, dict]:
+    """Default implementation for get_default_span_details
+    Args:
+        scope: the asgi scope dictionary
+    Returns:
+        a tuple of the span name, and any attributes to attach to the span.
+    """
+    span_name = (
+        scope.get("path", "").strip() or f"HTTP {scope.get('method', '').strip()}"
+    )
+
+    return span_name, {}
+
+
+class OpenTelemetryMiddleware:
+    """The ASGI application middleware.
+    This class is an ASGI middleware that starts and annotates spans for any
+    requests it is invoked with.
+    Args:
+        app: The ASGI application callable to forward requests to.
+        default_span_details: Callback which should return a string and a tuple, representing the desired default span name and a
+                      dictionary with any additional span attributes to set.
+                      Optional: Defaults to get_default_span_details.
+        server_request_hook: Optional callback which is called with the server span and ASGI
+                      scope object for every incoming request.
+        tracer_provider: The optional tracer provider to use. If omitted
+            the current globally configured one is used.
+    """
+
+    def __init__(
+        self,
+        app,
+        excluded_urls=None,
+        default_span_details=None,
+        server_request_hook: ServerRequestHookT = None,
+        tracer_provider=None,
+    ):
+        self.app = guarantee_single_callable(app)
+        self.tracer = trace.get_tracer(__name__, __version__, tracer_provider)
+        self.excluded_urls = excluded_urls
+        self.default_span_details = default_span_details or get_default_span_details
+        self.server_request_hook = server_request_hook
+
+    async def __call__(self, scope, receive, send):
+        """The ASGI application
+        Args:
+            scope: A ASGI environment.
+            receive: An awaitable callable yielding dictionaries
+            send: An awaitable callable taking a single dictionary as argument.
+        """
+        if scope["type"] not in ("http", "websocket"):
+            return await self.app(scope, receive, send)
+
+        _, _, url = get_host_port_url_tuple(scope)
+        if self.excluded_urls and self.excluded_urls.url_disabled(url):
+            return await self.app(scope, receive, send)
+
+        span_name, additional_attributes = self.default_span_details(scope)
+
+        span, token = _start_internal_or_server_span(
+            tracer=self.tracer,
+            span_name=span_name,
+            start_time=None,
+            context_carrier=scope,
+            context_getter=asgi_getter,
+        )
+
+        try:
+            with trace.use_span(span, end_on_exit=True) as current_span:
+                if current_span.is_recording():
+                    attributes = collect_request_attributes(scope)
+                    attributes.update(additional_attributes)
+                    for key, value in attributes.items():
+                        current_span.set_attribute(key, value)
+
+                    if current_span.kind == trace.SpanKind.SERVER:
+                        custom_attributes = collect_custom_request_headers_attributes(
+                            scope
+                        )
+                        if len(custom_attributes) > 0:
+                            current_span.set_attributes(custom_attributes)
+
+                if callable(self.server_request_hook):
+                    self.server_request_hook(current_span, scope)
+
+                otel_send = self._get_otel_send(
+                    current_span,
+                    span_name,
+                    scope,
+                    send,
+                )
+                await self.app(scope, receive, otel_send)
+
+        finally:
+            if token:
+                context.detach(token)
+
+    def _get_otel_send(self, server_span, server_span_name, scope, send):
+        @wraps(send)
+        async def otel_send(message):
+            if message["type"] == "http.response.start":
+                status_code = message["status"]
+                set_status_code(server_span, status_code)
+            elif message["type"] == "websocket.send":
+                set_status_code(server_span, 200)
+            if (
+                server_span.is_recording()
+                and server_span.kind == trace.SpanKind.SERVER
+                and "headers" in message
+            ):
+                custom_response_attributes = collect_custom_response_headers_attributes(
+                    message
+                )
+                if len(custom_response_attributes) > 0:
+                    server_span.set_attributes(custom_response_attributes)
+
+            propagator = get_global_response_propagator()
+            if propagator:
+                propagator.inject(
+                    message,
+                    context=set_span_in_context(
+                        server_span, trace.context_api.Context()
+                    ),
+                    setter=asgi_setter,
+                )
+
+            await send(message)
+
+        return otel_send
+
+
+class CaptureTraceIdMiddleware(BaseHTTPMiddleware):
+    def capture_trace_id(self, response):
+        span = trace.get_current_span()
+        if span is None:
+            return
+        trace_id = format_trace_id(span.get_span_context().trace_id)
+        response.headers[NUCLIA_TRACE_ID_HEADER] = trace_id
+
+    def expose_trace_id_header(self, response):
+        exposed_headers = []
+        if ACCESS_CONTROL_EXPOSE_HEADER in response.headers:
+            exposed_headers = response.headers[ACCESS_CONTROL_EXPOSE_HEADER].split(",")
+        if NUCLIA_TRACE_ID_HEADER not in exposed_headers:
+            exposed_headers.append(NUCLIA_TRACE_ID_HEADER)
+            response.headers[ACCESS_CONTROL_EXPOSE_HEADER] = ",".join(exposed_headers)
+
+    async def dispatch(
+        self, request: Request, call_next: RequestResponseEndpoint
+    ) -> Response:
+        response = None
+        try:
+            response = await call_next(request)
+        finally:
+            if response is not None:
+                self.capture_trace_id(response)
+                self.expose_trace_id_header(response)
+                return response
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/utils.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/utils.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from typing import NamedTuple, Optional
-
-from starlette.applications import Starlette
-from starlette.datastructures import URL
-from starlette.routing import Match, Mount, Route
-from starlette.types import Scope
-
-
-class FoundPathTemplate(NamedTuple):
-    path: str
-    scope: Optional[Scope]
-    match: bool
-
-
-def get_path_template(scope: Scope) -> FoundPathTemplate:
-    if "found_path_template" in scope:
-        return scope["found_path_template"]
-    app: Starlette = scope["app"]
-    path, sub_scope = find_route(scope, app.routes)  # type:ignore
-    if path is None:
-        path = URL(scope=scope).path
-    path_template = FoundPathTemplate(path, sub_scope, sub_scope is not None)
-    scope["found_path_template"] = path_template
-    return path_template
-
-
-def find_route(
-    scope: Scope, routes: list[Route]
-) -> tuple[Optional[str], Optional[Scope]]:
-    # we mutate scope, so we need a copy
-    scope = scope.copy()  # type:ignore
-    for route in routes:
-        if isinstance(route, Mount):
-            mount_match, child_scope = route.matches(scope)
-            if mount_match == Match.FULL:
-                scope.update(child_scope)
-                sub_path, sub_match = find_route(scope, route.routes)
-                if sub_match and sub_path is not None:
-                    return route.path + sub_path, sub_match
-        elif isinstance(route, Route):
-            match, child_scope = route.matches(scope)
-            if match == Match.FULL:
-                return route.path, child_scope
-    return None, None
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from typing import NamedTuple, Optional
+
+from starlette.applications import Starlette
+from starlette.datastructures import URL
+from starlette.routing import Match, Mount, Route
+from starlette.types import Scope
+
+
+class FoundPathTemplate(NamedTuple):
+    path: str
+    scope: Optional[Scope]
+    match: bool
+
+
+def get_path_template(scope: Scope) -> FoundPathTemplate:
+    if "found_path_template" in scope:
+        return scope["found_path_template"]
+    app: Starlette = scope["app"]
+    path, sub_scope = find_route(scope, app.routes)  # type:ignore
+    if path is None:
+        path = URL(scope=scope).path
+    path_template = FoundPathTemplate(path, sub_scope, sub_scope is not None)
+    scope["found_path_template"] = path_template
+    return path_template
+
+
+def find_route(
+    scope: Scope, routes: list[Route]
+) -> tuple[Optional[str], Optional[Scope]]:
+    # we mutate scope, so we need a copy
+    scope = scope.copy()  # type:ignore
+    for route in routes:
+        if isinstance(route, Mount):
+            mount_match, child_scope = route.matches(scope)
+            if mount_match == Match.FULL:
+                scope.update(child_scope)
+                sub_path, sub_match = find_route(scope, route.routes)
+                if sub_match and sub_path is not None:
+                    return route.path + sub_path, sub_match
+        elif isinstance(route, Route):
+            match, child_scope = route.matches(scope)
+            if match == Match.FULL:
+                return route.path, child_scope
+    return None, None
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,411 +1,411 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import functools
-from collections import OrderedDict
-from concurrent import futures
-from contextlib import contextmanager
-from typing import Any, Awaitable, Callable, List, Optional, Tuple
-
-import grpc
-from grpc import ChannelCredentials, ClientCallDetails, aio  # type: ignore
-from grpc.experimental import wrap_server_method_handler  # type: ignore
-from opentelemetry.context import attach, detach
-from opentelemetry.propagate import extract, inject
-from opentelemetry.propagators.textmap import CarrierT, Setter  # type: ignore
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
-from opentelemetry.trace import SpanKind  # type: ignore
-from opentelemetry.trace import Tracer  # type: ignore
-from opentelemetry.trace.status import Status, StatusCode  # type: ignore
-
-from nucliadb_telemetry import grpc_metrics, logger
-from nucliadb_telemetry.common import set_span_exception
-
-
-class _CarrierSetter(Setter):
-    """We use a custom setter in order to be able to lower case
-    keys as is required by grpc.
-    """
-
-    def set(self, carrier: CarrierT, key: str, value: str):  # type: ignore
-        carrier[key.lower()] = value  # type: ignore
-
-
-_carrier_setter = _CarrierSetter()
-
-
-def finish_span_grpc(span: Span, result):
-    code = result._cython_call._status.code()
-    if code != grpc.StatusCode.OK:
-        span.set_status(
-            Status(
-                status_code=StatusCode.OK,
-            )
-        )
-    else:
-        span.set_status(
-            Status(
-                status_code=StatusCode.ERROR,
-                description=result._cython_call._status.details(),
-            )
-        )
-    span.end()
-
-
-def start_span_client(
-    tracer: Tracer,
-    client_call_details: grpc.ClientCallDetails,
-    set_status_on_exception=False,
-):
-    if isinstance(client_call_details.method, bytes):
-        service, meth = client_call_details.method.decode().lstrip("/").split("/", 1)
-        method_name = client_call_details.method.decode()
-    else:
-        service, meth = client_call_details.method.lstrip("/").split("/", 1)
-        method_name = client_call_details.method
-
-    attributes = {
-        SpanAttributes.RPC_SYSTEM: "grpc",
-        SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
-        SpanAttributes.RPC_METHOD: meth,
-        SpanAttributes.RPC_SERVICE: service,
-    }
-
-    # add some attributes from the metadata
-    if client_call_details.metadata is not None:
-        mutable_metadata = OrderedDict(tuple(client_call_details.metadata))
-        inject(mutable_metadata, setter=_carrier_setter)  # type: ignore
-        for key, value in mutable_metadata.items():
-            client_call_details.metadata.add(key=key, value=value)  # type: ignore
-
-    span = tracer.start_span(  # type: ignore
-        name=method_name,
-        kind=SpanKind.CLIENT,
-        attributes=attributes,  # type: ignore
-        set_status_on_exception=set_status_on_exception,
-    )
-    return span
-
-
-class OpenTelemetryServerInterceptor(aio.ServerInterceptor):
-    """
-    A gRPC server interceptor, to add OpenTelemetry.
-    Usage::
-        tracer = some OpenTelemetry tracer
-        interceptors = [
-            OpenTelemetryServerInterceptor(tracer),
-        ]
-        server = grpc.server(
-            futures.ThreadPoolExecutor(max_workers=concurrency),
-            interceptors = interceptors)
-    """
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    def start_span_server(
-        self,
-        handler_call_details: grpc.HandlerCallDetails,
-        context: grpc.ServicerContext,
-        set_status_on_exception=False,
-    ):
-        service, meth = handler_call_details.method.lstrip("/").split("/", 1)  # type: ignore
-
-        attributes = {
-            SpanAttributes.RPC_SYSTEM: "grpc",
-            SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
-            SpanAttributes.RPC_METHOD: meth,
-            SpanAttributes.RPC_SERVICE: service,
-        }
-
-        # add some attributes from the metadata
-        metadata = dict(context.invocation_metadata())
-        if "user-agent" in metadata:
-            attributes["rpc.user_agent"] = metadata["user-agent"]
-
-        # Split up the peer to keep with how other telemetry sources
-        # do it.  This looks like:
-        # * ipv6:[::1]:57284
-        # * ipv4:127.0.0.1:57284
-        # * ipv4:10.2.1.1:57284,127.0.0.1:57284
-        #
-        try:
-            ip, port = context.peer().split(",")[0].split(":", 1)[1].rsplit(":", 1)
-            attributes.update(
-                {SpanAttributes.NET_PEER_IP: ip, SpanAttributes.NET_PEER_PORT: port}
-            )
-
-            # other telemetry sources add this, so we will too
-            if ip in ("[::1]", "127.0.0.1"):
-                attributes[SpanAttributes.NET_PEER_NAME] = "localhost"
-
-        except IndexError:
-            logger.warning("Failed to parse peer address '%s'", context.peer())
-
-        return self.tracer.start_as_current_span(  # type: ignore
-            name=handler_call_details.method,  # type: ignore
-            kind=SpanKind.SERVER,
-            attributes=attributes,
-            set_status_on_exception=set_status_on_exception,
-        )
-
-    # Handle streaming responses separately - we have to do this
-    # to return a *new* generator or various upstream things
-    # get confused, or we'll lose the consistent trace
-    async def _intercept_server_stream(
-        self, behavior, handler_call_details, request_or_iterator, context
-    ):
-        with self._set_remote_context(context):
-            with self.start_span_server(
-                handler_call_details, context, set_status_on_exception=False
-            ) as span:
-                try:
-                    async for response in behavior(request_or_iterator, context):
-                        yield response
-
-                except Exception as error:
-                    # pylint:disable=unidiomatic-typecheck
-                    if type(error) != Exception:
-                        span.record_exception(error)
-                    raise error
-
-    @contextmanager
-    def _set_remote_context(self, servicer_context):
-        metadata = servicer_context.invocation_metadata()
-        if metadata:
-            md_dict = {md.key: md.value for md in metadata}
-            ctx = extract(md_dict)
-            token = attach(ctx)
-            try:
-                yield
-            finally:
-                detach(token)
-        else:
-            yield
-
-    async def intercept_service(
-        self,
-        continuation: Callable[
-            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
-        ],
-        handler_call_details: grpc.HandlerCallDetails,
-    ) -> grpc.RpcMethodHandler:
-        handler = await continuation(handler_call_details)
-        if handler and (
-            handler.request_streaming or handler.response_streaming
-        ):  # pytype: disable=attribute-error
-            return handler
-
-        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
-            @functools.wraps(behavior)
-            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
-                with self._set_remote_context(context):
-                    with self.start_span_server(
-                        handler_call_details,
-                        context,
-                        set_status_on_exception=False,
-                    ) as span:
-                        # And now we run the actual RPC.
-                        try:
-                            value = await behavior(request, context)
-
-                        except Exception as error:
-                            # Bare exceptions are likely to be gRPC aborts, which
-                            # we handle in our context wrapper.
-                            # Here, we're interested in uncaught exceptions.
-                            # pylint:disable=unidiomatic-typecheck
-                            if type(error) != Exception:
-                                span.record_exception(error)
-                            raise error
-                return value
-
-            return wrapper
-
-        if "grpc.health.v1.Health" in handler_call_details.method:  # type: ignore
-            return handler
-
-        return wrap_server_method_handler(wrapper, handler)
-
-
-class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_unary_unary(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-        return call
-
-
-class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_unary_stream(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-
-        try:
-            call = await continuation(client_call_details, request)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_stream_stream(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request_iterator)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_stream_unary(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request_iterator)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-def get_client_interceptors(service_name: str, tracer_provider: TracerProvider):
-    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_client")
-    return [
-        UnaryUnaryClientInterceptor(tracer),
-        UnaryStreamClientInterceptor(tracer),
-        StreamUnaryClientInterceptor(tracer),
-        StreamStreamClientInterceptor(tracer),
-    ]
-
-
-def get_server_interceptors(service_name: str, tracer_provider: TracerProvider):
-    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_server")
-    return [OpenTelemetryServerInterceptor(tracer)]
-
-
-class GRPCTelemetry:
-    initialized: bool = False
-
-    def __init__(self, service_name: str, tracer_provider: TracerProvider):
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    def init_client(
-        self,
-        server_addr: str,
-        max_send_message: int = 100,
-        credentials: Optional[ChannelCredentials] = None,
-        options: Optional[List[Tuple[str, Any]]] = None,
-    ):
-        options = [
-            ("grpc.max_receive_message_length", max_send_message * 1024 * 1024),
-            ("grpc.max_send_message_length", max_send_message * 1024 * 1024),
-        ] + (options or [])
-        interceptors = (
-            get_client_interceptors(self.service_name, self.tracer_provider)
-            + grpc_metrics.CLIENT_INTERCEPTORS
-        )
-        if credentials is not None:
-            channel = aio.secure_channel(
-                server_addr,
-                options=options,
-                credentials=credentials,
-                interceptors=interceptors,
-            )
-        else:
-            channel = aio.insecure_channel(
-                server_addr, options=options, interceptors=interceptors
-            )
-        return channel
-
-    def init_server(
-        self,
-        concurrency: int = 4,
-        max_receive_message: int = 100,
-        interceptors: Optional[List[aio.ServerInterceptor]] = None,
-        options: Optional[List[Tuple[str, Any]]] = None,
-    ):
-        _interceptors = (
-            get_server_interceptors(self.service_name, self.tracer_provider)
-            + grpc_metrics.SERVER_INTERCEPTORS
-        )
-        if interceptors is not None:
-            _interceptors.extend(interceptors)
-        options = [
-            ("grpc.max_send_message_length", max_receive_message * 1024 * 1024),
-            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024),
-        ] + (options or [])
-        server = aio.server(
-            futures.ThreadPoolExecutor(max_workers=concurrency),
-            interceptors=_interceptors,
-            options=options,
-        )
-        return server
-
-
-OpenTelemetryGRPC = GRPCTelemetry  # b/w compat import
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import functools
+from collections import OrderedDict
+from concurrent import futures
+from contextlib import contextmanager
+from typing import Any, Awaitable, Callable, List, Optional, Tuple
+
+import grpc
+from grpc import ChannelCredentials, ClientCallDetails, aio  # type: ignore
+from grpc.experimental import wrap_server_method_handler  # type: ignore
+from opentelemetry.context import attach, detach
+from opentelemetry.propagate import extract, inject
+from opentelemetry.propagators.textmap import CarrierT, Setter  # type: ignore
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
+from opentelemetry.trace import SpanKind  # type: ignore
+from opentelemetry.trace import Tracer  # type: ignore
+from opentelemetry.trace.status import Status, StatusCode  # type: ignore
+
+from nucliadb_telemetry import grpc_metrics, logger
+from nucliadb_telemetry.common import set_span_exception
+
+
+class _CarrierSetter(Setter):
+    """We use a custom setter in order to be able to lower case
+    keys as is required by grpc.
+    """
+
+    def set(self, carrier: CarrierT, key: str, value: str):  # type: ignore
+        carrier[key.lower()] = value  # type: ignore
+
+
+_carrier_setter = _CarrierSetter()
+
+
+def finish_span_grpc(span: Span, result):
+    code = result._cython_call._status.code()
+    if code != grpc.StatusCode.OK:
+        span.set_status(
+            Status(
+                status_code=StatusCode.OK,
+            )
+        )
+    else:
+        span.set_status(
+            Status(
+                status_code=StatusCode.ERROR,
+                description=result._cython_call._status.details(),
+            )
+        )
+    span.end()
+
+
+def start_span_client(
+    tracer: Tracer,
+    client_call_details: grpc.ClientCallDetails,
+    set_status_on_exception=False,
+):
+    if isinstance(client_call_details.method, bytes):
+        service, meth = client_call_details.method.decode().lstrip("/").split("/", 1)
+        method_name = client_call_details.method.decode()
+    else:
+        service, meth = client_call_details.method.lstrip("/").split("/", 1)
+        method_name = client_call_details.method
+
+    attributes = {
+        SpanAttributes.RPC_SYSTEM: "grpc",
+        SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
+        SpanAttributes.RPC_METHOD: meth,
+        SpanAttributes.RPC_SERVICE: service,
+    }
+
+    # add some attributes from the metadata
+    if client_call_details.metadata is not None:
+        mutable_metadata = OrderedDict(tuple(client_call_details.metadata))
+        inject(mutable_metadata, setter=_carrier_setter)  # type: ignore
+        for key, value in mutable_metadata.items():
+            client_call_details.metadata.add(key=key, value=value)  # type: ignore
+
+    span = tracer.start_span(  # type: ignore
+        name=method_name,
+        kind=SpanKind.CLIENT,
+        attributes=attributes,  # type: ignore
+        set_status_on_exception=set_status_on_exception,
+    )
+    return span
+
+
+class OpenTelemetryServerInterceptor(aio.ServerInterceptor):
+    """
+    A gRPC server interceptor, to add OpenTelemetry.
+    Usage::
+        tracer = some OpenTelemetry tracer
+        interceptors = [
+            OpenTelemetryServerInterceptor(tracer),
+        ]
+        server = grpc.server(
+            futures.ThreadPoolExecutor(max_workers=concurrency),
+            interceptors = interceptors)
+    """
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    def start_span_server(
+        self,
+        handler_call_details: grpc.HandlerCallDetails,
+        context: grpc.ServicerContext,
+        set_status_on_exception=False,
+    ):
+        service, meth = handler_call_details.method.lstrip("/").split("/", 1)  # type: ignore
+
+        attributes = {
+            SpanAttributes.RPC_SYSTEM: "grpc",
+            SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
+            SpanAttributes.RPC_METHOD: meth,
+            SpanAttributes.RPC_SERVICE: service,
+        }
+
+        # add some attributes from the metadata
+        metadata = dict(context.invocation_metadata())
+        if "user-agent" in metadata:
+            attributes["rpc.user_agent"] = metadata["user-agent"]
+
+        # Split up the peer to keep with how other telemetry sources
+        # do it.  This looks like:
+        # * ipv6:[::1]:57284
+        # * ipv4:127.0.0.1:57284
+        # * ipv4:10.2.1.1:57284,127.0.0.1:57284
+        #
+        try:
+            ip, port = context.peer().split(",")[0].split(":", 1)[1].rsplit(":", 1)
+            attributes.update(
+                {SpanAttributes.NET_PEER_IP: ip, SpanAttributes.NET_PEER_PORT: port}
+            )
+
+            # other telemetry sources add this, so we will too
+            if ip in ("[::1]", "127.0.0.1"):
+                attributes[SpanAttributes.NET_PEER_NAME] = "localhost"
+
+        except IndexError:
+            logger.warning("Failed to parse peer address '%s'", context.peer())
+
+        return self.tracer.start_as_current_span(  # type: ignore
+            name=handler_call_details.method,  # type: ignore
+            kind=SpanKind.SERVER,
+            attributes=attributes,
+            set_status_on_exception=set_status_on_exception,
+        )
+
+    # Handle streaming responses separately - we have to do this
+    # to return a *new* generator or various upstream things
+    # get confused, or we'll lose the consistent trace
+    async def _intercept_server_stream(
+        self, behavior, handler_call_details, request_or_iterator, context
+    ):
+        with self._set_remote_context(context):
+            with self.start_span_server(
+                handler_call_details, context, set_status_on_exception=False
+            ) as span:
+                try:
+                    async for response in behavior(request_or_iterator, context):
+                        yield response
+
+                except Exception as error:
+                    # pylint:disable=unidiomatic-typecheck
+                    if type(error) != Exception:
+                        span.record_exception(error)
+                    raise error
+
+    @contextmanager
+    def _set_remote_context(self, servicer_context):
+        metadata = servicer_context.invocation_metadata()
+        if metadata:
+            md_dict = {md.key: md.value for md in metadata}
+            ctx = extract(md_dict)
+            token = attach(ctx)
+            try:
+                yield
+            finally:
+                detach(token)
+        else:
+            yield
+
+    async def intercept_service(
+        self,
+        continuation: Callable[
+            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
+        ],
+        handler_call_details: grpc.HandlerCallDetails,
+    ) -> grpc.RpcMethodHandler:
+        handler = await continuation(handler_call_details)
+        if handler and (
+            handler.request_streaming or handler.response_streaming
+        ):  # pytype: disable=attribute-error
+            return handler
+
+        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
+            @functools.wraps(behavior)
+            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
+                with self._set_remote_context(context):
+                    with self.start_span_server(
+                        handler_call_details,
+                        context,
+                        set_status_on_exception=False,
+                    ) as span:
+                        # And now we run the actual RPC.
+                        try:
+                            value = await behavior(request, context)
+
+                        except Exception as error:
+                            # Bare exceptions are likely to be gRPC aborts, which
+                            # we handle in our context wrapper.
+                            # Here, we're interested in uncaught exceptions.
+                            # pylint:disable=unidiomatic-typecheck
+                            if type(error) != Exception:
+                                span.record_exception(error)
+                            raise error
+                return value
+
+            return wrapper
+
+        if "grpc.health.v1.Health" in handler_call_details.method:  # type: ignore
+            return handler
+
+        return wrap_server_method_handler(wrapper, handler)
+
+
+class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_unary_unary(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+        return call
+
+
+class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_unary_stream(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+
+        try:
+            call = await continuation(client_call_details, request)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_stream_stream(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request_iterator)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_stream_unary(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request_iterator)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+def get_client_interceptors(service_name: str, tracer_provider: TracerProvider):
+    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_client")
+    return [
+        UnaryUnaryClientInterceptor(tracer),
+        UnaryStreamClientInterceptor(tracer),
+        StreamUnaryClientInterceptor(tracer),
+        StreamStreamClientInterceptor(tracer),
+    ]
+
+
+def get_server_interceptors(service_name: str, tracer_provider: TracerProvider):
+    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_server")
+    return [OpenTelemetryServerInterceptor(tracer)]
+
+
+class GRPCTelemetry:
+    initialized: bool = False
+
+    def __init__(self, service_name: str, tracer_provider: TracerProvider):
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    def init_client(
+        self,
+        server_addr: str,
+        max_send_message: int = 100,
+        credentials: Optional[ChannelCredentials] = None,
+        options: Optional[List[Tuple[str, Any]]] = None,
+    ):
+        options = [
+            ("grpc.max_receive_message_length", max_send_message * 1024 * 1024),
+            ("grpc.max_send_message_length", max_send_message * 1024 * 1024),
+        ] + (options or [])
+        interceptors = (
+            get_client_interceptors(self.service_name, self.tracer_provider)
+            + grpc_metrics.CLIENT_INTERCEPTORS
+        )
+        if credentials is not None:
+            channel = aio.secure_channel(
+                server_addr,
+                options=options,
+                credentials=credentials,
+                interceptors=interceptors,
+            )
+        else:
+            channel = aio.insecure_channel(
+                server_addr, options=options, interceptors=interceptors
+            )
+        return channel
+
+    def init_server(
+        self,
+        concurrency: int = 4,
+        max_receive_message: int = 100,
+        interceptors: Optional[List[aio.ServerInterceptor]] = None,
+        options: Optional[List[Tuple[str, Any]]] = None,
+    ):
+        _interceptors = (
+            get_server_interceptors(self.service_name, self.tracer_provider)
+            + grpc_metrics.SERVER_INTERCEPTORS
+        )
+        if interceptors is not None:
+            _interceptors.extend(interceptors)
+        options = [
+            ("grpc.max_send_message_length", max_receive_message * 1024 * 1024),
+            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024),
+        ] + (options or [])
+        server = aio.server(
+            futures.ThreadPoolExecutor(max_workers=concurrency),
+            interceptors=_interceptors,
+            options=options,
+        )
+        return server
+
+
+OpenTelemetryGRPC = GRPCTelemetry  # b/w compat import
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc_metrics.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc_metrics.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,166 +1,166 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import functools
-from typing import Any, Awaitable, Callable, Union
-
-import grpc
-from grpc import ClientCallDetails, aio  # type: ignore
-from grpc.experimental import wrap_server_method_handler  # type: ignore
-
-from nucliadb_telemetry import metrics
-
-histo_buckets = [
-    0.005,
-    0.01,
-    0.025,
-    0.05,
-    0.075,
-    0.1,
-    0.25,
-    0.5,
-    0.75,
-    1.0,
-    2.5,
-    5.0,
-    10.0,
-    30.0,
-    60.0,
-    metrics.INF,
-]
-grpc_client_observer = metrics.Observer(
-    "grpc_client_op", labels={"method": ""}, buckets=histo_buckets
-)
-grpc_server_observer = metrics.Observer(
-    "grpc_server_op", labels={"method": ""}, buckets=histo_buckets
-)
-
-
-class MetricsServerInterceptor(aio.ServerInterceptor):
-    """
-    A GRPC server interceptor that adds metrics to the server.
-    """
-
-    async def intercept_service(
-        self,
-        continuation: Callable[
-            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
-        ],
-        handler_call_details: grpc.HandlerCallDetails,
-    ) -> grpc.RpcMethodHandler:
-        handler = await continuation(handler_call_details)
-        if handler and (
-            handler.request_streaming or handler.response_streaming
-        ):  # pytype: disable=attribute-error
-            return handler
-
-        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
-            @functools.wraps(behavior)
-            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
-                with grpc_server_observer(
-                    labels={"method": handler_call_details.method}  # type: ignore
-                ) as observer:
-                    value = await behavior(request, context)
-                    observer.set_status(str(context.code()))
-                return value
-
-            return wrapper
-
-        return wrap_server_method_handler(wrapper, handler)
-
-
-def finish_metric_grpc(metric: metrics.ObserverRecorder, result):
-    code = result._cython_call._status.code()
-    metric.set_status(str(code))
-    metric.end()
-
-
-def _to_str(v: Union[str, bytes]) -> str:
-    if isinstance(v, str):
-        return v
-    return v.decode("utf-8")
-
-
-class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
-    async def intercept_unary_unary(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        metric = grpc_client_observer(
-            labels={"method": _to_str(client_call_details.method)}
-        )
-        metric.start()
-
-        call = await continuation(client_call_details, request)
-        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
-
-        return call
-
-
-class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
-    async def intercept_unary_stream(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        metric = grpc_client_observer(
-            labels={"method": _to_str(client_call_details.method)}
-        )
-        metric.start()
-
-        call = await continuation(client_call_details, request)
-        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
-
-        return call
-
-
-class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
-    async def intercept_stream_stream(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        metric = grpc_client_observer(
-            labels={"method": _to_str(client_call_details.method)}
-        )
-        metric.start()
-
-        call = await continuation(client_call_details, request_iterator)
-        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
-
-        return call
-
-
-class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
-    async def intercept_stream_unary(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        metric = grpc_client_observer(
-            labels={"method": _to_str(client_call_details.method)}
-        )
-        metric.start()
-
-        call = await continuation(client_call_details, request_iterator)
-        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
-
-        return call
-
-
-CLIENT_INTERCEPTORS = [
-    UnaryUnaryClientInterceptor(),
-    UnaryStreamClientInterceptor(),
-    StreamStreamClientInterceptor(),
-    StreamUnaryClientInterceptor(),
-]
-SERVER_INTERCEPTORS = [MetricsServerInterceptor()]
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import functools
+from typing import Any, Awaitable, Callable, Union
+
+import grpc
+from grpc import ClientCallDetails, aio  # type: ignore
+from grpc.experimental import wrap_server_method_handler  # type: ignore
+
+from nucliadb_telemetry import metrics
+
+histo_buckets = [
+    0.005,
+    0.01,
+    0.025,
+    0.05,
+    0.075,
+    0.1,
+    0.25,
+    0.5,
+    0.75,
+    1.0,
+    2.5,
+    5.0,
+    10.0,
+    30.0,
+    60.0,
+    metrics.INF,
+]
+grpc_client_observer = metrics.Observer(
+    "grpc_client_op", labels={"method": ""}, buckets=histo_buckets
+)
+grpc_server_observer = metrics.Observer(
+    "grpc_server_op", labels={"method": ""}, buckets=histo_buckets
+)
+
+
+class MetricsServerInterceptor(aio.ServerInterceptor):
+    """
+    A GRPC server interceptor that adds metrics to the server.
+    """
+
+    async def intercept_service(
+        self,
+        continuation: Callable[
+            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
+        ],
+        handler_call_details: grpc.HandlerCallDetails,
+    ) -> grpc.RpcMethodHandler:
+        handler = await continuation(handler_call_details)
+        if handler and (
+            handler.request_streaming or handler.response_streaming
+        ):  # pytype: disable=attribute-error
+            return handler
+
+        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
+            @functools.wraps(behavior)
+            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
+                with grpc_server_observer(
+                    labels={"method": handler_call_details.method}  # type: ignore
+                ) as observer:
+                    value = await behavior(request, context)
+                    observer.set_status(str(context.code()))
+                return value
+
+            return wrapper
+
+        return wrap_server_method_handler(wrapper, handler)
+
+
+def finish_metric_grpc(metric: metrics.ObserverRecorder, result):
+    code = result._cython_call._status.code()
+    metric.set_status(str(code))
+    metric.end()
+
+
+def _to_str(v: Union[str, bytes]) -> str:
+    if isinstance(v, str):
+        return v
+    return v.decode("utf-8")
+
+
+class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
+    async def intercept_unary_unary(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        metric = grpc_client_observer(
+            labels={"method": _to_str(client_call_details.method)}
+        )
+        metric.start()
+
+        call = await continuation(client_call_details, request)
+        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
+
+        return call
+
+
+class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
+    async def intercept_unary_stream(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        metric = grpc_client_observer(
+            labels={"method": _to_str(client_call_details.method)}
+        )
+        metric.start()
+
+        call = await continuation(client_call_details, request)
+        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
+
+        return call
+
+
+class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
+    async def intercept_stream_stream(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        metric = grpc_client_observer(
+            labels={"method": _to_str(client_call_details.method)}
+        )
+        metric.start()
+
+        call = await continuation(client_call_details, request_iterator)
+        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
+
+        return call
+
+
+class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
+    async def intercept_stream_unary(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        metric = grpc_client_observer(
+            labels={"method": _to_str(client_call_details.method)}
+        )
+        metric.start()
+
+        call = await continuation(client_call_details, request_iterator)
+        call.add_done_callback(functools.partial(finish_metric_grpc, metric))
+
+        return call
+
+
+CLIENT_INTERCEPTORS = [
+    UnaryUnaryClientInterceptor(),
+    UnaryStreamClientInterceptor(),
+    StreamStreamClientInterceptor(),
+    StreamUnaryClientInterceptor(),
+]
+SERVER_INTERCEPTORS = [MetricsServerInterceptor()]
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,189 +1,189 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import math
-import socket
-from asyncio import Future
-from functools import partial
-from typing import List
-
-from opentelemetry.exporter.jaeger.thrift import JaegerExporter  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.gen.agent import Agent  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.gen.jaeger import Collector  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.translate import Translate  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.translate import (  # type: ignore
-    ThriftTranslator,
-)
-from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.sdk.trace.export import SpanExportResult  # type: ignore
-from thrift.protocol import TCompactProtocol  # type: ignore
-from thrift.transport import TTransport  # type: ignore
-
-from nucliadb_telemetry import logger
-
-UDP_PACKET_MAX_LENGTH = 65000
-
-
-class JaegerExporterAsync(JaegerExporter):
-    def __init__(self, **kwags):
-        super(JaegerExporterAsync, self).__init__(**kwags)
-        self._agent_client = AgentClientUDPAsync(
-            host_name=self.agent_host_name,
-            port=self.agent_port,
-            split_oversized_batches=self.udp_split_oversized_batches,
-        )
-
-    async def async_export(self, spans: List[Span]) -> SpanExportResult:
-        # Populate service_name from first span
-        # We restrict any SpanProcessor to be only associated with a single
-        # TracerProvider, so it is safe to assume that all Spans in a single
-        # batch all originate from one TracerProvider (and in turn have all
-        # the same service.name)
-        if len(spans) == 0:
-            return SpanExportResult.SUCCESS
-        if spans:
-            service_name = spans[0].resource.attributes.get(SERVICE_NAME)
-            if service_name:
-                self.service_name = service_name
-        translator = Translate(spans)
-        thrift_translator = ThriftTranslator(self._max_tag_value_length)
-        jaeger_spans = translator._translate(thrift_translator)
-        batch = Collector.Batch(
-            spans=jaeger_spans,
-            process=Collector.Process(serviceName=self.service_name),
-        )
-        if self._collector_http_client is not None:
-            raise Exception("Not supported on asyncio")
-            # self._collector_http_client.submit(batch)
-        else:
-            await self._agent_client.emit(batch)
-
-        return SpanExportResult.SUCCESS
-
-
-class JaegerClientProtocol:
-    def __init__(self, message: bytes, on_con_lost: Future):
-        self.message = message
-        self.on_con_lost = on_con_lost
-        self.transport = None
-
-    def error_received(self, exc):
-        logger.exception("Error received from Jaeger", exc_info=exc)
-        if not self.on_con_lost.done():
-            self.on_con_lost.set_result(False)
-
-    def connection_lost(self, exc):
-        logger.exception("Connection lost with Jaeger", exc_info=exc)
-        if not self.on_con_lost.done():
-            self.on_con_lost.set_result(True)
-
-    def connection_made(self, transport):
-        self.transport = transport
-        self.transport.sendto(self.message)
-
-
-class AgentClientUDPAsync:
-    """Implement a UDP client to agent.
-
-    Args:
-        host_name: The host name of the Jaeger server.
-        port: The port of the Jaeger server.
-        max_packet_size: Maximum size of UDP packet.
-        client: Class for creating new client objects for agencies.
-        split_oversized_batches: Re-emit oversized batches in smaller chunks.
-    """
-
-    def __init__(
-        self,
-        host_name,
-        port,
-        max_packet_size=UDP_PACKET_MAX_LENGTH,
-        client=Agent.Client,
-        split_oversized_batches=False,
-    ):
-        self.host_name = host_name
-        self.port = port
-        self.max_packet_size = max_packet_size
-        self.buffer = TTransport.TMemoryBuffer()
-        self.client = client(iprot=TCompactProtocol.TCompactProtocol(trans=self.buffer))
-        self.split_oversized_batches = split_oversized_batches
-        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        self._sock.setblocking(False)
-        self._addr = (host_name, int(port))
-
-    async def emit(self, batch: Collector.Batch):
-        """
-        Args:
-            batch: Object to emit Jaeger spans.
-        """
-
-        # pylint: disable=protected-access
-        self.client._seqid = 0
-        #  truncate and reset the position of BytesIO object
-        self.buffer._buffer.truncate(0)
-        self.buffer._buffer.seek(0)
-        self.client.emitBatch(batch)
-        buff = self.buffer.getvalue()
-        if len(buff) > self.max_packet_size:
-            if self.split_oversized_batches and len(batch.spans) > 1:
-                packets = math.ceil(len(buff) / self.max_packet_size)
-                div = math.ceil(len(batch.spans) / packets)
-                for packet in range(packets):
-                    start = packet * div
-                    end = (packet + 1) * div
-                    if start < len(batch.spans):
-                        await self.emit(
-                            Collector.Batch(
-                                process=batch.process,
-                                spans=batch.spans[start:end],
-                            )
-                        )
-            else:
-                logger.warning(
-                    "Data exceeds the max UDP packet size; size %r, max %r",
-                    len(buff),
-                    self.max_packet_size,
-                )
-            return
-
-        loop = asyncio.get_running_loop()
-        on_con_lost = loop.create_future()
-
-        send_to = partial(self._sendto, buff, on_con_lost)
-        loop.add_writer(self._sock.fileno(), send_to)
-        try:
-            await on_con_lost
-        except Exception:
-            logger.exception("Exception on sending to jaeger", stack_info=True)
-        finally:
-            loop.remove_writer(self._sock.fileno())
-
-    def _sendto(self, buff, on_con_lost: asyncio.Future):
-        try:
-            self._sock.sendto(buff, self._addr)
-        except (BlockingIOError, InterruptedError):
-            return
-        except OSError as exc:
-            on_con_lost.set_exception(exc)
-        except Exception as exc:
-            on_con_lost.set_exception(exc)
-        else:
-            on_con_lost.set_result(True)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import math
+import socket
+from asyncio import Future
+from functools import partial
+from typing import List
+
+from opentelemetry.exporter.jaeger.thrift import JaegerExporter  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.gen.agent import Agent  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.gen.jaeger import Collector  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.translate import Translate  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.translate import (  # type: ignore
+    ThriftTranslator,
+)
+from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.sdk.trace.export import SpanExportResult  # type: ignore
+from thrift.protocol import TCompactProtocol  # type: ignore
+from thrift.transport import TTransport  # type: ignore
+
+from nucliadb_telemetry import logger
+
+UDP_PACKET_MAX_LENGTH = 65000
+
+
+class JaegerExporterAsync(JaegerExporter):
+    def __init__(self, **kwags):
+        super(JaegerExporterAsync, self).__init__(**kwags)
+        self._agent_client = AgentClientUDPAsync(
+            host_name=self.agent_host_name,
+            port=self.agent_port,
+            split_oversized_batches=self.udp_split_oversized_batches,
+        )
+
+    async def async_export(self, spans: List[Span]) -> SpanExportResult:
+        # Populate service_name from first span
+        # We restrict any SpanProcessor to be only associated with a single
+        # TracerProvider, so it is safe to assume that all Spans in a single
+        # batch all originate from one TracerProvider (and in turn have all
+        # the same service.name)
+        if len(spans) == 0:
+            return SpanExportResult.SUCCESS
+        if spans:
+            service_name = spans[0].resource.attributes.get(SERVICE_NAME)
+            if service_name:
+                self.service_name = service_name
+        translator = Translate(spans)
+        thrift_translator = ThriftTranslator(self._max_tag_value_length)
+        jaeger_spans = translator._translate(thrift_translator)
+        batch = Collector.Batch(
+            spans=jaeger_spans,
+            process=Collector.Process(serviceName=self.service_name),
+        )
+        if self._collector_http_client is not None:
+            raise Exception("Not supported on asyncio")
+            # self._collector_http_client.submit(batch)
+        else:
+            await self._agent_client.emit(batch)
+
+        return SpanExportResult.SUCCESS
+
+
+class JaegerClientProtocol:
+    def __init__(self, message: bytes, on_con_lost: Future):
+        self.message = message
+        self.on_con_lost = on_con_lost
+        self.transport = None
+
+    def error_received(self, exc):
+        logger.exception("Error received from Jaeger", exc_info=exc)
+        if not self.on_con_lost.done():
+            self.on_con_lost.set_result(False)
+
+    def connection_lost(self, exc):
+        logger.exception("Connection lost with Jaeger", exc_info=exc)
+        if not self.on_con_lost.done():
+            self.on_con_lost.set_result(True)
+
+    def connection_made(self, transport):
+        self.transport = transport
+        self.transport.sendto(self.message)
+
+
+class AgentClientUDPAsync:
+    """Implement a UDP client to agent.
+
+    Args:
+        host_name: The host name of the Jaeger server.
+        port: The port of the Jaeger server.
+        max_packet_size: Maximum size of UDP packet.
+        client: Class for creating new client objects for agencies.
+        split_oversized_batches: Re-emit oversized batches in smaller chunks.
+    """
+
+    def __init__(
+        self,
+        host_name,
+        port,
+        max_packet_size=UDP_PACKET_MAX_LENGTH,
+        client=Agent.Client,
+        split_oversized_batches=False,
+    ):
+        self.host_name = host_name
+        self.port = port
+        self.max_packet_size = max_packet_size
+        self.buffer = TTransport.TMemoryBuffer()
+        self.client = client(iprot=TCompactProtocol.TCompactProtocol(trans=self.buffer))
+        self.split_oversized_batches = split_oversized_batches
+        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        self._sock.setblocking(False)
+        self._addr = (host_name, int(port))
+
+    async def emit(self, batch: Collector.Batch):
+        """
+        Args:
+            batch: Object to emit Jaeger spans.
+        """
+
+        # pylint: disable=protected-access
+        self.client._seqid = 0
+        #  truncate and reset the position of BytesIO object
+        self.buffer._buffer.truncate(0)
+        self.buffer._buffer.seek(0)
+        self.client.emitBatch(batch)
+        buff = self.buffer.getvalue()
+        if len(buff) > self.max_packet_size:
+            if self.split_oversized_batches and len(batch.spans) > 1:
+                packets = math.ceil(len(buff) / self.max_packet_size)
+                div = math.ceil(len(batch.spans) / packets)
+                for packet in range(packets):
+                    start = packet * div
+                    end = (packet + 1) * div
+                    if start < len(batch.spans):
+                        await self.emit(
+                            Collector.Batch(
+                                process=batch.process,
+                                spans=batch.spans[start:end],
+                            )
+                        )
+            else:
+                logger.warning(
+                    "Data exceeds the max UDP packet size; size %r, max %r",
+                    len(buff),
+                    self.max_packet_size,
+                )
+            return
+
+        loop = asyncio.get_running_loop()
+        on_con_lost = loop.create_future()
+
+        send_to = partial(self._sendto, buff, on_con_lost)
+        loop.add_writer(self._sock.fileno(), send_to)
+        try:
+            await on_con_lost
+        except Exception:
+            logger.exception("Exception on sending to jaeger", stack_info=True)
+        finally:
+            loop.remove_writer(self._sock.fileno())
+
+    def _sendto(self, buff, on_con_lost: asyncio.Future):
+        try:
+            self._sock.sendto(buff, self._addr)
+        except (BlockingIOError, InterruptedError):
+            return
+        except OSError as exc:
+            on_con_lost.set_exception(exc)
+        except Exception as exc:
+            on_con_lost.set_exception(exc)
+        else:
+            on_con_lost.set_result(True)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,272 +1,272 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from datetime import datetime
-from functools import partial
-from typing import Any, Callable, Dict, List, Optional
-
-from nats.aio.client import Client
-from nats.aio.msg import Msg
-from nats.js.client import JetStreamContext
-from opentelemetry.context import attach
-from opentelemetry.propagate import extract, inject
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
-from opentelemetry.trace import SpanKind  # type: ignore
-from opentelemetry.trace import Tracer  # type: ignore
-
-from nucliadb_telemetry import logger, metrics
-from nucliadb_telemetry.common import set_span_exception
-
-msg_time_histo = metrics.Histogram(
-    # time it takes from when msg was queue to when it finished processing
-    "nuclia_nats_msg_op_time",
-    labels={
-        "stream": "",
-        "consumer": "",
-        "acked": "no",
-    },
-    buckets=[
-        0.005,
-        0.025,
-        0.05,
-        0.1,
-        0.5,
-        1.0,
-        5.0,
-        10.0,
-        30.0,
-        60.0,
-        120.0,
-        600.0,
-        metrics.INF,
-    ],
-)
-
-
-def start_span_message_receiver(tracer: Tracer, msg: Msg):
-    attributes = {
-        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
-        SpanAttributes.MESSAGING_MESSAGE_PAYLOAD_SIZE_BYTES: len(msg.data),
-        SpanAttributes.MESSAGING_MESSAGE_ID: msg.reply,
-    }
-
-    # add some attributes from the metadata
-    ctx = extract(msg.headers)
-    token = attach(ctx)
-
-    span = tracer.start_as_current_span(  # type: ignore
-        name=f"Received from {msg.subject}",
-        kind=SpanKind.SERVER,
-        attributes=attributes,
-    )
-    span._token = token
-    return span
-
-
-def start_span_message_publisher(tracer: Tracer, subject: str):
-    attributes = {
-        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
-        SpanAttributes.MESSAGING_DESTINATION: subject,
-    }
-
-    span = tracer.start_as_current_span(  # type: ignore
-        name=f"Published on {subject}",
-        kind=SpanKind.CLIENT,
-        attributes=attributes,
-    )
-    return span
-
-
-class JetStreamContextTelemetry:
-    def __init__(
-        self, js: JetStreamContext, service_name: str, tracer_provider: TracerProvider
-    ):
-        self.js = js
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    async def stream_info(self, name: str):
-        return await self.js.stream_info(name)
-
-    async def add_stream(self, name: str, subjects: List[str]):
-        return await self.js.add_stream(name=name, subjects=subjects)
-
-    async def subscribe(self, cb, **kwargs):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_subscriber")
-
-        async def wrapper(origin_cb, tracer, msg: Msg):
-            # Execute the callback without tracing
-            if msg.headers is None:
-                logger.warning("Message received without headers, skipping span")
-                await origin_cb(msg)
-                return
-
-            with start_span_message_receiver(tracer, msg) as span:
-                try:
-                    await origin_cb(msg)
-                except Exception as error:
-                    set_span_exception(span, error)
-                    raise error
-                finally:
-                    msg_time_histo.observe(
-                        (datetime.now() - msg.metadata.timestamp).total_seconds(),
-                        {
-                            "stream": msg.metadata.stream,
-                            "consumer": msg.metadata.consumer or "",
-                            "acked": "yes" if msg._ackd else "no",  # type: ignore
-                        },
-                    )
-
-        wrapped_cb = partial(wrapper, cb, tracer)
-        return await self.js.subscribe(cb=wrapped_cb, **kwargs)
-
-    async def publish(
-        self,
-        subject: str,
-        body: bytes,
-        headers: Optional[Dict[str, str]] = None,
-        **kwargs,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_publisher")
-        headers = {} if headers is None else headers
-        inject(headers)
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.js.publish(subject, body, headers=headers, **kwargs)
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-
-        return result
-
-    # Just for convenience, to wrap all we use in the context of
-    # telemetry-instrumented stuff using the JetStreamContextTelemetry class
-
-    async def pull_subscribe(
-        self, *args, **kwargs
-    ) -> JetStreamContext.PullSubscription:
-        return await self.js.pull_subscribe(*args, **kwargs)
-
-    async def pull_subscribe_bind(
-        self, *args, **kwargs
-    ) -> JetStreamContext.PullSubscription:
-        return await self.js.pull_subscribe_bind(*args, **kwargs)
-
-    async def pull_one(
-        self,
-        subscription: JetStreamContext.PullSubscription,
-        cb: Callable[[Msg], Any],
-        timeout: int = 5,
-    ) -> Msg:
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_pull_one")
-        messages = await subscription.fetch(1, timeout=timeout)
-
-        # If there is no message, fetch will raise a timeout
-        message = messages[0]
-
-        # Execute the callback without tracing
-        if message.headers is None:
-            logger.warning("Message received without headers, skipping span")
-            return await cb(message)
-
-        with start_span_message_receiver(tracer, message) as span:
-            try:
-                return await cb(message)
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-            finally:
-                msg_time_histo.observe(
-                    (datetime.now() - message.metadata.timestamp).total_seconds(),
-                    {
-                        "stream": message.metadata.stream,
-                        "consumer": message.metadata.consumer or "",
-                        "acked": "yes" if message._ackd else "no",
-                    },
-                )
-
-
-class NatsClientTelemetry:
-    def __init__(self, nc: Client, service_name: str, tracer_provider: TracerProvider):
-        self.nc = nc
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    async def subscribe(self, cb, **kwargs):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_subscriber")
-
-        async def wrapper(origin_cb, tracer, msg: Msg):
-            # Execute the callback without tracing
-            if msg.headers is None:
-                logger.warning("Message received without headers, skipping span")
-                await origin_cb(msg)
-                return
-
-            with start_span_message_receiver(tracer, msg) as span:
-                try:
-                    await origin_cb(msg)
-                except Exception as error:
-                    set_span_exception(span, error)
-                    raise error
-
-        wrapped_cb = partial(wrapper, cb, tracer)
-        return await self.nc.subscribe(cb=wrapped_cb, **kwargs)
-
-    async def publish(
-        self,
-        subject: str,
-        body: bytes,
-        headers: Optional[Dict[str, str]] = None,
-        **kwargs,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_publisher")
-        headers = {} if headers is None else headers
-        inject(headers)
-
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.nc.publish(subject, body, headers=headers, **kwargs)
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-
-        return result
-
-    async def request(
-        self,
-        subject: str,
-        payload: bytes = b"",
-        timeout: float = 0.5,
-        old_style: bool = False,
-        headers: Optional[Dict[str, Any]] = None,
-    ) -> Msg:
-        headers = {} if headers is None else headers
-        inject(headers)
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_request")
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.nc.request(
-                    subject, payload, timeout, old_style, headers  # type: ignore
-                )
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-
-        return result
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from datetime import datetime
+from functools import partial
+from typing import Any, Callable, Dict, List, Optional
+
+from nats.aio.client import Client
+from nats.aio.msg import Msg
+from nats.js.client import JetStreamContext
+from opentelemetry.context import attach
+from opentelemetry.propagate import extract, inject
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
+from opentelemetry.trace import SpanKind  # type: ignore
+from opentelemetry.trace import Tracer  # type: ignore
+
+from nucliadb_telemetry import logger, metrics
+from nucliadb_telemetry.common import set_span_exception
+
+msg_time_histo = metrics.Histogram(
+    # time it takes from when msg was queue to when it finished processing
+    "nuclia_nats_msg_op_time",
+    labels={
+        "stream": "",
+        "consumer": "",
+        "acked": "no",
+    },
+    buckets=[
+        0.005,
+        0.025,
+        0.05,
+        0.1,
+        0.5,
+        1.0,
+        5.0,
+        10.0,
+        30.0,
+        60.0,
+        120.0,
+        600.0,
+        metrics.INF,
+    ],
+)
+
+
+def start_span_message_receiver(tracer: Tracer, msg: Msg):
+    attributes = {
+        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
+        SpanAttributes.MESSAGING_MESSAGE_PAYLOAD_SIZE_BYTES: len(msg.data),
+        SpanAttributes.MESSAGING_MESSAGE_ID: msg.reply,
+    }
+
+    # add some attributes from the metadata
+    ctx = extract(msg.headers)
+    token = attach(ctx)
+
+    span = tracer.start_as_current_span(  # type: ignore
+        name=f"Received from {msg.subject}",
+        kind=SpanKind.SERVER,
+        attributes=attributes,
+    )
+    span._token = token
+    return span
+
+
+def start_span_message_publisher(tracer: Tracer, subject: str):
+    attributes = {
+        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
+        SpanAttributes.MESSAGING_DESTINATION: subject,
+    }
+
+    span = tracer.start_as_current_span(  # type: ignore
+        name=f"Published on {subject}",
+        kind=SpanKind.CLIENT,
+        attributes=attributes,
+    )
+    return span
+
+
+class JetStreamContextTelemetry:
+    def __init__(
+        self, js: JetStreamContext, service_name: str, tracer_provider: TracerProvider
+    ):
+        self.js = js
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    async def stream_info(self, name: str):
+        return await self.js.stream_info(name)
+
+    async def add_stream(self, name: str, subjects: List[str]):
+        return await self.js.add_stream(name=name, subjects=subjects)
+
+    async def subscribe(self, cb, **kwargs):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_subscriber")
+
+        async def wrapper(origin_cb, tracer, msg: Msg):
+            # Execute the callback without tracing
+            if msg.headers is None:
+                logger.warning("Message received without headers, skipping span")
+                await origin_cb(msg)
+                return
+
+            with start_span_message_receiver(tracer, msg) as span:
+                try:
+                    await origin_cb(msg)
+                except Exception as error:
+                    set_span_exception(span, error)
+                    raise error
+                finally:
+                    msg_time_histo.observe(
+                        (datetime.now() - msg.metadata.timestamp).total_seconds(),
+                        {
+                            "stream": msg.metadata.stream,
+                            "consumer": msg.metadata.consumer or "",
+                            "acked": "yes" if msg._ackd else "no",  # type: ignore
+                        },
+                    )
+
+        wrapped_cb = partial(wrapper, cb, tracer)
+        return await self.js.subscribe(cb=wrapped_cb, **kwargs)
+
+    async def publish(
+        self,
+        subject: str,
+        body: bytes,
+        headers: Optional[Dict[str, str]] = None,
+        **kwargs,
+    ):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_publisher")
+        headers = {} if headers is None else headers
+        inject(headers)
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.js.publish(subject, body, headers=headers, **kwargs)
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+
+        return result
+
+    # Just for convenience, to wrap all we use in the context of
+    # telemetry-instrumented stuff using the JetStreamContextTelemetry class
+
+    async def pull_subscribe(
+        self, *args, **kwargs
+    ) -> JetStreamContext.PullSubscription:
+        return await self.js.pull_subscribe(*args, **kwargs)
+
+    async def pull_subscribe_bind(
+        self, *args, **kwargs
+    ) -> JetStreamContext.PullSubscription:
+        return await self.js.pull_subscribe_bind(*args, **kwargs)
+
+    async def pull_one(
+        self,
+        subscription: JetStreamContext.PullSubscription,
+        cb: Callable[[Msg], Any],
+        timeout: int = 5,
+    ) -> Msg:
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_pull_one")
+        messages = await subscription.fetch(1, timeout=timeout)
+
+        # If there is no message, fetch will raise a timeout
+        message = messages[0]
+
+        # Execute the callback without tracing
+        if message.headers is None:
+            logger.warning("Message received without headers, skipping span")
+            return await cb(message)
+
+        with start_span_message_receiver(tracer, message) as span:
+            try:
+                return await cb(message)
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+            finally:
+                msg_time_histo.observe(
+                    (datetime.now() - message.metadata.timestamp).total_seconds(),
+                    {
+                        "stream": message.metadata.stream,
+                        "consumer": message.metadata.consumer or "",
+                        "acked": "yes" if message._ackd else "no",
+                    },
+                )
+
+
+class NatsClientTelemetry:
+    def __init__(self, nc: Client, service_name: str, tracer_provider: TracerProvider):
+        self.nc = nc
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    async def subscribe(self, cb, **kwargs):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_subscriber")
+
+        async def wrapper(origin_cb, tracer, msg: Msg):
+            # Execute the callback without tracing
+            if msg.headers is None:
+                logger.warning("Message received without headers, skipping span")
+                await origin_cb(msg)
+                return
+
+            with start_span_message_receiver(tracer, msg) as span:
+                try:
+                    await origin_cb(msg)
+                except Exception as error:
+                    set_span_exception(span, error)
+                    raise error
+
+        wrapped_cb = partial(wrapper, cb, tracer)
+        return await self.nc.subscribe(cb=wrapped_cb, **kwargs)
+
+    async def publish(
+        self,
+        subject: str,
+        body: bytes,
+        headers: Optional[Dict[str, str]] = None,
+        **kwargs,
+    ):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_publisher")
+        headers = {} if headers is None else headers
+        inject(headers)
+
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.nc.publish(subject, body, headers=headers, **kwargs)
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+
+        return result
+
+    async def request(
+        self,
+        subject: str,
+        payload: bytes = b"",
+        timeout: float = 0.5,
+        old_style: bool = False,
+        headers: Optional[Dict[str, Any]] = None,
+    ) -> Msg:
+        headers = {} if headers is None else headers
+        inject(headers)
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_request")
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.nc.request(
+                    subject, payload, timeout, old_style, headers  # type: ignore
+                )
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+
+        return result
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import enum
-from typing import Optional
-
-from pydantic import BaseSettings
-
-
-class TelemetrySettings(BaseSettings):
-    jaeger_agent_host: str = "localhost"
-    jaeger_agent_port: int = 6831
-    jaeger_enabled: bool = False
-    jaeger_query_port: int = 16686
-    jaeger_query_host: str = "jaeger.observability.svc.cluster.local"
-
-
-telemetry_settings = TelemetrySettings()
-
-
-class LogLevel(enum.Enum):
-    DEBUG = "DEBUG"
-    INFO = "INFO"
-    WARNING = "WARNING"
-    ERROR = "ERROR"
-    FATAL = "FATAL"
-    CRITICAL = "CRITICAL"
-
-
-class LogSettings(BaseSettings):
-    debug: bool = False
-    log_level: LogLevel = LogLevel.WARNING
-    logger_levels: Optional[dict[str, LogLevel]] = None
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import enum
+from typing import Optional
+
+from pydantic import BaseSettings
+
+
+class TelemetrySettings(BaseSettings):
+    jaeger_agent_host: str = "localhost"
+    jaeger_agent_port: int = 6831
+    jaeger_enabled: bool = False
+    jaeger_query_port: int = 16686
+    jaeger_query_host: str = "jaeger.observability.svc.cluster.local"
+
+
+telemetry_settings = TelemetrySettings()
+
+
+class LogLevel(enum.Enum):
+    DEBUG = "DEBUG"
+    INFO = "INFO"
+    WARNING = "WARNING"
+    ERROR = "ERROR"
+    FATAL = "FATAL"
+    CRITICAL = "CRITICAL"
+
+
+class LogSettings(BaseSettings):
+    debug: bool = False
+    log_level: LogLevel = LogLevel.WARNING
+    logger_levels: Optional[dict[str, LogLevel]] = None
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import logging
+
+logger = logging.getLogger("nucliadb_telemetry")
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,27 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-pytest_plugins = [
-    "pytest_docker_fixtures",
-    "nucliadb_utils.tests.nats",
-    "nucliadb_telemetry.tests.telemetry",
-]
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+pytest_plugins = [
+    "pytest_docker_fixtures",
+    "nucliadb_utils.tests.nats",
+    "nucliadb_utils.tests.gcs",
+    "nucliadb_utils.tests.s3",
+    "nucliadb_utils.tests.indexing",
+    "nucliadb_node.tests.fixtures",
+]
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-// Copyright 2015 gRPC authors.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-syntax = "proto3";
-
-option java_package = "ex.grpc";
-option objc_class_prefix = "HSW";
-
-package hellostreamingworld;
-
-// The greeting service definition.
-service MultiGreeter {
-  // Sends multiple greetings
-  rpc sayHello (HelloRequest) returns (stream HelloReply) {}
-}
-
-// The request message containing the user's name and how many greetings
-// they want.
-message HelloRequest {
-  string name = 1;
-  string num_greetings = 2;
-}
-
-// A response message containing a greeting
-message HelloReply {
-  string message = 1;
-}
+// Copyright 2015 gRPC authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto3";
+
+option java_package = "ex.grpc";
+option objc_class_prefix = "HSW";
+
+package hellostreamingworld;
+
+// The greeting service definition.
+service MultiGreeter {
+  // Sends multiple greetings
+  rpc sayHello (HelloRequest) returns (stream HelloReply) {}
+}
+
+// The request message containing the user's name and how many greetings
+// they want.
+message HelloRequest {
+  string name = 1;
+  string num_greetings = 2;
+}
+
+// A response message containing a greeting
+message HelloReply {
+  string message = 1;
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-# -*- coding: utf-8 -*-
-# Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
-"""Generated protocol buffer code."""
-from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
-from google.protobuf import symbol_database as _symbol_database
-
-# @@protoc_insertion_point(imports)
-
-_sym_db = _symbol_database.Default()
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
-    b'\n7nucliadb_telemetry/tests/grpc/hellostreamingworld.proto\x12\x13hellostreamingworld"3\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x15\n\rnum_greetings\x18\x02 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2b\n\x0cMultiGreeter\x12R\n\x08sayHello\x12!.hellostreamingworld.HelloRequest\x1a\x1f.hellostreamingworld.HelloReply"\x00\x30\x01\x42\x0f\n\x07\x65x.grpc\xa2\x02\x03HSWb\x06proto3'
-)
-
-
-_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
-_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
-HelloRequest = _reflection.GeneratedProtocolMessageType(
-    "HelloRequest",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREQUEST,
-        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
-        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloRequest)
-    },
-)
-_sym_db.RegisterMessage(HelloRequest)
-
-HelloReply = _reflection.GeneratedProtocolMessageType(
-    "HelloReply",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREPLY,
-        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
-        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloReply)
-    },
-)
-_sym_db.RegisterMessage(HelloReply)
-
-_MULTIGREETER = DESCRIPTOR.services_by_name["MultiGreeter"]
-if _descriptor._USE_C_DESCRIPTORS == False:
-    DESCRIPTOR._options = None
-    DESCRIPTOR._serialized_options = b"\n\007ex.grpc\242\002\003HSW"
-    _HELLOREQUEST._serialized_start = 80
-    _HELLOREQUEST._serialized_end = 131
-    _HELLOREPLY._serialized_start = 133
-    _HELLOREPLY._serialized_end = 162
-    _MULTIGREETER._serialized_start = 164
-    _MULTIGREETER._serialized_end = 262
-# @@protoc_insertion_point(module_scope)
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n7nucliadb_telemetry/tests/grpc/hellostreamingworld.proto\x12\x13hellostreamingworld"3\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x15\n\rnum_greetings\x18\x02 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2b\n\x0cMultiGreeter\x12R\n\x08sayHello\x12!.hellostreamingworld.HelloRequest\x1a\x1f.hellostreamingworld.HelloReply"\x00\x30\x01\x42\x0f\n\x07\x65x.grpc\xa2\x02\x03HSWb\x06proto3'
+)
+
+
+_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
+_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
+HelloRequest = _reflection.GeneratedProtocolMessageType(
+    "HelloRequest",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREQUEST,
+        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
+        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloRequest)
+    },
+)
+_sym_db.RegisterMessage(HelloRequest)
+
+HelloReply = _reflection.GeneratedProtocolMessageType(
+    "HelloReply",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREPLY,
+        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
+        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloReply)
+    },
+)
+_sym_db.RegisterMessage(HelloReply)
+
+_MULTIGREETER = DESCRIPTOR.services_by_name["MultiGreeter"]
+if _descriptor._USE_C_DESCRIPTORS == False:
+    DESCRIPTOR._options = None
+    DESCRIPTOR._serialized_options = b"\n\007ex.grpc\242\002\003HSW"
+    _HELLOREQUEST._serialized_start = 80
+    _HELLOREQUEST._serialized_end = 131
+    _HELLOREPLY._serialized_start = 133
+    _HELLOREPLY._serialized_end = 162
+    _MULTIGREETER._serialized_start = 164
+    _MULTIGREETER._serialized_end = 262
+# @@protoc_insertion_point(module_scope)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
-"""Client and server classes corresponding to protobuf-defined services."""
-import grpc
-
-from nucliadb_telemetry.tests.grpc import (
-    hellostreamingworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2,
-)
-
-
-class MultiGreeterStub(object):
-    """The greeting service definition."""
-
-    def __init__(self, channel):
-        """Constructor.
-
-        Args:
-            channel: A grpc.Channel.
-        """
-        self.sayHello = channel.unary_stream(
-            "/hellostreamingworld.MultiGreeter/sayHello",
-            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
-            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
-        )
-
-
-class MultiGreeterServicer(object):
-    """The greeting service definition."""
-
-    def sayHello(self, request, context):
-        """Sends multiple greetings"""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-
-def add_MultiGreeterServicer_to_server(servicer, server):
-    rpc_method_handlers = {
-        "sayHello": grpc.unary_stream_rpc_method_handler(
-            servicer.sayHello,
-            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.FromString,
-            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.SerializeToString,
-        ),
-    }
-    generic_handler = grpc.method_handlers_generic_handler(
-        "hellostreamingworld.MultiGreeter", rpc_method_handlers
-    )
-    server.add_generic_rpc_handlers((generic_handler,))
-
-
-# This class is part of an EXPERIMENTAL API.
-class MultiGreeter(object):
-    """The greeting service definition."""
-
-    @staticmethod
-    def sayHello(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_stream(
-            request,
-            target,
-            "/hellostreamingworld.MultiGreeter/sayHello",
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+from nucliadb_telemetry.tests.grpc import (
+    hellostreamingworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2,
+)
+
+
+class MultiGreeterStub(object):
+    """The greeting service definition."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.sayHello = channel.unary_stream(
+            "/hellostreamingworld.MultiGreeter/sayHello",
+            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
+            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
+        )
+
+
+class MultiGreeterServicer(object):
+    """The greeting service definition."""
+
+    def sayHello(self, request, context):
+        """Sends multiple greetings"""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details("Method not implemented!")
+        raise NotImplementedError("Method not implemented!")
+
+
+def add_MultiGreeterServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+        "sayHello": grpc.unary_stream_rpc_method_handler(
+            servicer.sayHello,
+            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.FromString,
+            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.SerializeToString,
+        ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+        "hellostreamingworld.MultiGreeter", rpc_method_handlers
+    )
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+# This class is part of an EXPERIMENTAL API.
+class MultiGreeter(object):
+    """The greeting service definition."""
+
+    @staticmethod
+    def sayHello(
+        request,
+        target,
+        options=(),
+        channel_credentials=None,
+        call_credentials=None,
+        insecure=False,
+        compression=None,
+        wait_for_ready=None,
+        timeout=None,
+        metadata=None,
+    ):
+        return grpc.experimental.unary_stream(
+            request,
+            target,
+            "/hellostreamingworld.MultiGreeter/sayHello",
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
+            options,
+            channel_credentials,
+            insecure,
+            call_credentials,
+            compression,
+            wait_for_ready,
+            timeout,
+            metadata,
+        )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import abc
-import grpc
-import nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2
-import typing
-
-class MultiGreeterStub:
-    """The greeting service definition."""
-
-    def __init__(self, channel: grpc.Channel) -> None: ...
-    sayHello: grpc.UnaryStreamMultiCallable[
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply,
-    ]
-    """Sends multiple greetings"""
-
-class MultiGreeterServicer(metaclass=abc.ABCMeta):
-    """The greeting service definition."""
-
-    @abc.abstractmethod
-    def sayHello(
-        self,
-        request: nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
-        context: grpc.ServicerContext,
-    ) -> typing.Iterator[
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply
-    ]:
-        """Sends multiple greetings"""
-        pass
-
-def add_MultiGreeterServicer_to_server(
-    servicer: MultiGreeterServicer, server: grpc.Server
-) -> None: ...
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import abc
+import grpc
+import nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2
+import typing
+
+class MultiGreeterStub:
+    """The greeting service definition."""
+
+    def __init__(self, channel: grpc.Channel) -> None: ...
+    sayHello: grpc.UnaryStreamMultiCallable[
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply,
+    ]
+    """Sends multiple greetings"""
+
+class MultiGreeterServicer(metaclass=abc.ABCMeta):
+    """The greeting service definition."""
+
+    @abc.abstractmethod
+    def sayHello(
+        self,
+        request: nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
+        context: grpc.ServicerContext,
+    ) -> typing.Iterator[
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply
+    ]:
+        """Sends multiple greetings"""
+        pass
+
+def add_MultiGreeterServicer_to_server(
+    servicer: MultiGreeterServicer, server: grpc.Server
+) -> None: ...
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-// Copyright 2015 gRPC authors.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-syntax = "proto3";
-
-option java_multiple_files = true;
-option java_package = "io.grpc.examples.helloworld";
-option java_outer_classname = "HelloWorldProto";
-option objc_class_prefix = "HLW";
-
-package helloworld;
-
-// The greeting service definition.
-service Greeter {
-  // Sends a greeting
-  rpc SayHello (HelloRequest) returns (HelloReply) {}
-}
-
-// The request message containing the user's name.
-message HelloRequest {
-  string name = 1;
-}
-
-// The response message containing the greetings
-message HelloReply {
-  string message = 1;
+// Copyright 2015 gRPC authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto3";
+
+option java_multiple_files = true;
+option java_package = "io.grpc.examples.helloworld";
+option java_outer_classname = "HelloWorldProto";
+option objc_class_prefix = "HLW";
+
+package helloworld;
+
+// The greeting service definition.
+service Greeter {
+  // Sends a greeting
+  rpc SayHello (HelloRequest) returns (HelloReply) {}
+}
+
+// The request message containing the user's name.
+message HelloRequest {
+  string name = 1;
+}
+
+// The response message containing the greetings
+message HelloReply {
+  string message = 1;
 }
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-# -*- coding: utf-8 -*-
-# Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: nucliadb_telemetry/tests/grpc/helloworld.proto
-"""Generated protocol buffer code."""
-from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
-from google.protobuf import symbol_database as _symbol_database
-
-# @@protoc_insertion_point(imports)
-
-_sym_db = _symbol_database.Default()
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
-    b'\n.nucliadb_telemetry/tests/grpc/helloworld.proto\x12\nhelloworld"\x1c\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2I\n\x07Greeter\x12>\n\x08SayHello\x12\x18.helloworld.HelloRequest\x1a\x16.helloworld.HelloReply"\x00\x42\x36\n\x1bio.grpc.examples.helloworldB\x0fHelloWorldProtoP\x01\xa2\x02\x03HLWb\x06proto3'
-)
-
-
-_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
-_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
-HelloRequest = _reflection.GeneratedProtocolMessageType(
-    "HelloRequest",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREQUEST,
-        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
-        # @@protoc_insertion_point(class_scope:helloworld.HelloRequest)
-    },
-)
-_sym_db.RegisterMessage(HelloRequest)
-
-HelloReply = _reflection.GeneratedProtocolMessageType(
-    "HelloReply",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREPLY,
-        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
-        # @@protoc_insertion_point(class_scope:helloworld.HelloReply)
-    },
-)
-_sym_db.RegisterMessage(HelloReply)
-
-_GREETER = DESCRIPTOR.services_by_name["Greeter"]
-if _descriptor._USE_C_DESCRIPTORS == False:
-    DESCRIPTOR._options = None
-    DESCRIPTOR._serialized_options = (
-        b"\n\033io.grpc.examples.helloworldB\017HelloWorldProtoP\001\242\002\003HLW"
-    )
-    _HELLOREQUEST._serialized_start = 62
-    _HELLOREQUEST._serialized_end = 90
-    _HELLOREPLY._serialized_start = 92
-    _HELLOREPLY._serialized_end = 121
-    _GREETER._serialized_start = 123
-    _GREETER._serialized_end = 196
-# @@protoc_insertion_point(module_scope)
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: nucliadb_telemetry/tests/grpc/helloworld.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n.nucliadb_telemetry/tests/grpc/helloworld.proto\x12\nhelloworld"\x1c\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2I\n\x07Greeter\x12>\n\x08SayHello\x12\x18.helloworld.HelloRequest\x1a\x16.helloworld.HelloReply"\x00\x42\x36\n\x1bio.grpc.examples.helloworldB\x0fHelloWorldProtoP\x01\xa2\x02\x03HLWb\x06proto3'
+)
+
+
+_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
+_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
+HelloRequest = _reflection.GeneratedProtocolMessageType(
+    "HelloRequest",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREQUEST,
+        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
+        # @@protoc_insertion_point(class_scope:helloworld.HelloRequest)
+    },
+)
+_sym_db.RegisterMessage(HelloRequest)
+
+HelloReply = _reflection.GeneratedProtocolMessageType(
+    "HelloReply",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREPLY,
+        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
+        # @@protoc_insertion_point(class_scope:helloworld.HelloReply)
+    },
+)
+_sym_db.RegisterMessage(HelloReply)
+
+_GREETER = DESCRIPTOR.services_by_name["Greeter"]
+if _descriptor._USE_C_DESCRIPTORS == False:
+    DESCRIPTOR._options = None
+    DESCRIPTOR._serialized_options = (
+        b"\n\033io.grpc.examples.helloworldB\017HelloWorldProtoP\001\242\002\003HLW"
+    )
+    _HELLOREQUEST._serialized_start = 62
+    _HELLOREQUEST._serialized_end = 90
+    _HELLOREPLY._serialized_start = 92
+    _HELLOREPLY._serialized_end = 121
+    _GREETER._serialized_start = 123
+    _GREETER._serialized_end = 196
+# @@protoc_insertion_point(module_scope)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi`

 * *Files 19% similar despite different names*

```diff
@@ -1,45 +1,53 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import builtins
-import google.protobuf.descriptor
-import google.protobuf.message
-import typing
-import typing_extensions
-
-DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
-
-class HelloRequest(google.protobuf.message.Message):
-    """The request message containing the user's name."""
-
-    DESCRIPTOR: google.protobuf.descriptor.Descriptor
-    NAME_FIELD_NUMBER: builtins.int
-    name: typing.Text
-    def __init__(
-        self,
-        *,
-        name: typing.Text = ...,
-    ) -> None: ...
-    def ClearField(
-        self, field_name: typing_extensions.Literal["name", b"name"]
-    ) -> None: ...
-
-global___HelloRequest = HelloRequest
-
-class HelloReply(google.protobuf.message.Message):
-    """The response message containing the greetings"""
-
-    DESCRIPTOR: google.protobuf.descriptor.Descriptor
-    MESSAGE_FIELD_NUMBER: builtins.int
-    message: typing.Text
-    def __init__(
-        self,
-        *,
-        message: typing.Text = ...,
-    ) -> None: ...
-    def ClearField(
-        self, field_name: typing_extensions.Literal["message", b"message"]
-    ) -> None: ...
-
-global___HelloReply = HelloReply
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import builtins
+import google.protobuf.descriptor
+import google.protobuf.message
+import typing
+import typing_extensions
+
+DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
+
+class HelloRequest(google.protobuf.message.Message):
+    """The request message containing the user's name and how many greetings
+    they want.
+    """
+
+    DESCRIPTOR: google.protobuf.descriptor.Descriptor
+    NAME_FIELD_NUMBER: builtins.int
+    NUM_GREETINGS_FIELD_NUMBER: builtins.int
+    name: typing.Text
+    num_greetings: typing.Text
+    def __init__(
+        self,
+        *,
+        name: typing.Text = ...,
+        num_greetings: typing.Text = ...,
+    ) -> None: ...
+    def ClearField(
+        self,
+        field_name: typing_extensions.Literal[
+            "name", b"name", "num_greetings", b"num_greetings"
+        ],
+    ) -> None: ...
+
+global___HelloRequest = HelloRequest
+
+class HelloReply(google.protobuf.message.Message):
+    """A response message containing a greeting"""
+
+    DESCRIPTOR: google.protobuf.descriptor.Descriptor
+    MESSAGE_FIELD_NUMBER: builtins.int
+    message: typing.Text
+    def __init__(
+        self,
+        *,
+        message: typing.Text = ...,
+    ) -> None: ...
+    def ClearField(
+        self, field_name: typing_extensions.Literal["message", b"message"]
+    ) -> None: ...
+
+global___HelloReply = HelloReply
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
-"""Client and server classes corresponding to protobuf-defined services."""
-import grpc
-
-from nucliadb_telemetry.tests.grpc import (
-    helloworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2,
-)
-
-
-class GreeterStub(object):
-    """The greeting service definition."""
-
-    def __init__(self, channel):
-        """Constructor.
-
-        Args:
-            channel: A grpc.Channel.
-        """
-        self.SayHello = channel.unary_unary(
-            "/helloworld.Greeter/SayHello",
-            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
-            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
-        )
-
-
-class GreeterServicer(object):
-    """The greeting service definition."""
-
-    def SayHello(self, request, context):
-        """Sends a greeting"""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-
-def add_GreeterServicer_to_server(servicer, server):
-    rpc_method_handlers = {
-        "SayHello": grpc.unary_unary_rpc_method_handler(
-            servicer.SayHello,
-            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.FromString,
-            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.SerializeToString,
-        ),
-    }
-    generic_handler = grpc.method_handlers_generic_handler(
-        "helloworld.Greeter", rpc_method_handlers
-    )
-    server.add_generic_rpc_handlers((generic_handler,))
-
-
-# This class is part of an EXPERIMENTAL API.
-class Greeter(object):
-    """The greeting service definition."""
-
-    @staticmethod
-    def SayHello(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
-            target,
-            "/helloworld.Greeter/SayHello",
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+from nucliadb_telemetry.tests.grpc import (
+    helloworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2,
+)
+
+
+class GreeterStub(object):
+    """The greeting service definition."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.SayHello = channel.unary_unary(
+            "/helloworld.Greeter/SayHello",
+            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
+            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
+        )
+
+
+class GreeterServicer(object):
+    """The greeting service definition."""
+
+    def SayHello(self, request, context):
+        """Sends a greeting"""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details("Method not implemented!")
+        raise NotImplementedError("Method not implemented!")
+
+
+def add_GreeterServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+        "SayHello": grpc.unary_unary_rpc_method_handler(
+            servicer.SayHello,
+            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.FromString,
+            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.SerializeToString,
+        ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+        "helloworld.Greeter", rpc_method_handlers
+    )
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+# This class is part of an EXPERIMENTAL API.
+class Greeter(object):
+    """The greeting service definition."""
+
+    @staticmethod
+    def SayHello(
+        request,
+        target,
+        options=(),
+        channel_credentials=None,
+        call_credentials=None,
+        insecure=False,
+        compression=None,
+        wait_for_ready=None,
+        timeout=None,
+        metadata=None,
+    ):
+        return grpc.experimental.unary_unary(
+            request,
+            target,
+            "/helloworld.Greeter/SayHello",
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
+            options,
+            channel_credentials,
+            insecure,
+            call_credentials,
+            compression,
+            wait_for_ready,
+            timeout,
+            metadata,
+        )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import abc
-import grpc
-import nucliadb_telemetry.tests.grpc.helloworld_pb2
-
-class GreeterStub:
-    """The greeting service definition."""
-
-    def __init__(self, channel: grpc.Channel) -> None: ...
-    SayHello: grpc.UnaryUnaryMultiCallable[
-        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
-        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply,
-    ]
-    """Sends a greeting"""
-
-class GreeterServicer(metaclass=abc.ABCMeta):
-    """The greeting service definition."""
-
-    @abc.abstractmethod
-    def SayHello(
-        self,
-        request: nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
-        context: grpc.ServicerContext,
-    ) -> nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply:
-        """Sends a greeting"""
-        pass
-
-def add_GreeterServicer_to_server(
-    servicer: GreeterServicer, server: grpc.Server
-) -> None: ...
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import abc
+import grpc
+import nucliadb_telemetry.tests.grpc.helloworld_pb2
+
+class GreeterStub:
+    """The greeting service definition."""
+
+    def __init__(self, channel: grpc.Channel) -> None: ...
+    SayHello: grpc.UnaryUnaryMultiCallable[
+        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
+        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply,
+    ]
+    """Sends a greeting"""
+
+class GreeterServicer(metaclass=abc.ABCMeta):
+    """The greeting service definition."""
+
+    @abc.abstractmethod
+    def SayHello(
+        self,
+        request: nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
+        context: grpc.ServicerContext,
+    ) -> nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply:
+        """Sends a greeting"""
+        pass
+
+def add_GreeterServicer_to_server(
+    servicer: GreeterServicer, server: grpc.Server
+) -> None: ...
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_fastapi.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_fastapi.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import pytest
-from starlette.applications import Starlette
-from starlette.responses import PlainTextResponse
-from starlette.testclient import TestClient
-
-from nucliadb_telemetry.fastapi import PrometheusMiddleware, metrics_endpoint
-from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
-
-
-class TestCasePrometheusMiddleware:
-    @pytest.fixture(scope="class")
-    def app(self):
-        app_ = Starlette()
-        app_.add_middleware(PrometheusMiddleware)
-        app_.add_route("/metrics/", metrics_endpoint)
-
-        @app_.route("/foo/")
-        def foo(request):
-            return PlainTextResponse("Foo")
-
-        @app_.route("/bar/")
-        def bar(request):
-            raise ValueError("bar")
-
-        @app_.route("/foo/{bar}/")
-        def foobar(request):
-            return PlainTextResponse(f"Foo: {request.path_params['bar']}")
-
-        sub_app = Starlette()
-
-        @sub_app.route("/foobar/")
-        def sub_foobar(request):
-            return PlainTextResponse("Foobar")
-
-        app_.mount("/sub", sub_app)
-
-        return app_
-
-    @pytest.fixture
-    def client(self, app):
-        return TestClient(app)
-
-    def test_view_ok(self, client):
-        # Do a request
-        client.get("/foo/")
-
-        # Get metrics
-        response = client.get("/metrics/")
-        metrics_text = response.content.decode()
-
-        # Asserts: Requests
-        assert (
-            'starlette_requests_total{method="GET",path_template="/foo/"} 1.0'
-            in metrics_text
-        )
-
-        # Asserts: Responses
-        assert (
-            'starlette_responses_total{method="GET",path_template="/foo/",status_code="200"} 1.0'
-            in metrics_text
-        )
-
-        # Asserts: Requests in progress
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/foo/"} 0.0'
-            in metrics_text
-        )
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
-            in metrics_text
-        )
-
-    def test_view_exception(self, client):
-        # Do a request
-        with pytest.raises(ValueError):
-            client.get("/bar/")
-
-        # Get metrics
-        response = client.get("/metrics/")
-        metrics_text = response.content.decode()
-
-        # Asserts: Requests
-        assert (
-            'starlette_requests_total{method="GET",path_template="/bar/"} 1.0'
-            in metrics_text
-        )
-
-        # Asserts: Responses
-        assert (
-            "starlette_exceptions_total{"
-            'exception_type="ValueError",method="GET",path_template="/bar/"'
-            "} 1.0" in metrics_text
-        )
-        assert (
-            "starlette_responses_total{"
-            'method="GET",path_template="/bar/",status_code="500"'
-            "} 1.0" in metrics_text
-        )
-
-        # Asserts: Requests in progress
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/bar/"} 0.0'
-            in metrics_text
-        )
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
-            in metrics_text
-        )
-
-    def test_path_substitution(self, client):
-        # Do a request
-        client.get("/foo/baz/")
-
-        # Get metrics
-        response = client.get("/metrics/")
-        metrics_text = response.content.decode()
-
-        # Asserts: Headers
-        assert (
-            response.headers["content-type"]
-            == "text/plain; version=0.0.4; charset=utf-8"
-        )
-
-        # Asserts: Requests
-        assert (
-            'starlette_requests_total{method="GET",path_template="/foo/{bar}/"} 1.0'
-            in metrics_text
-        )
-
-        # Asserts: Responses
-        assert (
-            'starlette_responses_total{method="GET",path_template="/foo/{bar}/",status_code="200"} 1.0'
-            in metrics_text
-        )
-
-        # Asserts: Requests in progress
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/foo/{bar}/"} 0.0'
-            in metrics_text
-        )
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
-            in metrics_text
-        )
-
-    def test_sub_path_match(self, client):
-        # Do a request
-        client.get("/sub/foobar/")
-
-        # Get metrics
-        response = client.get("/metrics/")
-        metrics_text = response.content.decode()
-
-        # Asserts: Requests
-        assert (
-            'starlette_requests_total{method="GET",path_template="/sub/foobar/"} 1.0'
-            in metrics_text
-        )
-
-
-class TestCasePrometheusMiddlewareFilterUnhandledPaths:
-    @pytest.fixture(scope="class")
-    def app(self):
-        app_ = Starlette()
-        app_.add_middleware(PrometheusMiddleware)
-        app_.add_route("/metrics/", metrics_endpoint)
-
-        return app_
-
-    @pytest.fixture
-    def client(self, app):
-        return TestClient(app)
-
-    def test_filter_unhandled_paths(self, client):
-        # Do a request
-        path = "/other/unhandled/path"
-        client.get(path)
-
-        # Get metrics
-        response = client.get("/metrics/")
-        metrics_text = response.content.decode()
-
-        # Asserts: metric is filtered
-        assert path not in metrics_text
-
-        # Asserts: Requests in progress
-        assert (
-            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
-            in metrics_text
-        )
-
-
-class TestCaseCaptureTraceIdMiddleware:
-    @pytest.fixture(scope="class")
-    def app(self):
-        app_ = Starlette()
-        app_.add_middleware(CaptureTraceIdMiddleware)
-
-        @app_.route("/foo/")
-        def foo(request):
-            return PlainTextResponse("Foo")
-
-        return app_
-
-    @pytest.fixture
-    def client(self, app):
-        return TestClient(app)
-
-    def test_trace_id_header_is_returned(self, client):
-        response = client.get("/foo/")
-
-        assert response.headers["x-nuclia-trace-id"]
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import pytest
+from starlette.applications import Starlette
+from starlette.responses import PlainTextResponse
+from starlette.testclient import TestClient
+
+from nucliadb_telemetry.fastapi import PrometheusMiddleware, metrics_endpoint
+from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
+
+
+class TestCasePrometheusMiddleware:
+    @pytest.fixture(scope="class")
+    def app(self):
+        app_ = Starlette()
+        app_.add_middleware(PrometheusMiddleware)
+        app_.add_route("/metrics/", metrics_endpoint)
+
+        @app_.route("/foo/")
+        def foo(request):
+            return PlainTextResponse("Foo")
+
+        @app_.route("/bar/")
+        def bar(request):
+            raise ValueError("bar")
+
+        @app_.route("/foo/{bar}/")
+        def foobar(request):
+            return PlainTextResponse(f"Foo: {request.path_params['bar']}")
+
+        sub_app = Starlette()
+
+        @sub_app.route("/foobar/")
+        def sub_foobar(request):
+            return PlainTextResponse("Foobar")
+
+        app_.mount("/sub", sub_app)
+
+        return app_
+
+    @pytest.fixture
+    def client(self, app):
+        return TestClient(app)
+
+    def test_view_ok(self, client):
+        # Do a request
+        client.get("/foo/")
+
+        # Get metrics
+        response = client.get("/metrics/")
+        metrics_text = response.content.decode()
+
+        # Asserts: Requests
+        assert (
+            'starlette_requests_total{method="GET",path_template="/foo/"} 1.0'
+            in metrics_text
+        )
+
+        # Asserts: Responses
+        assert (
+            'starlette_responses_total{method="GET",path_template="/foo/",status_code="200"} 1.0'
+            in metrics_text
+        )
+
+        # Asserts: Requests in progress
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/foo/"} 0.0'
+            in metrics_text
+        )
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
+            in metrics_text
+        )
+
+    def test_view_exception(self, client):
+        # Do a request
+        with pytest.raises(ValueError):
+            client.get("/bar/")
+
+        # Get metrics
+        response = client.get("/metrics/")
+        metrics_text = response.content.decode()
+
+        # Asserts: Requests
+        assert (
+            'starlette_requests_total{method="GET",path_template="/bar/"} 1.0'
+            in metrics_text
+        )
+
+        # Asserts: Responses
+        assert (
+            "starlette_exceptions_total{"
+            'exception_type="ValueError",method="GET",path_template="/bar/"'
+            "} 1.0" in metrics_text
+        )
+        assert (
+            "starlette_responses_total{"
+            'method="GET",path_template="/bar/",status_code="500"'
+            "} 1.0" in metrics_text
+        )
+
+        # Asserts: Requests in progress
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/bar/"} 0.0'
+            in metrics_text
+        )
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
+            in metrics_text
+        )
+
+    def test_path_substitution(self, client):
+        # Do a request
+        client.get("/foo/baz/")
+
+        # Get metrics
+        response = client.get("/metrics/")
+        metrics_text = response.content.decode()
+
+        # Asserts: Headers
+        assert (
+            response.headers["content-type"]
+            == "text/plain; version=0.0.4; charset=utf-8"
+        )
+
+        # Asserts: Requests
+        assert (
+            'starlette_requests_total{method="GET",path_template="/foo/{bar}/"} 1.0'
+            in metrics_text
+        )
+
+        # Asserts: Responses
+        assert (
+            'starlette_responses_total{method="GET",path_template="/foo/{bar}/",status_code="200"} 1.0'
+            in metrics_text
+        )
+
+        # Asserts: Requests in progress
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/foo/{bar}/"} 0.0'
+            in metrics_text
+        )
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
+            in metrics_text
+        )
+
+    def test_sub_path_match(self, client):
+        # Do a request
+        client.get("/sub/foobar/")
+
+        # Get metrics
+        response = client.get("/metrics/")
+        metrics_text = response.content.decode()
+
+        # Asserts: Requests
+        assert (
+            'starlette_requests_total{method="GET",path_template="/sub/foobar/"} 1.0'
+            in metrics_text
+        )
+
+
+class TestCasePrometheusMiddlewareFilterUnhandledPaths:
+    @pytest.fixture(scope="class")
+    def app(self):
+        app_ = Starlette()
+        app_.add_middleware(PrometheusMiddleware)
+        app_.add_route("/metrics/", metrics_endpoint)
+
+        return app_
+
+    @pytest.fixture
+    def client(self, app):
+        return TestClient(app)
+
+    def test_filter_unhandled_paths(self, client):
+        # Do a request
+        path = "/other/unhandled/path"
+        client.get(path)
+
+        # Get metrics
+        response = client.get("/metrics/")
+        metrics_text = response.content.decode()
+
+        # Asserts: metric is filtered
+        assert path not in metrics_text
+
+        # Asserts: Requests in progress
+        assert (
+            'starlette_requests_in_progress{method="GET",path_template="/metrics/"} 1.0'
+            in metrics_text
+        )
+
+
+class TestCaseCaptureTraceIdMiddleware:
+    @pytest.fixture(scope="class")
+    def app(self):
+        app_ = Starlette()
+        app_.add_middleware(CaptureTraceIdMiddleware)
+
+        @app_.route("/foo/")
+        def foo(request):
+            return PlainTextResponse("Foo")
+
+        return app_
+
+    @pytest.fixture
+    def client(self, app):
+        return TestClient(app)
+
+    def test_trace_id_header_is_returned(self, client):
+        response = client.get("/foo/")
+
+        assert response.headers["x-nuclia-trace-id"]
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,115 +1,115 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import json
-
-import pytest
-from httpx import AsyncClient
-
-from nucliadb_telemetry import grpc_metrics
-from nucliadb_telemetry.jetstream import msg_time_histo
-from nucliadb_telemetry.settings import telemetry_settings
-from nucliadb_telemetry.tests.telemetry import Greeter
-
-
-def fmt_span(span):
-    tags_by_key = {tag["key"]: tag["value"] for tag in span["tags"]}
-    return {
-        "time": span["startTime"],
-        "id": span["spanID"],
-        "parent": span["references"][0]["spanID"],
-        "process": span["processID"],
-        "scope": tags_by_key["otel.scope.name"],
-        "operation": span["operationName"],
-    }
-
-
-def debug_spans(spans):
-    print(
-        json.dumps(
-            sorted([fmt_span(span) for span in spans], key=lambda x: x["time"]),
-            indent=4,
-        )
-    )
-
-
-@pytest.mark.asyncio
-async def test_telemetry_dict(http_service: AsyncClient, greeter: Greeter):
-    resp = await http_service.get(
-        "http://test/",
-        headers={
-            "x-b3-traceid": "f13dc5318bf3bef64a0a5ea607db93a1",
-            "x-b3-spanid": "bfc2225c60b39d97",
-            "x-b3-sampled": "1",
-        },
-    )
-    assert resp.status_code == 200
-
-    # Check that trace ids are returned in response headers
-    assert resp.headers["X-NUCLIA-TRACE-ID"]
-    assert resp.headers["X-NUCLIA-TRACE-ID"] != "0"
-    assert "X-NUCLIA-TRACE-ID" in resp.headers["Access-Control-Expose-Headers"]
-
-    for i in range(10):
-        if len(greeter.messages) == 0:
-            await asyncio.sleep(1)
-    assert (
-        greeter.messages[0].headers["x-b3-traceid"]
-        == "f13dc5318bf3bef64a0a5ea607db93a1"
-    )
-    assert len(greeter.messages) == 4
-
-    expected_spans = 17
-
-    await asyncio.sleep(2)
-    client = AsyncClient()
-    for _ in range(10):
-        resp = await client.get(
-            f"http://localhost:{telemetry_settings.jaeger_query_port}/api/traces/f13dc5318bf3bef64a0a5ea607db93a1",
-            headers={"Accept": "application/json"},
-        )
-        if (
-            resp.status_code != 200
-            or len(resp.json()["data"][0]["spans"]) < expected_spans
-        ):
-            await asyncio.sleep(2)
-        else:
-            break
-
-    assert resp.json()["data"][0]["traceID"] == "f13dc5318bf3bef64a0a5ea607db93a1"
-
-    # Enable this block for debugging purposes, to see sunmmarized and sorted details of all spans
-    # debug_spans(resp.json()["data"][0]["spans"])
-
-    assert len(resp.json()["data"][0]["spans"]) == expected_spans
-    assert len(resp.json()["data"][0]["processes"]) == 3
-
-    assert grpc_metrics.grpc_client_observer.histogram.collect()[0].samples  # type: ignore
-    assert grpc_metrics.grpc_server_observer.histogram.collect()[0].samples  # type: ignore
-
-    assert msg_time_histo.histo.collect()[0].samples  # type: ignore
-
-    sample = [
-        sam.labels
-        for sam in msg_time_histo.histo.collect()[0].samples  # type: ignore
-        if sam.labels.get("le") == "0.005"
-    ][0]
-    sample.pop("consumer")
-    assert sample == {"stream": "testing", "acked": "no", "le": "0.005"}
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import json
+
+import pytest
+from httpx import AsyncClient
+
+from nucliadb_telemetry import grpc_metrics
+from nucliadb_telemetry.jetstream import msg_time_histo
+from nucliadb_telemetry.settings import telemetry_settings
+from nucliadb_telemetry.tests.telemetry import Greeter
+
+
+def fmt_span(span):
+    tags_by_key = {tag["key"]: tag["value"] for tag in span["tags"]}
+    return {
+        "time": span["startTime"],
+        "id": span["spanID"],
+        "parent": span["references"][0]["spanID"],
+        "process": span["processID"],
+        "scope": tags_by_key["otel.scope.name"],
+        "operation": span["operationName"],
+    }
+
+
+def debug_spans(spans):
+    print(
+        json.dumps(
+            sorted([fmt_span(span) for span in spans], key=lambda x: x["time"]),
+            indent=4,
+        )
+    )
+
+
+@pytest.mark.asyncio
+async def test_telemetry_dict(http_service: AsyncClient, greeter: Greeter):
+    resp = await http_service.get(
+        "http://test/",
+        headers={
+            "x-b3-traceid": "f13dc5318bf3bef64a0a5ea607db93a1",
+            "x-b3-spanid": "bfc2225c60b39d97",
+            "x-b3-sampled": "1",
+        },
+    )
+    assert resp.status_code == 200
+
+    # Check that trace ids are returned in response headers
+    assert resp.headers["X-NUCLIA-TRACE-ID"]
+    assert resp.headers["X-NUCLIA-TRACE-ID"] != "0"
+    assert "X-NUCLIA-TRACE-ID" in resp.headers["Access-Control-Expose-Headers"]
+
+    for i in range(10):
+        if len(greeter.messages) == 0:
+            await asyncio.sleep(1)
+    assert (
+        greeter.messages[0].headers["x-b3-traceid"]
+        == "f13dc5318bf3bef64a0a5ea607db93a1"
+    )
+    assert len(greeter.messages) == 4
+
+    expected_spans = 17
+
+    await asyncio.sleep(2)
+    client = AsyncClient()
+    for _ in range(10):
+        resp = await client.get(
+            f"http://localhost:{telemetry_settings.jaeger_query_port}/api/traces/f13dc5318bf3bef64a0a5ea607db93a1",
+            headers={"Accept": "application/json"},
+        )
+        if (
+            resp.status_code != 200
+            or len(resp.json()["data"][0]["spans"]) < expected_spans
+        ):
+            await asyncio.sleep(2)
+        else:
+            break
+
+    assert resp.json()["data"][0]["traceID"] == "f13dc5318bf3bef64a0a5ea607db93a1"
+
+    # Enable this block for debugging purposes, to see sunmmarized and sorted details of all spans
+    # debug_spans(resp.json()["data"][0]["spans"])
+
+    assert len(resp.json()["data"][0]["spans"]) == expected_spans
+    assert len(resp.json()["data"][0]["processes"]) == 3
+
+    assert grpc_metrics.grpc_client_observer.histogram.collect()[0].samples  # type: ignore
+    assert grpc_metrics.grpc_server_observer.histogram.collect()[0].samples  # type: ignore
+
+    assert msg_time_histo.histo.collect()[0].samples  # type: ignore
+
+    sample = [
+        sam.labels
+        for sam in msg_time_histo.histo.collect()[0].samples  # type: ignore
+        if sam.labels.get("le") == "0.005"
+    ][0]
+    sample.pop("consumer")
+    assert sample == {"stream": "testing", "acked": "no", "le": "0.005"}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,354 +1,354 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import os
-
-import nats
-import pytest
-import requests
-from fastapi import FastAPI
-from grpc import aio  # type: ignore
-from httpx import AsyncClient
-from nats.aio.msg import Msg
-from nats.js import api
-from opentelemetry.propagate import set_global_textmap
-from opentelemetry.propagators.b3 import B3MultiFormat
-from pytest_docker_fixtures import images  # type: ignore
-from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
-
-from nucliadb_telemetry.fastapi import instrument_app
-from nucliadb_telemetry.grpc import GRPCTelemetry
-from nucliadb_telemetry.jetstream import JetStreamContextTelemetry, NatsClientTelemetry
-from nucliadb_telemetry.settings import telemetry_settings
-from nucliadb_telemetry.tests.grpc import (
-    hellostreamingworld_pb2,
-    hellostreamingworld_pb2_grpc,
-    helloworld_pb2,
-    helloworld_pb2_grpc,
-)
-from nucliadb_telemetry.utils import (
-    clean_telemetry,
-    get_telemetry,
-    init_telemetry,
-    set_info_on_span,
-)
-
-images.settings["jaeger"] = {
-    "image": "jaegertracing/all-in-one",
-    "version": "1.33",
-    "options": {"ports": {"6831/udp": None, "16686": None}},
-}
-
-
-class Jaeger(BaseImage):
-    name = "jaeger"
-    port = 6831
-
-    def get_port(self, port=None):
-        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
-            return port if port else self.port
-        network = self.container_obj.attrs["NetworkSettings"]
-        service_port = "{0}/udp".format(port if port else self.port)
-        for netport in network["Ports"].keys():
-            if netport == service_port:
-                return network["Ports"][service_port][0]["HostPort"]
-
-    def get_http_port(self, port=None):
-        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
-            return 16686
-        network = self.container_obj.attrs["NetworkSettings"]
-        service_port = "16686/tcp"
-        for netport in network["Ports"].keys():
-            if netport == service_port:
-                return network["Ports"][service_port][0]["HostPort"]
-
-    def check(self):
-        resp = requests.get(f"http://{self.host}:{self.get_http_port()}")
-        return resp.status_code == 200
-
-
-@pytest.fixture(scope="function")
-async def jaeger_server():
-    server = Jaeger()
-    server.run()
-    yield server
-    server.stop()
-
-
-@pytest.fixture(scope="function")
-async def set_telemetry_settings(jaeger_server: Jaeger):
-    telemetry_settings.jaeger_enabled = True
-    telemetry_settings.jaeger_agent_host = "127.0.0.1"
-    telemetry_settings.jaeger_agent_port = jaeger_server.get_port()
-    telemetry_settings.jaeger_query_port = jaeger_server.get_http_port()
-
-
-@pytest.fixture(scope="function")
-async def telemetry_grpc(set_telemetry_settings):
-    tracer_provider = get_telemetry("GRPC_SERVICE")
-    await init_telemetry(tracer_provider)
-    util = GRPCTelemetry("test_telemetry", tracer_provider)
-    yield util
-
-
-class GreeterStreaming(hellostreamingworld_pb2_grpc.MultiGreeterServicer):
-    def __init__(self, natsd):
-        self.natsd = natsd
-        self.nc = None
-        self.push_subscription = None
-        self.tracer_provider = None
-        self.messages = []
-
-    async def push_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("message_worker")
-        with tracer.start_as_current_span("message_worker_span") as _:
-            self.messages.append(msg)
-
-    async def initialize(self):
-        self.nc = await nats.connect(servers=[self.natsd])
-        self.js = self.nc.jetstream()
-
-        try:
-            await self.js.stream_info("testing")
-        except nats.js.errors.NotFoundError:
-            await self.js.add_stream(name="testing", subjects=["testing.*"])
-
-        self.tracer_provider = get_telemetry("NATS_SERVICE")
-        await init_telemetry(self.tracer_provider)
-        self.jsotel = JetStreamContextTelemetry(
-            self.js, "nats_service", self.tracer_provider
-        )
-
-        self.push_subscription = await self.jsotel.subscribe(
-            subject="testing.stelemetry",
-            stream="testing",
-            cb=self.push_subscription_worker,
-        )
-
-    async def finalize(self):
-        await self.push_subscription.unsubscribe()
-        await self.nc.drain()
-        await self.nc.close()
-
-    async def sayHello(self, request, context):
-        await self.jsotel.publish("testing.stelemetry", request.name.encode())
-        for _ in range(10):
-            yield hellostreamingworld_pb2.HelloReply(
-                message="Hello, %s!" % request.name
-            )
-
-
-class Greeter(helloworld_pb2_grpc.GreeterServicer):
-    def __init__(self, natsd):
-        self.natsd = natsd
-        self.nc = None
-        self.push_subscription = None
-        self.pull_subscription_one = None
-        self.puller_task = None
-        self.pubsub_subscription = None
-        self.tracer_provider = None
-        self.messages = []
-        self.puller_task_one = None
-
-    async def push_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("message_worker")
-        with tracer.start_as_current_span("message_worker_span") as _:
-            self.messages.append(msg)
-
-    async def pull_subscription_worker_one(self):
-        async def callback(message):
-            self.messages.append(message)
-
-        await self.jsotel.pull_one(self.pull_subscription_one, callback)
-
-    async def pubsub_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("pubsub_worker")
-        with tracer.start_as_current_span("pubsub_worker_span") as _:
-            self.messages.append(msg)
-
-    async def reqresp_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("reqresp_worker")
-        with tracer.start_as_current_span("reqresp_worker_span") as _:
-            self.messages.append(msg)
-            await msg.respond(b"Bye Bye!")
-
-    async def initialize(self):
-        self.nc = await nats.connect(servers=[self.natsd])
-        self.js = self.nc.jetstream()
-
-        try:
-            await self.js.stream_info("testing")
-        except nats.js.errors.NotFoundError:
-            await self.js.add_stream(name="testing", subjects=["testing.*"])
-
-        self.tracer_provider = get_telemetry("NATS_SERVICE")
-        await init_telemetry(self.tracer_provider)
-        self.jsotel = JetStreamContextTelemetry(
-            self.js, "nats_service", self.tracer_provider
-        )
-
-        self.push_subscription = await self.jsotel.subscribe(
-            subject="testing.telemetry",
-            stream="testing",
-            cb=self.push_subscription_worker,
-        )
-
-        # Nats Jetstream Pull subscription including consumer creation
-        # and task to pull one message
-        config = api.ConsumerConfig()
-        config.filter_subject = "testing.telemetry_pull_one"
-        config.durable_name = "testing_consumer_one"
-        await self.js._jsm.add_consumer(stream="testing", config=config)
-
-        self.pull_subscription_one = await self.jsotel.pull_subscribe(
-            subject=config.filter_subject, durable=config.durable_name, stream="testing"
-        )
-
-        self.puller_task_one = asyncio.create_task(self.pull_subscription_worker_one())
-
-        # Plain nats instrumentation and subscription
-        # (no streams neither consumers used here)
-        self.ncotel = NatsClientTelemetry(self.nc, "nats_service", self.tracer_provider)
-
-        self.pubsub_subscription = await self.ncotel.subscribe(
-            subject="testing.telemetry_nats_pubsub",
-            queue="telemetry_nats_pubsub",
-            cb=self.pubsub_subscription_worker,
-        )
-
-        # Plain nats request-response
-
-        self.reqresp_subscription = await self.ncotel.subscribe(
-            subject="testing.telemetry_nats_reqresp",
-            queue="telemetry_nats_reqresp",
-            cb=self.reqresp_subscription_worker,
-        )
-
-    async def finalize(self):
-        await self.push_subscription.unsubscribe()
-
-        await self.pull_subscription_one.unsubscribe()
-        self.puller_task_one.cancel()
-        await self.js._jsm.delete_consumer(
-            stream="testing", consumer="testing_consumer_one"
-        )
-
-        await self.pubsub_subscription.unsubscribe()
-        await self.nc.drain()
-        await self.nc.close()
-
-    async def SayHello(self, request, context):
-        # Send message to test Jetstream publish and subscribe message
-        await self.jsotel.publish("testing.telemetry", request.name.encode())
-
-        # Send message to test Jetstream pull subscriber one
-        await self.jsotel.publish("testing.telemetry_pull_one", request.name.encode())
-
-        # Test regular nats pubsub
-        await self.ncotel.publish(
-            "testing.telemetry_nats_pubsub", request.name.encode()
-        )
-
-        # Test regular nats request-response
-        await self.ncotel.request(
-            "testing.telemetry_nats_reqresp", request.name.encode()
-        )
-
-        return helloworld_pb2.HelloReply(
-            message=("Hello, %s!" % request.name) * 2_000_000
-        )
-
-
-@pytest.fixture(scope="function")
-async def greeter(set_telemetry_settings, natsd: str):
-    obj = Greeter(natsd)
-    await obj.initialize()
-    yield obj
-    await obj.finalize()
-
-
-@pytest.fixture(scope="function")
-async def greeter_streaming(set_telemetry_settings, natsd: str):
-    obj = GreeterStreaming(natsd)
-    await obj.initialize()
-    yield obj
-    await obj.finalize()
-
-
-@pytest.fixture(scope="function")
-async def grpc_service(
-    telemetry_grpc: GRPCTelemetry,
-    greeter: Greeter,
-    greeter_streaming: GreeterStreaming,
-):
-    server = telemetry_grpc.init_server()
-    helloworld_pb2_grpc.add_GreeterServicer_to_server(greeter, server)
-    hellostreamingworld_pb2_grpc.add_MultiGreeterServicer_to_server(
-        greeter_streaming, server
-    )
-    port = server.add_insecure_port("[::]:0")
-    await server.start()
-    yield port
-    await server.stop(grace=True)
-
-
-@pytest.fixture(scope="function")
-async def http_service(
-    set_telemetry_settings, telemetry_grpc: GRPCTelemetry, grpc_service: int
-):
-    tracer_provider = get_telemetry("HTTP_SERVICE")
-    await init_telemetry(tracer_provider)
-    app = FastAPI(title="Test API")  # type: ignore
-    set_global_textmap(B3MultiFormat())
-    instrument_app(
-        app,
-        tracer_provider=tracer_provider,
-        excluded_urls=[],
-        metrics=True,
-        trace_id_on_responses=True,
-    )
-
-    @app.get("/")
-    async def simple_api():
-        set_info_on_span({"my.data": "is this"})
-        tracer = tracer_provider.get_tracer(__name__)
-        with tracer.start_as_current_span("simple_api_work") as _:
-            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-            stub = helloworld_pb2_grpc.GreeterStub(channel)
-            response = await stub.SayHello(
-                helloworld_pb2.HelloRequest(name="you"),
-                # This metadata is here to make sure our instrumentor handles correctly
-                # requests with metadata, as it does some manipulation of in on start_client_span
-                # The servicer endpoint does not use this metadata
-                metadata=aio.Metadata.from_tuple((("header1", "value1"),)),
-            )
-        with tracer.start_as_current_span("simple_stream_api_work") as _:
-            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-            stub = hellostreamingworld_pb2_grpc.MultiGreeterStub(channel)
-            async for sresponse in stub.sayHello(
-                helloworld_pb2.HelloRequest(name="you")
-            ):
-                assert sresponse
-        return response.message
-
-    client_base_url = "http://test"
-    client = AsyncClient(app=app, base_url=client_base_url)  # type: ignore
-    yield client
-    await clean_telemetry("HTTP_SERVICE")
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import os
+
+import nats
+import pytest
+import requests
+from fastapi import FastAPI
+from grpc import aio  # type: ignore
+from httpx import AsyncClient
+from nats.aio.msg import Msg
+from nats.js import api
+from opentelemetry.propagate import set_global_textmap
+from opentelemetry.propagators.b3 import B3MultiFormat
+from pytest_docker_fixtures import images  # type: ignore
+from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
+
+from nucliadb_telemetry.fastapi import instrument_app
+from nucliadb_telemetry.grpc import GRPCTelemetry
+from nucliadb_telemetry.jetstream import JetStreamContextTelemetry, NatsClientTelemetry
+from nucliadb_telemetry.settings import telemetry_settings
+from nucliadb_telemetry.tests.grpc import (
+    hellostreamingworld_pb2,
+    hellostreamingworld_pb2_grpc,
+    helloworld_pb2,
+    helloworld_pb2_grpc,
+)
+from nucliadb_telemetry.utils import (
+    clean_telemetry,
+    get_telemetry,
+    init_telemetry,
+    set_info_on_span,
+)
+
+images.settings["jaeger"] = {
+    "image": "jaegertracing/all-in-one",
+    "version": "1.33",
+    "options": {"ports": {"6831/udp": None, "16686": None}},
+}
+
+
+class Jaeger(BaseImage):
+    name = "jaeger"
+    port = 6831
+
+    def get_port(self, port=None):
+        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
+            return port if port else self.port
+        network = self.container_obj.attrs["NetworkSettings"]
+        service_port = "{0}/udp".format(port if port else self.port)
+        for netport in network["Ports"].keys():
+            if netport == service_port:
+                return network["Ports"][service_port][0]["HostPort"]
+
+    def get_http_port(self, port=None):
+        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
+            return 16686
+        network = self.container_obj.attrs["NetworkSettings"]
+        service_port = "16686/tcp"
+        for netport in network["Ports"].keys():
+            if netport == service_port:
+                return network["Ports"][service_port][0]["HostPort"]
+
+    def check(self):
+        resp = requests.get(f"http://{self.host}:{self.get_http_port()}")
+        return resp.status_code == 200
+
+
+@pytest.fixture(scope="function")
+async def jaeger_server():
+    server = Jaeger()
+    server.run()
+    yield server
+    server.stop()
+
+
+@pytest.fixture(scope="function")
+async def set_telemetry_settings(jaeger_server: Jaeger):
+    telemetry_settings.jaeger_enabled = True
+    telemetry_settings.jaeger_agent_host = "127.0.0.1"
+    telemetry_settings.jaeger_agent_port = jaeger_server.get_port()
+    telemetry_settings.jaeger_query_port = jaeger_server.get_http_port()
+
+
+@pytest.fixture(scope="function")
+async def telemetry_grpc(set_telemetry_settings):
+    tracer_provider = get_telemetry("GRPC_SERVICE")
+    await init_telemetry(tracer_provider)
+    util = GRPCTelemetry("test_telemetry", tracer_provider)
+    yield util
+
+
+class GreeterStreaming(hellostreamingworld_pb2_grpc.MultiGreeterServicer):
+    def __init__(self, natsd):
+        self.natsd = natsd
+        self.nc = None
+        self.push_subscription = None
+        self.tracer_provider = None
+        self.messages = []
+
+    async def push_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("message_worker")
+        with tracer.start_as_current_span("message_worker_span") as _:
+            self.messages.append(msg)
+
+    async def initialize(self):
+        self.nc = await nats.connect(servers=[self.natsd])
+        self.js = self.nc.jetstream()
+
+        try:
+            await self.js.stream_info("testing")
+        except nats.js.errors.NotFoundError:
+            await self.js.add_stream(name="testing", subjects=["testing.*"])
+
+        self.tracer_provider = get_telemetry("NATS_SERVICE")
+        await init_telemetry(self.tracer_provider)
+        self.jsotel = JetStreamContextTelemetry(
+            self.js, "nats_service", self.tracer_provider
+        )
+
+        self.push_subscription = await self.jsotel.subscribe(
+            subject="testing.stelemetry",
+            stream="testing",
+            cb=self.push_subscription_worker,
+        )
+
+    async def finalize(self):
+        await self.push_subscription.unsubscribe()
+        await self.nc.drain()
+        await self.nc.close()
+
+    async def sayHello(self, request, context):
+        await self.jsotel.publish("testing.stelemetry", request.name.encode())
+        for _ in range(10):
+            yield hellostreamingworld_pb2.HelloReply(
+                message="Hello, %s!" % request.name
+            )
+
+
+class Greeter(helloworld_pb2_grpc.GreeterServicer):
+    def __init__(self, natsd):
+        self.natsd = natsd
+        self.nc = None
+        self.push_subscription = None
+        self.pull_subscription_one = None
+        self.puller_task = None
+        self.pubsub_subscription = None
+        self.tracer_provider = None
+        self.messages = []
+        self.puller_task_one = None
+
+    async def push_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("message_worker")
+        with tracer.start_as_current_span("message_worker_span") as _:
+            self.messages.append(msg)
+
+    async def pull_subscription_worker_one(self):
+        async def callback(message):
+            self.messages.append(message)
+
+        await self.jsotel.pull_one(self.pull_subscription_one, callback)
+
+    async def pubsub_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("pubsub_worker")
+        with tracer.start_as_current_span("pubsub_worker_span") as _:
+            self.messages.append(msg)
+
+    async def reqresp_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("reqresp_worker")
+        with tracer.start_as_current_span("reqresp_worker_span") as _:
+            self.messages.append(msg)
+            await msg.respond(b"Bye Bye!")
+
+    async def initialize(self):
+        self.nc = await nats.connect(servers=[self.natsd])
+        self.js = self.nc.jetstream()
+
+        try:
+            await self.js.stream_info("testing")
+        except nats.js.errors.NotFoundError:
+            await self.js.add_stream(name="testing", subjects=["testing.*"])
+
+        self.tracer_provider = get_telemetry("NATS_SERVICE")
+        await init_telemetry(self.tracer_provider)
+        self.jsotel = JetStreamContextTelemetry(
+            self.js, "nats_service", self.tracer_provider
+        )
+
+        self.push_subscription = await self.jsotel.subscribe(
+            subject="testing.telemetry",
+            stream="testing",
+            cb=self.push_subscription_worker,
+        )
+
+        # Nats Jetstream Pull subscription including consumer creation
+        # and task to pull one message
+        config = api.ConsumerConfig()
+        config.filter_subject = "testing.telemetry_pull_one"
+        config.durable_name = "testing_consumer_one"
+        await self.js._jsm.add_consumer(stream="testing", config=config)
+
+        self.pull_subscription_one = await self.jsotel.pull_subscribe(
+            subject=config.filter_subject, durable=config.durable_name, stream="testing"
+        )
+
+        self.puller_task_one = asyncio.create_task(self.pull_subscription_worker_one())
+
+        # Plain nats instrumentation and subscription
+        # (no streams neither consumers used here)
+        self.ncotel = NatsClientTelemetry(self.nc, "nats_service", self.tracer_provider)
+
+        self.pubsub_subscription = await self.ncotel.subscribe(
+            subject="testing.telemetry_nats_pubsub",
+            queue="telemetry_nats_pubsub",
+            cb=self.pubsub_subscription_worker,
+        )
+
+        # Plain nats request-response
+
+        self.reqresp_subscription = await self.ncotel.subscribe(
+            subject="testing.telemetry_nats_reqresp",
+            queue="telemetry_nats_reqresp",
+            cb=self.reqresp_subscription_worker,
+        )
+
+    async def finalize(self):
+        await self.push_subscription.unsubscribe()
+
+        await self.pull_subscription_one.unsubscribe()
+        self.puller_task_one.cancel()
+        await self.js._jsm.delete_consumer(
+            stream="testing", consumer="testing_consumer_one"
+        )
+
+        await self.pubsub_subscription.unsubscribe()
+        await self.nc.drain()
+        await self.nc.close()
+
+    async def SayHello(self, request, context):
+        # Send message to test Jetstream publish and subscribe message
+        await self.jsotel.publish("testing.telemetry", request.name.encode())
+
+        # Send message to test Jetstream pull subscriber one
+        await self.jsotel.publish("testing.telemetry_pull_one", request.name.encode())
+
+        # Test regular nats pubsub
+        await self.ncotel.publish(
+            "testing.telemetry_nats_pubsub", request.name.encode()
+        )
+
+        # Test regular nats request-response
+        await self.ncotel.request(
+            "testing.telemetry_nats_reqresp", request.name.encode()
+        )
+
+        return helloworld_pb2.HelloReply(
+            message=("Hello, %s!" % request.name) * 2_000_000
+        )
+
+
+@pytest.fixture(scope="function")
+async def greeter(set_telemetry_settings, natsd: str):
+    obj = Greeter(natsd)
+    await obj.initialize()
+    yield obj
+    await obj.finalize()
+
+
+@pytest.fixture(scope="function")
+async def greeter_streaming(set_telemetry_settings, natsd: str):
+    obj = GreeterStreaming(natsd)
+    await obj.initialize()
+    yield obj
+    await obj.finalize()
+
+
+@pytest.fixture(scope="function")
+async def grpc_service(
+    telemetry_grpc: GRPCTelemetry,
+    greeter: Greeter,
+    greeter_streaming: GreeterStreaming,
+):
+    server = telemetry_grpc.init_server()
+    helloworld_pb2_grpc.add_GreeterServicer_to_server(greeter, server)
+    hellostreamingworld_pb2_grpc.add_MultiGreeterServicer_to_server(
+        greeter_streaming, server
+    )
+    port = server.add_insecure_port("[::]:0")
+    await server.start()
+    yield port
+    await server.stop(grace=True)
+
+
+@pytest.fixture(scope="function")
+async def http_service(
+    set_telemetry_settings, telemetry_grpc: GRPCTelemetry, grpc_service: int
+):
+    tracer_provider = get_telemetry("HTTP_SERVICE")
+    await init_telemetry(tracer_provider)
+    app = FastAPI(title="Test API")  # type: ignore
+    set_global_textmap(B3MultiFormat())
+    instrument_app(
+        app,
+        tracer_provider=tracer_provider,
+        excluded_urls=[],
+        metrics=True,
+        trace_id_on_responses=True,
+    )
+
+    @app.get("/")
+    async def simple_api():
+        set_info_on_span({"my.data": "is this"})
+        tracer = tracer_provider.get_tracer(__name__)
+        with tracer.start_as_current_span("simple_api_work") as _:
+            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+            stub = helloworld_pb2_grpc.GreeterStub(channel)
+            response = await stub.SayHello(
+                helloworld_pb2.HelloRequest(name="you"),
+                # This metadata is here to make sure our instrumentor handles correctly
+                # requests with metadata, as it does some manipulation of in on start_client_span
+                # The servicer endpoint does not use this metadata
+                metadata=aio.Metadata.from_tuple((("header1", "value1"),)),
+            )
+        with tracer.start_as_current_span("simple_stream_api_work") as _:
+            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+            stub = hellostreamingworld_pb2_grpc.MultiGreeterStub(channel)
+            async for sresponse in stub.sayHello(
+                helloworld_pb2.HelloRequest(name="you")
+            ):
+                assert sresponse
+        return response.message
+
+    client_base_url = "http://test"
+    client = AsyncClient(app=app, base_url=client_base_url)  # type: ignore
+    yield client
+    await clean_telemetry("HTTP_SERVICE")
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_context.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_context.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from unittest.mock import AsyncMock
-
-import pytest
-from fastapi import FastAPI
-
-from nucliadb_telemetry import context
-from nucliadb_telemetry.fastapi.context import ContextInjectorMiddleware
-
-app = FastAPI()
-
-
-@app.get("/api/v1/kb/{kbid}")
-def get_kb(kbid: str):
-    return {"kbid": kbid}
-
-
-@pytest.mark.asyncio
-async def test_context_injected():
-    scope = {
-        "app": app,
-        "path": "/api/v1/kb/123",
-        "method": "GET",
-        "type": "http",
-    }
-
-    mdlw = ContextInjectorMiddleware(app)
-
-    found_ctx = {}
-
-    async def receive(*args, **kwargs):
-        found_ctx.update(context.get_context())
-        return {
-            "type": "http.disconnect",
-        }
-
-    await mdlw(scope, receive, AsyncMock())
-
-    assert found_ctx == {"kbid": "123"}
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from unittest.mock import AsyncMock
+
+import pytest
+from fastapi import FastAPI
+
+from nucliadb_telemetry import context
+from nucliadb_telemetry.fastapi.context import ContextInjectorMiddleware
+
+app = FastAPI()
+
+
+@app.get("/api/v1/kb/{kbid}")
+def get_kb(kbid: str):
+    return {"kbid": kbid}
+
+
+@pytest.mark.asyncio
+async def test_context_injected():
+    scope = {
+        "app": app,
+        "path": "/api/v1/kb/123",
+        "method": "GET",
+        "type": "http",
+    }
+
+    mdlw = ContextInjectorMiddleware(app)
+
+    found_ctx = {}
+
+    async def receive(*args, **kwargs):
+        found_ctx.update(context.get_context())
+        return {
+            "type": "http.disconnect",
+        }
+
+    await mdlw(scope, receive, AsyncMock())
+
+    assert found_ctx == {"kbid": "123"}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_fastapi.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_fastapi.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from unittest.mock import Mock
-
-from nucliadb_telemetry.fastapi import instrument_app
-from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
-
-
-def test_instrument_app_adds_capture_trace_id_middleware():
-    app = Mock()
-    instrument_app(app, [])
-    for middleware_call in app.add_middleware.call_args_list:
-        assert middleware_call[0][0] != CaptureTraceIdMiddleware
-
-    app = Mock()
-    instrument_app(app, [], trace_id_on_responses=True)
-    assert app.add_middleware.call_args_list[0][0][0] == CaptureTraceIdMiddleware
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from unittest.mock import Mock
+
+from nucliadb_telemetry.fastapi import instrument_app
+from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
+
+
+def test_instrument_app_adds_capture_trace_id_middleware():
+    app = Mock()
+    instrument_app(app, [])
+    for middleware_call in app.add_middleware.call_args_list:
+        assert middleware_call[0][0] != CaptureTraceIdMiddleware
+
+    app = Mock()
+    instrument_app(app, [], trace_id_on_responses=True)
+    assert app.add_middleware.call_args_list[0][0][0] == CaptureTraceIdMiddleware
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_tracing.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_tracing.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,64 +1,64 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from unittest import mock
-
-import pytest
-from opentelemetry.trace import format_trace_id
-
-from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
-
-
-@pytest.fixture(scope="function")
-def trace_id():
-    tid = 123
-    context = mock.Mock()
-    context.trace_id = tid
-    current_span = mock.Mock()
-    current_span.get_span_context = mock.Mock(return_value=context)
-    with mock.patch(
-        "nucliadb_telemetry.fastapi.tracing.trace.get_current_span",
-        return_value=current_span,
-    ):
-        yield tid
-
-
-async def test_capture_trace_id_middleware(trace_id):
-    request = mock.Mock()
-    response = mock.Mock(headers={})
-    call_next = mock.AsyncMock(return_value=response)
-
-    mdw = CaptureTraceIdMiddleware(mock.Mock())
-    response = await mdw.dispatch(request, call_next)
-
-    assert response.headers["X-NUCLIA-TRACE-ID"] == format_trace_id(trace_id)
-    assert response.headers["Access-Control-Expose-Headers"] == "X-NUCLIA-TRACE-ID"
-
-
-async def test_capture_trace_id_middleware_appends_trace_id_header_to_exposed(trace_id):
-    request = mock.Mock()
-    response = mock.Mock(headers={"Access-Control-Expose-Headers": "Foo-Bar,X-Header"})
-    call_next = mock.AsyncMock(return_value=response)
-
-    mdw = CaptureTraceIdMiddleware(mock.Mock())
-    response = await mdw.dispatch(request, call_next)
-
-    assert (
-        response.headers["Access-Control-Expose-Headers"]
-        == "Foo-Bar,X-Header,X-NUCLIA-TRACE-ID"
-    )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from unittest import mock
+
+import pytest
+from opentelemetry.trace import format_trace_id
+
+from nucliadb_telemetry.fastapi.tracing import CaptureTraceIdMiddleware
+
+
+@pytest.fixture(scope="function")
+def trace_id():
+    tid = 123
+    context = mock.Mock()
+    context.trace_id = tid
+    current_span = mock.Mock()
+    current_span.get_span_context = mock.Mock(return_value=context)
+    with mock.patch(
+        "nucliadb_telemetry.fastapi.tracing.trace.get_current_span",
+        return_value=current_span,
+    ):
+        yield tid
+
+
+async def test_capture_trace_id_middleware(trace_id):
+    request = mock.Mock()
+    response = mock.Mock(headers={})
+    call_next = mock.AsyncMock(return_value=response)
+
+    mdw = CaptureTraceIdMiddleware(mock.Mock())
+    response = await mdw.dispatch(request, call_next)
+
+    assert response.headers["X-NUCLIA-TRACE-ID"] == format_trace_id(trace_id)
+    assert response.headers["Access-Control-Expose-Headers"] == "X-NUCLIA-TRACE-ID"
+
+
+async def test_capture_trace_id_middleware_appends_trace_id_header_to_exposed(trace_id):
+    request = mock.Mock()
+    response = mock.Mock(headers={"Access-Control-Expose-Headers": "Foo-Bar,X-Header"})
+    call_next = mock.AsyncMock(return_value=response)
+
+    mdw = CaptureTraceIdMiddleware(mock.Mock())
+    response = await mdw.dispatch(request, call_next)
+
+    assert (
+        response.headers["Access-Control-Expose-Headers"]
+        == "Foo-Bar,X-Header,X-NUCLIA-TRACE-ID"
+    )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_utils.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/fastapi/test_utils.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-from fastapi import FastAPI
-
-from nucliadb_telemetry.fastapi import utils
-
-app = FastAPI()
-
-
-@app.get("/api/v1/kb/{kbid}")
-def get_kb(kbid: str):
-    return {"kbid": kbid}
-
-
-def test_get_path_template():
-    scope = {
-        "app": app,
-        "path": "/api/v1/kb/123",
-        "method": "GET",
-        "type": "http",
-    }
-
-    path_template = utils.get_path_template(scope)
-    assert path_template.path == "/api/v1/kb/{kbid}"
-    assert path_template.match is True
-    assert path_template.scope["path_params"] == {"kbid": "123"}
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+from fastapi import FastAPI
+
+from nucliadb_telemetry.fastapi import utils
+
+app = FastAPI()
+
+
+@app.get("/api/v1/kb/{kbid}")
+def get_kb(kbid: str):
+    return {"kbid": kbid}
+
+
+def test_get_path_template():
+    scope = {
+        "app": app,
+        "path": "/api/v1/kb/123",
+        "method": "GET",
+        "type": "http",
+    }
+
+    path_template = utils.get_path_template(scope)
+    assert path_template.path == "/api/v1/kb/{kbid}"
+    assert path_template.match is True
+    assert path_template.scope["path_params"] == {"kbid": "123"}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_context.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_context.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-
-import pytest
-
-from nucliadb_telemetry import context
-
-
-@pytest.mark.asyncio
-async def test_logger_with_context(caplog):
-    context_lvl_1 = {}
-    context_lvl_1_after = {}
-    context_lvl_2 = {}
-    context_lvl_2_after = {}
-    context_lvl_3 = {}
-
-    async def task3():
-        context.add_context({"task3": "value", "foo": "daz"})
-        context_lvl_3.update(context.get_context())
-
-    async def task2():
-        context.add_context({"task2": "value", "foo": "baz"})
-        context_lvl_2.update(context.get_context())
-        await asyncio.create_task(task3())
-        context_lvl_2_after.update(context.get_context())
-
-    async def task1():
-        context.add_context({"task1": "value", "foo": "bar"})
-        context_lvl_1.update(context.get_context())
-        await asyncio.create_task(task2())
-        context_lvl_1_after.update(context.get_context())
-
-    await asyncio.create_task(task1())
-
-    assert context_lvl_1 == {
-        "task1": "value",
-        "foo": "bar",
-    }
-    assert context_lvl_1 == context_lvl_1_after
-
-    assert context_lvl_2 == {
-        "task1": "value",
-        "task2": "value",
-        "foo": "baz",
-    }
-    assert context_lvl_2 == context_lvl_2_after
-
-    assert context_lvl_3 == {
-        "task1": "value",
-        "task2": "value",
-        "task3": "value",
-        "foo": "daz",
-    }
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+
+import pytest
+
+from nucliadb_telemetry import context
+
+
+@pytest.mark.asyncio
+async def test_logger_with_context(caplog):
+    context_lvl_1 = {}
+    context_lvl_1_after = {}
+    context_lvl_2 = {}
+    context_lvl_2_after = {}
+    context_lvl_3 = {}
+
+    async def task3():
+        context.add_context({"task3": "value", "foo": "daz"})
+        context_lvl_3.update(context.get_context())
+
+    async def task2():
+        context.add_context({"task2": "value", "foo": "baz"})
+        context_lvl_2.update(context.get_context())
+        await asyncio.create_task(task3())
+        context_lvl_2_after.update(context.get_context())
+
+    async def task1():
+        context.add_context({"task1": "value", "foo": "bar"})
+        context_lvl_1.update(context.get_context())
+        await asyncio.create_task(task2())
+        context_lvl_1_after.update(context.get_context())
+
+    await asyncio.create_task(task1())
+
+    assert context_lvl_1 == {
+        "task1": "value",
+        "foo": "bar",
+    }
+    assert context_lvl_1 == context_lvl_1_after
+
+    assert context_lvl_2 == {
+        "task1": "value",
+        "task2": "value",
+        "foo": "baz",
+    }
+    assert context_lvl_2 == context_lvl_2_after
+
+    assert context_lvl_3 == {
+        "task1": "value",
+        "task2": "value",
+        "task3": "value",
+        "foo": "daz",
+    }
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,97 +1,97 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from unittest.mock import patch
-
-from nucliadb_telemetry import errors
-
-
-def test_capture_exception() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        ex = Exception("test")
-        errors.capture_exception(ex)
-        mock_sentry_sdk.capture_exception.assert_called_once_with(ex)
-
-
-def test_capture_exception_no_sentry() -> None:
-    with patch.object(errors, "SENTRY", False), patch(
-        "nucliadb_telemetry.errors.sentry_sdk"
-    ) as mock_sentry_sdk:
-        errors.capture_exception(Exception())
-        mock_sentry_sdk.capture_exception.assert_not_called()
-
-
-def test_capture_message() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        errors.capture_message("error_msg", "level", "scope")
-        mock_sentry_sdk.capture_message.assert_called_once_with(
-            "error_msg", "level", "scope"
-        )
-
-
-def test_capture_message_no_sentry() -> None:
-    with patch.object(errors, "SENTRY", False), patch(
-        "nucliadb_telemetry.errors.sentry_sdk"
-    ) as mock_sentry_sdk:
-        errors.capture_message("error_msg", "level", "scope")
-        mock_sentry_sdk.capture_message.assert_not_called()
-
-
-def test_setup_error_handling(monkeypatch):
-    monkeypatch.setenv("sentry_url", "sentry_url")
-    monkeypatch.setenv("environment", "environment")
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        errors.setup_error_handling("1.0.0")
-        mock_sentry_sdk.init.assert_called_once_with(
-            release="1.0.0",
-            environment="environment",
-            dsn="sentry_url",
-            integrations=[],
-            default_integrations=False,
-        )
-
-
-def test_setup_error_handling_no_sentry(monkeypatch):
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk:
-        errors.setup_error_handling("1.0.0")
-        mock_sentry_sdk.init.assert_not_called()
-
-
-def test_push_scope() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        with errors.push_scope() as scope:
-            scope.set_extra("key", "value")
-        mock_sentry_sdk.push_scope.assert_called_once_with()
-
-
-def test_push_scope_no_sentry() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", False
-    ):
-        with errors.push_scope() as scope:
-            scope.set_extra("key", "value")
-        mock_sentry_sdk.push_scope.assert_not_called()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from unittest.mock import patch
+
+from nucliadb_telemetry import errors
+
+
+def test_capture_exception() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        ex = Exception("test")
+        errors.capture_exception(ex)
+        mock_sentry_sdk.capture_exception.assert_called_once_with(ex)
+
+
+def test_capture_exception_no_sentry() -> None:
+    with patch.object(errors, "SENTRY", False), patch(
+        "nucliadb_telemetry.errors.sentry_sdk"
+    ) as mock_sentry_sdk:
+        errors.capture_exception(Exception())
+        mock_sentry_sdk.capture_exception.assert_not_called()
+
+
+def test_capture_message() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        errors.capture_message("error_msg", "level", "scope")
+        mock_sentry_sdk.capture_message.assert_called_once_with(
+            "error_msg", "level", "scope"
+        )
+
+
+def test_capture_message_no_sentry() -> None:
+    with patch.object(errors, "SENTRY", False), patch(
+        "nucliadb_telemetry.errors.sentry_sdk"
+    ) as mock_sentry_sdk:
+        errors.capture_message("error_msg", "level", "scope")
+        mock_sentry_sdk.capture_message.assert_not_called()
+
+
+def test_setup_error_handling(monkeypatch):
+    monkeypatch.setenv("sentry_url", "sentry_url")
+    monkeypatch.setenv("environment", "environment")
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        errors.setup_error_handling("1.0.0")
+        mock_sentry_sdk.init.assert_called_once_with(
+            release="1.0.0",
+            environment="environment",
+            dsn="sentry_url",
+            integrations=[],
+            default_integrations=False,
+        )
+
+
+def test_setup_error_handling_no_sentry(monkeypatch):
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk:
+        errors.setup_error_handling("1.0.0")
+        mock_sentry_sdk.init.assert_not_called()
+
+
+def test_push_scope() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        with errors.push_scope() as scope:
+            scope.set_extra("key", "value")
+        mock_sentry_sdk.push_scope.assert_called_once_with()
+
+
+def test_push_scope_no_sentry() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", False
+    ):
+        with errors.push_scope() as scope:
+            scope.set_extra("key", "value")
+        mock_sentry_sdk.push_scope.assert_not_called()
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_logs.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_logs.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import logging
-from unittest.mock import MagicMock, patch
-
-import orjson
-import pydantic
-import pytest
-from opentelemetry.trace import format_span_id, format_trace_id
-
-from nucliadb_telemetry import context, logs
-
-
-def test_setup_logging(monkeypatch):
-    monkeypatch.setenv("LOG_LEVEL", "INFO")
-    monkeypatch.setenv("LOGGER_LEVELS", '{"foo": "WARNING"}')
-    with patch("nucliadb_telemetry.logs.logging") as logging:
-        logs.setup_logging()
-
-        logging.getLogger.assert_any_call("foo")
-        assert len(logging.getLogger().addHandler.mock_calls) == 5
-
-        logger = logging.getLogger()
-        handler = logger.addHandler.mock_calls[0].args[0]
-        assert isinstance(
-            handler.setFormatter.mock_calls[0].args[0], logs.JSONFormatter
-        )
-
-
-class _TestLogMessage(pydantic.BaseModel):
-    message: str
-    foo: str
-    bar: int
-
-
-def test_logger_with_formatter(caplog):
-    logger = logging.getLogger("test.logger")
-    formatter = logs.JSONFormatter()
-
-    outputted_records = []
-
-    class Handler(logging.Handler):
-        def emit(self, record):
-            msg = self.format(record)
-            data = orjson.loads(msg)
-            outputted_records.append(data)
-            assert data["foo"] == "bar", msg
-            assert data["bar"] == 42, msg
-            assert data["message"] == "foobar", msg
-
-    handler = Handler()
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-
-    logger.setLevel(logging.ERROR)
-    logger.propagate = False
-
-    logger.error("foobar", extra={"foo": "bar", "bar": 42})
-    logger.error({"message": "foobar", "foo": "bar", "bar": 42})
-    logger.error(_TestLogMessage(message="foobar", foo="bar", bar=42))
-
-    assert len(outputted_records) == 3
-
-
-def test_logger_with_access_formatter(caplog):
-    logger = logging.getLogger("test.logger2")
-    formatter = logs.UvicornAccessFormatter()
-
-    outputted_records = []
-
-    class Handler(logging.Handler):
-        def emit(self, record):
-            msg = self.format(record)
-            data = orjson.loads(msg)
-            outputted_records.append(data)
-
-    handler = Handler()
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-
-    logger.setLevel(logging.ERROR)
-    logger.propagate = False
-
-    logger.error(
-        '%s - "%s %s HTTP/%s" %d',
-        "client_addr",
-        "method",
-        "full_path",
-        "http_version",
-        200,
-    )
-
-    assert len(outputted_records) == 1
-
-    assert outputted_records[0]["httpRequest"] == {
-        "requestMethod": "method",
-        "requestUrl": "full_path",
-        "status": 200,
-        "remoteIp": "client_addr",
-        "protocol": "http_version",
-    }
-
-
-def test_logger_with_formatter_and_active_span(caplog):
-    logger = logging.getLogger("test.logger3")
-    formatter = logs.JSONFormatter()
-
-    outputted_records = []
-
-    class Handler(logging.Handler):
-        def emit(self, record):
-            msg = self.format(record)
-            data = orjson.loads(msg)
-            outputted_records.append(data)
-
-    handler = Handler()
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-
-    logger.setLevel(logging.ERROR)
-    logger.propagate = False
-
-    span = MagicMock()
-    span.get_span_context.return_value = MagicMock(
-        # make sure to give potential very large numbers to make sure they are
-        # serializable
-        trace_id=9999999999999999999999,
-        span_id=9999999999999999999999,
-    )
-    with patch("nucliadb_telemetry.logs.trace.get_current_span", return_value=span):
-        logger.error("foobar")
-
-    assert len(outputted_records) == 1
-    assert outputted_records[0]["trace_id"] == format_trace_id(9999999999999999999999)
-    assert outputted_records[0]["span_id"] == format_span_id(9999999999999999999999)
-
-
-@pytest.mark.asyncio
-async def test_logger_with_context(caplog):
-    logger = logging.getLogger("test.logger4")
-    formatter = logs.JSONFormatter()
-
-    outputted_records = []
-
-    class Handler(logging.Handler):
-        def emit(self, record):
-            msg = self.format(record)
-            data = orjson.loads(msg)
-            outputted_records.append(data)
-
-    handler = Handler()
-    handler.setFormatter(formatter)
-    logger.addHandler(handler)
-
-    logger.setLevel(logging.ERROR)
-    logger.propagate = False
-
-    async def task2():
-        context.add_context({"task2": "value", "foo": "baz"})
-        logger.error("baz")
-
-    async def task1():
-        context.add_context({"task1": "value", "foo": "bar"})
-        logger.error("bar")
-        await asyncio.create_task(task2())
-
-    await asyncio.create_task(task1())
-    assert len(outputted_records) == 2
-
-    assert outputted_records[0]["context"] == {
-        "task1": "value",
-        "foo": "bar",
-    }
-    assert outputted_records[1]["context"] == {
-        "task1": "value",
-        "task2": "value",
-        "foo": "baz",
-    }
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import logging
+from unittest.mock import MagicMock, patch
+
+import orjson
+import pydantic
+import pytest
+from opentelemetry.trace import format_span_id, format_trace_id
+
+from nucliadb_telemetry import context, logs
+
+
+def test_setup_logging(monkeypatch):
+    monkeypatch.setenv("LOG_LEVEL", "INFO")
+    monkeypatch.setenv("LOGGER_LEVELS", '{"foo": "WARNING"}')
+    with patch("nucliadb_telemetry.logs.logging") as logging:
+        logs.setup_logging()
+
+        logging.getLogger.assert_any_call("foo")
+        assert len(logging.getLogger().addHandler.mock_calls) == 5
+
+        logger = logging.getLogger()
+        handler = logger.addHandler.mock_calls[0].args[0]
+        assert isinstance(
+            handler.setFormatter.mock_calls[0].args[0], logs.JSONFormatter
+        )
+
+
+class _TestLogMessage(pydantic.BaseModel):
+    message: str
+    foo: str
+    bar: int
+
+
+def test_logger_with_formatter(caplog):
+    logger = logging.getLogger("test.logger")
+    formatter = logs.JSONFormatter()
+
+    outputted_records = []
+
+    class Handler(logging.Handler):
+        def emit(self, record):
+            msg = self.format(record)
+            data = orjson.loads(msg)
+            outputted_records.append(data)
+            assert data["foo"] == "bar", msg
+            assert data["bar"] == 42, msg
+            assert data["message"] == "foobar", msg
+
+    handler = Handler()
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+
+    logger.setLevel(logging.ERROR)
+    logger.propagate = False
+
+    logger.error("foobar", extra={"foo": "bar", "bar": 42})
+    logger.error({"message": "foobar", "foo": "bar", "bar": 42})
+    logger.error(_TestLogMessage(message="foobar", foo="bar", bar=42))
+
+    assert len(outputted_records) == 3
+
+
+def test_logger_with_access_formatter(caplog):
+    logger = logging.getLogger("test.logger2")
+    formatter = logs.UvicornAccessFormatter()
+
+    outputted_records = []
+
+    class Handler(logging.Handler):
+        def emit(self, record):
+            msg = self.format(record)
+            data = orjson.loads(msg)
+            outputted_records.append(data)
+
+    handler = Handler()
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+
+    logger.setLevel(logging.ERROR)
+    logger.propagate = False
+
+    logger.error(
+        '%s - "%s %s HTTP/%s" %d',
+        "client_addr",
+        "method",
+        "full_path",
+        "http_version",
+        200,
+    )
+
+    assert len(outputted_records) == 1
+
+    assert outputted_records[0]["httpRequest"] == {
+        "requestMethod": "method",
+        "requestUrl": "full_path",
+        "status": 200,
+        "remoteIp": "client_addr",
+        "protocol": "http_version",
+    }
+
+
+def test_logger_with_formatter_and_active_span(caplog):
+    logger = logging.getLogger("test.logger3")
+    formatter = logs.JSONFormatter()
+
+    outputted_records = []
+
+    class Handler(logging.Handler):
+        def emit(self, record):
+            msg = self.format(record)
+            data = orjson.loads(msg)
+            outputted_records.append(data)
+
+    handler = Handler()
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+
+    logger.setLevel(logging.ERROR)
+    logger.propagate = False
+
+    span = MagicMock()
+    span.get_span_context.return_value = MagicMock(
+        # make sure to give potential very large numbers to make sure they are
+        # serializable
+        trace_id=9999999999999999999999,
+        span_id=9999999999999999999999,
+    )
+    with patch("nucliadb_telemetry.logs.trace.get_current_span", return_value=span):
+        logger.error("foobar")
+
+    assert len(outputted_records) == 1
+    assert outputted_records[0]["trace_id"] == format_trace_id(9999999999999999999999)
+    assert outputted_records[0]["span_id"] == format_span_id(9999999999999999999999)
+
+
+@pytest.mark.asyncio
+async def test_logger_with_context(caplog):
+    logger = logging.getLogger("test.logger4")
+    formatter = logs.JSONFormatter()
+
+    outputted_records = []
+
+    class Handler(logging.Handler):
+        def emit(self, record):
+            msg = self.format(record)
+            data = orjson.loads(msg)
+            outputted_records.append(data)
+
+    handler = Handler()
+    handler.setFormatter(formatter)
+    logger.addHandler(handler)
+
+    logger.setLevel(logging.ERROR)
+    logger.propagate = False
+
+    async def task2():
+        context.add_context({"task2": "value", "foo": "baz"})
+        logger.error("baz")
+
+    async def task1():
+        context.add_context({"task1": "value", "foo": "bar"})
+        logger.error("bar")
+        await asyncio.create_task(task2())
+
+    await asyncio.create_task(task1())
+    assert len(outputted_records) == 2
+
+    assert outputted_records[0]["context"] == {
+        "task1": "value",
+        "foo": "bar",
+    }
+    assert outputted_records[1]["context"] == {
+        "task1": "value",
+        "task2": "value",
+        "foo": "baz",
+    }
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,217 +1,217 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-import asyncio
-import time
-from unittest.mock import MagicMock, patch
-
-import pytest
-
-from nucliadb_telemetry import metrics
-
-
-class TestObserver:
-    @pytest.fixture(autouse=True)
-    def histogram(self):
-        mock = MagicMock()
-        with patch(
-            "nucliadb_telemetry.metrics.prometheus_client.Histogram", return_value=mock
-        ):
-            yield mock
-
-    @pytest.fixture(autouse=True)
-    def counter(self):
-        mock = MagicMock()
-        with patch(
-            "nucliadb_telemetry.metrics.prometheus_client.Counter", return_value=mock
-        ):
-            yield mock
-
-    def test_observer(self, histogram, counter):
-        observer = metrics.Observer(
-            "my_metric", buckets=[1, 2, 3], labels={"foo": "bar"}
-        )
-        with observer(labels={"foo": "baz"}):
-            pass
-
-        histogram.labels.assert_called_once_with(foo="baz")
-        histogram.labels().observe.assert_called_once()
-        counter.labels.assert_called_once_with(status=metrics.OK, foo="baz")
-        counter.labels().inc.assert_called_once()
-
-    def test_observer_error_labels(self, histogram, counter):
-        class MyError(Exception):
-            pass
-
-        observer = metrics.Observer("my_metric", error_mappings={"my_error": MyError})
-        with pytest.raises(MyError), observer():
-            raise MyError("my_error")
-
-        histogram.observe.assert_called_once()
-        counter.labels.assert_called_once_with(status="my_error")
-        counter.labels().inc.assert_called_once()
-
-    def test_sync_decorator(self, histogram, counter):
-        observer = metrics.Observer("my_metric")
-
-        @observer.wrap()
-        def my_func():
-            pass
-
-        my_func()
-
-        histogram.observe.assert_called_once()
-        counter.labels().inc.assert_called_once()
-
-    @pytest.mark.asyncio
-    async def test_async_decorator(self, histogram, counter):
-        observer = metrics.Observer("my_metric")
-
-        @observer.wrap()
-        async def my_func():
-            pass
-
-        await my_func()
-
-        histogram.observe.assert_called_once()
-        counter.labels().inc.assert_called_once()
-
-    def test_gen_decorator(self, histogram, counter):
-        observer = metrics.Observer("my_metric")
-
-        @observer.wrap()
-        def my_func():
-            for i in range(1):
-                time.sleep(0.2)
-                yield i
-
-        for _ in my_func():
-            pass
-
-        histogram.observe.assert_called_once()
-        assert histogram.observe.call_args[0][0] >= 0.2
-        counter.labels().inc.assert_called_once()
-
-    @pytest.mark.asyncio
-    async def test_async_gen_decorator(self, histogram, counter):
-        observer = metrics.Observer("my_metric")
-
-        @observer.wrap()
-        async def my_func():
-            for i in range(1):
-                await asyncio.sleep(0.2)
-                yield i
-
-        async for _ in my_func():
-            pass
-
-        histogram.observe.assert_called_once()
-        assert histogram.observe.call_args[0][0] >= 0.2
-        counter.labels().inc.assert_called_once()
-
-    def test_observer_with_env(self, histogram, counter, monkeypatch):
-        monkeypatch.setenv("VERSION", "1.0.0")
-        observer = metrics.Observer(
-            "my_metric", buckets=[1, 2, 3], labels={"foo": "bar"}
-        )
-        with observer(labels={"foo": "baz"}):
-            pass
-
-        histogram.labels.assert_called_once_with(foo="baz", version="1.0.0")
-        histogram.labels().observe.assert_called_once()
-        counter.labels.assert_called_once_with(
-            status=metrics.OK, foo="baz", version="1.0.0"
-        )
-        counter.labels().inc.assert_called_once()
-
-
-class TestGauge:
-    def test_guage(self):
-        gauge = metrics.Gauge("my_guage")
-        gauge.set(5)
-
-        assert gauge.gauge._value.get() == 5.0
-
-    def test_guage_with_labels(self):
-        gauge = metrics.Gauge("my_guage2", labels={"foo": "", "bar": ""})
-        gauge.set(5, labels={"foo": "baz", "bar": "qux"})
-
-        assert gauge.gauge.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 5.0
-
-        gauge.remove({"foo": "baz", "bar": "qux"})
-
-        assert gauge.gauge.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 0.0
-
-    def test_guage_with_env_label(self, monkeypatch):
-        monkeypatch.setenv("VERSION", "1.0.0")
-        gauge = metrics.Gauge("my_guage3")
-        gauge.set(5)
-
-        assert gauge.gauge.labels(**{"version": "1.0.0"})._value.get() == 5.0
-
-
-class TestCounter:
-    def test_counter(self):
-        counter = metrics.Counter("my_counter")
-        counter.inc()
-
-        assert counter.counter._value.get() == 1.0
-
-    def test_counter_with_labels(self):
-        counter = metrics.Counter("my_counter2", labels={"foo": "", "bar": ""})
-        counter.inc(labels={"foo": "baz", "bar": "qux"})
-
-        assert (
-            counter.counter.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 1.0
-        )
-
-    def test_counter_with_env_label(self, monkeypatch):
-        monkeypatch.setenv("VERSION", "1.0.0")
-        counter = metrics.Counter("my_counter3")
-        counter.inc(labels={"version": "1.0.0"})
-
-        assert counter.counter.labels(**{"version": "1.0.0"})._value.get() == 1.0
-
-
-class TestHistogram:
-    def test_histo(self):
-        histo = metrics.Histogram("my_histo")
-        histo.observe(5)
-
-        assert [
-            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "5.0"
-        ][0].value == 1.0
-
-    def test_histo_with_labels(self):
-        histo = metrics.Histogram(
-            "my_histo2", labels={"foo": "", "bar": ""}, buckets=[1, 2, 3]
-        )
-        histo.observe(1, labels={"foo": "baz", "bar": "qux"})
-
-        assert [
-            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "1.0"
-        ][0].value == 1.0
-
-    def test_histo_with_env_label(self, monkeypatch):
-        monkeypatch.setenv("VERSION", "1.0.0")
-        histo = metrics.Histogram("my_histo3", buckets=[1, 2, 3])
-        histo.observe(1)
-
-        assert [
-            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "1.0"
-        ][0].value == 1.0
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+import asyncio
+import time
+from unittest.mock import MagicMock, patch
+
+import pytest
+
+from nucliadb_telemetry import metrics
+
+
+class TestObserver:
+    @pytest.fixture(autouse=True)
+    def histogram(self):
+        mock = MagicMock()
+        with patch(
+            "nucliadb_telemetry.metrics.prometheus_client.Histogram", return_value=mock
+        ):
+            yield mock
+
+    @pytest.fixture(autouse=True)
+    def counter(self):
+        mock = MagicMock()
+        with patch(
+            "nucliadb_telemetry.metrics.prometheus_client.Counter", return_value=mock
+        ):
+            yield mock
+
+    def test_observer(self, histogram, counter):
+        observer = metrics.Observer(
+            "my_metric", buckets=[1, 2, 3], labels={"foo": "bar"}
+        )
+        with observer(labels={"foo": "baz"}):
+            pass
+
+        histogram.labels.assert_called_once_with(foo="baz")
+        histogram.labels().observe.assert_called_once()
+        counter.labels.assert_called_once_with(status=metrics.OK, foo="baz")
+        counter.labels().inc.assert_called_once()
+
+    def test_observer_error_labels(self, histogram, counter):
+        class MyError(Exception):
+            pass
+
+        observer = metrics.Observer("my_metric", error_mappings={"my_error": MyError})
+        with pytest.raises(MyError), observer():
+            raise MyError("my_error")
+
+        histogram.observe.assert_called_once()
+        counter.labels.assert_called_once_with(status="my_error")
+        counter.labels().inc.assert_called_once()
+
+    def test_sync_decorator(self, histogram, counter):
+        observer = metrics.Observer("my_metric")
+
+        @observer.wrap()
+        def my_func():
+            pass
+
+        my_func()
+
+        histogram.observe.assert_called_once()
+        counter.labels().inc.assert_called_once()
+
+    @pytest.mark.asyncio
+    async def test_async_decorator(self, histogram, counter):
+        observer = metrics.Observer("my_metric")
+
+        @observer.wrap()
+        async def my_func():
+            pass
+
+        await my_func()
+
+        histogram.observe.assert_called_once()
+        counter.labels().inc.assert_called_once()
+
+    def test_gen_decorator(self, histogram, counter):
+        observer = metrics.Observer("my_metric")
+
+        @observer.wrap()
+        def my_func():
+            for i in range(1):
+                time.sleep(0.2)
+                yield i
+
+        for _ in my_func():
+            pass
+
+        histogram.observe.assert_called_once()
+        assert histogram.observe.call_args[0][0] >= 0.2
+        counter.labels().inc.assert_called_once()
+
+    @pytest.mark.asyncio
+    async def test_async_gen_decorator(self, histogram, counter):
+        observer = metrics.Observer("my_metric")
+
+        @observer.wrap()
+        async def my_func():
+            for i in range(1):
+                await asyncio.sleep(0.2)
+                yield i
+
+        async for _ in my_func():
+            pass
+
+        histogram.observe.assert_called_once()
+        assert histogram.observe.call_args[0][0] >= 0.2
+        counter.labels().inc.assert_called_once()
+
+    def test_observer_with_env(self, histogram, counter, monkeypatch):
+        monkeypatch.setenv("VERSION", "1.0.0")
+        observer = metrics.Observer(
+            "my_metric", buckets=[1, 2, 3], labels={"foo": "bar"}
+        )
+        with observer(labels={"foo": "baz"}):
+            pass
+
+        histogram.labels.assert_called_once_with(foo="baz", version="1.0.0")
+        histogram.labels().observe.assert_called_once()
+        counter.labels.assert_called_once_with(
+            status=metrics.OK, foo="baz", version="1.0.0"
+        )
+        counter.labels().inc.assert_called_once()
+
+
+class TestGauge:
+    def test_guage(self):
+        gauge = metrics.Gauge("my_guage")
+        gauge.set(5)
+
+        assert gauge.gauge._value.get() == 5.0
+
+    def test_guage_with_labels(self):
+        gauge = metrics.Gauge("my_guage2", labels={"foo": "", "bar": ""})
+        gauge.set(5, labels={"foo": "baz", "bar": "qux"})
+
+        assert gauge.gauge.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 5.0
+
+        gauge.remove({"foo": "baz", "bar": "qux"})
+
+        assert gauge.gauge.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 0.0
+
+    def test_guage_with_env_label(self, monkeypatch):
+        monkeypatch.setenv("VERSION", "1.0.0")
+        gauge = metrics.Gauge("my_guage3")
+        gauge.set(5)
+
+        assert gauge.gauge.labels(**{"version": "1.0.0"})._value.get() == 5.0
+
+
+class TestCounter:
+    def test_counter(self):
+        counter = metrics.Counter("my_counter")
+        counter.inc()
+
+        assert counter.counter._value.get() == 1.0
+
+    def test_counter_with_labels(self):
+        counter = metrics.Counter("my_counter2", labels={"foo": "", "bar": ""})
+        counter.inc(labels={"foo": "baz", "bar": "qux"})
+
+        assert (
+            counter.counter.labels(**{"foo": "baz", "bar": "qux"})._value.get() == 1.0
+        )
+
+    def test_counter_with_env_label(self, monkeypatch):
+        monkeypatch.setenv("VERSION", "1.0.0")
+        counter = metrics.Counter("my_counter3")
+        counter.inc(labels={"version": "1.0.0"})
+
+        assert counter.counter.labels(**{"version": "1.0.0"})._value.get() == 1.0
+
+
+class TestHistogram:
+    def test_histo(self):
+        histo = metrics.Histogram("my_histo")
+        histo.observe(5)
+
+        assert [
+            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "5.0"
+        ][0].value == 1.0
+
+    def test_histo_with_labels(self):
+        histo = metrics.Histogram(
+            "my_histo2", labels={"foo": "", "bar": ""}, buckets=[1, 2, 3]
+        )
+        histo.observe(1, labels={"foo": "baz", "bar": "qux"})
+
+        assert [
+            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "1.0"
+        ][0].value == 1.0
+
+    def test_histo_with_env_label(self, monkeypatch):
+        monkeypatch.setenv("VERSION", "1.0.0")
+        histo = metrics.Histogram("my_histo3", buckets=[1, 2, 3])
+        histo.observe(1)
+
+        assert [
+            s for s in histo.histo.collect()[0].samples if s.labels.get("le") == "1.0"
+        ][0].value == 1.0
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tikv.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tikv.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,138 +1,138 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import sys
-
-import tikv_client  # type: ignore
-from opentelemetry import trace
-from opentelemetry.instrumentation.instrumentor import BaseInstrumentor  # type: ignore
-from opentelemetry.instrumentation.utils import unwrap
-from opentelemetry.semconv.trace import SpanAttributes
-from opentelemetry.trace import SpanKind, Tracer
-from wrapt import wrap_function_wrapper  # type: ignore
-
-from nucliadb_telemetry.common import set_span_exception
-
-_instruments = ("tikv-client ~= 0.0.3",)
-
-
-def _instrument(
-    tracer: Tracer,
-):
-    async def _traced_async_wrapper(func, instance, args, kwargs):
-        operation = func.__name__
-        attributes = {
-            SpanAttributes.DB_SYSTEM: "tikv",
-            SpanAttributes.DB_OPERATION: operation,
-        }
-
-        # Frame 0 is this wrapper call, frame 1 is it's caller
-        sysframe = sys._getframe(1)
-        attributes.update(
-            {
-                SpanAttributes.CODE_FILEPATH: sysframe.f_code.co_filename,
-                SpanAttributes.CODE_LINENO: sysframe.f_lineno,
-                SpanAttributes.CODE_FUNCTION: sysframe.f_code.co_name,
-            }
-        )
-
-        span_name = f"TiKV {operation}"
-        with tracer.start_as_current_span(
-            name=span_name,
-            kind=SpanKind.CLIENT,
-            attributes=attributes,
-        ) as span:
-            try:
-                result = await func(*args, **kwargs)
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-            else:
-                return result
-
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "TransactionClient.begin", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.get", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.get_for_update", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.key_exists", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.batch_get", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous",
-        "Transaction.batch_get_for_update",
-        _traced_async_wrapper,
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.scan", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.scan_keys", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.lock_keys", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.put", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.insert", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.delete", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.commit", _traced_async_wrapper
-    )
-    wrap_function_wrapper(
-        "tikv_client.asynchronous", "Transaction.rollback", _traced_async_wrapper
-    )
-
-
-class TiKVInstrumentor(BaseInstrumentor):
-    def instrumentation_dependencies(self):
-        return _instruments
-
-    def _instrument(self, **kwargs):
-        tracer_provider = kwargs.get("tracer_provider")
-        tracer = trace.get_tracer("tikv_client", tracer_provider=tracer_provider)
-        _instrument(tracer)
-
-    def _uninstrument(self, **kwargs):
-        unwrap(tikv_client.asynchronous.TransactionClient, "begin")
-        unwrap(tikv_client.asynchronous.TransactionClient, "get")
-        unwrap(tikv_client.asynchronous.TransactionClient, "get_for_update")
-        unwrap(tikv_client.asynchronous.TransactionClient, "key_exists")
-        unwrap(tikv_client.asynchronous.TransactionClient, "batch_get")
-        unwrap(tikv_client.asynchronous.TransactionClient, "batch_get_for_update")
-        unwrap(tikv_client.asynchronous.TransactionClient, "scan")
-        unwrap(tikv_client.asynchronous.TransactionClient, "scan_keys")
-        unwrap(tikv_client.asynchronous.TransactionClient, "lock_keys")
-        unwrap(tikv_client.asynchronous.TransactionClient, "put")
-        unwrap(tikv_client.asynchronous.TransactionClient, "insert")
-        unwrap(tikv_client.asynchronous.TransactionClient, "delete")
-        unwrap(tikv_client.asynchronous.TransactionClient, "commit")
-        unwrap(tikv_client.asynchronous.TransactionClient, "rollback")
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import sys
+
+import tikv_client  # type: ignore
+from opentelemetry import trace
+from opentelemetry.instrumentation.instrumentor import BaseInstrumentor  # type: ignore
+from opentelemetry.instrumentation.utils import unwrap
+from opentelemetry.semconv.trace import SpanAttributes
+from opentelemetry.trace import SpanKind, Tracer
+from wrapt import wrap_function_wrapper  # type: ignore
+
+from nucliadb_telemetry.common import set_span_exception
+
+_instruments = ("tikv-client ~= 0.0.3",)
+
+
+def _instrument(
+    tracer: Tracer,
+):
+    async def _traced_async_wrapper(func, instance, args, kwargs):
+        operation = func.__name__
+        attributes = {
+            SpanAttributes.DB_SYSTEM: "tikv",
+            SpanAttributes.DB_OPERATION: operation,
+        }
+
+        # Frame 0 is this wrapper call, frame 1 is it's caller
+        sysframe = sys._getframe(1)
+        attributes.update(
+            {
+                SpanAttributes.CODE_FILEPATH: sysframe.f_code.co_filename,
+                SpanAttributes.CODE_LINENO: sysframe.f_lineno,
+                SpanAttributes.CODE_FUNCTION: sysframe.f_code.co_name,
+            }
+        )
+
+        span_name = f"TiKV {operation}"
+        with tracer.start_as_current_span(
+            name=span_name,
+            kind=SpanKind.CLIENT,
+            attributes=attributes,
+        ) as span:
+            try:
+                result = await func(*args, **kwargs)
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+            else:
+                return result
+
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "TransactionClient.begin", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.get", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.get_for_update", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.key_exists", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.batch_get", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous",
+        "Transaction.batch_get_for_update",
+        _traced_async_wrapper,
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.scan", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.scan_keys", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.lock_keys", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.put", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.insert", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.delete", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.commit", _traced_async_wrapper
+    )
+    wrap_function_wrapper(
+        "tikv_client.asynchronous", "Transaction.rollback", _traced_async_wrapper
+    )
+
+
+class TiKVInstrumentor(BaseInstrumentor):
+    def instrumentation_dependencies(self):
+        return _instruments
+
+    def _instrument(self, **kwargs):
+        tracer_provider = kwargs.get("tracer_provider")
+        tracer = trace.get_tracer("tikv_client", tracer_provider=tracer_provider)
+        _instrument(tracer)
+
+    def _uninstrument(self, **kwargs):
+        unwrap(tikv_client.asynchronous.TransactionClient, "begin")
+        unwrap(tikv_client.asynchronous.TransactionClient, "get")
+        unwrap(tikv_client.asynchronous.TransactionClient, "get_for_update")
+        unwrap(tikv_client.asynchronous.TransactionClient, "key_exists")
+        unwrap(tikv_client.asynchronous.TransactionClient, "batch_get")
+        unwrap(tikv_client.asynchronous.TransactionClient, "batch_get_for_update")
+        unwrap(tikv_client.asynchronous.TransactionClient, "scan")
+        unwrap(tikv_client.asynchronous.TransactionClient, "scan_keys")
+        unwrap(tikv_client.asynchronous.TransactionClient, "lock_keys")
+        unwrap(tikv_client.asynchronous.TransactionClient, "put")
+        unwrap(tikv_client.asynchronous.TransactionClient, "insert")
+        unwrap(tikv_client.asynchronous.TransactionClient, "delete")
+        unwrap(tikv_client.asynchronous.TransactionClient, "commit")
+        unwrap(tikv_client.asynchronous.TransactionClient, "rollback")
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import Optional
-
-from opentelemetry.context import Context  # type: ignore
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
-from opentelemetry.util._time import _time_ns  # type: ignore
-
-
-class AsyncMultiSpanProcessor(SpanProcessor):
-    """Implementation of class:`SpanProcessor` that forwards all received
-    events to a list of span processors sequentially.
-
-    The underlying span processors are called in sequential order as they were
-    added.
-    """
-
-    def __init__(self):
-        # use a tuple to avoid race conditions when adding a new span and
-        # iterating through it on "on_start" and "on_end".
-        self._span_processors = ()
-        self._lock = asyncio.Lock()
-
-    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
-        """Adds a SpanProcessor to the list handled by this instance."""
-        async with self._lock:
-            self._span_processors += (span_processor,)
-
-    def on_start(
-        self,
-        span: Span,
-        parent_context: Optional[Context] = None,
-    ) -> None:
-        for sp in self._span_processors:
-            sp.on_start(span, parent_context=parent_context)
-
-    def on_end(self, span: ReadableSpan) -> None:
-        for sp in self._span_processors:
-            sp.on_end(span)
-
-    def shutdown(self) -> None:
-        """Sequentially shuts down all underlying span processors."""
-        for sp in self._span_processors:
-            sp.shutdown()
-
-    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:  # type: ignore
-        """Sequentially calls async_force_flush on all underlying
-        :class:`SpanProcessor`
-
-        Args:
-            timeout_millis: The maximum amount of time over all span processors
-                to wait for spans to be exported. In case the first n span
-                processors exceeded the timeout followup span processors will be
-                skipped.
-
-        Returns:
-            True if all span processors flushed their spans within the
-            given timeout, False otherwise.
-        """
-        deadline_ns = _time_ns() + timeout_millis * 1000000
-        for sp in self._span_processors:
-            current_time_ns = _time_ns()
-            if current_time_ns >= deadline_ns:
-                return False
-
-            if not await sp.async_force_flush(
-                (deadline_ns - current_time_ns) // 1000000
-            ):
-                return False
-
-        return True
-
-
-class AsyncTracerProvider(TracerProvider):
-    initialized: bool = False
-    _active_span_processor: AsyncMultiSpanProcessor  # type: ignore
-
-    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
-        await self._active_span_processor.async_add_span_processor(span_processor)
-
-    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:
-        return await self._active_span_processor.async_force_flush(timeout_millis)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import Optional
+
+from opentelemetry.context import Context  # type: ignore
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
+from opentelemetry.util._time import _time_ns  # type: ignore
+
+
+class AsyncMultiSpanProcessor(SpanProcessor):
+    """Implementation of class:`SpanProcessor` that forwards all received
+    events to a list of span processors sequentially.
+
+    The underlying span processors are called in sequential order as they were
+    added.
+    """
+
+    def __init__(self):
+        # use a tuple to avoid race conditions when adding a new span and
+        # iterating through it on "on_start" and "on_end".
+        self._span_processors = ()
+        self._lock = asyncio.Lock()
+
+    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
+        """Adds a SpanProcessor to the list handled by this instance."""
+        async with self._lock:
+            self._span_processors += (span_processor,)
+
+    def on_start(
+        self,
+        span: Span,
+        parent_context: Optional[Context] = None,
+    ) -> None:
+        for sp in self._span_processors:
+            sp.on_start(span, parent_context=parent_context)
+
+    def on_end(self, span: ReadableSpan) -> None:
+        for sp in self._span_processors:
+            sp.on_end(span)
+
+    def shutdown(self) -> None:
+        """Sequentially shuts down all underlying span processors."""
+        for sp in self._span_processors:
+            sp.shutdown()
+
+    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:  # type: ignore
+        """Sequentially calls async_force_flush on all underlying
+        :class:`SpanProcessor`
+
+        Args:
+            timeout_millis: The maximum amount of time over all span processors
+                to wait for spans to be exported. In case the first n span
+                processors exceeded the timeout followup span processors will be
+                skipped.
+
+        Returns:
+            True if all span processors flushed their spans within the
+            given timeout, False otherwise.
+        """
+        deadline_ns = _time_ns() + timeout_millis * 1000000
+        for sp in self._span_processors:
+            current_time_ns = _time_ns()
+            if current_time_ns >= deadline_ns:
+                return False
+
+            if not await sp.async_force_flush(
+                (deadline_ns - current_time_ns) // 1000000
+            ):
+                return False
+
+        return True
+
+
+class AsyncTracerProvider(TracerProvider):
+    initialized: bool = False
+    _active_span_processor: AsyncMultiSpanProcessor  # type: ignore
+
+    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
+        await self._active_span_processor.async_add_span_processor(span_processor)
+
+    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:
+        return await self._active_span_processor.async_force_flush(timeout_millis)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,133 +1,133 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import os
-from typing import Dict, Optional
-
-from opentelemetry.propagate import set_global_textmap
-from opentelemetry.propagators.b3 import B3MultiFormat
-from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
-from opentelemetry.sdk.resources import Resource  # type: ignore
-
-from nucliadb_telemetry.batch_span import BatchSpanProcessor
-from nucliadb_telemetry.jaeger import JaegerExporterAsync
-from nucliadb_telemetry.settings import telemetry_settings
-from nucliadb_telemetry.tikv import TiKVInstrumentor
-from nucliadb_telemetry.tracerprovider import (
-    AsyncMultiSpanProcessor,
-    AsyncTracerProvider,
-)
-
-from .context import set_info_on_span  # noqa: F401
-
-set_info_on_span  # b/w compatible import
-
-GLOBAL_PROVIDER: Dict[str, AsyncTracerProvider] = {}
-
-
-def get_telemetry(service_name: Optional[str] = None) -> Optional[AsyncTracerProvider]:
-    if service_name is None:
-        return None
-    if service_name not in GLOBAL_PROVIDER and service_name is not None:
-        provider = create_telemetry(service_name)
-
-        if provider is not None:
-            GLOBAL_PROVIDER[service_name] = provider
-    return GLOBAL_PROVIDER.get(service_name)
-
-
-def create_telemetry(service_name: str) -> Optional[AsyncTracerProvider]:
-    if telemetry_settings.jaeger_enabled is False:
-        return None
-
-    tracer_provider = AsyncTracerProvider(
-        active_span_processor=AsyncMultiSpanProcessor(),  # type: ignore
-        resource=Resource.create({SERVICE_NAME: service_name}),
-    )
-
-    return tracer_provider
-
-
-async def clean_telemetry(service_name: str):
-    if service_name in GLOBAL_PROVIDER and service_name:
-        tracer_provider = GLOBAL_PROVIDER[service_name]
-        await tracer_provider.async_force_flush()
-        # Without this sleep, async_force_flush fails on exporting pending spans
-        await asyncio.sleep(0)
-        tracer_provider.shutdown()
-        del GLOBAL_PROVIDER[service_name]
-
-
-async def init_telemetry(tracer_provider: Optional[AsyncTracerProvider] = None):
-    if tracer_provider is None:
-        return
-
-    if tracer_provider.initialized:
-        return
-
-    # create a JaegerExporter
-    jaeger_exporter = JaegerExporterAsync(
-        # configure agent
-        agent_host_name=telemetry_settings.jaeger_agent_host,
-        agent_port=telemetry_settings.jaeger_agent_port,
-        # optional: configure also collector
-        # collector_endpoint='http://localhost:14268/api/traces?format=jaeger.thrift',
-        # username=xxxx, # optional
-        # password=xxxx, # optional
-        # max_tag_value_length=None # optional
-    )
-
-    # Create a BatchSpanProcessor and add the exporter to it
-    schedule_delay_millis = int(os.environ.get("OTEL_BSP_SCHEDULE_DELAY", 5000))
-    max_queue_size = int(os.environ.get("OTEL_BSP_MAX_QUEUE_SIZE", 2048))
-    max_export_batch_size = int(os.environ.get("OTEL_BSP_MAX_EXPORT_BATCH_SIZE", 512))
-    export_timeout_millis = int(os.environ.get("OTEL_BSP_EXPORT_TIMEOUT", 30000))
-
-    span_processor = BatchSpanProcessor(
-        jaeger_exporter,
-        schedule_delay_millis=schedule_delay_millis,
-        max_queue_size=max_queue_size,
-        max_export_batch_size=max_export_batch_size,
-        export_timeout_millis=export_timeout_millis,
-    )
-
-    # add to the tracer
-    await tracer_provider.async_add_span_processor(span_processor)
-    tracer_provider.initialized = True
-
-
-async def setup_telemetry(service_name: str) -> Optional[AsyncTracerProvider]:
-    """
-    Setup telemetry for a service if it is enabled
-    """
-    tracer_provider = get_telemetry(service_name)
-    if tracer_provider is not None:  # pragma: no cover
-        await init_telemetry(tracer_provider)
-        set_global_textmap(B3MultiFormat())
-        TiKVInstrumentor().instrument(tracer_provider=tracer_provider)
-        try:
-            from opentelemetry.instrumentation.aiohttp_client import (  # type: ignore
-                AioHttpClientInstrumentor,
-            )
-
-            AioHttpClientInstrumentor().instrument(tracer_provider=tracer_provider)
-        except ImportError:
-            pass
-    return tracer_provider
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import os
+from typing import Dict, Optional
+
+from opentelemetry.propagate import set_global_textmap
+from opentelemetry.propagators.b3 import B3MultiFormat
+from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
+from opentelemetry.sdk.resources import Resource  # type: ignore
+
+from nucliadb_telemetry.batch_span import BatchSpanProcessor
+from nucliadb_telemetry.jaeger import JaegerExporterAsync
+from nucliadb_telemetry.settings import telemetry_settings
+from nucliadb_telemetry.tikv import TiKVInstrumentor
+from nucliadb_telemetry.tracerprovider import (
+    AsyncMultiSpanProcessor,
+    AsyncTracerProvider,
+)
+
+from .context import set_info_on_span  # noqa: F401
+
+set_info_on_span  # b/w compatible import
+
+GLOBAL_PROVIDER: Dict[str, AsyncTracerProvider] = {}
+
+
+def get_telemetry(service_name: Optional[str] = None) -> Optional[AsyncTracerProvider]:
+    if service_name is None:
+        return None
+    if service_name not in GLOBAL_PROVIDER and service_name is not None:
+        provider = create_telemetry(service_name)
+
+        if provider is not None:
+            GLOBAL_PROVIDER[service_name] = provider
+    return GLOBAL_PROVIDER.get(service_name)
+
+
+def create_telemetry(service_name: str) -> Optional[AsyncTracerProvider]:
+    if telemetry_settings.jaeger_enabled is False:
+        return None
+
+    tracer_provider = AsyncTracerProvider(
+        active_span_processor=AsyncMultiSpanProcessor(),  # type: ignore
+        resource=Resource.create({SERVICE_NAME: service_name}),
+    )
+
+    return tracer_provider
+
+
+async def clean_telemetry(service_name: str):
+    if service_name in GLOBAL_PROVIDER and service_name:
+        tracer_provider = GLOBAL_PROVIDER[service_name]
+        await tracer_provider.async_force_flush()
+        # Without this sleep, async_force_flush fails on exporting pending spans
+        await asyncio.sleep(0)
+        tracer_provider.shutdown()
+        del GLOBAL_PROVIDER[service_name]
+
+
+async def init_telemetry(tracer_provider: Optional[AsyncTracerProvider] = None):
+    if tracer_provider is None:
+        return
+
+    if tracer_provider.initialized:
+        return
+
+    # create a JaegerExporter
+    jaeger_exporter = JaegerExporterAsync(
+        # configure agent
+        agent_host_name=telemetry_settings.jaeger_agent_host,
+        agent_port=telemetry_settings.jaeger_agent_port,
+        # optional: configure also collector
+        # collector_endpoint='http://localhost:14268/api/traces?format=jaeger.thrift',
+        # username=xxxx, # optional
+        # password=xxxx, # optional
+        # max_tag_value_length=None # optional
+    )
+
+    # Create a BatchSpanProcessor and add the exporter to it
+    schedule_delay_millis = int(os.environ.get("OTEL_BSP_SCHEDULE_DELAY", 5000))
+    max_queue_size = int(os.environ.get("OTEL_BSP_MAX_QUEUE_SIZE", 2048))
+    max_export_batch_size = int(os.environ.get("OTEL_BSP_MAX_EXPORT_BATCH_SIZE", 512))
+    export_timeout_millis = int(os.environ.get("OTEL_BSP_EXPORT_TIMEOUT", 30000))
+
+    span_processor = BatchSpanProcessor(
+        jaeger_exporter,
+        schedule_delay_millis=schedule_delay_millis,
+        max_queue_size=max_queue_size,
+        max_export_batch_size=max_export_batch_size,
+        export_timeout_millis=export_timeout_millis,
+    )
+
+    # add to the tracer
+    await tracer_provider.async_add_span_processor(span_processor)
+    tracer_provider.initialized = True
+
+
+async def setup_telemetry(service_name: str) -> Optional[AsyncTracerProvider]:
+    """
+    Setup telemetry for a service if it is enabled
+    """
+    tracer_provider = get_telemetry(service_name)
+    if tracer_provider is not None:  # pragma: no cover
+        await init_telemetry(tracer_provider)
+        set_global_textmap(B3MultiFormat())
+        TiKVInstrumentor().instrument(tracer_provider=tracer_provider)
+        try:
+            from opentelemetry.instrumentation.aiohttp_client import (  # type: ignore
+                AioHttpClientInstrumentor,
+            )
+
+            AioHttpClientInstrumentor().instrument(tracer_provider=tracer_provider)
+        except ImportError:
+            pass
+    return tracer_provider
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/requirements.txt` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/requirements.txt`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-nats-py[nkeys]==2.2.0
-httpx==0.23.0
-grpcio>=1.44.0
-grpcio-health-checking>=1.44.0
-grpcio-channelz>=1.44.0
-grpcio-status>=1.44.0
-grpcio-tools>=1.44.0
-grpcio-testing>=1.44.0
-grpcio-reflection>=1.44.0
-opentelemetry-sdk==1.11.1
-opentelemetry-api==1.11.1
-opentelemetry-proto==1.11.1
-opentelemetry-exporter-jaeger==1.11.1
-opentelemetry-propagator-b3==1.11.1
-opentelemetry-instrumentation-fastapi==0.30b1
-opentelemetry-instrumentation-aiohttp-client>=0.30b1
-opentelemetry-semantic-conventions>=0.30b1
-pydantic
-requests
-fastapi
-tikv-client>=0.0.3
-types-protobuf>=3.19.5,<4.0
-types-requests
-prometheus-client>=0.12.0
-orjson>=3.6.7
-urllib3<1.27,>=1.21.1
-wrapt>=1.14.1
+nats-py[nkeys]==2.2.0
+httpx==0.23.0
+grpcio>=1.44.0
+grpcio-health-checking>=1.44.0
+grpcio-channelz>=1.44.0
+grpcio-status>=1.44.0
+grpcio-tools>=1.44.0
+grpcio-testing>=1.44.0
+grpcio-reflection>=1.44.0
+opentelemetry-sdk==1.11.1
+opentelemetry-api==1.11.1
+opentelemetry-proto==1.11.1
+opentelemetry-exporter-jaeger==1.11.1
+opentelemetry-propagator-b3==1.11.1
+opentelemetry-instrumentation-fastapi==0.30b1
+opentelemetry-instrumentation-aiohttp-client>=0.30b1
+opentelemetry-semantic-conventions>=0.30b1
+pydantic<2.0
+requests
+fastapi
+tikv-client>=0.0.3
+types-protobuf>=3.19.5,<4.0
+types-requests
+prometheus-client>=0.12.0
+orjson>=3.6.7
+urllib3<1.27,>=1.21.1
+wrapt>=1.14.1
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/setup.cfg` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/setup.cfg`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-[flake8]
-ignore =
-  E302
-  W292
-  W391
-  E722
-  E501
-  W503
-  E203
-  F541
-  E402
-max-line-length = 110
-exclude = nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py,nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py,nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py,nucliadb_telemetry/tests/grpc/helloworld_pb2.py
-
-[zest.releaser]
-tag-format = nucliadb_telemetry-{version}
-tag-message = NucliaDB Telemetry {version}
-
-[tool:pytest]
-# pytest is requiring this for async projects otherwise
-# we'd need to change all our fixtures
+[flake8]
+ignore =
+  E302
+  W292
+  W391
+  E722
+  E501
+  W503
+  E203
+  F541
+  E402
+max-line-length = 110
+exclude = nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py,nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py,nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py,nucliadb_telemetry/tests/grpc/helloworld_pb2.py
+
+[zest.releaser]
+tag-format = nucliadb_telemetry-{version}
+tag-message = NucliaDB Telemetry {version}
+
+[tool:pytest]
+# pytest is requiring this for async projects otherwise
+# we'd need to change all our fixtures
 asyncio_mode=auto
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/setup.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/setup.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-import re
-
-from setuptools import find_packages, setup  # type: ignore
-
-VERSION = open("VERSION").read().strip()
-README = open("README.md").read()
-
-
-def load_reqs(filename):
-    with open(filename) as reqs_file:
-        return [
-            # pin nucliadb-xxx to the same version as nucliadb
-            line.strip() + f"=={VERSION}"
-            if line.startswith("nucliadb-") and "=" not in line
-            else line.strip()
-            for line in reqs_file.readlines()
-            if not (
-                re.match(r"\s*#", line) or re.match("-e", line) or re.match("-r", line)
-            )
-        ]
-
-
-requirements = load_reqs("requirements.txt")
-
-setup(
-    name="nucliadb_telemetry",
-    version=VERSION,
-    author="nucliadb Authors",
-    author_email="nucliadb@nuclia.com",
-    description="NucliaDB Telemetry Library Python process",
-    long_description=README,
-    long_description_content_type="text/markdown",
-    license="MIT",
-    url="https://github.com/nuclia/nucliadb",
-    classifiers=[
-        "Development Status :: 3 - Alpha",
-        "Intended Audience :: Developers",
-        "Intended Audience :: Information Technology",
-        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
-        "Framework :: AsyncIO",
-        "Programming Language :: Python",
-        "Programming Language :: Python :: 3.9",
-        "Topic :: System :: Monitoring",
-    ],
-    python_requires=">=3.7",
-    include_package_data=True,
-    package_data={"": ["*.txt", "*.md"], "nucliadb_telemetry": ["py.typed"]},
-    packages=find_packages(),
-    install_requires=requirements,
-)
+import re
+
+from setuptools import find_packages, setup  # type: ignore
+
+VERSION = open("VERSION").read().strip()
+README = open("README.md").read()
+
+
+def load_reqs(filename):
+    with open(filename) as reqs_file:
+        return [
+            # pin nucliadb-xxx to the same version as nucliadb
+            line.strip() + f"=={VERSION}"
+            if line.startswith("nucliadb-") and "=" not in line
+            else line.strip()
+            for line in reqs_file.readlines()
+            if not (
+                re.match(r"\s*#", line) or re.match("-e", line) or re.match("-r", line)
+            )
+        ]
+
+
+requirements = load_reqs("requirements.txt")
+
+setup(
+    name="nucliadb_telemetry",
+    version=VERSION,
+    author="nucliadb Authors",
+    author_email="nucliadb@nuclia.com",
+    description="NucliaDB Telemetry Library Python process",
+    long_description=README,
+    long_description_content_type="text/markdown",
+    license="MIT",
+    url="https://github.com/nuclia/nucliadb",
+    classifiers=[
+        "Development Status :: 3 - Alpha",
+        "Intended Audience :: Developers",
+        "Intended Audience :: Information Technology",
+        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
+        "Framework :: AsyncIO",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3.9",
+        "Topic :: System :: Monitoring",
+    ],
+    python_requires=">=3.7",
+    include_package_data=True,
+    package_data={"": ["*.txt", "*.md"], "nucliadb_telemetry": ["py.typed"]},
+    packages=find_packages(),
+    install_requires=requirements,
+)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/lib.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/lib.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,30 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-#![allow(clippy::bool_assert_comparison)]
-pub mod payload;
-/// This crate contains  the code responsible for sending usage data to Nuclia's server.
-mod sender;
-pub(crate) mod sink;
-
-pub mod blocking;
-pub mod sync;
-
-/// This environment variable can be set to disable sending telemetry events.
-pub const DISABLE_TELEMETRY_ENV_KEY: &str = "NUCLIADB_DISABLE_TELEMETRY";
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+#![allow(clippy::bool_assert_comparison)]
+pub mod payload;
+/// This crate contains  the code responsible for sending usage data to Nuclia's server.
+mod sender;
+pub(crate) mod sink;
+
+pub mod blocking;
+pub mod sync;
+
+/// This environment variable can be set to disable sending telemetry events.
+pub const DISABLE_TELEMETRY_ENV_KEY: &str = "NUCLIADB_DISABLE_TELEMETRY";
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/payload.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/payload.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::env;
-use std::time::UNIX_EPOCH;
-
-use serde::{Deserialize, Serialize};
-use uuid::Uuid;
-
-/// Represents the payload of the request sent with telemetry requests.
-#[derive(Debug, Serialize, Deserialize)]
-pub struct TelemetryPayload {
-    /// Client information. See details in `[ClientInformation]`.
-    pub client_information: ClientInformation,
-    pub events: Vec<EventWithTimestamp>,
-    /// Represents the number of events that where drops due to the
-    /// combination of the `TELEMETRY_PUSH_COOLDOWN` and `MAX_EVENT_IN_QUEUE`.
-    pub num_dropped_events: usize,
-}
-
-impl TelemetryPayload {
-    pub fn from_single_event(event: TelemetryEvent) -> TelemetryPayload {
-        TelemetryPayload {
-            client_information: ClientInformation::default(),
-            events: vec![EventWithTimestamp::from(event)],
-            num_dropped_events: 0,
-        }
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize)]
-pub struct EventWithTimestamp {
-    /// Unix time in seconds.
-    pub timestamp: u64,
-    /// Telemetry event.
-    pub r#type: TelemetryEvent,
-}
-
-/// Returns the number of seconds elapsed since UNIX_EPOCH.
-///
-/// If the system clock is set before 1970, returns 0.
-fn unixtime() -> u64 {
-    match UNIX_EPOCH.elapsed() {
-        Ok(duration) => duration.as_secs(),
-        Err(_) => 0u64,
-    }
-}
-
-impl From<TelemetryEvent> for EventWithTimestamp {
-    fn from(event: TelemetryEvent) -> Self {
-        EventWithTimestamp {
-            timestamp: unixtime(),
-            r#type: event,
-        }
-    }
-}
-
-/// Represents a Telemetry Event send to nucliadb's server for usage information.
-#[derive(Debug, Serialize, Deserialize)]
-pub enum TelemetryEvent {
-    /// Create command is called.
-    Create,
-    /// Delete command
-    Delete,
-    /// Garbage Collect command
-    GarbageCollect,
-    /// Serve command is called.
-    Serve(ServeEvent),
-    /// EndCommand (with the return code)
-    EndCommand { return_code: i32 },
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-pub struct ServeEvent {
-    pub has_seed: bool,
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-pub struct ClientInformation {
-    session_uuid: uuid::Uuid,
-    nucliadb_version: String,
-    os: String,
-    arch: String,
-    hashed_host_username: String,
-    component: Option<String>,
-    kubernetes: bool,
-}
-
-fn hashed_host_username() -> String {
-    let hostname = hostname::get()
-        .map(|hostname| hostname.to_string_lossy().to_string())
-        .unwrap_or_else(|_| "".to_string());
-    let username = username::get_user_name().unwrap_or_else(|_| "".to_string());
-    let hashed_value = format!("{hostname}:{username}");
-    let digest = md5::compute(hashed_value.as_bytes());
-    format!("{digest:x}")
-}
-
-impl Default for ClientInformation {
-    fn default() -> ClientInformation {
-        ClientInformation {
-            session_uuid: Uuid::new_v4(),
-            nucliadb_version: env!("CARGO_PKG_VERSION").to_string(),
-            os: env::consts::OS.to_string(),
-            arch: env::consts::ARCH.to_string(),
-            hashed_host_username: hashed_host_username(),
-            component: Some("Node".to_string()),
-            kubernetes: std::env::var_os("KUBERNETES_SERVICE_HOST").is_some(),
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::env;
+use std::time::UNIX_EPOCH;
+
+use serde::{Deserialize, Serialize};
+use uuid::Uuid;
+
+/// Represents the payload of the request sent with telemetry requests.
+#[derive(Debug, Serialize, Deserialize)]
+pub struct TelemetryPayload {
+    /// Client information. See details in `[ClientInformation]`.
+    pub client_information: ClientInformation,
+    pub events: Vec<EventWithTimestamp>,
+    /// Represents the number of events that where drops due to the
+    /// combination of the `TELEMETRY_PUSH_COOLDOWN` and `MAX_EVENT_IN_QUEUE`.
+    pub num_dropped_events: usize,
+}
+
+impl TelemetryPayload {
+    pub fn from_single_event(event: TelemetryEvent) -> TelemetryPayload {
+        TelemetryPayload {
+            client_information: ClientInformation::default(),
+            events: vec![EventWithTimestamp::from(event)],
+            num_dropped_events: 0,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize)]
+pub struct EventWithTimestamp {
+    /// Unix time in seconds.
+    pub timestamp: u64,
+    /// Telemetry event.
+    pub r#type: TelemetryEvent,
+}
+
+/// Returns the number of seconds elapsed since UNIX_EPOCH.
+///
+/// If the system clock is set before 1970, returns 0.
+fn unixtime() -> u64 {
+    match UNIX_EPOCH.elapsed() {
+        Ok(duration) => duration.as_secs(),
+        Err(_) => 0u64,
+    }
+}
+
+impl From<TelemetryEvent> for EventWithTimestamp {
+    fn from(event: TelemetryEvent) -> Self {
+        EventWithTimestamp {
+            timestamp: unixtime(),
+            r#type: event,
+        }
+    }
+}
+
+/// Represents a Telemetry Event send to nucliadb's server for usage information.
+#[derive(Debug, Serialize, Deserialize)]
+pub enum TelemetryEvent {
+    /// Create command is called.
+    Create,
+    /// Delete command
+    Delete,
+    /// Garbage Collect command
+    GarbageCollect,
+    /// Serve command is called.
+    Serve(ServeEvent),
+    /// EndCommand (with the return code)
+    EndCommand { return_code: i32 },
+}
+
+#[derive(Clone, Debug, Serialize, Deserialize)]
+pub struct ServeEvent {
+    pub has_seed: bool,
+}
+
+#[derive(Clone, Debug, Serialize, Deserialize)]
+pub struct ClientInformation {
+    session_uuid: uuid::Uuid,
+    nucliadb_version: String,
+    os: String,
+    arch: String,
+    hashed_host_username: String,
+    component: Option<String>,
+    kubernetes: bool,
+}
+
+fn hashed_host_username() -> String {
+    let hostname = hostname::get()
+        .map(|hostname| hostname.to_string_lossy().to_string())
+        .unwrap_or_else(|_| "".to_string());
+    let username = username::get_user_name().unwrap_or_else(|_| "".to_string());
+    let hashed_value = format!("{hostname}:{username}");
+    let digest = md5::compute(hashed_value.as_bytes());
+    format!("{digest:x}")
+}
+
+impl Default for ClientInformation {
+    fn default() -> ClientInformation {
+        ClientInformation {
+            session_uuid: Uuid::new_v4(),
+            nucliadb_version: env!("CARGO_PKG_VERSION").to_string(),
+            os: env::consts::OS.to_string(),
+            arch: env::consts::ARCH.to_string(),
+            hashed_host_username: hashed_host_username(),
+            component: Some("Node".to_string()),
+            kubernetes: std::env::var_os("KUBERNETES_SERVICE_HOST").is_some(),
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/sender.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/sender.rs`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,401 +1,401 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::mem;
-use std::sync::atomic::{AtomicBool, Ordering};
-use std::sync::Arc;
-use std::time::Duration;
-
-use tokio::sync::mpsc::{Receiver, Sender};
-use tokio::sync::{oneshot, Mutex, RwLock};
-use tokio::task::JoinHandle;
-use tokio::time::Interval;
-use tracing::info;
-
-use crate::payload::{ClientInformation, EventWithTimestamp, TelemetryEvent, TelemetryPayload};
-use crate::sink::{HttpClient, Sink};
-
-/// At most 1 Request per minutes.
-const TELEMETRY_PUSH_COOLDOWN: Duration = Duration::from_secs(60);
-
-/// Upon termination of the program, we send one last telemetry request with pending events.
-/// This duration is the amount of time we wait for at most to send that last telemetry request.
-const LAST_REQUEST_TIMEOUT: Duration = Duration::from_secs(1);
-
-const MAX_NUM_EVENTS_IN_QUEUE: usize = 10;
-
-#[cfg(test)]
-struct ClockButton(Sender<()>);
-
-#[cfg(test)]
-impl ClockButton {
-    async fn tick(&self) {
-        let _ = self.0.send(()).await;
-    }
-}
-
-enum Clock {
-    Periodical(Mutex<Interval>),
-    #[cfg(test)]
-    Manual(Mutex<Receiver<()>>),
-}
-
-impl Clock {
-    pub fn periodical(period: Duration) -> Clock {
-        let interval = tokio::time::interval(period);
-        Clock::Periodical(Mutex::new(interval))
-    }
-
-    #[cfg(test)]
-    pub async fn manual() -> (ClockButton, Clock) {
-        let (tx, rx) = tokio::sync::mpsc::channel(1);
-        let _ = tx.send(()).await;
-        let button = ClockButton(tx);
-        (button, Clock::Manual(Mutex::new(rx)))
-    }
-
-    async fn tick(&self) {
-        match self {
-            Clock::Periodical(interval) => {
-                interval.lock().await.tick().await;
-            }
-            #[cfg(test)]
-            Clock::Manual(channel) => {
-                channel.lock().await.recv().await;
-            }
-        }
-    }
-}
-
-#[derive(Default)]
-struct EventsState {
-    events: Vec<EventWithTimestamp>,
-    num_dropped_events: usize,
-}
-
-impl EventsState {
-    fn drain_events(&mut self) -> EventsState {
-        mem::replace(
-            self,
-            EventsState {
-                events: Vec::new(),
-                num_dropped_events: 0,
-            },
-        )
-    }
-
-    /// Adds an event.
-    /// If the queue is already saturated, (ie. it has reached the len `MAX_NUM_EVENTS_IN_QUEUE`)
-    // Returns true iff it was the first event in the queue.
-    fn push_event(&mut self, event: TelemetryEvent) -> bool {
-        if self.events.len() >= MAX_NUM_EVENTS_IN_QUEUE {
-            self.num_dropped_events += 1;
-            return false;
-        }
-        let events_was_empty = self.events.is_empty();
-        self.events.push(EventWithTimestamp::from(event));
-        events_was_empty
-    }
-}
-
-struct Events {
-    state: RwLock<EventsState>,
-    items_available_tx: Sender<()>,
-    items_available_rx: RwLock<Receiver<()>>,
-}
-
-impl Default for Events {
-    fn default() -> Self {
-        let (items_available_tx, items_available_rx) = tokio::sync::mpsc::channel(1);
-        Events {
-            state: RwLock::new(EventsState::default()),
-            items_available_tx,
-            items_available_rx: RwLock::new(items_available_rx),
-        }
-    }
-}
-
-impl Events {
-    /// Wait for events to be available (if there are pending events, then do not wait)
-    /// and then send them to the PushAPI server.
-    async fn drain_events(&self) -> EventsState {
-        self.items_available_rx.write().await.recv().await;
-        self.state.write().await.drain_events()
-    }
-
-    async fn push_event(&self, event: TelemetryEvent) {
-        let is_first_event = self.state.write().await.push_event(event);
-        if is_first_event {
-            let _ = self.items_available_tx.send(()).await;
-        }
-    }
-}
-
-pub(crate) struct Inner {
-    sink: Option<Box<dyn Sink>>,
-    client_information: ClientInformation,
-    /// This channel is just used to signal there are new items available.
-    events: Events,
-    clock: Clock,
-    is_started: AtomicBool,
-}
-
-impl Inner {
-    pub fn is_disabled(&self) -> bool {
-        self.sink.is_none()
-    }
-
-    async fn create_telemetry_payload(&self) -> TelemetryPayload {
-        let events_state = self.events.drain_events().await;
-        TelemetryPayload {
-            client_information: self.client_information.clone(),
-            events: events_state.events,
-            num_dropped_events: events_state.num_dropped_events,
-        }
-    }
-
-    /// Wait for events to be available (if there are pending events, then do not wait)
-    /// and then send them to the PushAPI server.
-    ///
-    /// If the requests fails, it fails silently.
-    async fn send_pending_events(&self) {
-        if let Some(sink) = self.sink.as_ref() {
-            let payload = self.create_telemetry_payload().await;
-            sink.send_payload(payload).await;
-        }
-    }
-
-    async fn send(&self, event: TelemetryEvent) {
-        if self.is_disabled() {
-            return;
-        }
-        self.events.push_event(event).await;
-    }
-}
-
-pub struct TelemetrySender {
-    pub(crate) inner: Arc<Inner>,
-}
-
-pub enum TelemetryLoopHandle {
-    NoLoop,
-    WithLoop {
-        join_handle: JoinHandle<()>,
-        terminate_command_tx: oneshot::Sender<()>,
-    },
-}
-
-impl TelemetryLoopHandle {
-    /// Terminate telemetry will exit the telemetry loop
-    /// and possibly send the last request, possibly ignoring the
-    /// telemetry cooldown.
-    pub async fn terminate_telemetry(self) {
-        if let Self::WithLoop {
-            join_handle,
-            terminate_command_tx,
-        } = self
-        {
-            let _ = terminate_command_tx.send(());
-            let _ = tokio::time::timeout(LAST_REQUEST_TIMEOUT, join_handle).await;
-        }
-    }
-}
-
-impl TelemetrySender {
-    fn new<S: Sink>(sink_opt: Option<S>, clock: Clock) -> TelemetrySender {
-        let sink_opt: Option<Box<dyn Sink>> = if let Some(sink) = sink_opt {
-            Some(Box::new(sink))
-        } else {
-            None
-        };
-        TelemetrySender {
-            inner: Arc::new(Inner {
-                sink: sink_opt,
-                client_information: ClientInformation::default(),
-                events: Events::default(),
-                clock,
-                is_started: AtomicBool::new(false),
-            }),
-        }
-    }
-
-    pub fn start_loop(&self) -> TelemetryLoopHandle {
-        let (terminate_command_tx, mut terminate_command_rx) = oneshot::channel();
-        if self.inner.is_disabled() {
-            return TelemetryLoopHandle::NoLoop;
-        }
-
-        assert!(
-            self.inner
-                .is_started
-                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)
-                .is_ok(),
-            "The telemetry loop is already started."
-        );
-
-        let inner = self.inner.clone();
-        let join_handle = tokio::task::spawn(async move {
-            // This channel is used to send the command to terminate telemetry.
-            loop {
-                let quit_loop = tokio::select! {
-                    _ = (&mut terminate_command_rx) => { true }
-                    _ = inner.clock.tick() => { false }
-                };
-                inner.send_pending_events().await;
-                if quit_loop {
-                    break;
-                }
-            }
-        });
-        TelemetryLoopHandle::WithLoop {
-            join_handle,
-            terminate_command_tx,
-        }
-    }
-
-    pub async fn send(&self, event: TelemetryEvent) {
-        self.inner.send(event).await;
-    }
-}
-
-/// Check to see if telemetry is enabled.
-pub fn is_telemetry_enabled() -> bool {
-    std::env::var_os(crate::DISABLE_TELEMETRY_ENV_KEY).is_none()
-}
-
-fn create_http_client() -> Option<HttpClient> {
-    // TODO add telemetry URL.
-    let client_opt = if is_telemetry_enabled() {
-        HttpClient::try_new()
-    } else {
-        None
-    };
-    if let Some(client) = client_opt.as_ref() {
-        info!("telemetry to {} is enabled.", client.endpoint());
-    } else {
-        info!("telemetry to nucliadb is disabled.");
-    }
-    client_opt
-}
-
-impl Default for TelemetrySender {
-    fn default() -> Self {
-        let http_client = create_http_client();
-        TelemetrySender::new(http_client, Clock::periodical(TELEMETRY_PUSH_COOLDOWN))
-    }
-}
-
-#[cfg(test)]
-mod tests {
-
-    use std::env;
-
-    use super::*;
-
-    #[ignore]
-    #[tokio::test]
-    async fn test_enabling_and_disabling_telemetry() {
-        // We group the two in a single test to ensure it happens on the same thread.
-        env::set_var(crate::DISABLE_TELEMETRY_ENV_KEY, "");
-        assert_eq!(TelemetrySender::default().inner.is_disabled(), true);
-        env::remove_var(crate::DISABLE_TELEMETRY_ENV_KEY);
-        assert_eq!(TelemetrySender::default().inner.is_disabled(), false);
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_no_wait_for_first_event() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (_clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        let payload_opt = rx.recv().await;
-        assert!(payload_opt.is_some());
-        let payload = payload_opt.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_two_events() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        clock_btn.tick().await;
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_cooldown_observed() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        tokio::task::yield_now().await;
-        telemetry_sender.send(TelemetryEvent::Create).await;
-
-        let timeout_res = tokio::time::timeout(Duration::from_millis(1), rx.recv()).await;
-        assert!(timeout_res.is_err());
-
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        clock_btn.tick().await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 2);
-        }
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_terminate_telemetry_sends_pending_events() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (_clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        let payload = rx.recv().await.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        telemetry_sender
-            .send(TelemetryEvent::EndCommand { return_code: 2i32 })
-            .await;
-        loop_handler.terminate_telemetry().await;
-        let payload = rx.recv().await.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        assert!(matches!(
-            &payload.events[0].r#type,
-            &TelemetryEvent::EndCommand { .. }
-        ));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::mem;
+use std::sync::atomic::{AtomicBool, Ordering};
+use std::sync::Arc;
+use std::time::Duration;
+
+use tokio::sync::mpsc::{Receiver, Sender};
+use tokio::sync::{oneshot, Mutex, RwLock};
+use tokio::task::JoinHandle;
+use tokio::time::Interval;
+use tracing::info;
+
+use crate::payload::{ClientInformation, EventWithTimestamp, TelemetryEvent, TelemetryPayload};
+use crate::sink::{HttpClient, Sink};
+
+/// At most 1 Request per minutes.
+const TELEMETRY_PUSH_COOLDOWN: Duration = Duration::from_secs(60);
+
+/// Upon termination of the program, we send one last telemetry request with pending events.
+/// This duration is the amount of time we wait for at most to send that last telemetry request.
+const LAST_REQUEST_TIMEOUT: Duration = Duration::from_secs(1);
+
+const MAX_NUM_EVENTS_IN_QUEUE: usize = 10;
+
+#[cfg(test)]
+struct ClockButton(Sender<()>);
+
+#[cfg(test)]
+impl ClockButton {
+    async fn tick(&self) {
+        let _ = self.0.send(()).await;
+    }
+}
+
+enum Clock {
+    Periodical(Mutex<Interval>),
+    #[cfg(test)]
+    Manual(Mutex<Receiver<()>>),
+}
+
+impl Clock {
+    pub fn periodical(period: Duration) -> Clock {
+        let interval = tokio::time::interval(period);
+        Clock::Periodical(Mutex::new(interval))
+    }
+
+    #[cfg(test)]
+    pub async fn manual() -> (ClockButton, Clock) {
+        let (tx, rx) = tokio::sync::mpsc::channel(1);
+        let _ = tx.send(()).await;
+        let button = ClockButton(tx);
+        (button, Clock::Manual(Mutex::new(rx)))
+    }
+
+    async fn tick(&self) {
+        match self {
+            Clock::Periodical(interval) => {
+                interval.lock().await.tick().await;
+            }
+            #[cfg(test)]
+            Clock::Manual(channel) => {
+                channel.lock().await.recv().await;
+            }
+        }
+    }
+}
+
+#[derive(Default)]
+struct EventsState {
+    events: Vec<EventWithTimestamp>,
+    num_dropped_events: usize,
+}
+
+impl EventsState {
+    fn drain_events(&mut self) -> EventsState {
+        mem::replace(
+            self,
+            EventsState {
+                events: Vec::new(),
+                num_dropped_events: 0,
+            },
+        )
+    }
+
+    /// Adds an event.
+    /// If the queue is already saturated, (ie. it has reached the len `MAX_NUM_EVENTS_IN_QUEUE`)
+    // Returns true iff it was the first event in the queue.
+    fn push_event(&mut self, event: TelemetryEvent) -> bool {
+        if self.events.len() >= MAX_NUM_EVENTS_IN_QUEUE {
+            self.num_dropped_events += 1;
+            return false;
+        }
+        let events_was_empty = self.events.is_empty();
+        self.events.push(EventWithTimestamp::from(event));
+        events_was_empty
+    }
+}
+
+struct Events {
+    state: RwLock<EventsState>,
+    items_available_tx: Sender<()>,
+    items_available_rx: RwLock<Receiver<()>>,
+}
+
+impl Default for Events {
+    fn default() -> Self {
+        let (items_available_tx, items_available_rx) = tokio::sync::mpsc::channel(1);
+        Events {
+            state: RwLock::new(EventsState::default()),
+            items_available_tx,
+            items_available_rx: RwLock::new(items_available_rx),
+        }
+    }
+}
+
+impl Events {
+    /// Wait for events to be available (if there are pending events, then do not wait)
+    /// and then send them to the PushAPI server.
+    async fn drain_events(&self) -> EventsState {
+        self.items_available_rx.write().await.recv().await;
+        self.state.write().await.drain_events()
+    }
+
+    async fn push_event(&self, event: TelemetryEvent) {
+        let is_first_event = self.state.write().await.push_event(event);
+        if is_first_event {
+            let _ = self.items_available_tx.send(()).await;
+        }
+    }
+}
+
+pub(crate) struct Inner {
+    sink: Option<Box<dyn Sink>>,
+    client_information: ClientInformation,
+    /// This channel is just used to signal there are new items available.
+    events: Events,
+    clock: Clock,
+    is_started: AtomicBool,
+}
+
+impl Inner {
+    pub fn is_disabled(&self) -> bool {
+        self.sink.is_none()
+    }
+
+    async fn create_telemetry_payload(&self) -> TelemetryPayload {
+        let events_state = self.events.drain_events().await;
+        TelemetryPayload {
+            client_information: self.client_information.clone(),
+            events: events_state.events,
+            num_dropped_events: events_state.num_dropped_events,
+        }
+    }
+
+    /// Wait for events to be available (if there are pending events, then do not wait)
+    /// and then send them to the PushAPI server.
+    ///
+    /// If the requests fails, it fails silently.
+    async fn send_pending_events(&self) {
+        if let Some(sink) = self.sink.as_ref() {
+            let payload = self.create_telemetry_payload().await;
+            sink.send_payload(payload).await;
+        }
+    }
+
+    async fn send(&self, event: TelemetryEvent) {
+        if self.is_disabled() {
+            return;
+        }
+        self.events.push_event(event).await;
+    }
+}
+
+pub struct TelemetrySender {
+    pub(crate) inner: Arc<Inner>,
+}
+
+pub enum TelemetryLoopHandle {
+    NoLoop,
+    WithLoop {
+        join_handle: JoinHandle<()>,
+        terminate_command_tx: oneshot::Sender<()>,
+    },
+}
+
+impl TelemetryLoopHandle {
+    /// Terminate telemetry will exit the telemetry loop
+    /// and possibly send the last request, possibly ignoring the
+    /// telemetry cooldown.
+    pub async fn terminate_telemetry(self) {
+        if let Self::WithLoop {
+            join_handle,
+            terminate_command_tx,
+        } = self
+        {
+            let _ = terminate_command_tx.send(());
+            let _ = tokio::time::timeout(LAST_REQUEST_TIMEOUT, join_handle).await;
+        }
+    }
+}
+
+impl TelemetrySender {
+    fn new<S: Sink>(sink_opt: Option<S>, clock: Clock) -> TelemetrySender {
+        let sink_opt: Option<Box<dyn Sink>> = if let Some(sink) = sink_opt {
+            Some(Box::new(sink))
+        } else {
+            None
+        };
+        TelemetrySender {
+            inner: Arc::new(Inner {
+                sink: sink_opt,
+                client_information: ClientInformation::default(),
+                events: Events::default(),
+                clock,
+                is_started: AtomicBool::new(false),
+            }),
+        }
+    }
+
+    pub fn start_loop(&self) -> TelemetryLoopHandle {
+        let (terminate_command_tx, mut terminate_command_rx) = oneshot::channel();
+        if self.inner.is_disabled() {
+            return TelemetryLoopHandle::NoLoop;
+        }
+
+        assert!(
+            self.inner
+                .is_started
+                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)
+                .is_ok(),
+            "The telemetry loop is already started."
+        );
+
+        let inner = self.inner.clone();
+        let join_handle = tokio::task::spawn(async move {
+            // This channel is used to send the command to terminate telemetry.
+            loop {
+                let quit_loop = tokio::select! {
+                    _ = (&mut terminate_command_rx) => { true }
+                    _ = inner.clock.tick() => { false }
+                };
+                inner.send_pending_events().await;
+                if quit_loop {
+                    break;
+                }
+            }
+        });
+        TelemetryLoopHandle::WithLoop {
+            join_handle,
+            terminate_command_tx,
+        }
+    }
+
+    pub async fn send(&self, event: TelemetryEvent) {
+        self.inner.send(event).await;
+    }
+}
+
+/// Check to see if telemetry is enabled.
+pub fn is_telemetry_enabled() -> bool {
+    std::env::var_os(crate::DISABLE_TELEMETRY_ENV_KEY).is_none()
+}
+
+fn create_http_client() -> Option<HttpClient> {
+    // TODO add telemetry URL.
+    let client_opt = if is_telemetry_enabled() {
+        HttpClient::try_new()
+    } else {
+        None
+    };
+    if let Some(client) = client_opt.as_ref() {
+        info!("telemetry to {} is enabled.", client.endpoint());
+    } else {
+        info!("telemetry to nucliadb is disabled.");
+    }
+    client_opt
+}
+
+impl Default for TelemetrySender {
+    fn default() -> Self {
+        let http_client = create_http_client();
+        TelemetrySender::new(http_client, Clock::periodical(TELEMETRY_PUSH_COOLDOWN))
+    }
+}
+
+#[cfg(test)]
+mod tests {
+
+    use std::env;
+
+    use super::*;
+
+    #[ignore]
+    #[tokio::test]
+    async fn test_enabling_and_disabling_telemetry() {
+        // We group the two in a single test to ensure it happens on the same thread.
+        env::set_var(crate::DISABLE_TELEMETRY_ENV_KEY, "");
+        assert_eq!(TelemetrySender::default().inner.is_disabled(), true);
+        env::remove_var(crate::DISABLE_TELEMETRY_ENV_KEY);
+        assert_eq!(TelemetrySender::default().inner.is_disabled(), false);
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_no_wait_for_first_event() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (_clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        let payload_opt = rx.recv().await;
+        assert!(payload_opt.is_some());
+        let payload = payload_opt.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_two_events() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        clock_btn.tick().await;
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_cooldown_observed() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        tokio::task::yield_now().await;
+        telemetry_sender.send(TelemetryEvent::Create).await;
+
+        let timeout_res = tokio::time::timeout(Duration::from_millis(1), rx.recv()).await;
+        assert!(timeout_res.is_err());
+
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        clock_btn.tick().await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 2);
+        }
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_terminate_telemetry_sends_pending_events() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (_clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        let payload = rx.recv().await.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        telemetry_sender
+            .send(TelemetryEvent::EndCommand { return_code: 2i32 })
+            .await;
+        loop_handler.terminate_telemetry().await;
+        let payload = rx.recv().await.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        assert!(matches!(
+            &payload.events[0].r#type,
+            &TelemetryEvent::EndCommand { .. }
+        ));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_telemetry/src/sync.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/src/sync.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use once_cell::sync::OnceCell;
-
-use crate::payload::TelemetryEvent;
-pub use crate::sender::is_telemetry_enabled;
-use crate::sender::{TelemetryLoopHandle, TelemetrySender};
-
-pub fn start_telemetry_loop() -> TelemetryLoopHandle {
-    get_telemetry_sender_singleton().start_loop()
-}
-
-fn get_telemetry_sender_singleton() -> &'static TelemetrySender {
-    static INSTANCE: OnceCell<TelemetrySender> = OnceCell::new();
-    INSTANCE.get_or_init(TelemetrySender::default)
-}
-
-/// Sends a telemetry event to Nuclia's server via HTTP.
-///
-/// Telemetry guarantees to send at most 1 request per minute.
-/// Each requests can ship at most 10 messages.
-///
-/// If this methods is called too often, some events will be dropped.
-///
-/// If the http requests fail, the error will be silent.
-///
-/// We voluntarily use an enum here to make it easier for reader
-/// to audit the type of information that is send home.
-pub async fn send_telemetry_event(event: TelemetryEvent) {
-    get_telemetry_sender_singleton().send(event).await
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use once_cell::sync::OnceCell;
+
+use crate::payload::TelemetryEvent;
+pub use crate::sender::is_telemetry_enabled;
+use crate::sender::{TelemetryLoopHandle, TelemetrySender};
+
+pub fn start_telemetry_loop() -> TelemetryLoopHandle {
+    get_telemetry_sender_singleton().start_loop()
+}
+
+fn get_telemetry_sender_singleton() -> &'static TelemetrySender {
+    static INSTANCE: OnceCell<TelemetrySender> = OnceCell::new();
+    INSTANCE.get_or_init(TelemetrySender::default)
+}
+
+/// Sends a telemetry event to Nuclia's server via HTTP.
+///
+/// Telemetry guarantees to send at most 1 request per minute.
+/// Each requests can ship at most 10 messages.
+///
+/// If this methods is called too often, some events will be dropped.
+///
+/// If the http requests fail, the error will be silent.
+///
+/// We voluntarily use an enum here to make it easier for reader
+/// to audit the type of information that is send home.
+pub async fn send_telemetry_event(event: TelemetryEvent) {
+    get_telemetry_sender_singleton().send(event).await
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/Cargo.toml`

 * *Files 12% similar despite different names*

```diff
@@ -29,45 +29,40 @@
 
 tonic = "0.7"
 tonic-health = "0.6"
 async-std = "1.10.0"
 futures-core = "0.3.17"
 futures-util = "0.3.17"
 futures = "0.3.17"
-tokio = { version = "1.12.0", features = [
-    "rt-multi-thread",
-    "macros",
-    "sync",
-    "time",
-    "signal",
-    "fs",
+tokio = { version = "1.12.0", features = [
+    "rt-multi-thread",
+    "macros",
+    "sync",
+    "time",
+    "signal",
+    "fs",
 ] }
 tokio-stream = "0.1.7"
 log = "0.4.14"
 serde_json = "1"
 serde = { version = "1.0", features = ["derive"] }
 uuid = { version = "1.1", features = ["serde", "v4"] }
 bincode = "1.3.3"
 async-trait = "0.1.51"
 time = "0.3.3"
 itertools = "0.10"
-anyhow = "1"
+anyhow = { version = "1", features = ["backtrace"] }
 http = "0.2"
 hyper = "0.14.26"
 tower = "0.4.13"
 thiserror = "1"
 opentelemetry = { version = "0.17", features = ["rt-tokio", "trace"] }
 tracing-opentelemetry = "0.17.2"
 reqwest = "0.11.16"
-
-# Test dependencies
-tempfile = "3.2.0"
-regex = "1.5.5"
-lazy_static = "1.4.0"
-openssl = { version = "0.10", features = ["vendored"] }
+derive_builder = "0.12.0"
 
 # Text Service
 async-stream = "0.3.2"
 
 rand = "0.8.4"
 
 # nucliadb dependencies
@@ -75,26 +70,22 @@
 nucliadb_telemetry = { path = "../nucliadb_telemetry" }
 nucliadb_core = { path = "../nucliadb_core" }
 nucliadb_texts= { path = "../nucliadb_texts" }
 nucliadb_paragraphs= { path = "../nucliadb_paragraphs" }
 nucliadb_vectors= { path = "../nucliadb_vectors" }
 nucliadb_relations= { path = "../nucliadb_relations" }
 
-# test
-tempdir = "0.3.7"
-portpicker = "0.1.1"
-
 # sentry sdk
 sentry = "0.26.0"
 opentelemetry-jaeger = { version = "0.16.0", features = ["rt-tokio"] }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
-    "json",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
+    "json",
 ] }
 dotenvy = "0.15.1"
 tracing-log = { version = "0.1.3", features = ["env_logger"] }
 opentelemetry-zipkin = "0.15.0"
 sentry-tracing = "0.27.0"
 
 parse_duration = "2.1.1"
@@ -113,24 +104,32 @@
 lto = true
 
 [dev-dependencies]
 backoff = { version = "0.4.0", features = ["tokio"] }
 
 once_cell = { version = "1.17" }
 
-tokio = { version = "1.12.0", features = [
-    "rt-multi-thread",
-    "macros",
-    "sync",
-    "time",
-    "signal",
-    "fs",
+tokio = { version = "1.12.0", features = [
+    "rt-multi-thread",
+    "macros",
+    "sync",
+    "time",
+    "signal",
+    "fs",
 ] }
 
 tracing = { version = "0.1.29" }
 tracing-log = { version = "0.1.3", features = ["env_logger"] }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
 ] }
 uuid = { version = "1.1", features = ["v4", "fast-rng", "macro-diagnostics"] }
+serial_test = "2.0.0"
+
+tempfile = "3.2.0"
+regex = "1.5.5"
+lazy_static = "1.4.0"
+openssl = { version = "0.10", features = ["vendored"] }
+
+portpicker = "0.1.1"
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/.rustc_info.json` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/.rustc_info.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/Makefile` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/Makefile`

 * *Files 27% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-.PHONY: install-dev
-install-dev:
-	pip install --upgrade pip wheel
-	cd .. && pip install \
-		-r test-requirements.txt \
-		-r code-requirements.txt \
-		-r nucliadb_node/requirements-sources.txt \
-		-r nucliadb_node/requirements.txt
-	pip install -e .
-
-.PHONY: format
-format:
-	isort --profile black .
-	black .
-
-.PHONY: lint
-lint:
-	flake8 nucliadb_node --config=setup.cfg
-	isort -c --profile black .
-	black --check .
-	MYPYPATH=../mypy_stubs mypy --config-file=../mypy.ini .
-
-.PHONY: test
-test:
-	pytest -s --tb=native -v nucliadb_node
-
-.PHONY: test-cov
-test-cov:
-	pytest -rfE --cov=nucliadb_node --cov-config=../.coveragerc -s --tb=native -v --cov-report term-missing:skip-covered --cov-report xml nucliadb_node
-
-.PHONY: build
-build:
-	export RUSTFLAGS="--cfg=prometheus_metrics" && cargo build
+.PHONY: install-dev
+install-dev:
+	pip install --upgrade pip wheel
+	cd .. && pip install \
+		-r test-requirements.txt \
+		-r code-requirements.txt \
+		-r nucliadb_node/requirements-sources.txt \
+		-r nucliadb_node/requirements.txt
+	pip install -e .
+
+.PHONY: format
+format:
+	cd .. && isort --profile black nucliadb_node
+	black .
+
+.PHONY: lint
+lint:
+	flake8 nucliadb_node --config=setup.cfg
+	cd .. && isort -c --profile black nucliadb_node
+	black --check .
+	MYPYPATH=../mypy_stubs mypy --config-file=../mypy.ini .
+
+.PHONY: test
+test:
+	pytest -s --tb=native -v nucliadb_node
+
+.PHONY: test-cov
+test-cov:
+	pytest -rfE --cov=nucliadb_node --cov-config=../.coveragerc -s --tb=native -v --cov-report term-missing:skip-covered --cov-report xml nucliadb_node
+
+.PHONY: build
+build:
+	export RUSTFLAGS="--cfg=prometheus_metrics" && cargo build
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/README.md` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/README.md`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-# NucliaDB Node
-
-<p align="center">
-  <img src="../docs/assets/images/node_scheme.png" alt="nucliadb_node"  width="500px" style="background-color: #fff">
-</p>
-
-The node is nucliadb's own indexing system, used for storing and retrieving complex AI data like vector embeddings or relations.
-Currently the main crates involved in making this system possible are shown in the previous image, along with how they interact with each other.
-
-### nucliadb_core
-
-The node contains four different indexes that need to share certain functionality like error handling, tracing or handling protos types. `nucliadb_core` is the 
-crate where all this shared dependencies live.
-
-### nucliadb_node
-
-Interacting with nucliadb's four indexes requiers going through `nucliadb_node`. This crate provides a grpc interface for adding, removing and searching informacion stored in 
-the four indexes that live inside the node.
-
-### nucliadb_vectors
-
-Is possible to index vector embeddings using `nucliadb_node` thanks to this crate, nucliadb's own [HNSW](https://arxiv.org/abs/1603.09320) implementaion.
-
-### nucliadb_relations
-
-Is possible to index knowledge graphs using `nucliadb_node` thanks to this crate, nucliadb's own knowledge graph implementation built on top of [heed](https://github.com/meilisearch/heed).
-
-
-### nucliadb_paragraphs and nucliadb_fields
-
-`nucliadb_node` uses [Tantivy](https://github.com/quickwit-oss/tantivy) to obtain full-text search capabilities, we offer complex querying, BM25 and fuzzy search.
+# NucliaDB Node
+
+<p align="center">
+  <img src="../docs/assets/images/node_scheme.png" alt="nucliadb_node"  width="500px" style="background-color: #fff">
+</p>
+
+The node is nucliadb's own indexing system, used for storing and retrieving complex AI data like vector embeddings or relations.
+Currently the main crates involved in making this system possible are shown in the previous image, along with how they interact with each other.
+
+### nucliadb_core
+
+The node contains four different indexes that need to share certain functionality like error handling, tracing or handling protos types. `nucliadb_core` is the 
+crate where all this shared dependencies live.
+
+### nucliadb_node
+
+Interacting with nucliadb's four indexes requiers going through `nucliadb_node`. This crate provides a grpc interface for adding, removing and searching informacion stored in 
+the four indexes that live inside the node.
+
+### nucliadb_vectors
+
+Is possible to index vector embeddings using `nucliadb_node` thanks to this crate, nucliadb's own [HNSW](https://arxiv.org/abs/1603.09320) implementaion.
+
+### nucliadb_relations
+
+Is possible to index knowledge graphs using `nucliadb_node` thanks to this crate, nucliadb's own knowledge graph implementation built on top of [heed](https://github.com/meilisearch/heed).
+
+
+### nucliadb_paragraphs and nucliadb_fields
+
+`nucliadb_node` uses [Tantivy](https://github.com/quickwit-oss/tantivy) to obtain full-text search capabilities, we offer complex querying, BM25 and fuzzy search.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/__init__.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import logging
-
-SERVICE_NAME = "nucliadb_node"
-
-logger = logging.getLogger(SERVICE_NAME)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import logging
+
+SERVICE_NAME = "nucliadb_node"
+
+logger = logging.getLogger(SERVICE_NAME)
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/app.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/app.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-
-import asyncio
-import uuid
-
-import pkg_resources
-from nucliadb_telemetry import errors
-from nucliadb_telemetry.logs import setup_logging
-from nucliadb_telemetry.utils import setup_telemetry
-from nucliadb_utils.fastapi.run import serve_metrics
-from nucliadb_utils.run import run_until_exit
-
-from nucliadb_node import SERVICE_NAME, logger
-from nucliadb_node.pull import Worker
-from nucliadb_node.reader import Reader
-from nucliadb_node.service import start_grpc
-from nucliadb_node.settings import settings
-from nucliadb_node.writer import Writer
-
-
-async def start_worker(writer: Writer, reader: Reader) -> Worker:
-    if settings.force_host_id is None:  # pragma: no cover
-        node = None
-        i = 0
-        while node is None and i < 20:
-            try:
-                with open(settings.host_key_path, "rb") as file_key:
-                    uuid_bytes = file_key.read()
-                    node = str(uuid.UUID(bytes=uuid_bytes))
-            except FileNotFoundError:
-                logger.error("Could not find key")
-                node = None
-                i += 1
-                await asyncio.sleep(2)
-    else:
-        node = settings.force_host_id
-
-    if node is None:
-        raise Exception("No Key defined")
-
-    worker = Worker(writer=writer, reader=reader, node=node)
-    await worker.initialize()
-
-    return worker
-
-
-async def main():
-    writer = Writer(settings.writer_listen_address)
-    reader = Reader(settings.reader_listen_address)
-    worker = await start_worker(writer, reader)
-
-    await setup_telemetry(SERVICE_NAME)
-
-    logger.info(f"Node ID : {worker.node}")
-
-    grpc_finalizer = await start_grpc(writer=writer, reader=reader)
-
-    logger.info(f"======= Node sidecar started ======")
-
-    metrics_server = await serve_metrics()
-
-    finalizers = [
-        grpc_finalizer,
-        worker.finalize,
-        metrics_server.shutdown,
-        writer.close,
-        reader.close,
-    ]
-
-    await run_until_exit(finalizers)
-
-
-def run():  # pragma: no cover
-    setup_logging()
-
-    errors.setup_error_handling(pkg_resources.get_distribution("nucliadb_node").version)
-
-    asyncio.run(main())
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+
+import asyncio
+import uuid
+
+import pkg_resources
+
+from nucliadb_node import SERVICE_NAME, logger
+from nucliadb_node.pull import Worker
+from nucliadb_node.reader import Reader
+from nucliadb_node.service import start_grpc
+from nucliadb_node.settings import settings
+from nucliadb_node.writer import Writer
+from nucliadb_telemetry import errors
+from nucliadb_telemetry.logs import setup_logging
+from nucliadb_telemetry.utils import setup_telemetry
+from nucliadb_utils.fastapi.run import serve_metrics
+from nucliadb_utils.run import run_until_exit
+
+
+async def start_worker(writer: Writer, reader: Reader) -> Worker:
+    if settings.force_host_id is None:  # pragma: no cover
+        node = None
+        i = 0
+        while node is None and i < 20:
+            try:
+                with open(settings.host_key_path, "rb") as file_key:
+                    uuid_bytes = file_key.read()
+                    node = str(uuid.UUID(bytes=uuid_bytes))
+            except FileNotFoundError:
+                logger.error("Could not find key")
+                node = None
+                i += 1
+                await asyncio.sleep(2)
+    else:
+        node = settings.force_host_id
+
+    if node is None:
+        raise Exception("No Key defined")
+
+    worker = Worker(writer=writer, reader=reader, node=node)
+    await worker.initialize()
+
+    return worker
+
+
+async def main():
+    writer = Writer(settings.writer_listen_address)
+    reader = Reader(settings.reader_listen_address)
+    worker = await start_worker(writer, reader)
+
+    await setup_telemetry(SERVICE_NAME)
+
+    logger.info(f"Node ID : {worker.node}")
+
+    grpc_finalizer = await start_grpc(writer=writer, reader=reader)
+
+    logger.info(f"======= Node sidecar started ======")
+
+    metrics_server = await serve_metrics()
+
+    finalizers = [
+        grpc_finalizer,
+        worker.finalize,
+        metrics_server.shutdown,
+        writer.close,
+        reader.close,
+    ]
+
+    await run_until_exit(finalizers)
+
+
+def run():  # pragma: no cover
+    setup_logging()
+
+    errors.setup_error_handling(pkg_resources.get_distribution("nucliadb_node").version)
+
+    asyncio.run(main())
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/pull.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/pull.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,351 +1,351 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# We need to pull from jetstream key partition
-
-import asyncio
-from typing import List, Optional
-
-import nats
-from grpc import StatusCode
-from grpc.aio import AioRpcError  # type: ignore
-from nats.aio.client import Msg
-from nats.aio.subscription import Subscription
-from nats.js.errors import NotFoundError as StreamNotFoundError
-from nucliadb_protos.noderesources_pb2 import Resource, ResourceID, ShardIds
-from nucliadb_protos.nodewriter_pb2 import IndexMessage, OpStatus, TypeMessage
-from nucliadb_protos.writer_pb2 import Notification
-from nucliadb_telemetry import errors, metrics
-from nucliadb_utils import const
-from nucliadb_utils.nats import get_traced_jetstream
-from nucliadb_utils.storages.exceptions import IndexDataNotFound
-from nucliadb_utils.storages.storage import Storage
-from nucliadb_utils.utilities import get_pubsub, get_storage
-
-from nucliadb_node import SERVICE_NAME, logger
-from nucliadb_node.reader import Reader
-from nucliadb_node.settings import indexing_settings, settings
-from nucliadb_node.writer import Writer
-
-subscriber_observer = metrics.Observer(
-    "message_processor",
-    buckets=[
-        0.01,
-        0.025,
-        0.05,
-        0.1,
-        0.5,
-        1.0,
-        2.5,
-        5.0,
-        7.5,
-        10.0,
-        30.0,
-        60.0,
-        120.0,
-        float("inf"),
-    ],
-)
-
-
-class Worker:
-    subscriptions: List[Subscription]
-    storage: Storage
-
-    def __init__(
-        self,
-        writer: Writer,
-        reader: Reader,
-        node: str,
-    ):
-        self.writer = writer
-        self.reader = reader
-        self.subscriptions = []
-        self.ack_wait = 10 * 60
-        self.lock = asyncio.Lock()
-        self.event = asyncio.Event()
-        self.node = node
-        self.gc_task = None
-        self.publisher = IndexedPublisher()
-        self.load_seqid()
-        self.brain: Optional[Resource] = None
-
-    async def finalize(self):
-        if self.gc_task:
-            self.gc_task.cancel()
-
-        await self.publisher.finalize()
-        await self.subscriber_finalize()
-
-        await self.storage.finalize()
-
-    async def disconnected_cb(self):
-        logger.info("Got disconnected from NATS!")
-
-    async def reconnected_cb(self):
-        # See who we are connected to on reconnect
-        logger.warning(
-            f"Got reconnected to NATS {self.nc.connected_url}. Attempting reconnect"
-        )
-        await self.drain_subscriptions()
-        await self.subscribe()
-
-    async def error_cb(self, e):
-        errors.capture_exception(e)
-        logger.error(
-            "There was an error on the worker, check sentry: {}".format(e),
-            exc_info=True,
-        )
-
-    async def closed_cb(self):
-        logger.info("Connection is closed on NATS")
-
-    async def initialize(self):
-        self.storage = await get_storage(service_name=SERVICE_NAME)
-        self.event.clear()
-        await self.publisher.initialize()
-        await self.subscriber_initialize()
-        self.gc_task = asyncio.create_task(self.garbage())
-
-    async def subscriber_initialize(self):
-        options = {
-            "error_cb": self.error_cb,
-            "closed_cb": self.closed_cb,
-            "reconnected_cb": self.reconnected_cb,
-        }
-
-        if indexing_settings.index_jetstream_auth is not None:
-            options["user_credentials"] = indexing_settings.index_jetstream_auth
-
-        if len(indexing_settings.index_jetstream_servers) > 0:
-            options["servers"] = indexing_settings.index_jetstream_servers
-
-        self.nc = await nats.connect(**options)
-        self.js = get_traced_jetstream(self.nc, SERVICE_NAME)
-        logger.info(f"Nats: Connected to {indexing_settings.index_jetstream_servers}")
-        await self.subscribe()
-
-    async def drain_subscriptions(self) -> None:
-        for subscription in self.subscriptions:
-            try:
-                await subscription.drain()
-            except nats.errors.ConnectionClosedError:
-                pass
-        self.subscriptions = []
-
-    async def subscriber_finalize(self):
-        await self.drain_subscriptions()
-        try:
-            await self.nc.close()
-        except (RuntimeError, AttributeError):  # pragma: no cover
-            # RuntimeError: can be thrown if event loop is closed
-            # AttributeError: can be thrown by nats-py when handling shutdown
-            pass
-
-    async def garbage(self) -> None:
-        try:
-            await self._garbage()
-        except (asyncio.CancelledError, RuntimeError):  # pragma: no cover
-            return
-
-    async def _garbage(self) -> None:
-        while True:
-            await self.event.wait()
-            await asyncio.sleep(10)
-            if self.event.is_set():
-                async with self.lock:
-                    try:
-                        logger.info(f"Mr Propper working")
-                        shards: ShardIds = await self.writer.shards()
-                        for shard in shards.ids:
-                            await self.writer.garbage_collector(shard)
-                        logger.info(f"Garbaged {len(shards.ids)}")
-                    except Exception:
-                        logger.exception(
-                            f"Could not garbage {shard.id}", stack_info=True
-                        )
-                await asyncio.sleep(24 * 3660)
-
-    def store_seqid(self, seqid: int):
-        if settings.data_path is None:
-            raise Exception("We need a DATA_PATH env")
-        with open(f"{settings.data_path}/seqid", "w+") as seqfile:
-            seqfile.write(str(seqid))
-        self.last_seqid = seqid
-
-    def load_seqid(self):
-        if settings.data_path is None:
-            raise Exception("We need a DATA_PATH env")
-        try:
-            with open(f"{settings.data_path}/seqid", "r") as seqfile:
-                self.last_seqid = int(seqfile.read())
-        except FileNotFoundError:
-            # First time the consumer is started
-            self.last_seqid = None
-
-    async def set_resource(self, pb: IndexMessage) -> OpStatus:
-        self.brain = await self.storage.get_indexing(pb)
-        self.brain.shard_id = self.brain.resource.shard_id = pb.shard
-        logger.info(
-            f"Added {self.brain.resource.uuid} at {self.brain.shard_id} otx:{pb.txid}"
-        )
-        status = await self.writer.set_resource(self.brain)
-        logger.info(f"...done")
-        del self.brain
-        self.brain = None
-        return status
-
-    async def delete_resource(self, pb: IndexMessage) -> OpStatus:
-        logger.info(f"Deleting {pb.resource} otx:{pb.txid}")
-        rid = ResourceID(uuid=pb.resource, shard_id=pb.shard)
-        status = await self.writer.delete_resource(rid)
-        logger.info(f"...done")
-        return status
-
-    @subscriber_observer.wrap()
-    async def subscription_worker(self, msg: Msg):
-        subject = msg.subject
-        reply = msg.reply
-        seqid = int(msg.reply.split(".")[5])
-        logger.info(
-            f"Message received: subject:{subject}, seqid: {seqid}, reply: {reply}"
-        )
-        if self.last_seqid and self.last_seqid >= seqid:
-            logger.warning(
-                f"Skipping already processed message. Msg seqid {seqid} vs Last seqid {self.last_seqid}"
-            )
-            await msg.ack()
-            return
-
-        self.event.clear()
-        status: Optional[OpStatus] = None
-        async with self.lock:
-            try:
-                pb = IndexMessage()
-                pb.ParseFromString(msg.data)
-                if pb.typemessage == TypeMessage.CREATION:
-                    status = await self.set_resource(pb)
-                elif pb.typemessage == TypeMessage.DELETION:
-                    status = await self.delete_resource(pb)
-                if status:
-                    self.reader.update(pb.shard, status)
-
-            except AioRpcError as grpc_error:
-                if grpc_error.code() == StatusCode.NOT_FOUND:
-                    logger.error(f"Shard does not exist {pb.shard}")
-                else:
-                    event_id = errors.capture_exception(grpc_error)
-                    logger.error(
-                        f"An error on subscription_worker. Check sentry for more details. Event id: {event_id}"
-                    )
-                    if (
-                        pb.typemessage == TypeMessage.CREATION
-                        and self.brain
-                        and self.brain.HasField("metadata")
-                    ):
-                        # Hard fail if we have the correct data
-                        raise grpc_error
-
-            except IndexDataNotFound as storage_error:
-                # This should never happen now.
-                # Remove this block in the future once we're confident it's not needed.
-                errors.capture_exception(storage_error)
-                logger.warning(
-                    "Error retrieving the indexing payload we do not block as that means its already deleted!"
-                )
-            except Exception as e:
-                event_id = errors.capture_exception(e)
-                logger.error(
-                    f"An error on subscription_worker. Check sentry for more details. Event id: {event_id}"
-                )
-                raise e
-        try:
-            self.store_seqid(seqid)
-            await msg.ack()
-            self.event.set()
-            await self.publisher.indexed(pb)
-        except Exception as e:  # pragma: no cover
-            errors.capture_exception(e)
-            logger.error(
-                f"An error on subscription_worker. Check sentry for more details."
-            )
-            raise e
-
-    async def subscribe(self):
-        logger.info(f"Last seqid {self.last_seqid}")
-        try:
-            await self.js.stream_info(const.Streams.INDEX.name)
-        except StreamNotFoundError:
-            logger.info("Creating stream")
-            res = await self.js.add_stream(
-                name=const.Streams.INDEX.name,
-                subjects=[
-                    const.Streams.INDEX.subject.format(node=">"),
-                ],
-            )
-            await self.js.stream_info(const.Streams.INDEX.name)
-
-        subject = const.Streams.INDEX.subject.format(node=self.node)
-        res = await self.js.subscribe(
-            subject=subject,
-            queue=const.Streams.INDEX.group.format(node=self.node),
-            stream=const.Streams.INDEX.name,
-            flow_control=True,
-            cb=self.subscription_worker,
-            config=nats.js.api.ConsumerConfig(
-                deliver_policy=nats.js.api.DeliverPolicy.BY_START_SEQUENCE,
-                opt_start_seq=self.last_seqid or 1,
-                ack_policy=nats.js.api.AckPolicy.EXPLICIT,
-                max_deliver=10000,
-                max_ack_pending=1,
-                ack_wait=self.ack_wait,
-                idle_heartbeat=5,
-            ),
-        )
-        self.subscriptions.append(res)
-        logger.info(f"Subscribed to {subject} on stream {const.Streams.INDEX.name}")
-
-
-class IndexedPublisher:
-    def __init__(self):
-        self.pubsub = None
-
-    async def initialize(self):
-        self.pubsub = await get_pubsub()
-
-    async def finalize(self):
-        await self.pubsub.finalize()
-
-    async def indexed(self, indexpb: IndexMessage):
-        if not indexpb.HasField("partition"):
-            logger.warning(f"Could not publish message without partition")
-            return
-
-        message = Notification(
-            partition=int(indexpb.partition),
-            seqid=indexpb.txid,
-            uuid=indexpb.resource,
-            kbid=indexpb.kbid,
-            action=Notification.INDEXED,
-        )
-
-        await self.pubsub.publish(
-            const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=indexpb.kbid),
-            message.SerializeToString(),
-        )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# We need to pull from jetstream key partition
+
+import asyncio
+from typing import List, Optional
+
+import nats
+from grpc import StatusCode
+from grpc.aio import AioRpcError  # type: ignore
+from nats.aio.client import Msg
+from nats.aio.subscription import Subscription
+from nats.js.errors import NotFoundError as StreamNotFoundError
+from nucliadb_protos.noderesources_pb2 import Resource, ResourceID, ShardIds
+from nucliadb_protos.nodewriter_pb2 import IndexMessage, OpStatus, TypeMessage
+from nucliadb_protos.writer_pb2 import Notification
+
+from nucliadb_node import SERVICE_NAME, logger
+from nucliadb_node.reader import Reader
+from nucliadb_node.settings import indexing_settings, settings
+from nucliadb_node.writer import Writer
+from nucliadb_telemetry import errors, metrics
+from nucliadb_utils import const
+from nucliadb_utils.nats import get_traced_jetstream
+from nucliadb_utils.storages.exceptions import IndexDataNotFound
+from nucliadb_utils.storages.storage import Storage
+from nucliadb_utils.utilities import get_pubsub, get_storage
+
+subscriber_observer = metrics.Observer(
+    "message_processor",
+    buckets=[
+        0.01,
+        0.025,
+        0.05,
+        0.1,
+        0.5,
+        1.0,
+        2.5,
+        5.0,
+        7.5,
+        10.0,
+        30.0,
+        60.0,
+        120.0,
+        float("inf"),
+    ],
+)
+
+
+class Worker:
+    subscriptions: List[Subscription]
+    storage: Storage
+
+    def __init__(
+        self,
+        writer: Writer,
+        reader: Reader,
+        node: str,
+    ):
+        self.writer = writer
+        self.reader = reader
+        self.subscriptions = []
+        self.ack_wait = 10 * 60
+        self.lock = asyncio.Lock()
+        self.event = asyncio.Event()
+        self.node = node
+        self.gc_task = None
+        self.publisher = IndexedPublisher()
+        self.load_seqid()
+        self.brain: Optional[Resource] = None
+
+    async def finalize(self):
+        if self.gc_task:
+            self.gc_task.cancel()
+
+        await self.publisher.finalize()
+        await self.subscriber_finalize()
+
+        await self.storage.finalize()
+
+    async def disconnected_cb(self):
+        logger.info("Got disconnected from NATS!")
+
+    async def reconnected_cb(self):
+        # See who we are connected to on reconnect
+        logger.warning(
+            f"Got reconnected to NATS {self.nc.connected_url}. Attempting reconnect"
+        )
+        await self.drain_subscriptions()
+        await self.subscribe()
+
+    async def error_cb(self, e):
+        errors.capture_exception(e)
+        logger.error(
+            "There was an error on the worker, check sentry: {}".format(e),
+            exc_info=True,
+        )
+
+    async def closed_cb(self):
+        logger.info("Connection is closed on NATS")
+
+    async def initialize(self):
+        self.storage = await get_storage(service_name=SERVICE_NAME)
+        self.event.clear()
+        await self.publisher.initialize()
+        await self.subscriber_initialize()
+        self.gc_task = asyncio.create_task(self.garbage())
+
+    async def subscriber_initialize(self):
+        options = {
+            "error_cb": self.error_cb,
+            "closed_cb": self.closed_cb,
+            "reconnected_cb": self.reconnected_cb,
+        }
+
+        if indexing_settings.index_jetstream_auth is not None:
+            options["user_credentials"] = indexing_settings.index_jetstream_auth
+
+        if len(indexing_settings.index_jetstream_servers) > 0:
+            options["servers"] = indexing_settings.index_jetstream_servers
+
+        self.nc = await nats.connect(**options)
+        self.js = get_traced_jetstream(self.nc, SERVICE_NAME)
+        logger.info(f"Nats: Connected to {indexing_settings.index_jetstream_servers}")
+        await self.subscribe()
+
+    async def drain_subscriptions(self) -> None:
+        for subscription in self.subscriptions:
+            try:
+                await subscription.drain()
+            except nats.errors.ConnectionClosedError:
+                pass
+        self.subscriptions = []
+
+    async def subscriber_finalize(self):
+        await self.drain_subscriptions()
+        try:
+            await self.nc.close()
+        except (RuntimeError, AttributeError):  # pragma: no cover
+            # RuntimeError: can be thrown if event loop is closed
+            # AttributeError: can be thrown by nats-py when handling shutdown
+            pass
+
+    async def garbage(self) -> None:
+        try:
+            await self._garbage()
+        except (asyncio.CancelledError, RuntimeError):  # pragma: no cover
+            return
+
+    async def _garbage(self) -> None:
+        while True:
+            await self.event.wait()
+            await asyncio.sleep(10)
+            if self.event.is_set():
+                async with self.lock:
+                    try:
+                        logger.info(f"Mr Propper working")
+                        shards: ShardIds = await self.writer.shards()
+                        for shard in shards.ids:
+                            await self.writer.garbage_collector(shard)
+                        logger.info(f"Garbaged {len(shards.ids)}")
+                    except Exception:
+                        logger.exception(
+                            f"Could not garbage {shard.id}", stack_info=True
+                        )
+                await asyncio.sleep(24 * 3660)
+
+    def store_seqid(self, seqid: int):
+        if settings.data_path is None:
+            raise Exception("We need a DATA_PATH env")
+        with open(f"{settings.data_path}/seqid", "w+") as seqfile:
+            seqfile.write(str(seqid))
+        self.last_seqid = seqid
+
+    def load_seqid(self):
+        if settings.data_path is None:
+            raise Exception("We need a DATA_PATH env")
+        try:
+            with open(f"{settings.data_path}/seqid", "r") as seqfile:
+                self.last_seqid = int(seqfile.read())
+        except FileNotFoundError:
+            # First time the consumer is started
+            self.last_seqid = None
+
+    async def set_resource(self, pb: IndexMessage) -> OpStatus:
+        self.brain = await self.storage.get_indexing(pb)
+        self.brain.shard_id = self.brain.resource.shard_id = pb.shard
+        logger.info(
+            f"Added {self.brain.resource.uuid} at {self.brain.shard_id} otx:{pb.txid}"
+        )
+        status = await self.writer.set_resource(self.brain)
+        logger.info(f"...done")
+        del self.brain
+        self.brain = None
+        return status
+
+    async def delete_resource(self, pb: IndexMessage) -> OpStatus:
+        logger.info(f"Deleting {pb.resource} otx:{pb.txid}")
+        rid = ResourceID(uuid=pb.resource, shard_id=pb.shard)
+        status = await self.writer.delete_resource(rid)
+        logger.info(f"...done")
+        return status
+
+    @subscriber_observer.wrap()
+    async def subscription_worker(self, msg: Msg):
+        subject = msg.subject
+        reply = msg.reply
+        seqid = int(msg.reply.split(".")[5])
+        logger.info(
+            f"Message received: subject:{subject}, seqid: {seqid}, reply: {reply}"
+        )
+        if self.last_seqid and self.last_seqid >= seqid:
+            logger.warning(
+                f"Skipping already processed message. Msg seqid {seqid} vs Last seqid {self.last_seqid}"
+            )
+            await msg.ack()
+            return
+
+        self.event.clear()
+        status: Optional[OpStatus] = None
+        async with self.lock:
+            try:
+                pb = IndexMessage()
+                pb.ParseFromString(msg.data)
+                if pb.typemessage == TypeMessage.CREATION:
+                    status = await self.set_resource(pb)
+                elif pb.typemessage == TypeMessage.DELETION:
+                    status = await self.delete_resource(pb)
+                if status:
+                    self.reader.update(pb.shard, status)
+
+            except AioRpcError as grpc_error:
+                if grpc_error.code() == StatusCode.NOT_FOUND:
+                    logger.error(f"Shard does not exist {pb.shard}")
+                else:
+                    event_id = errors.capture_exception(grpc_error)
+                    logger.error(
+                        f"An error on subscription_worker. Check sentry for more details. Event id: {event_id}"
+                    )
+                    if (
+                        pb.typemessage == TypeMessage.CREATION
+                        and self.brain
+                        and self.brain.HasField("metadata")
+                    ):
+                        # Hard fail if we have the correct data
+                        raise grpc_error
+
+            except IndexDataNotFound as storage_error:
+                # This should never happen now.
+                # Remove this block in the future once we're confident it's not needed.
+                errors.capture_exception(storage_error)
+                logger.warning(
+                    "Error retrieving the indexing payload we do not block as that means its already deleted!"
+                )
+            except Exception as e:
+                event_id = errors.capture_exception(e)
+                logger.error(
+                    f"An error on subscription_worker. Check sentry for more details. Event id: {event_id}"
+                )
+                raise e
+        try:
+            self.store_seqid(seqid)
+            await msg.ack()
+            self.event.set()
+            await self.publisher.indexed(pb)
+        except Exception as e:  # pragma: no cover
+            errors.capture_exception(e)
+            logger.error(
+                f"An error on subscription_worker. Check sentry for more details."
+            )
+            raise e
+
+    async def subscribe(self):
+        logger.info(f"Last seqid {self.last_seqid}")
+        try:
+            await self.js.stream_info(const.Streams.INDEX.name)
+        except StreamNotFoundError:
+            logger.info("Creating stream")
+            res = await self.js.add_stream(
+                name=const.Streams.INDEX.name,
+                subjects=[
+                    const.Streams.INDEX.subject.format(node=">"),
+                ],
+            )
+            await self.js.stream_info(const.Streams.INDEX.name)
+
+        subject = const.Streams.INDEX.subject.format(node=self.node)
+        res = await self.js.subscribe(
+            subject=subject,
+            queue=const.Streams.INDEX.group.format(node=self.node),
+            stream=const.Streams.INDEX.name,
+            flow_control=True,
+            cb=self.subscription_worker,
+            config=nats.js.api.ConsumerConfig(
+                deliver_policy=nats.js.api.DeliverPolicy.BY_START_SEQUENCE,
+                opt_start_seq=self.last_seqid or 1,
+                ack_policy=nats.js.api.AckPolicy.EXPLICIT,
+                max_deliver=10000,
+                max_ack_pending=1,
+                ack_wait=self.ack_wait,
+                idle_heartbeat=5,
+            ),
+        )
+        self.subscriptions.append(res)
+        logger.info(f"Subscribed to {subject} on stream {const.Streams.INDEX.name}")
+
+
+class IndexedPublisher:
+    def __init__(self):
+        self.pubsub = None
+
+    async def initialize(self):
+        self.pubsub = await get_pubsub()
+
+    async def finalize(self):
+        await self.pubsub.finalize()
+
+    async def indexed(self, indexpb: IndexMessage):
+        if not indexpb.HasField("partition"):
+            logger.warning(f"Could not publish message without partition")
+            return
+
+        message = Notification(
+            partition=int(indexpb.partition),
+            seqid=indexpb.txid,
+            uuid=indexpb.resource,
+            kbid=indexpb.kbid,
+            action=Notification.INDEXED,
+        )
+
+        await self.pubsub.publish(
+            const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=indexpb.kbid),
+            message.SerializeToString(),
+        )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/service.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/service.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-
-from grpc import aio  # type: ignore
-from grpc_health.v1 import health, health_pb2_grpc
-from nucliadb_protos import nodesidecar_pb2_grpc
-from nucliadb_utils.grpc import get_traced_grpc_server
-
-from nucliadb_node import SERVICE_NAME
-from nucliadb_node.reader import Reader  # type: ignore
-from nucliadb_node.servicer import SidecarServicer
-from nucliadb_node.settings import settings
-from nucliadb_node.writer import Writer
-
-
-async def start_grpc(writer: Writer, reader: Reader):
-    aio.init_grpc_aio()
-
-    server = get_traced_grpc_server(SERVICE_NAME)
-    servicer = SidecarServicer(reader=reader, writer=writer)
-    await servicer.initialize()
-    health_servicer = health.aio.HealthServicer()  # type: ignore
-    server.add_insecure_port(settings.sidecar_listen_address)
-
-    nodesidecar_pb2_grpc.add_NodeSidecarServicer_to_server(servicer, server)
-    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
-    await server.start()
-
-    async def finalizer():
-        await servicer.finalize()
-        await server.stop(grace=False)
-
-    return finalizer
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+
+from grpc import aio  # type: ignore
+from grpc_health.v1 import health, health_pb2_grpc
+
+from nucliadb_node import SERVICE_NAME
+from nucliadb_node.reader import Reader  # type: ignore
+from nucliadb_node.servicer import SidecarServicer
+from nucliadb_node.settings import settings
+from nucliadb_node.writer import Writer
+from nucliadb_protos import nodesidecar_pb2_grpc
+from nucliadb_utils.grpc import get_traced_grpc_server
+
+
+async def start_grpc(writer: Writer, reader: Reader):
+    aio.init_grpc_aio()
+
+    server = get_traced_grpc_server(SERVICE_NAME)
+    servicer = SidecarServicer(reader=reader, writer=writer)
+    await servicer.initialize()
+    health_servicer = health.aio.HealthServicer()  # type: ignore
+    server.add_insecure_port(settings.sidecar_listen_address)
+
+    nodesidecar_pb2_grpc.add_NodeSidecarServicer_to_server(servicer, server)
+    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
+    await server.start()
+
+    async def finalizer():
+        await servicer.finalize()
+        await server.stop(grace=False)
+
+    return finalizer
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/settings.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/settings.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,46 +1,47 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-from typing import Optional
-
-from nucliadb_utils import settings as utils_settings
-from pydantic import BaseSettings
-
-
-class Settings(BaseSettings):
-    host_key_path: str = "node.key"
-    force_host_id: Optional[str] = None
-
-    writer_listen_address: str = "0.0.0.0:10000"
-    reader_listen_address: str = "0.0.0.0:10001"
-    sidecar_listen_address: str = "0.0.0.0:10002"
-
-    data_path: Optional[str] = None
-
-
-settings = Settings()
-indexing_settings = utils_settings.IndexingSettings()
-
-
-class RunningSettings(BaseSettings):
-    debug: bool = True
-    log_level: str = "DEBUG"
-
-
-running_settings = RunningSettings()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+from typing import Optional
+
+from pydantic import BaseSettings
+
+from nucliadb_utils import settings as utils_settings
+
+
+class Settings(BaseSettings):
+    host_key_path: str = "node.key"
+    force_host_id: Optional[str] = None
+
+    writer_listen_address: str = "0.0.0.0:10000"
+    reader_listen_address: str = "0.0.0.0:10001"
+    sidecar_listen_address: str = "0.0.0.0:10002"
+
+    data_path: Optional[str] = None
+
+
+settings = Settings()
+indexing_settings = utils_settings.IndexingSettings()
+
+
+class RunningSettings(BaseSettings):
+    debug: bool = True
+    log_level: str = "DEBUG"
+
+
+running_settings = RunningSettings()
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/mod.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,24 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-pytest_plugins = [
-    "pytest_docker_fixtures",
-    "nucliadb_utils.tests.nats",
-    "nucliadb_utils.tests.gcs",
-    "nucliadb_utils.tests.s3",
-    "nucliadb_utils.tests.indexing",
-    "nucliadb_node.tests.fixtures",
-]
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+mod debug;
+mod telemetry;
+
+pub use debug::GrpcDebugLogsLayer;
+pub use telemetry::GrpcInstrumentorLayer;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_app.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_app.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from unittest.mock import AsyncMock, MagicMock, patch
-
-import pytest
-
-from nucliadb_node import app
-
-pytestmark = pytest.mark.asyncio
-
-
-async def test_main():
-    with patch("nucliadb_node.app.start_worker", AsyncMock()) as start_worker, patch(
-        "nucliadb_node.app.start_grpc", AsyncMock()
-    ) as start_grpc, patch(
-        "nucliadb_node.app.serve_metrics", AsyncMock()
-    ) as serve_metrics, patch(
-        "nucliadb_node.app.run_until_exit", AsyncMock()
-    ) as run_until_exit, patch(
-        "nucliadb_node.app.Writer", MagicMock()
-    ) as writer, patch(
-        "nucliadb_node.app.Reader", MagicMock()
-    ) as reader:
-        await app.main()
-
-        run_until_exit.assert_awaited_once_with(
-            [
-                start_grpc.return_value,
-                start_worker.return_value.finalize,
-                serve_metrics.return_value.shutdown,
-                writer.return_value.close,
-                reader.return_value.close,
-            ]
-        )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from unittest.mock import AsyncMock, MagicMock, patch
+
+import pytest
+
+from nucliadb_node import app
+
+pytestmark = pytest.mark.asyncio
+
+
+async def test_main():
+    with patch("nucliadb_node.app.start_worker", AsyncMock()) as start_worker, patch(
+        "nucliadb_node.app.start_grpc", AsyncMock()
+    ) as start_grpc, patch(
+        "nucliadb_node.app.serve_metrics", AsyncMock()
+    ) as serve_metrics, patch(
+        "nucliadb_node.app.run_until_exit", AsyncMock()
+    ) as run_until_exit, patch(
+        "nucliadb_node.app.Writer", MagicMock()
+    ) as writer, patch(
+        "nucliadb_node.app.Reader", MagicMock()
+    ) as reader:
+        await app.main()
+
+        run_until_exit.assert_awaited_once_with(
+            [
+                start_grpc.return_value,
+                start_worker.return_value.finalize,
+                serve_metrics.return_value.shutdown,
+                writer.return_value.close,
+                reader.return_value.close,
+            ]
+        )
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/nucliadb_node/writer.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/nucliadb_node/writer.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import Optional
-
-from nucliadb_protos.noderesources_pb2 import (
-    EmptyQuery,
-    Resource,
-    ResourceID,
-    ShardId,
-    ShardIds,
-)
-from nucliadb_protos.nodewriter_pb2 import OpStatus
-from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
-from nucliadb_utils.grpc import get_traced_grpc_channel
-
-from nucliadb_node import SERVICE_NAME  # type: ignore
-
-
-class Writer:
-    _stub: Optional[NodeWriterStub] = None
-    lock: asyncio.Lock
-
-    def __init__(self, grpc_writer_address: str):
-        self.lock = asyncio.Lock()
-        self.channel = get_traced_grpc_channel(
-            grpc_writer_address, SERVICE_NAME, max_send_message=250
-        )
-        self.stub = NodeWriterStub(self.channel)
-
-    async def set_resource(self, pb: Resource) -> OpStatus:
-        return await self.stub.SetResource(pb)  # type: ignore
-
-    async def delete_resource(self, pb: ResourceID) -> OpStatus:
-        return await self.stub.RemoveResource(pb)  # type: ignore
-
-    async def garbage_collector(self, pb: ShardId):
-        await self.stub.GC(pb)  # type: ignore
-
-    async def shards(self) -> ShardIds:
-        pb = EmptyQuery()
-        return await self.stub.ListShards(pb)  # type: ignore
-
-    async def close(self):
-        await self.channel.close()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import Optional
+
+from nucliadb_protos.noderesources_pb2 import (
+    EmptyQuery,
+    Resource,
+    ResourceID,
+    ShardId,
+    ShardIds,
+)
+from nucliadb_protos.nodewriter_pb2 import OpStatus
+from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
+
+from nucliadb_node import SERVICE_NAME  # type: ignore
+from nucliadb_utils.grpc import get_traced_grpc_channel
+
+
+class Writer:
+    _stub: Optional[NodeWriterStub] = None
+    lock: asyncio.Lock
+
+    def __init__(self, grpc_writer_address: str):
+        self.lock = asyncio.Lock()
+        self.channel = get_traced_grpc_channel(
+            grpc_writer_address, SERVICE_NAME, max_send_message=250
+        )
+        self.stub = NodeWriterStub(self.channel)
+
+    async def set_resource(self, pb: Resource) -> OpStatus:
+        return await self.stub.SetResource(pb)  # type: ignore
+
+    async def delete_resource(self, pb: ResourceID) -> OpStatus:
+        return await self.stub.RemoveResource(pb)  # type: ignore
+
+    async def garbage_collector(self, pb: ShardId):
+        await self.stub.GC(pb)  # type: ignore
+
+    async def shards(self) -> ShardIds:
+        pb = EmptyQuery()
+        return await self.stub.ListShards(pb)  # type: ignore
+
+    async def close(self):
+        await self.channel.close()
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/bin/payload_test.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/bin/payload_test.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,63 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::io::Cursor;
-
-use nucliadb_core::protos::*;
-use nucliadb_core::NodeResult;
-use nucliadb_node::reader::NodeReaderService;
-use nucliadb_node::writer::NodeWriterService;
-use prost::Message;
-
-fn main() -> NodeResult<()> {
-    let writer = NodeWriterService::new()?;
-    let reader = NodeReaderService::new();
-
-    let resources_dir = std::path::Path::new("/path/to/data");
-    let new_shard = NodeWriterService::new_shard(&NewShardRequest::default())?;
-    let shard_id = ShardId { id: new_shard.id };
-    assert!(resources_dir.exists());
-    for file_path in std::fs::read_dir(resources_dir).unwrap() {
-        let file_path = file_path.unwrap().path();
-        println!("processing {file_path:?}");
-        let content = std::fs::read(&file_path).unwrap();
-        let resource = Resource::decode(&mut Cursor::new(content)).unwrap();
-        println!("Adding resource {}", file_path.display());
-        let res = writer.set_resource(&shard_id, &resource).unwrap();
-        assert!(res.is_some());
-        println!("Resource added: {:?}", res.unwrap());
-        let info = reader
-            .get_info(&shard_id, GetShardRequest::default())?
-            .unwrap();
-        println!("Sentences {}", info.sentences);
-        println!("Paragraphs {}", info.paragraphs);
-        println!("resources {}", info.resources);
-    }
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::io::Cursor;
+
+use nucliadb_core::protos::*;
+use nucliadb_core::NodeResult;
+use nucliadb_node::env;
+use nucliadb_node::shard_metadata::ShardMetadata;
+use nucliadb_node::shards::{
+    ReaderShardsProvider, UnboundedShardReaderCache, UnboundedShardWriterCache,
+    WriterShardsProvider,
+};
+use prost::Message;
+
+fn main() -> NodeResult<()> {
+    let writer = UnboundedShardWriterCache::new(env::shards_path());
+    let reader = UnboundedShardReaderCache::new(env::shards_path());
+    let resources_dir = std::path::Path::new("/path/to/data");
+
+    let metadata = ShardMetadata::from(NewShardRequest::default());
+    let new_shard = writer.create(metadata)?;
+    let shard_id = ShardId { id: new_shard.id };
+    assert!(resources_dir.exists());
+    for file_path in std::fs::read_dir(resources_dir).unwrap() {
+        let file_path = file_path.unwrap().path();
+        println!("processing {file_path:?}");
+
+        let content = std::fs::read(&file_path).unwrap();
+        let resource = Resource::decode(&mut Cursor::new(content)).unwrap();
+        println!("Adding resource {}", file_path.display());
+
+        writer.load(shard_id.id.clone())?;
+        let shard_writer = writer.get(shard_id.id.clone()).unwrap();
+        let res = shard_writer.set_resource(&resource);
+        assert!(res.is_ok());
+        println!("Resource added: {:?}", res.unwrap());
+
+        reader.load(shard_id.id.clone())?;
+        let shard_reader = reader.get(shard_id.id.clone()).unwrap();
+        let info = shard_reader.get_info(&GetShardRequest::default())?;
+        println!("Sentences {}", info.sentences);
+        println!("Paragraphs {}", info.paragraphs);
+        println!("Fields {}", info.fields);
+    }
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/bin/reader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/bin/reader.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,112 +1,116 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::time::Instant;
-
-use nucliadb_core::metrics::middleware::MetricsLayer;
-use nucliadb_core::protos::node_reader_server::NodeReaderServer;
-use nucliadb_core::tracing::*;
-use nucliadb_core::{node_error, NodeResult};
-use nucliadb_node::env;
-use nucliadb_node::http_server::{run_http_metrics_server, MetricsServerOptions};
-use nucliadb_node::middleware::{GrpcDebugLogsLayer, GrpcInstrumentorLayer};
-use nucliadb_node::reader::grpc_driver::{GrpcReaderOptions, NodeReaderGRPCDriver};
-use nucliadb_node::shards::{AsyncReaderShardsProvider, AsyncUnboundedShardReaderCache};
-use nucliadb_node::telemetry::init_telemetry;
-use tokio::signal::unix::SignalKind;
-use tokio::signal::{ctrl_c, unix};
-use tonic::transport::Server;
-
-type GrpcServer = NodeReaderServer<NodeReaderGRPCDriver>;
-
-#[tokio::main]
-async fn main() -> NodeResult<()> {
-    eprintln!("NucliaDB Reader Node starting...");
-
-    if !env::data_path().exists() {
-        return Err(node_error!("Data directory missing"));
-    }
-
-    let _guard = init_telemetry()?;
-    let start_bootstrap = Instant::now();
-    let shards_provider = AsyncUnboundedShardReaderCache::new();
-
-    if !env::lazy_loading() {
-        shards_provider.load_all().await?;
-    }
-
-    let grpc_options = GrpcReaderOptions {
-        lazy_loading: env::lazy_loading(),
-    };
-    let grpc_driver = NodeReaderGRPCDriver::new(grpc_options);
-    grpc_driver.initialize().await?;
-
-    let _grpc_task = tokio::spawn(start_grpc_service(grpc_driver));
-    let metrics_task = tokio::spawn(run_http_metrics_server(MetricsServerOptions {
-        default_http_port: 3031,
-    }));
-
-    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
-    eprintln!("Running");
-
-    wait_for_sigkill().await?;
-    info!("Shutting down NucliaDB Reader Node...");
-    // wait some time to handle latest gRPC calls
-    tokio::time::sleep(env::shutdown_delay()).await;
-    metrics_task.abort();
-    let _ = metrics_task.await;
-
-    Ok(())
-}
-
-async fn wait_for_sigkill() -> NodeResult<()> {
-    let mut sigterm = unix::signal(SignalKind::terminate())?;
-    let mut sigquit = unix::signal(SignalKind::quit())?;
-
-    tokio::select! {
-        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
-        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
-        _ = ctrl_c() => println!("Terminating on ctrl-c"),
-    }
-
-    Ok(())
-}
-
-pub async fn start_grpc_service(grpc_driver: NodeReaderGRPCDriver) {
-    let addr = env::reader_listen_address();
-
-    info!("Reader listening for gRPC requests at: {:?}", addr);
-
-    let tracing_middleware = GrpcInstrumentorLayer::default();
-    let debug_logs_middleware = GrpcDebugLogsLayer::default();
-    let metrics_middleware = MetricsLayer::default();
-
-    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
-    health_reporter.set_serving::<GrpcServer>().await;
-
-    Server::builder()
-        .layer(tracing_middleware)
-        .layer(debug_logs_middleware)
-        .layer(metrics_middleware)
-        .add_service(health_service)
-        .add_service(GrpcServer::new(grpc_driver))
-        .serve(addr)
-        .await
-        .expect("Error starting gRPC reader");
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::net::SocketAddr;
+use std::sync::Arc;
+use std::time::Instant;
+
+use nucliadb_core::metrics::middleware::MetricsLayer;
+use nucliadb_core::protos::node_reader_server::NodeReaderServer;
+use nucliadb_core::tracing::*;
+use nucliadb_core::{node_error, NodeResult};
+use nucliadb_node::http_server::{run_http_metrics_server, MetricsServerOptions};
+use nucliadb_node::middleware::{GrpcDebugLogsLayer, GrpcInstrumentorLayer};
+use nucliadb_node::reader;
+use nucliadb_node::reader::grpc_driver::NodeReaderGRPCDriver;
+use nucliadb_node::settings::providers::env::EnvSettingsProvider;
+use nucliadb_node::settings::providers::SettingsProvider;
+use nucliadb_node::telemetry::init_telemetry;
+use tokio::signal::unix::SignalKind;
+use tokio::signal::{ctrl_c, unix};
+use tonic::transport::Server;
+
+type GrpcServer = NodeReaderServer<NodeReaderGRPCDriver>;
+
+#[tokio::main]
+async fn main() -> NodeResult<()> {
+    eprintln!("NucliaDB Reader Node starting...");
+    let start_bootstrap = Instant::now();
+
+    let settings = Arc::new(EnvSettingsProvider::generate_settings()?);
+
+    if !settings.data_path().exists() {
+        return Err(node_error!("Data directory missing"));
+    }
+
+    // XXX it probably should be moved to a more clear abstraction
+    reader::initialize();
+
+    let _guard = init_telemetry(&settings)?;
+
+    let grpc_driver = NodeReaderGRPCDriver::new(Arc::clone(&settings));
+    grpc_driver.initialize().await?;
+
+    let _grpc_task = tokio::spawn(start_grpc_service(
+        grpc_driver,
+        settings.reader_listen_address(),
+    ));
+    let metrics_task = tokio::spawn(run_http_metrics_server(MetricsServerOptions {
+        default_http_port: 3031,
+    }));
+
+    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
+    eprintln!("Running");
+
+    wait_for_sigkill().await?;
+    info!("Shutting down NucliaDB Reader Node...");
+    // wait some time to handle latest gRPC calls
+    tokio::time::sleep(settings.shutdown_delay()).await;
+    metrics_task.abort();
+    let _ = metrics_task.await;
+
+    Ok(())
+}
+
+async fn wait_for_sigkill() -> NodeResult<()> {
+    let mut sigterm = unix::signal(SignalKind::terminate())?;
+    let mut sigquit = unix::signal(SignalKind::quit())?;
+
+    tokio::select! {
+        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
+        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
+        _ = ctrl_c() => println!("Terminating on ctrl-c"),
+    }
+
+    Ok(())
+}
+
+pub async fn start_grpc_service(grpc_driver: NodeReaderGRPCDriver, listen_address: SocketAddr) {
+    info!(
+        "Reader listening for gRPC requests at: {:?}",
+        listen_address
+    );
+
+    let tracing_middleware = GrpcInstrumentorLayer::default();
+    let debug_logs_middleware = GrpcDebugLogsLayer::default();
+    let metrics_middleware = MetricsLayer::default();
+
+    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
+    health_reporter.set_serving::<GrpcServer>().await;
+
+    Server::builder()
+        .layer(tracing_middleware)
+        .layer(debug_logs_middleware)
+        .layer(metrics_middleware)
+        .add_service(health_service)
+        .add_service(GrpcServer::new(grpc_driver))
+        .serve(listen_address)
+        .await
+        .expect("Error starting gRPC reader");
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use nucliadb_core::{metrics, tracing};
-
-pub async fn metrics_service() -> String {
-    let metrics = metrics::get_metrics();
-    match metrics.export() {
-        Ok(m) => m,
-        Err(err) => {
-            tracing::error!("Could not collect metrics due to {err:?}");
-            Default::default()
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use nucliadb_core::{metrics, tracing};
+
+pub async fn metrics_service() -> String {
+    let metrics = metrics::get_metrics();
+    match metrics.export() {
+        Ok(m) => m,
+        Err(err) => {
+            tracing::error!("Could not collect metrics due to {err:?}");
+            Default::default()
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/http_server/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/http_server/mod.rs`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod metrics_service;
-
-use std::net::SocketAddr;
-
-use axum::routing::get;
-use axum::Router;
-
-use crate::env::metrics_http_port;
-
-pub struct MetricsServerOptions {
-    pub default_http_port: u16,
-}
-
-pub async fn run_http_metrics_server(options: MetricsServerOptions) {
-    // Add routes to services
-    let addr = SocketAddr::from(([0, 0, 0, 0], metrics_http_port(options.default_http_port)));
-    let metrics = Router::new().route("/metrics", get(metrics_service::metrics_service));
-    axum_server::bind(addr)
-        // Services will be added here
-        .serve(metrics.into_make_service())
-        .await
-        .expect("Error starting the HTTP server");
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod metrics_service;
+
+use std::net::SocketAddr;
+
+use axum::routing::get;
+use axum::Router;
+
+use crate::env::metrics_http_port;
+
+pub struct MetricsServerOptions {
+    pub default_http_port: u16,
+}
+
+pub async fn run_http_metrics_server(options: MetricsServerOptions) {
+    // Add routes to services
+    let addr = SocketAddr::from(([0, 0, 0, 0], metrics_http_port(options.default_http_port)));
+    let metrics = Router::new().route("/metrics", get(metrics_service::metrics_service));
+    axum_server::bind(addr)
+        // Services will be added here
+        .serve(metrics.into_make_service())
+        .await
+        .expect("Error starting the HTTP server");
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/lib.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/lib.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,51 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-// NucliaDB Node component
-
-// #![warn(missing_docs)]
-
-/// Shard metadata, defined at the moment of creation.
-mod shard_metadata;
-
-pub mod node_metadata;
-
-pub mod services;
-
-pub mod middleware;
-
-pub mod shards;
-
-/// Global configuration enviromental variables
-pub mod env;
-
-/// GRPC reading service
-pub mod reader;
-
-/// Utilities
-pub mod utils;
-
-// Telemetry
-pub mod telemetry;
-
-/// GRPC writing service
-pub mod writer;
-
-/// Node's http service
-pub mod http_server;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod bfs_engine;
+mod errors;
+mod graph_db;
+#[cfg(test)]
+mod graph_test_utils;
+pub mod index;
+mod node_dictionary;
+mod relations_io;
+pub mod service;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/debug.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/debug.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,87 +1,87 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::task::{Context, Poll};
-
-use futures::future::BoxFuture;
-use hyper::Body;
-use nucliadb_core::tracing::debug;
-use tonic::body::BoxBody;
-use tower::{Layer, Service};
-
-#[derive(Debug, Clone, Default)]
-pub struct GrpcDebugLogsLayer;
-
-impl<S> Layer<S> for GrpcDebugLogsLayer {
-    type Service = GrpcDebugLogs<S>;
-
-    fn layer(&self, service: S) -> Self::Service {
-        GrpcDebugLogs { inner: service }
-    }
-}
-
-#[derive(Debug, Clone)]
-pub struct GrpcDebugLogs<S> {
-    inner: S,
-}
-
-impl<S> Service<hyper::Request<Body>> for GrpcDebugLogs<S>
-where
-    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
-    S::Error: std::fmt::Debug,
-    S::Future: Send + 'static,
-{
-    type Response = S::Response;
-    type Error = S::Error;
-    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
-
-    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
-        self.inner.poll_ready(cx)
-    }
-
-    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
-        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
-        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
-        // for details on why this is necessary
-        let clone = self.inner.clone();
-        // We need to swap the clone and the original to avoid a not ready
-        // service. See
-        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
-        // for more details
-        let mut inner = std::mem::replace(&mut self.inner, clone);
-
-        let name = req.uri().path();
-        let method = name.split('/').last().unwrap_or(name).to_string();
-
-        Box::pin(async move {
-            debug!("gRPC call {method} starts");
-            let response = inner.call(req).await;
-            match response {
-                Ok(response) => {
-                    debug!("gRPC call {method} ended correctly");
-                    Ok(response)
-                }
-                Err(error) => {
-                    debug!("gRPC call {method} failed: {error:?}");
-                    Err(error)
-                }
-            }
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::task::{Context, Poll};
+
+use futures::future::BoxFuture;
+use hyper::Body;
+use nucliadb_core::tracing::debug;
+use tonic::body::BoxBody;
+use tower::{Layer, Service};
+
+#[derive(Debug, Clone, Default)]
+pub struct GrpcDebugLogsLayer;
+
+impl<S> Layer<S> for GrpcDebugLogsLayer {
+    type Service = GrpcDebugLogs<S>;
+
+    fn layer(&self, service: S) -> Self::Service {
+        GrpcDebugLogs { inner: service }
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct GrpcDebugLogs<S> {
+    inner: S,
+}
+
+impl<S> Service<hyper::Request<Body>> for GrpcDebugLogs<S>
+where
+    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
+    S::Error: std::fmt::Debug,
+    S::Future: Send + 'static,
+{
+    type Response = S::Response;
+    type Error = S::Error;
+    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
+        self.inner.poll_ready(cx)
+    }
+
+    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
+        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
+        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
+        // for details on why this is necessary
+        let clone = self.inner.clone();
+        // We need to swap the clone and the original to avoid a not ready
+        // service. See
+        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
+        // for more details
+        let mut inner = std::mem::replace(&mut self.inner, clone);
+
+        let name = req.uri().path();
+        let method = name.split('/').last().unwrap_or(name).to_string();
+
+        Box::pin(async move {
+            debug!("gRPC call {method} starts");
+            let response = inner.call(req).await;
+            match response {
+                Ok(response) => {
+                    debug!("gRPC call {method} ended correctly");
+                    Ok(response)
+                }
+                Err(error) => {
+                    debug!("gRPC call {method} failed: {error:?}");
+                    Err(error)
+                }
+            }
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-mod debug;
-mod telemetry;
-
-pub use debug::GrpcDebugLogsLayer;
-pub use telemetry::GrpcInstrumentorLayer;
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+pytest_plugins = [
+    "pytest_docker_fixtures",
+    "nucliadb_utils.tests.nats",
+    "nucliadb_telemetry.tests.telemetry",
+]
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/middleware/telemetry.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/middleware/telemetry.rs`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,119 +1,119 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::task::{Context, Poll};
-
-use futures::future::BoxFuture;
-use hyper::Body;
-use nucliadb_core::tracing::instrument::Instrument;
-use nucliadb_core::tracing::{info_span, warn};
-use opentelemetry::propagation::Extractor;
-use tonic::body::BoxBody;
-use tower::{Layer, Service};
-use tracing_opentelemetry::OpenTelemetrySpanExt;
-
-#[derive(Debug, Clone, Default)]
-pub struct GrpcInstrumentorLayer;
-
-impl<S> Layer<S> for GrpcInstrumentorLayer {
-    type Service = GrpcInstrumentor<S>;
-
-    fn layer(&self, service: S) -> Self::Service {
-        GrpcInstrumentor { inner: service }
-    }
-}
-
-/// Dynamically instrument gRPC server endpoints which continue traces injected
-/// by clients.
-#[derive(Debug, Clone)]
-pub struct GrpcInstrumentor<S> {
-    inner: S,
-}
-
-impl<S> Service<hyper::Request<Body>> for GrpcInstrumentor<S>
-where
-    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
-    S::Future: Send + 'static,
-{
-    type Response = S::Response;
-    type Error = S::Error;
-    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
-
-    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
-        self.inner.poll_ready(cx)
-    }
-
-    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
-        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
-        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
-        // for details on why this is necessary
-        let clone = self.inner.clone();
-        // We need to swap the clone and the original to avoid a not ready
-        // service. See
-        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
-        // for more details
-        let mut inner = std::mem::replace(&mut self.inner, clone);
-
-        let name = req.uri().path();
-        let (service, method) = match name.strip_prefix('/').and_then(|s| s.split_once('/')) {
-            Some((service, method)) => (service, method),
-            None => {
-                warn!("gRPC server called with unexpected format: {name:?}");
-                ("Unknown", name)
-            }
-        };
-
-        let span = info_span!(
-            target: "NUCLIADB_NODE",
-            "nucliadb_node:grpc-call", // placeholder that will be substituted by otel.name
-            otel.name = name,
-            rpc.system = "grpc",
-            rpc.service = service,
-            rpc.method = method
-        );
-        let parent_context = opentelemetry::global::get_text_map_propagator(|propagator| {
-            propagator.extract(&HeaderMapWrapper {
-                inner: req.headers(),
-            })
-        });
-
-        span.set_parent(parent_context);
-
-        let fut = inner.call(req).instrument(span);
-        Box::pin(fut)
-    }
-}
-
-struct HeaderMapWrapper<'a> {
-    inner: &'a http::header::HeaderMap,
-}
-
-impl<'a> Extractor for HeaderMapWrapper<'a> {
-    fn get(&self, key: &str) -> Option<&str> {
-        self.inner
-            .get(key)
-            .map(|value| value.to_str())
-            .transpose()
-            .unwrap_or(None)
-    }
-
-    fn keys(&self) -> Vec<&str> {
-        self.inner.keys().map(|key| key.as_str()).collect()
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::task::{Context, Poll};
+
+use futures::future::BoxFuture;
+use hyper::Body;
+use nucliadb_core::tracing::instrument::Instrument;
+use nucliadb_core::tracing::{info_span, warn};
+use opentelemetry::propagation::Extractor;
+use tonic::body::BoxBody;
+use tower::{Layer, Service};
+use tracing_opentelemetry::OpenTelemetrySpanExt;
+
+#[derive(Debug, Clone, Default)]
+pub struct GrpcInstrumentorLayer;
+
+impl<S> Layer<S> for GrpcInstrumentorLayer {
+    type Service = GrpcInstrumentor<S>;
+
+    fn layer(&self, service: S) -> Self::Service {
+        GrpcInstrumentor { inner: service }
+    }
+}
+
+/// Dynamically instrument gRPC server endpoints which continue traces injected
+/// by clients.
+#[derive(Debug, Clone)]
+pub struct GrpcInstrumentor<S> {
+    inner: S,
+}
+
+impl<S> Service<hyper::Request<Body>> for GrpcInstrumentor<S>
+where
+    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
+    S::Future: Send + 'static,
+{
+    type Response = S::Response;
+    type Error = S::Error;
+    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
+        self.inner.poll_ready(cx)
+    }
+
+    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
+        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
+        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
+        // for details on why this is necessary
+        let clone = self.inner.clone();
+        // We need to swap the clone and the original to avoid a not ready
+        // service. See
+        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
+        // for more details
+        let mut inner = std::mem::replace(&mut self.inner, clone);
+
+        let name = req.uri().path();
+        let (service, method) = match name.strip_prefix('/').and_then(|s| s.split_once('/')) {
+            Some((service, method)) => (service, method),
+            None => {
+                warn!("gRPC server called with unexpected format: {name:?}");
+                ("Unknown", name)
+            }
+        };
+
+        let span = info_span!(
+            target: "NUCLIADB_NODE",
+            "nucliadb_node:grpc-call", // placeholder that will be substituted by otel.name
+            otel.name = name,
+            rpc.system = "grpc",
+            rpc.service = service,
+            rpc.method = method
+        );
+        let parent_context = opentelemetry::global::get_text_map_propagator(|propagator| {
+            propagator.extract(&HeaderMapWrapper {
+                inner: req.headers(),
+            })
+        });
+
+        span.set_parent(parent_context);
+
+        let fut = inner.call(req).instrument(span);
+        Box::pin(fut)
+    }
+}
+
+struct HeaderMapWrapper<'a> {
+    inner: &'a http::header::HeaderMap,
+}
+
+impl<'a> Extractor for HeaderMapWrapper<'a> {
+    fn get(&self, key: &str) -> Option<&str> {
+        self.inner
+            .get(key)
+            .map(|value| value.to_str())
+            .transpose()
+            .unwrap_or(None)
+    }
+
+    fn keys(&self) -> Vec<&str> {
+        self.inner.keys().map(|key| key.as_str()).collect()
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/node_metadata.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/node_metadata.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,104 +1,107 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs::File;
-use std::io::{BufReader, BufWriter, Write};
-use std::path::Path;
-
-use nucliadb_core::tracing::*;
-use nucliadb_core::NodeResult;
-use serde::{Deserialize, Serialize};
-
-use crate::env;
-
-fn number_of_shards() -> NodeResult<usize> {
-    Ok(std::fs::read_dir(env::shards_path())?
-        .flatten()
-        .filter(|entry| entry.path().is_dir())
-        .count())
-}
-
-#[derive(Debug, Clone, Default, Deserialize, Serialize)]
-pub struct NodeMetadata {
-    #[serde(default)]
-    shard_count: u64,
-}
-
-impl From<NodeMetadata> for nucliadb_core::protos::NodeMetadata {
-    fn from(node_metadata: NodeMetadata) -> Self {
-        nucliadb_core::protos::NodeMetadata {
-            shard_count: node_metadata.shard_count,
-            ..Default::default()
-        }
-    }
-}
-
-impl NodeMetadata {
-    pub fn shard_count(&self) -> u64 {
-        self.shard_count
-    }
-
-    pub fn new_shard(&mut self) {
-        self.shard_count += 1;
-    }
-
-    pub fn delete_shard(&mut self) {
-        self.shard_count -= 1;
-    }
-    pub fn load_or_create(path: &Path) -> NodeResult<Self> {
-        if !path.exists() {
-            debug!("Node metadata file does not exist.");
-
-            let node_metadata = Self::create(path).unwrap_or_else(|e| {
-                warn!("Cannot create metadata file '{}': {e}", path.display());
-                debug!("Create default metadata file '{}'", path.display());
-
-                Self::default()
-            });
-
-            node_metadata.save(path)?;
-            Ok(node_metadata)
-        } else {
-            Self::load(path)
-        }
-    }
-
-    pub fn save(&self, path: &Path) -> NodeResult<()> {
-        debug!("Saving node metadata file '{}'", path.display());
-        let file = File::create(path)?;
-        let mut writer = BufWriter::new(file);
-        serde_json::to_writer(&mut writer, &self)?;
-        Ok(writer.flush()?)
-    }
-
-    pub fn load(path: &Path) -> NodeResult<Self> {
-        debug!("Loading node metadata file '{}'", path.display());
-        let file = File::open(path)?;
-        let reader = BufReader::new(file);
-        Ok(serde_json::from_reader(reader)?)
-    }
-
-    pub fn create(path: &Path) -> NodeResult<Self> {
-        debug!("Creating node metadata file '{}'", path.display());
-        number_of_shards()
-            .map(|i| i as u64)
-            .map(|shard_count| NodeMetadata { shard_count })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs::File;
+use std::io::{BufReader, BufWriter, Write};
+use std::path::Path;
+
+use nucliadb_core::tracing::*;
+use nucliadb_core::NodeResult;
+use serde::{Deserialize, Serialize};
+
+use crate::{env, utils};
+
+fn number_of_shards() -> NodeResult<usize> {
+    Ok(std::fs::read_dir(env::shards_path())?
+        .flatten()
+        .filter(|entry| entry.path().is_dir())
+        .count())
+}
+
+#[derive(Debug, Clone, Default, Deserialize, Serialize)]
+pub struct NodeMetadata {
+    #[serde(default)]
+    shard_count: u64,
+}
+
+impl From<NodeMetadata> for nucliadb_core::protos::NodeMetadata {
+    fn from(node_metadata: NodeMetadata) -> Self {
+        nucliadb_core::protos::NodeMetadata {
+            shard_count: node_metadata.shard_count,
+            node_id: utils::read_host_key(env::host_key_path())
+                .unwrap()
+                .to_string(),
+            ..Default::default()
+        }
+    }
+}
+
+impl NodeMetadata {
+    pub fn shard_count(&self) -> u64 {
+        self.shard_count
+    }
+
+    pub fn new_shard(&mut self) {
+        self.shard_count += 1;
+    }
+
+    pub fn delete_shard(&mut self) {
+        self.shard_count -= 1;
+    }
+    pub fn load_or_create(path: &Path) -> NodeResult<Self> {
+        if !path.exists() {
+            debug!("Node metadata file does not exist.");
+
+            let node_metadata = Self::create(path).unwrap_or_else(|e| {
+                warn!("Cannot create metadata file '{}': {e}", path.display());
+                debug!("Create default metadata file '{}'", path.display());
+
+                Self::default()
+            });
+
+            node_metadata.save(path)?;
+            Ok(node_metadata)
+        } else {
+            Self::load(path)
+        }
+    }
+
+    pub fn save(&self, path: &Path) -> NodeResult<()> {
+        debug!("Saving node metadata file '{}'", path.display());
+        let file = File::create(path)?;
+        let mut writer = BufWriter::new(file);
+        serde_json::to_writer(&mut writer, &self)?;
+        Ok(writer.flush()?)
+    }
+
+    pub fn load(path: &Path) -> NodeResult<Self> {
+        debug!("Loading node metadata file '{}'", path.display());
+        let file = File::open(path)?;
+        let reader = BufReader::new(file);
+        Ok(serde_json::from_reader(reader)?)
+    }
+
+    pub fn create(path: &Path) -> NodeResult<Self> {
+        debug!("Creating node metadata file '{}'", path.display());
+        number_of_shards()
+            .map(|i| i as u64)
+            .map(|shard_count| NodeMetadata { shard_count })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/reader/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/reader/mod.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,258 +1,255 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod grpc_driver;
-use std::collections::HashMap;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::{
-    DocumentSearchRequest, DocumentSearchResponse, EdgeList, GetShardRequest, IdCollection,
-    ParagraphSearchRequest, ParagraphSearchResponse, RelationSearchRequest, RelationSearchResponse,
-    SearchRequest, SearchResponse, Shard as ShardPB, ShardId, StreamRequest, SuggestRequest,
-    SuggestResponse, TypeList, VectorSearchRequest, VectorSearchResponse,
-};
-use nucliadb_core::thread::*;
-use nucliadb_core::tracing::{self, *};
-
-use crate::env;
-use crate::services::reader::ShardReaderService;
-
-#[derive(Default)]
-pub struct NodeReaderService {
-    pub cache: HashMap<String, ShardReaderService>,
-}
-
-impl NodeReaderService {
-    pub fn new() -> NodeReaderService {
-        // We shallow the error if the threadpool was already initialized
-        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
-        Self::default()
-    }
-
-    /// Stop all shards on memory
-    #[tracing::instrument(skip_all)]
-    pub fn shutdown(&mut self) {
-        for (shard_id, shard) in &mut self.cache {
-            debug!("Stopping shard {}", shard_id);
-            ShardReaderService::stop(shard);
-        }
-    }
-
-    /// Load all shards on the shards memory structure
-    #[tracing::instrument(skip_all)]
-    pub fn load_shards(&mut self) -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        debug!("Recovering shards from {shards_path:?}...");
-        for entry in std::fs::read_dir(&shards_path)? {
-            let entry = entry?;
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            match ShardReaderService::new(file_name.clone(), &shard_path) {
-                Err(err) => error!("Loading {shard_path:?} raised {err}"),
-                Ok(shard) => {
-                    debug!("Shard loaded: {shard_path:?}");
-                    self.cache.insert(file_name, shard);
-                }
-            }
-        }
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shard(&mut self, shard_id: &ShardId) {
-        let shard_name = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id.id);
-
-        if self.cache.contains_key(&shard_id.id) {
-            debug!("Shard {shard_path:?} is already on memory");
-            return;
-        }
-        if !shard_path.is_dir() {
-            error!("Shard {shard_path:?} is not on disk");
-            return;
-        }
-        match ShardReaderService::new(shard_name, &shard_path) {
-            Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
-            Ok(shard) => {
-                self.cache.insert(shard_id.id.clone(), shard);
-                debug!("{shard_path:?}: Shard loaded");
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardReaderService> {
-        self.cache.get(&shard_id.id)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn suggest(
-        &self,
-        shard_id: &ShardId,
-        request: SuggestRequest,
-    ) -> NodeResult<Option<SuggestResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let suggest_response = shard.suggest(request)?;
-        Ok(Some(suggest_response))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn search(
-        &self,
-        shard_id: &ShardId,
-        request: SearchRequest,
-    ) -> NodeResult<Option<SearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_search(
-        &self,
-        shard_id: &ShardId,
-        request: RelationSearchRequest,
-    ) -> NodeResult<Option<RelationSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.relation_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_search(
-        &self,
-        shard_id: &ShardId,
-        request: VectorSearchRequest,
-    ) -> NodeResult<Option<VectorSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.vector_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_search(
-        &self,
-        shard_id: &ShardId,
-        request: ParagraphSearchRequest,
-    ) -> NodeResult<Option<ParagraphSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.paragraph_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_iterator(
-        &self,
-        shard_id: &ShardId,
-        request: StreamRequest,
-    ) -> NodeResult<Option<ParagraphIterator>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.paragraph_iterator(request)?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_iterator(
-        &self,
-        shard_id: &ShardId,
-        request: StreamRequest,
-    ) -> NodeResult<Option<DocumentIterator>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.document_iterator(request)?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_search(
-        &self,
-        shard_id: &ShardId,
-        request: DocumentSearchRequest,
-    ) -> NodeResult<Option<DocumentSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.document_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_text_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_paragraphs_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_vectors_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_relations_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_edges(&self, shard_id: &ShardId) -> NodeResult<Option<EdgeList>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.get_relations_edges()?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_types(&self, shard_id: &ShardId) -> NodeResult<Option<TypeList>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.get_relations_types()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn get_info(
-        &self,
-        shard_id: &ShardId,
-        request: GetShardRequest,
-    ) -> NodeResult<Option<ShardPB>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.get_info(&request).map(Some)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod grpc_driver;
+use std::collections::HashMap;
+
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::{
+    DocumentSearchRequest, DocumentSearchResponse, EdgeList, GetShardRequest, IdCollection,
+    ParagraphSearchRequest, ParagraphSearchResponse, RelationSearchRequest, RelationSearchResponse,
+    SearchRequest, SearchResponse, Shard as ShardPB, ShardId, StreamRequest, SuggestRequest,
+    SuggestResponse, TypeList, VectorSearchRequest, VectorSearchResponse,
+};
+use nucliadb_core::thread::*;
+use nucliadb_core::tracing::{self, *};
+
+use crate::services::reader::ShardReaderService;
+use crate::{disk_structure, env};
+
+/// Initialize the index node reader. This function must be called before using
+/// a reader
+pub fn initialize() {
+    // We swallow the error if the threadpool was already initialized
+    let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
+}
+
+#[derive(Default)]
+pub struct NodeReaderService {
+    pub cache: HashMap<String, ShardReaderService>,
+}
+
+impl NodeReaderService {
+    pub fn new() -> NodeReaderService {
+        initialize();
+        Self::default()
+    }
+
+    /// Load all shards on the shards memory structure
+    #[tracing::instrument(skip_all)]
+    pub fn load_shards(&mut self) -> NodeResult<()> {
+        let shards_path = env::shards_path();
+        debug!("Recovering shards from {shards_path:?}...");
+        for entry in std::fs::read_dir(&shards_path)? {
+            let entry = entry?;
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            match ShardReaderService::new(file_name.clone(), &shard_path) {
+                Err(err) => error!("Loading {shard_path:?} raised {err}"),
+                Ok(shard) => {
+                    debug!("Shard loaded: {shard_path:?}");
+                    self.cache.insert(file_name, shard);
+                }
+            }
+        }
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shard(&mut self, shard_id: &ShardId) {
+        let shard_name = shard_id.id.clone();
+        let shard_path = disk_structure::shard_path_by_id(&env::shards_path(), &shard_id.id);
+
+        if self.cache.contains_key(&shard_id.id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return;
+        }
+        if !shard_path.is_dir() {
+            error!("Shard {shard_path:?} is not on disk");
+            return;
+        }
+        match ShardReaderService::new(shard_name, &shard_path) {
+            Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
+            Ok(shard) => {
+                self.cache.insert(shard_id.id.clone(), shard);
+                debug!("{shard_path:?}: Shard loaded");
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardReaderService> {
+        self.cache.get(&shard_id.id)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn suggest(
+        &self,
+        shard_id: &ShardId,
+        request: SuggestRequest,
+    ) -> NodeResult<Option<SuggestResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let suggest_response = shard.suggest(request)?;
+        Ok(Some(suggest_response))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn search(
+        &self,
+        shard_id: &ShardId,
+        request: SearchRequest,
+    ) -> NodeResult<Option<SearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_search(
+        &self,
+        shard_id: &ShardId,
+        request: RelationSearchRequest,
+    ) -> NodeResult<Option<RelationSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.relation_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_search(
+        &self,
+        shard_id: &ShardId,
+        request: VectorSearchRequest,
+    ) -> NodeResult<Option<VectorSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.vector_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_search(
+        &self,
+        shard_id: &ShardId,
+        request: ParagraphSearchRequest,
+    ) -> NodeResult<Option<ParagraphSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.paragraph_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_iterator(
+        &self,
+        shard_id: &ShardId,
+        request: StreamRequest,
+    ) -> NodeResult<Option<ParagraphIterator>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.paragraph_iterator(request)?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_iterator(
+        &self,
+        shard_id: &ShardId,
+        request: StreamRequest,
+    ) -> NodeResult<Option<DocumentIterator>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.document_iterator(request)?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_search(
+        &self,
+        shard_id: &ShardId,
+        request: DocumentSearchRequest,
+    ) -> NodeResult<Option<DocumentSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.document_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_text_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_paragraphs_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_vectors_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_relations_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_edges(&self, shard_id: &ShardId) -> NodeResult<Option<EdgeList>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.get_relations_edges()?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_types(&self, shard_id: &ShardId) -> NodeResult<Option<TypeList>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.get_relations_types()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_info(
+        &self,
+        shard_id: &ShardId,
+        request: GetShardRequest,
+    ) -> NodeResult<Option<ShardPB>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.get_info(&request).map(Some)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/mod.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,34 +1,24 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod versions;
-// Main services
-pub mod reader;
-pub mod writer;
-
-mod shard_disk_structure {
-    pub const VERSION_FILE: &str = "versions.json";
-    pub const VECTORS_DIR: &str = "vectors";
-    pub const VECTORSET_DIR: &str = "vectorset";
-    pub const TEXTS_DIR: &str = "text";
-    pub const PARAGRAPHS_DIR: &str = "paragraph";
-    pub const RELATIONS_DIR: &str = "relations";
-    pub const METADATA_FILE: &str = "metadata.json";
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod versions;
+// Main services
+pub mod reader;
+pub mod writer;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/versions.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/versions.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,220 +1,220 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::path::Path;
-
-use nucliadb_core::prelude::*;
-use serde::{Deserialize, Serialize};
-
-const VECTORS_VERSION: u32 = 1;
-const PARAGRAPHS_VERSION: u32 = 1;
-const RELATIONS_VERSION: u32 = 1;
-const TEXTS_VERSION: u32 = 1;
-const DEPRECATED_CONFIG: &str = "config.json";
-
-#[derive(Serialize, Deserialize)]
-pub struct Versions {
-    #[serde(default)]
-    version_paragraphs: Option<u32>,
-    #[serde(default)]
-    version_vectors: Option<u32>,
-    #[serde(default)]
-    version_texts: Option<u32>,
-    #[serde(default)]
-    version_relations: Option<u32>,
-}
-
-impl Versions {
-    fn deprecated_versions_exists(versions_file: &Path) -> bool {
-        versions_file
-            .parent()
-            .map(|v| v.join(DEPRECATED_CONFIG))
-            .map(|v| v.exists())
-            .unwrap_or_default()
-    }
-    fn new_from_deprecated() -> Versions {
-        Versions {
-            version_paragraphs: Some(1),
-            version_vectors: Some(1),
-            version_texts: Some(1),
-            version_relations: Some(1),
-        }
-    }
-    fn new() -> Versions {
-        Versions {
-            version_paragraphs: Some(PARAGRAPHS_VERSION),
-            version_vectors: Some(VECTORS_VERSION),
-            version_texts: Some(TEXTS_VERSION),
-            version_relations: Some(RELATIONS_VERSION),
-        }
-    }
-    fn fill_gaps(&mut self) -> bool {
-        let mut modified = false;
-        if self.version_paragraphs.is_none() {
-            self.version_paragraphs = Some(PARAGRAPHS_VERSION);
-            modified = true;
-        }
-        if self.version_relations.is_none() {
-            self.version_relations = Some(RELATIONS_VERSION);
-            modified = true;
-        }
-        if self.version_texts.is_none() {
-            self.version_texts = Some(TEXTS_VERSION);
-            modified = true;
-        }
-        if self.version_vectors.is_none() {
-            self.version_vectors = Some(VECTORS_VERSION);
-            modified = true;
-        }
-        modified
-    }
-    pub fn version_paragraphs(&self) -> u32 {
-        self.version_paragraphs.unwrap_or(PARAGRAPHS_VERSION)
-    }
-    pub fn version_vectors(&self) -> u32 {
-        self.version_vectors.unwrap_or(VECTORS_VERSION)
-    }
-    pub fn version_texts(&self) -> u32 {
-        self.version_texts.unwrap_or(TEXTS_VERSION)
-    }
-    pub fn version_relations(&self) -> u32 {
-        self.version_relations.unwrap_or(RELATIONS_VERSION)
-    }
-    pub fn get_vectors_reader(&self, config: &VectorConfig) -> NodeResult<VectorsReaderPointer> {
-        match self.version_vectors {
-            Some(1) => nucliadb_vectors::service::VectorReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as VectorsReaderPointer),
-            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-    pub fn get_paragraphs_reader(
-        &self,
-        config: &ParagraphConfig,
-    ) -> NodeResult<ParagraphsReaderPointer> {
-        match self.version_paragraphs {
-            Some(1) => nucliadb_paragraphs::reader::ParagraphReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as ParagraphsReaderPointer),
-            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_texts_reader(&self, config: &TextConfig) -> NodeResult<TextsReaderPointer> {
-        match self.version_texts {
-            Some(1) => nucliadb_texts::reader::TextReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as TextsReaderPointer),
-            Some(v) => Err(node_error!("Invalid text  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_relations_reader(
-        &self,
-        config: &RelationConfig,
-    ) -> NodeResult<RelationsReaderPointer> {
-        match self.version_relations {
-            Some(1) => nucliadb_relations::service::RelationsReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as RelationsReaderPointer),
-            Some(v) => Err(node_error!("Invalid relations  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_vectors_writer(&self, config: &VectorConfig) -> NodeResult<VectorsWriterPointer> {
-        match self.version_vectors {
-            Some(1) => nucliadb_vectors::service::VectorWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as VectorsWriterPointer),
-            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-    pub fn get_paragraphs_writer(
-        &self,
-        config: &ParagraphConfig,
-    ) -> NodeResult<ParagraphsWriterPointer> {
-        match self.version_paragraphs {
-            Some(1) => nucliadb_paragraphs::writer::ParagraphWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as ParagraphsWriterPointer),
-            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_texts_writer(&self, config: &TextConfig) -> NodeResult<TextsWriterPointer> {
-        match self.version_texts {
-            Some(1) => nucliadb_texts::writer::TextWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as TextsWriterPointer),
-            Some(v) => Err(node_error!("Invalid text  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_relations_writer(
-        &self,
-        config: &RelationConfig,
-    ) -> NodeResult<RelationsWriterPointer> {
-        match self.version_relations {
-            Some(1) => nucliadb_relations::service::RelationsWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as RelationsWriterPointer),
-            Some(v) => Err(node_error!("Invalid relations  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn load(versions_file: &Path) -> NodeResult<Versions> {
-        if versions_file.exists() {
-            let versions_json = std::fs::read_to_string(versions_file)?;
-            let mut versions: Versions = serde_json::from_str(&versions_json)?;
-            versions.fill_gaps();
-            Ok(versions)
-        } else if Versions::deprecated_versions_exists(versions_file) {
-            // In this case is an old index, therefore we create the versions file
-            // with the index versions that where available that moment.
-            // The writer will create the file at some point
-            Ok(Versions::new_from_deprecated())
-        } else {
-            Err(node_error!("Versions not found"))
-        }
-    }
-    pub fn load_or_create(versions_file: &Path) -> NodeResult<Versions> {
-        if versions_file.exists() {
-            let versions_json = std::fs::read_to_string(versions_file)?;
-            let mut versions: Versions = serde_json::from_str(&versions_json)?;
-            if versions.fill_gaps() {
-                let serialized = serde_json::to_string(&versions)?;
-                std::fs::write(versions_file, serialized)?;
-            }
-            Ok(versions)
-        } else if Versions::deprecated_versions_exists(versions_file) {
-            // In this case is an old index, therefore we create the versions file
-            // with the index versions that where available that moment.
-            let versions = Versions::new_from_deprecated();
-            let serialized = serde_json::to_string(&versions)?;
-            std::fs::write(versions_file, serialized)?;
-            Ok(versions)
-        } else {
-            let versions = Versions::new();
-            let serialized = serde_json::to_string(&versions)?;
-            std::fs::write(versions_file, serialized)?;
-            Ok(versions)
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::path::Path;
+
+use nucliadb_core::prelude::*;
+use serde::{Deserialize, Serialize};
+
+const VECTORS_VERSION: u32 = 1;
+const PARAGRAPHS_VERSION: u32 = 1;
+const RELATIONS_VERSION: u32 = 1;
+const TEXTS_VERSION: u32 = 1;
+const DEPRECATED_CONFIG: &str = "config.json";
+
+#[derive(Serialize, Deserialize)]
+pub struct Versions {
+    #[serde(default)]
+    version_paragraphs: Option<u32>,
+    #[serde(default)]
+    version_vectors: Option<u32>,
+    #[serde(default)]
+    version_texts: Option<u32>,
+    #[serde(default)]
+    version_relations: Option<u32>,
+}
+
+impl Versions {
+    fn deprecated_versions_exists(versions_file: &Path) -> bool {
+        versions_file
+            .parent()
+            .map(|v| v.join(DEPRECATED_CONFIG))
+            .map(|v| v.exists())
+            .unwrap_or_default()
+    }
+    fn new_from_deprecated() -> Versions {
+        Versions {
+            version_paragraphs: Some(1),
+            version_vectors: Some(1),
+            version_texts: Some(1),
+            version_relations: Some(1),
+        }
+    }
+    fn new() -> Versions {
+        Versions {
+            version_paragraphs: Some(PARAGRAPHS_VERSION),
+            version_vectors: Some(VECTORS_VERSION),
+            version_texts: Some(TEXTS_VERSION),
+            version_relations: Some(RELATIONS_VERSION),
+        }
+    }
+    fn fill_gaps(&mut self) -> bool {
+        let mut modified = false;
+        if self.version_paragraphs.is_none() {
+            self.version_paragraphs = Some(PARAGRAPHS_VERSION);
+            modified = true;
+        }
+        if self.version_relations.is_none() {
+            self.version_relations = Some(RELATIONS_VERSION);
+            modified = true;
+        }
+        if self.version_texts.is_none() {
+            self.version_texts = Some(TEXTS_VERSION);
+            modified = true;
+        }
+        if self.version_vectors.is_none() {
+            self.version_vectors = Some(VECTORS_VERSION);
+            modified = true;
+        }
+        modified
+    }
+    pub fn version_paragraphs(&self) -> u32 {
+        self.version_paragraphs.unwrap_or(PARAGRAPHS_VERSION)
+    }
+    pub fn version_vectors(&self) -> u32 {
+        self.version_vectors.unwrap_or(VECTORS_VERSION)
+    }
+    pub fn version_texts(&self) -> u32 {
+        self.version_texts.unwrap_or(TEXTS_VERSION)
+    }
+    pub fn version_relations(&self) -> u32 {
+        self.version_relations.unwrap_or(RELATIONS_VERSION)
+    }
+    pub fn get_vectors_reader(&self, config: &VectorConfig) -> NodeResult<VectorsReaderPointer> {
+        match self.version_vectors {
+            Some(1) => nucliadb_vectors::service::VectorReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as VectorsReaderPointer),
+            Some(v) => Err(node_error!("Invalid vectors version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+    pub fn get_paragraphs_reader(
+        &self,
+        config: &ParagraphConfig,
+    ) -> NodeResult<ParagraphsReaderPointer> {
+        match self.version_paragraphs {
+            Some(1) => nucliadb_paragraphs::reader::ParagraphReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as ParagraphsReaderPointer),
+            Some(v) => Err(node_error!("Invalid paragraphs version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_texts_reader(&self, config: &TextConfig) -> NodeResult<TextsReaderPointer> {
+        match self.version_texts {
+            Some(1) => nucliadb_texts::reader::TextReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as TextsReaderPointer),
+            Some(v) => Err(node_error!("Invalid text version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_relations_reader(
+        &self,
+        config: &RelationConfig,
+    ) -> NodeResult<RelationsReaderPointer> {
+        match self.version_relations {
+            Some(1) => nucliadb_relations::service::RelationsReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as RelationsReaderPointer),
+            Some(v) => Err(node_error!("Invalid relations version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_vectors_writer(&self, config: &VectorConfig) -> NodeResult<VectorsWriterPointer> {
+        match self.version_vectors {
+            Some(1) => nucliadb_vectors::service::VectorWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as VectorsWriterPointer),
+            Some(v) => Err(node_error!("Invalid vectors version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+    pub fn get_paragraphs_writer(
+        &self,
+        config: &ParagraphConfig,
+    ) -> NodeResult<ParagraphsWriterPointer> {
+        match self.version_paragraphs {
+            Some(1) => nucliadb_paragraphs::writer::ParagraphWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as ParagraphsWriterPointer),
+            Some(v) => Err(node_error!("Invalid paragraphs version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_texts_writer(&self, config: &TextConfig) -> NodeResult<TextsWriterPointer> {
+        match self.version_texts {
+            Some(1) => nucliadb_texts::writer::TextWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as TextsWriterPointer),
+            Some(v) => Err(node_error!("Invalid text version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_relations_writer(
+        &self,
+        config: &RelationConfig,
+    ) -> NodeResult<RelationsWriterPointer> {
+        match self.version_relations {
+            Some(1) => nucliadb_relations::service::RelationsWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as RelationsWriterPointer),
+            Some(v) => Err(node_error!("Invalid relations version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn load(versions_file: &Path) -> NodeResult<Versions> {
+        if versions_file.exists() {
+            let versions_json = std::fs::read_to_string(versions_file)?;
+            let mut versions: Versions = serde_json::from_str(&versions_json)?;
+            versions.fill_gaps();
+            Ok(versions)
+        } else if Versions::deprecated_versions_exists(versions_file) {
+            // In this case is an old index, therefore we create the versions file
+            // with the index versions that where available that moment.
+            // The writer will create the file at some point
+            Ok(Versions::new_from_deprecated())
+        } else {
+            Err(node_error!("Versions not found"))
+        }
+    }
+    pub fn load_or_create(versions_file: &Path) -> NodeResult<Versions> {
+        if versions_file.exists() {
+            let versions_json = std::fs::read_to_string(versions_file)?;
+            let mut versions: Versions = serde_json::from_str(&versions_json)?;
+            if versions.fill_gaps() {
+                let serialized = serde_json::to_string(&versions)?;
+                std::fs::write(versions_file, serialized)?;
+            }
+            Ok(versions)
+        } else if Versions::deprecated_versions_exists(versions_file) {
+            // In this case is an old index, therefore we create the versions file
+            // with the index versions that where available that moment.
+            let versions = Versions::new_from_deprecated();
+            let serialized = serde_json::to_string(&versions)?;
+            std::fs::write(versions_file, serialized)?;
+            Ok(versions)
+        } else {
+            let versions = Versions::new();
+            let serialized = serde_json::to_string(&versions)?;
+            std::fs::write(versions_file, serialized)?;
+            Ok(versions)
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/services/writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/services/writer.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,518 +1,465 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::path::{Path, PathBuf};
-use std::time::SystemTime;
-
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::shard_created::{
-    DocumentService, ParagraphService, RelationService, VectorService,
-};
-use nucliadb_core::protos::{
-    DeleteGraphNodes, JoinGraph, NewShardRequest, OpStatus, Resource, ResourceId, VectorSetId,
-    VectorSimilarity,
-};
-use nucliadb_core::tracing::{self, *};
-use nucliadb_core::{metrics, thread};
-
-use super::shard_disk_structure::*;
-use crate::services::versions::Versions;
-use crate::shard_metadata::ShardMetadata;
-use crate::telemetry::run_with_telemetry;
-
-#[derive(Debug)]
-pub struct ShardWriterService {
-    pub metadata: ShardMetadata,
-    pub id: String,
-    pub path: PathBuf,
-    text_writer: TextsWriterPointer,
-    paragraph_writer: ParagraphsWriterPointer,
-    vector_writer: VectorsWriterPointer,
-    relation_writer: RelationsWriterPointer,
-    document_service_version: i32,
-    paragraph_service_version: i32,
-    vector_service_version: i32,
-    relation_service_version: i32,
-}
-
-impl ShardWriterService {
-    #[tracing::instrument(skip_all)]
-    fn initialize(
-        id: String,
-        path: &Path,
-        metadata: ShardMetadata,
-        tsc: TextConfig,
-        psc: ParagraphConfig,
-        vsc: VectorConfig,
-        rsc: RelationConfig,
-    ) -> NodeResult<ShardWriterService> {
-        let versions = Versions::load_or_create(&path.join(VERSION_FILE))?;
-        let text_task = || Some(versions.get_texts_writer(&tsc));
-        let paragraph_task = || Some(versions.get_paragraphs_writer(&psc));
-        let vector_task = || Some(versions.get_vectors_writer(&vsc));
-        let relation_task = || Some(versions.get_relations_writer(&rsc));
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text start");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph start");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector start");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation start");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = None;
-        let mut paragraph_result = None;
-        let mut vector_result = None;
-        let mut relation_result = None;
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        let fields = text_result.transpose()?;
-        let paragraphs = paragraph_result.transpose()?;
-        let vectors = vector_result.transpose()?;
-        let relations = relation_result.transpose()?;
-
-        Ok(ShardWriterService {
-            id,
-            metadata,
-            path: path.to_path_buf(),
-            text_writer: fields.unwrap(),
-            paragraph_writer: paragraphs.unwrap(),
-            vector_writer: vectors.unwrap(),
-            relation_writer: relations.unwrap(),
-            document_service_version: versions.version_texts() as i32,
-            paragraph_service_version: versions.version_paragraphs() as i32,
-            vector_service_version: versions.version_vectors() as i32,
-            relation_service_version: versions.version_relations() as i32,
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn document_version(&self) -> DocumentService {
-        match self.document_service_version {
-            0 => DocumentService::DocumentV0,
-            1 => DocumentService::DocumentV1,
-            i => panic!("Unknown document version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_version(&self) -> ParagraphService {
-        match self.paragraph_service_version {
-            0 => ParagraphService::ParagraphV0,
-            1 => ParagraphService::ParagraphV1,
-            i => panic!("Unknown paragraph version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_version(&self) -> VectorService {
-        match self.vector_service_version {
-            0 => VectorService::VectorV0,
-            1 => VectorService::VectorV1,
-            i => panic!("Unknown vector version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_version(&self) -> RelationService {
-        match self.relation_service_version {
-            0 => RelationService::RelationV0,
-            1 => RelationService::RelationV1,
-            i => panic!("Unknown relation version {i}"),
-        }
-    }
-    pub fn clean_and_create(id: String, path: &Path) -> NodeResult<ShardWriterService> {
-        let metadata = ShardMetadata::open(&path.join(METADATA_FILE))?;
-        std::fs::remove_dir_all(path)?;
-        std::fs::create_dir(path)?;
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            similarity: Some(metadata.similarity()),
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
-    }
-    pub fn new(
-        id: String,
-        path: &Path,
-        request: &NewShardRequest,
-    ) -> NodeResult<ShardWriterService> {
-        let time = SystemTime::now();
-
-        std::fs::create_dir(path)?;
-        let metadata_path = path.join(METADATA_FILE);
-        let similarity = request.similarity();
-        let metadata = ShardMetadata::from(request.clone());
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            similarity: Some(similarity),
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-
-        metadata.serialize(&metadata_path)?;
-        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/new".to_string());
-        metrics.record_request_time(metric, took);
-
-        result
-    }
-
-    pub fn open(id: String, path: &Path) -> NodeResult<ShardWriterService> {
-        let time = SystemTime::now();
-
-        let metadata_path = path.join(METADATA_FILE);
-        let metadata = ShardMetadata::open(&metadata_path)?;
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            similarity: None,
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/open".to_string());
-        metrics.record_request_time(metric, took);
-
-        result
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn stop(&self) {
-        debug!("Stopping shard {}...", { &self.id });
-        let span = tracing::Span::current();
-        let time = SystemTime::now();
-
-        let texts = self.text_writer.clone();
-        let paragraphs = self.paragraph_writer.clone();
-        let vectors = self.vector_writer.clone();
-        let relations = self.relation_writer.clone();
-
-        let text_task = move || text_write(&texts).stop();
-        let paragraph_task = move || paragraph_write(&paragraphs).stop();
-        let vector_task = move || vector_write(&vectors).stop();
-        let relation_task = move || relation_write(&relations).stop();
-
-        let info = info_span!(parent: &span, "text stop");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph stop");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector stop");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation stop");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        if let Err(e) = text_result {
-            error!("Error stopping the Field writer service: {}", e);
-        }
-        if let Err(e) = paragraph_result {
-            error!("Error stopping the Paragraph writer service: {}", e);
-        }
-        if let Err(e) = vector_result {
-            error!("Error stopping the Vector writer service: {}", e);
-        }
-        if let Err(e) = relation_result {
-            error!("Error stopping the Relation writer service: {}", e);
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/stop".to_string());
-        metrics.record_request_time(metric, took);
-
-        debug!("Shard stopped {}...", { &self.id });
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn set_resource(&self, resource: &Resource) -> NodeResult<()> {
-        let span = tracing::Span::current();
-        let time = SystemTime::now();
-
-        let text_writer_service = self.text_writer.clone();
-        let field_resource = resource.clone();
-        let text_task = move || {
-            debug!("Field service starts set_resource");
-            let mut writer = text_write(&text_writer_service);
-            let result = writer.set_resource(&field_resource);
-            debug!("Field service ends set_resource");
-            result
-        };
-
-        let paragraph_resource = resource.clone();
-        let paragraph_writer_service = self.paragraph_writer.clone();
-        let paragraph_task = move || {
-            debug!("Paragraph service starts set_resource");
-            let mut writer = paragraph_write(&paragraph_writer_service);
-            let result = writer.set_resource(&paragraph_resource);
-            debug!("Paragraph service ends set_resource");
-            result
-        };
-
-        let vector_writer_service = self.vector_writer.clone();
-        let vector_resource = resource.clone();
-        let vector_task = move || {
-            debug!("Vector service starts set_resource");
-            let mut writer = vector_write(&vector_writer_service);
-            let result = writer.set_resource(&vector_resource);
-            debug!("Vector service ends set_resource");
-            result
-        };
-
-        let relation_writer_service = self.relation_writer.clone();
-        let relation_resource = resource.clone();
-        let relation_task = move || {
-            debug!("Relation service starts set_resource");
-            let mut writer = relation_write(&relation_writer_service);
-            let result = writer.set_resource(&relation_resource);
-            debug!("Relation service ends set_resource");
-            result
-        };
-
-        let info = info_span!(parent: &span, "text set_resource");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph set_resource");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector set_resource");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation set_resource");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/set_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        text_result?;
-        paragraph_result?;
-        vector_result?;
-        relation_result?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn remove_resource(&self, resource: &ResourceId) -> NodeResult<()> {
-        let span = tracing::Span::current();
-        let time = SystemTime::now();
-
-        let text_writer_service = self.text_writer.clone();
-        let field_resource = resource.clone();
-        let text_task = move || {
-            let mut writer = text_write(&text_writer_service);
-            writer.delete_resource(&field_resource)
-        };
-        let paragraph_resource = resource.clone();
-        let paragraph_writer_service = self.paragraph_writer.clone();
-        let paragraph_task = move || {
-            let mut writer = paragraph_write(&paragraph_writer_service);
-            writer.delete_resource(&paragraph_resource)
-        };
-        let vector_writer_service = self.vector_writer.clone();
-        let vector_resource = resource.clone();
-        let vector_task = move || {
-            let mut writer = vector_write(&vector_writer_service);
-            writer.delete_resource(&vector_resource)
-        };
-        let relation_writer_service = self.relation_writer.clone();
-        let relation_resource = resource.clone();
-        let relation_task = move || {
-            let mut writer = relation_write(&relation_writer_service);
-            writer.delete_resource(&relation_resource)
-        };
-
-        let info = info_span!(parent: &span, "text remove");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph remove");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector remove");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation remove");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/remove_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        text_result?;
-        paragraph_result?;
-        vector_result?;
-        relation_result?;
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn get_opstatus(&self) -> NodeResult<OpStatus> {
-        let span = tracing::Span::current();
-        let time = SystemTime::now();
-
-        let paragraphs = self.paragraph_writer.clone();
-        let vectors = self.vector_writer.clone();
-        let texts = self.text_writer.clone();
-        let info = info_span!(parent: &span, "text count");
-        let text_task = || run_with_telemetry(info, move || text_read(&texts).count());
-        let info = info_span!(parent: &span, "paragraph count");
-        let paragraph_task =
-            || run_with_telemetry(info, move || paragraph_read(&paragraphs).count());
-        let info = info_span!(parent: &span, "vector count");
-        let vector_task = || run_with_telemetry(info, move || vector_read(&vectors).count());
-
-        let mut text_result = Ok(0);
-        let mut paragraph_result = Ok(0);
-        let mut vector_result = Ok(0);
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-        });
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::shard("writer/get_opstatus".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(OpStatus {
-            shard_id: self.id.clone(),
-            count: text_result? as u64,
-            count_paragraphs: paragraph_result? as u64,
-            count_sentences: vector_result? as u64,
-            ..Default::default()
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
-        let reader = vector_read(&self.vector_writer);
-        let keys = reader.list_vectorsets()?;
-        Ok(keys)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn add_vectorset(
-        &self,
-        setid: &VectorSetId,
-        similarity: VectorSimilarity,
-    ) -> NodeResult<()> {
-        let mut writer = vector_write(&self.vector_writer);
-        writer.add_vectorset(setid, similarity)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn remove_vectorset(&self, setid: &VectorSetId) -> NodeResult<()> {
-        let mut writer = vector_write(&self.vector_writer);
-        writer.remove_vectorset(setid)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn delete_relation_nodes(&self, nodes: &DeleteGraphNodes) -> NodeResult<()> {
-        let mut writer = relation_write(&self.relation_writer);
-        writer.delete_nodes(nodes)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn join_relations_graph(&self, graph: &JoinGraph) -> NodeResult<()> {
-        let mut writer = relation_write(&self.relation_writer);
-        writer.join_graph(graph)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_count(&self) -> NodeResult<usize> {
-        paragraph_read(&self.paragraph_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_count(&self) -> NodeResult<usize> {
-        vector_read(&self.vector_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn text_count(&self) -> NodeResult<usize> {
-        text_read(&self.text_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn gc(&self) -> NodeResult<()> {
-        vector_write(&self.vector_writer).garbage_collection()
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::path::{Path, PathBuf};
+use std::time::SystemTime;
+
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::shard_created::{
+    DocumentService, ParagraphService, RelationService, VectorService,
+};
+use nucliadb_core::protos::{
+    DeleteGraphNodes, JoinGraph, OpStatus, Resource, ResourceId, VectorSetId, VectorSimilarity,
+};
+use nucliadb_core::tracing::{self, *};
+use nucliadb_core::{metrics, thread};
+
+use crate::disk_structure::*;
+use crate::services::versions::Versions;
+use crate::shard_metadata::ShardMetadata;
+use crate::telemetry::run_with_telemetry;
+
+#[derive(Debug)]
+pub struct ShardWriterService {
+    pub metadata: ShardMetadata,
+    pub id: String,
+    pub path: PathBuf,
+    text_writer: TextsWriterPointer,
+    paragraph_writer: ParagraphsWriterPointer,
+    vector_writer: VectorsWriterPointer,
+    relation_writer: RelationsWriterPointer,
+    document_service_version: i32,
+    paragraph_service_version: i32,
+    vector_service_version: i32,
+    relation_service_version: i32,
+}
+
+impl ShardWriterService {
+    #[tracing::instrument(skip_all)]
+    fn initialize(
+        id: String,
+        path: &Path,
+        metadata: ShardMetadata,
+        tsc: TextConfig,
+        psc: ParagraphConfig,
+        vsc: VectorConfig,
+        rsc: RelationConfig,
+    ) -> NodeResult<ShardWriterService> {
+        let versions = Versions::load_or_create(&path.join(VERSION_FILE))?;
+        let text_task = || Some(versions.get_texts_writer(&tsc));
+        let paragraph_task = || Some(versions.get_paragraphs_writer(&psc));
+        let vector_task = || Some(versions.get_vectors_writer(&vsc));
+        let relation_task = || Some(versions.get_relations_writer(&rsc));
+
+        let span = tracing::Span::current();
+        let info = info_span!(parent: &span, "text start");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph start");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector start");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation start");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = None;
+        let mut paragraph_result = None;
+        let mut vector_result = None;
+        let mut relation_result = None;
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let fields = text_result.transpose()?;
+        let paragraphs = paragraph_result.transpose()?;
+        let vectors = vector_result.transpose()?;
+        let relations = relation_result.transpose()?;
+
+        Ok(ShardWriterService {
+            id,
+            metadata,
+            path: path.to_path_buf(),
+            text_writer: fields.unwrap(),
+            paragraph_writer: paragraphs.unwrap(),
+            vector_writer: vectors.unwrap(),
+            relation_writer: relations.unwrap(),
+            document_service_version: versions.version_texts() as i32,
+            paragraph_service_version: versions.version_paragraphs() as i32,
+            vector_service_version: versions.version_vectors() as i32,
+            relation_service_version: versions.version_relations() as i32,
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn document_version(&self) -> DocumentService {
+        match self.document_service_version {
+            0 => DocumentService::DocumentV0,
+            1 => DocumentService::DocumentV1,
+            i => panic!("Unknown document version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_version(&self) -> ParagraphService {
+        match self.paragraph_service_version {
+            0 => ParagraphService::ParagraphV0,
+            1 => ParagraphService::ParagraphV1,
+            i => panic!("Unknown paragraph version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_version(&self) -> VectorService {
+        match self.vector_service_version {
+            0 => VectorService::VectorV0,
+            1 => VectorService::VectorV1,
+            i => panic!("Unknown vector version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_version(&self) -> RelationService {
+        match self.relation_service_version {
+            0 => RelationService::RelationV0,
+            1 => RelationService::RelationV1,
+            i => panic!("Unknown relation version {i}"),
+        }
+    }
+    pub fn clean_and_create(id: String, path: &Path) -> NodeResult<ShardWriterService> {
+        let metadata = ShardMetadata::open(&path.join(METADATA_FILE))?;
+        std::fs::remove_dir_all(path)?;
+        std::fs::create_dir(path)?;
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            similarity: Some(metadata.similarity()),
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
+    }
+
+    pub fn new(id: String, path: &Path, metadata: ShardMetadata) -> NodeResult<ShardWriterService> {
+        let time = SystemTime::now();
+
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            similarity: Some(metadata.similarity()),
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+
+        std::fs::create_dir(path)?;
+        let metadata_path = path.join(METADATA_FILE);
+        metadata.serialize(&metadata_path)?;
+        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/new".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+
+    pub fn open(id: String, path: &Path) -> NodeResult<ShardWriterService> {
+        let time = SystemTime::now();
+
+        let metadata_path = path.join(METADATA_FILE);
+        let metadata = ShardMetadata::open(&metadata_path)?;
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            similarity: None,
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/open".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn set_resource(&self, resource: &Resource) -> NodeResult<()> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let text_writer_service = self.text_writer.clone();
+        let field_resource = resource.clone();
+        let text_task = move || {
+            debug!("Field service starts set_resource");
+            let mut writer = text_write(&text_writer_service);
+            let result = writer.set_resource(&field_resource);
+            debug!("Field service ends set_resource");
+            result
+        };
+
+        let paragraph_resource = resource.clone();
+        let paragraph_writer_service = self.paragraph_writer.clone();
+        let paragraph_task = move || {
+            debug!("Paragraph service starts set_resource");
+            let mut writer = paragraph_write(&paragraph_writer_service);
+            let result = writer.set_resource(&paragraph_resource);
+            debug!("Paragraph service ends set_resource");
+            result
+        };
+
+        let vector_writer_service = self.vector_writer.clone();
+        let vector_resource = resource.clone();
+        let vector_task = move || {
+            debug!("Vector service starts set_resource");
+            let mut writer = vector_write(&vector_writer_service);
+            let result = writer.set_resource(&vector_resource);
+            debug!("Vector service ends set_resource");
+            result
+        };
+
+        let relation_writer_service = self.relation_writer.clone();
+        let relation_resource = resource.clone();
+        let relation_task = move || {
+            debug!("Relation service starts set_resource");
+            let mut writer = relation_write(&relation_writer_service);
+            let result = writer.set_resource(&relation_resource);
+            debug!("Relation service ends set_resource");
+            result
+        };
+
+        let info = info_span!(parent: &span, "text set_resource");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph set_resource");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector set_resource");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation set_resource");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/set_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        text_result?;
+        paragraph_result?;
+        vector_result?;
+        relation_result?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn remove_resource(&self, resource: &ResourceId) -> NodeResult<()> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let text_writer_service = self.text_writer.clone();
+        let field_resource = resource.clone();
+        let text_task = move || {
+            let mut writer = text_write(&text_writer_service);
+            writer.delete_resource(&field_resource)
+        };
+        let paragraph_resource = resource.clone();
+        let paragraph_writer_service = self.paragraph_writer.clone();
+        let paragraph_task = move || {
+            let mut writer = paragraph_write(&paragraph_writer_service);
+            writer.delete_resource(&paragraph_resource)
+        };
+        let vector_writer_service = self.vector_writer.clone();
+        let vector_resource = resource.clone();
+        let vector_task = move || {
+            let mut writer = vector_write(&vector_writer_service);
+            writer.delete_resource(&vector_resource)
+        };
+        let relation_writer_service = self.relation_writer.clone();
+        let relation_resource = resource.clone();
+        let relation_task = move || {
+            let mut writer = relation_write(&relation_writer_service);
+            writer.delete_resource(&relation_resource)
+        };
+
+        let info = info_span!(parent: &span, "text remove");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph remove");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector remove");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation remove");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/remove_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        text_result?;
+        paragraph_result?;
+        vector_result?;
+        relation_result?;
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_opstatus(&self) -> NodeResult<OpStatus> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let paragraphs = self.paragraph_writer.clone();
+        let vectors = self.vector_writer.clone();
+        let texts = self.text_writer.clone();
+
+        let count_fields = || {
+            run_with_telemetry(info_span!(parent: &span, "field count"), move || {
+                text_read(&texts).count()
+            })
+        };
+        let count_paragraphs = || {
+            run_with_telemetry(info_span!(parent: &span, "paragraph count"), move || {
+                paragraph_read(&paragraphs).count()
+            })
+        };
+        let count_vectors = || {
+            run_with_telemetry(info_span!(parent: &span, "vector count"), move || {
+                vector_read(&vectors).count()
+            })
+        };
+
+        let mut field_count = Ok(0);
+        let mut paragraph_count = Ok(0);
+        let mut vector_count = Ok(0);
+        thread::scope(|s| {
+            s.spawn(|_| field_count = count_fields());
+            s.spawn(|_| paragraph_count = count_paragraphs());
+            s.spawn(|_| vector_count = count_vectors());
+        });
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/get_opstatus".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(OpStatus {
+            shard_id: self.id.clone(),
+            field_count: field_count? as u64,
+            paragraph_count: paragraph_count? as u64,
+            sentence_count: vector_count? as u64,
+            ..Default::default()
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
+        let reader = vector_read(&self.vector_writer);
+        let keys = reader.list_vectorsets()?;
+        Ok(keys)
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn add_vectorset(
+        &self,
+        setid: &VectorSetId,
+        similarity: VectorSimilarity,
+    ) -> NodeResult<()> {
+        let mut writer = vector_write(&self.vector_writer);
+        writer.add_vectorset(setid, similarity)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn remove_vectorset(&self, setid: &VectorSetId) -> NodeResult<()> {
+        let mut writer = vector_write(&self.vector_writer);
+        writer.remove_vectorset(setid)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn delete_relation_nodes(&self, nodes: &DeleteGraphNodes) -> NodeResult<()> {
+        let mut writer = relation_write(&self.relation_writer);
+        writer.delete_nodes(nodes)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn join_relations_graph(&self, graph: &JoinGraph) -> NodeResult<()> {
+        let mut writer = relation_write(&self.relation_writer);
+        writer.join_graph(graph)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_count(&self) -> NodeResult<usize> {
+        paragraph_read(&self.paragraph_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_count(&self) -> NodeResult<usize> {
+        vector_read(&self.vector_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn text_count(&self) -> NodeResult<usize> {
+        text_read(&self.text_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn gc(&self) -> NodeResult<()> {
+        vector_write(&self.vector_writer).garbage_collection()
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shard_metadata.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shard_metadata.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,126 +1,126 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs::File;
-use std::io::{BufReader, BufWriter, Write};
-use std::path::Path;
-
-use nucliadb_core::protos::{NewShardRequest, ShardMetadata as GrpcMetadata, VectorSimilarity};
-use nucliadb_core::{node_error, NodeResult};
-use serde::*;
-
-#[derive(Serialize, Deserialize, Default, Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
-enum Similarity {
-    #[default]
-    Cosine,
-    Dot,
-}
-
-impl From<VectorSimilarity> for Similarity {
-    fn from(value: VectorSimilarity) -> Self {
-        match value {
-            VectorSimilarity::Cosine => Similarity::Cosine,
-            VectorSimilarity::Dot => Similarity::Dot,
-        }
-    }
-}
-impl From<Similarity> for VectorSimilarity {
-    fn from(value: Similarity) -> Self {
-        match value {
-            Similarity::Cosine => VectorSimilarity::Cosine,
-            Similarity::Dot => VectorSimilarity::Dot,
-        }
-    }
-}
-
-#[derive(Serialize, Deserialize, Default, Clone, Debug)]
-pub struct ShardMetadata {
-    kbid: Option<String>,
-    similarity: Option<Similarity>,
-}
-
-impl From<ShardMetadata> for GrpcMetadata {
-    fn from(x: ShardMetadata) -> GrpcMetadata {
-        GrpcMetadata {
-            kbid: x.kbid.unwrap_or_default(),
-        }
-    }
-}
-impl From<NewShardRequest> for ShardMetadata {
-    fn from(value: NewShardRequest) -> Self {
-        ShardMetadata {
-            similarity: Some(value.similarity().into()),
-            kbid: Some(value.kbid).filter(|s| !s.is_empty()),
-        }
-    }
-}
-
-impl ShardMetadata {
-    pub fn open(metadata: &Path) -> NodeResult<ShardMetadata> {
-        if !metadata.exists() {
-            return Ok(ShardMetadata::default());
-        }
-
-        let mut reader = BufReader::new(File::open(metadata)?);
-        Ok(serde_json::from_reader(&mut reader)?)
-    }
-    pub fn serialize(&self, metadata: &Path) -> NodeResult<()> {
-        if metadata.exists() {
-            return Err(node_error!("Metadata file already exists at {metadata:?}"));
-        }
-
-        let mut writer = BufWriter::new(File::create(metadata)?);
-        serde_json::to_writer(&mut writer, self)?;
-        Ok(writer.flush()?)
-    }
-    pub fn kbid(&self) -> Option<&str> {
-        self.kbid.as_deref()
-    }
-    pub fn similarity(&self) -> VectorSimilarity {
-        self.similarity.unwrap_or(Similarity::Cosine).into()
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use tempfile::TempDir;
-
-    use super::*;
-    #[test]
-    fn create() {
-        let dir = TempDir::new().unwrap();
-        let metadata_path = dir.path().join("metadata.json");
-        let meta = ShardMetadata {
-            kbid: Some("KB".to_string()),
-            similarity: Some(Similarity::Cosine),
-        };
-        meta.serialize(&metadata_path).unwrap();
-        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
-        assert_eq!(meta.kbid, meta_disk.kbid);
-        assert_eq!(meta.similarity, meta_disk.similarity);
-    }
-    #[test]
-    fn open_empty() {
-        let dir = TempDir::new().unwrap();
-        let metadata_path = dir.path().join("metadata.json");
-        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
-        assert!(meta_disk.kbid.is_none());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs::File;
+use std::io::{BufReader, BufWriter, Write};
+use std::path::Path;
+
+use nucliadb_core::protos::{NewShardRequest, ShardMetadata as GrpcMetadata, VectorSimilarity};
+use nucliadb_core::{node_error, NodeResult};
+use serde::*;
+
+#[derive(Serialize, Deserialize, Default, Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
+enum Similarity {
+    #[default]
+    Cosine,
+    Dot,
+}
+
+impl From<VectorSimilarity> for Similarity {
+    fn from(value: VectorSimilarity) -> Self {
+        match value {
+            VectorSimilarity::Cosine => Similarity::Cosine,
+            VectorSimilarity::Dot => Similarity::Dot,
+        }
+    }
+}
+impl From<Similarity> for VectorSimilarity {
+    fn from(value: Similarity) -> Self {
+        match value {
+            Similarity::Cosine => VectorSimilarity::Cosine,
+            Similarity::Dot => VectorSimilarity::Dot,
+        }
+    }
+}
+
+#[derive(Serialize, Deserialize, Default, Clone, Debug)]
+pub struct ShardMetadata {
+    kbid: Option<String>,
+    similarity: Option<Similarity>,
+}
+
+impl From<ShardMetadata> for GrpcMetadata {
+    fn from(x: ShardMetadata) -> GrpcMetadata {
+        GrpcMetadata {
+            kbid: x.kbid.unwrap_or_default(),
+        }
+    }
+}
+impl From<NewShardRequest> for ShardMetadata {
+    fn from(value: NewShardRequest) -> Self {
+        ShardMetadata {
+            similarity: Some(value.similarity().into()),
+            kbid: Some(value.kbid).filter(|s| !s.is_empty()),
+        }
+    }
+}
+
+impl ShardMetadata {
+    pub fn open(metadata: &Path) -> NodeResult<ShardMetadata> {
+        if !metadata.exists() {
+            return Ok(ShardMetadata::default());
+        }
+
+        let mut reader = BufReader::new(File::open(metadata)?);
+        Ok(serde_json::from_reader(&mut reader)?)
+    }
+    pub fn serialize(&self, metadata: &Path) -> NodeResult<()> {
+        if metadata.exists() {
+            return Err(node_error!("Metadata file already exists at {metadata:?}"));
+        }
+
+        let mut writer = BufWriter::new(File::create(metadata)?);
+        serde_json::to_writer(&mut writer, self)?;
+        Ok(writer.flush()?)
+    }
+    pub fn kbid(&self) -> Option<&str> {
+        self.kbid.as_deref()
+    }
+    pub fn similarity(&self) -> VectorSimilarity {
+        self.similarity.unwrap_or(Similarity::Cosine).into()
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use tempfile::TempDir;
+
+    use super::*;
+    #[test]
+    fn create() {
+        let dir = TempDir::new().unwrap();
+        let metadata_path = dir.path().join("metadata.json");
+        let meta = ShardMetadata {
+            kbid: Some("KB".to_string()),
+            similarity: Some(Similarity::Cosine),
+        };
+        meta.serialize(&metadata_path).unwrap();
+        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
+        assert_eq!(meta.kbid, meta_disk.kbid);
+        assert_eq!(meta.similarity, meta_disk.similarity);
+    }
+    #[test]
+    fn open_empty() {
+        let dir = TempDir::new().unwrap();
+        let metadata_path = dir.path().join("metadata.json");
+        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
+        assert!(meta_disk.kbid.is_none());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/mod.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,27 +1,33 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-mod shards_provider;
-mod unbounded_cache_provider;
-
-pub use shards_provider::{AsyncReaderShardsProvider, ReaderShardsProvider, ShardId};
-pub use unbounded_cache_provider::{AsyncUnboundedShardReaderCache, UnboundedShardReaderCache};
-
-pub use crate::services::reader::ShardReaderService as ShardReader;
-pub use crate::services::writer::ShardWriterService as ShardWriter;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+mod shards_provider;
+mod unbounded_cache_provider;
+
+pub use shards_provider::{
+    AsyncReaderShardsProvider, AsyncWriterShardsProvider, ReaderShardsProvider, ShardId,
+    ShardNotFoundError, WriterShardsProvider,
+};
+pub use unbounded_cache_provider::{
+    AsyncUnboundedShardReaderCache, AsyncUnboundedShardWriterCache, UnboundedShardReaderCache,
+    UnboundedShardWriterCache,
+};
+
+pub use crate::services::reader::ShardReaderService as ShardReader;
+pub use crate::services::writer::ShardWriterService as ShardWriter;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/shards_provider.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/mod.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,56 +1,44 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::sync::Arc;
-
-use async_trait::async_trait;
-// use nucliadb_vectors::data_point::Similarity;
-use nucliadb_core::NodeResult;
-
-use super::ShardReader;
-// use super::ShardWriter;
-
-pub type ShardId = String;
-
-pub trait ReaderShardsProvider: Send + Sync {
-    fn load(&self, id: ShardId) -> NodeResult<()>;
-    fn load_all(&self) -> NodeResult<()>;
-
-    fn get(&self, id: ShardId) -> Option<Arc<ShardReader>>;
-}
-
-#[async_trait]
-pub trait AsyncReaderShardsProvider: Send + Sync {
-    async fn load(&self, id: ShardId) -> NodeResult<()>;
-    async fn load_all(&self) -> NodeResult<()>;
-
-    async fn get(&self, id: ShardId) -> Option<Arc<ShardReader>>;
-}
-
-// pub trait WriterShardsProvider {
-//     fn create(&self, id: ShardId, kbid: String, similarity: Similarity);
-
-//     fn load(&self, id: ShardId) -> NodeResult<()>;
-//     fn load_all(&mut self) -> NodeResult<()>;
-
-//     fn get(&self, id: ShardId) -> Option<&ShardWriter>;
-//     fn get_mut(&self, id: ShardId) -> Option<&mut ShardWriter>;
-
-//     fn delete(&self, id: ShardId) -> NodeResult<()>;
-// }
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+mod console;
+mod noop;
+mod prometheus;
+
+pub use console::ConsoleMeter;
+pub use noop::NoOpMeter;
+pub use prometheus::PrometheusMeter;
+
+use crate::metrics::metric::request_time;
+use crate::metrics::task_monitor::{Monitor, TaskId};
+use crate::NodeResult;
+
+pub trait Meter: Send + Sync {
+    fn record_request_time(
+        &self,
+        metric: request_time::RequestTimeKey,
+        value: request_time::RequestTimeValue,
+    );
+
+    fn export(&self) -> NodeResult<String>;
+
+    fn task_monitor(&self, _task_id: TaskId) -> Option<Monitor> {
+        None
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_reader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_reader.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,112 +1,115 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::collections::HashMap;
-use std::sync::Arc;
-
-use async_std::sync::RwLock;
-use async_trait::async_trait;
-use nucliadb_core::tracing::{debug, error};
-use nucliadb_core::{node_error, Context, Error, NodeResult};
-
-pub use crate::env;
-use crate::shards::shards_provider::{AsyncReaderShardsProvider, ShardId};
-use crate::shards::ShardReader;
-
-#[derive(Default)]
-pub struct AsyncUnboundedShardReaderCache {
-    cache: RwLock<HashMap<ShardId, Arc<ShardReader>>>,
-}
-
-impl AsyncUnboundedShardReaderCache {
-    pub fn new() -> Self {
-        Self {
-            // NOTE: we use max shards per node as initial capacity to avoid
-            // hashmap resizing, as it would block the current thread while
-            // doing it.
-            //
-            // REVIEW: if resize don't take more than 10s, it's acceptable
-            // (blocking in tokio means CPU bound during 10-100s)
-            cache: RwLock::new(HashMap::with_capacity(env::max_shards_per_node())),
-        }
-    }
-}
-
-#[async_trait]
-impl AsyncReaderShardsProvider for AsyncUnboundedShardReaderCache {
-    async fn load(&self, id: ShardId) -> NodeResult<()> {
-        let shard_path = env::shards_path_id(&id);
-
-        if self.cache.read().await.contains_key(&id) {
-            debug!("Shard {shard_path:?} is already on memory");
-            return Ok(());
-        }
-
-        // Avoid blocking while interacting with the file system (reads and
-        // writes to disk)
-        let _id = id.clone();
-        let shard = tokio::task::spawn_blocking(move || {
-            if !shard_path.is_dir() {
-                return Err(node_error!("Shard {shard_path:?} is not on disk"));
-            }
-            ShardReader::new(id.clone(), &shard_path).map_err(|error| {
-                node_error!("Shard {shard_path:?} could not be loaded from disk: {error:?}")
-            })
-        })
-        .await
-        .context("Blocking task panicked")??;
-
-        self.cache.write().await.insert(_id, Arc::new(shard));
-        Ok(())
-    }
-
-    async fn load_all(&self) -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        let mut shards = tokio::task::spawn_blocking(move || {
-            let mut shards = HashMap::new();
-            for entry in std::fs::read_dir(&shards_path)? {
-                let entry = entry?;
-                let file_name = entry.file_name().to_str().unwrap().to_string();
-                let shard_path = entry.path();
-                match ShardReader::new(file_name.clone(), &shard_path) {
-                    Err(err) => error!("Loading {shard_path:?} raised {err}"),
-                    Ok(shard) => {
-                        debug!("Shard loaded: {shard_path:?}");
-                        shards.insert(file_name, shard);
-                    }
-                }
-            }
-            Ok::<HashMap<String, ShardReader>, Error>(shards)
-        })
-        .await
-        .context("Blocking task panicked")??;
-
-        {
-            let mut cache = self.cache.write().await;
-            shards.drain().for_each(|(k, v)| {
-                cache.insert(k, Arc::new(v));
-            });
-        }
-        Ok(())
-    }
-
-    async fn get(&self, id: ShardId) -> Option<Arc<ShardReader>> {
-        self.cache.read().await.get(&id).map(Arc::clone)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::collections::HashMap;
+use std::path::PathBuf;
+use std::sync::Arc;
+
+use async_std::sync::RwLock;
+use async_trait::async_trait;
+use nucliadb_core::tracing::{debug, error};
+use nucliadb_core::{node_error, Context, NodeResult};
+
+use crate::shards::shards_provider::{AsyncReaderShardsProvider, ShardId};
+use crate::shards::ShardReader;
+use crate::{disk_structure, env};
+
+#[derive(Default)]
+pub struct AsyncUnboundedShardReaderCache {
+    cache: RwLock<HashMap<ShardId, Arc<ShardReader>>>,
+    shards_path: PathBuf,
+}
+
+impl AsyncUnboundedShardReaderCache {
+    pub fn new(shards_path: PathBuf) -> Self {
+        Self {
+            // NOTE: we use max shards per node as initial capacity to avoid
+            // hashmap resizing, as it would block the current thread while
+            // doing it.
+            //
+            // REVIEW: if resize don't take more than 10s, it's acceptable
+            // (blocking in tokio means CPU bound during 10-100s)
+            cache: RwLock::new(HashMap::with_capacity(env::max_shards_per_node())),
+            shards_path,
+        }
+    }
+}
+
+#[async_trait]
+impl AsyncReaderShardsProvider for AsyncUnboundedShardReaderCache {
+    async fn load(&self, id: ShardId) -> NodeResult<()> {
+        let shard_path = disk_structure::shard_path_by_id(&self.shards_path, &id);
+
+        if self.cache.read().await.contains_key(&id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return Ok(());
+        }
+
+        // Avoid blocking while interacting with the file system (reads and
+        // writes to disk)
+        let id_ = id.clone();
+        let shard = tokio::task::spawn_blocking(move || {
+            if !shard_path.is_dir() {
+                return Err(node_error!("Shard {shard_path:?} is not on disk"));
+            }
+            ShardReader::new(id.clone(), &shard_path).map_err(|error| {
+                node_error!("Shard {shard_path:?} could not be loaded from disk: {error:?}")
+            })
+        })
+        .await
+        .context("Blocking task panicked")??;
+
+        self.cache.write().await.insert(id_, Arc::new(shard));
+        Ok(())
+    }
+
+    async fn load_all(&self) -> NodeResult<()> {
+        let shards_path = self.shards_path.clone();
+        let mut shards = tokio::task::spawn_blocking(move || -> NodeResult<_> {
+            let mut shards = HashMap::new();
+            for entry in std::fs::read_dir(&shards_path)? {
+                let entry = entry?;
+                let file_name = entry.file_name().to_str().unwrap().to_string();
+                let shard_path = entry.path();
+                match ShardReader::new(file_name.clone(), &shard_path) {
+                    Err(err) => error!("Loading shard {shard_path:?} from disk raised {err}"),
+                    Ok(shard) => {
+                        debug!("Shard loaded: {shard_path:?}");
+                        shards.insert(file_name, shard);
+                    }
+                }
+            }
+            Ok(shards)
+        })
+        .await
+        .context("Blocking task panicked")??;
+
+        {
+            let mut cache = self.cache.write().await;
+            shards.drain().for_each(|(k, v)| {
+                cache.insert(k, Arc::new(v));
+            });
+        }
+        Ok(())
+    }
+
+    async fn get(&self, id: ShardId) -> Option<Arc<ShardReader>> {
+        self.cache.read().await.get(&id).map(Arc::clone)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/async_unbounded_writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/lib.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,18 +1,24 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod reader;
+mod schema;
+mod search_query;
+pub mod writer;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_reader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_reader.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,93 +1,96 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::collections::HashMap;
-use std::sync::{Arc, RwLock, RwLockReadGuard, RwLockWriteGuard};
-
-use nucliadb_core::tracing::{debug, error};
-use nucliadb_core::{node_error, NodeResult};
-
-pub use crate::env;
-use crate::shards::shards_provider::{ReaderShardsProvider, ShardId};
-use crate::shards::ShardReader;
-
-#[derive(Default)]
-pub struct UnboundedShardReaderCache {
-    cache: RwLock<HashMap<ShardId, Arc<ShardReader>>>,
-}
-
-impl UnboundedShardReaderCache {
-    pub fn new() -> Self {
-        Self {
-            cache: RwLock::new(HashMap::new()),
-        }
-    }
-
-    fn read(&self) -> RwLockReadGuard<HashMap<ShardId, Arc<ShardReader>>> {
-        self.cache.read().expect("Poisoned lock while reading")
-    }
-
-    fn write(&self) -> RwLockWriteGuard<HashMap<ShardId, Arc<ShardReader>>> {
-        self.cache.write().expect("Poisoned lock while reading")
-    }
-}
-
-impl ReaderShardsProvider for UnboundedShardReaderCache {
-    fn load(&self, id: ShardId) -> NodeResult<()> {
-        let shard_path = env::shards_path_id(&id);
-
-        if self.read().contains_key(&id) {
-            debug!("Shard {shard_path:?} is already on memory");
-            return Ok(());
-        }
-
-        if !shard_path.is_dir() {
-            return Err(node_error!("Shard {shard_path:?} is not on disk"));
-        }
-        let shard = ShardReader::new(id.clone(), &shard_path).map_err(|error| {
-            node_error!("Shard {shard_path:?} could not be loaded from disk: {error:?}")
-        })?;
-
-        self.write().insert(id, Arc::new(shard));
-        Ok(())
-    }
-
-    fn load_all(&self) -> NodeResult<()> {
-        let mut cache = self.write();
-        let shards_path = env::shards_path();
-        debug!("Recovering shards from {shards_path:?}...");
-        for entry in std::fs::read_dir(&shards_path)? {
-            let entry = entry?;
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            match ShardReader::new(file_name.clone(), &shard_path) {
-                Err(err) => error!("Loading {shard_path:?} raised {err}"),
-                Ok(shard) => {
-                    debug!("Shard loaded: {shard_path:?}");
-                    cache.insert(file_name, Arc::new(shard));
-                }
-            }
-        }
-        Ok(())
-    }
-
-    fn get(&self, id: ShardId) -> Option<Arc<ShardReader>> {
-        self.read().get(&id).map(Arc::clone)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::collections::HashMap;
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock, RwLockReadGuard, RwLockWriteGuard};
+
+use nucliadb_core::tracing::{debug, error};
+use nucliadb_core::{node_error, NodeResult};
+
+use crate::disk_structure;
+use crate::shards::shards_provider::{ReaderShardsProvider, ShardId};
+use crate::shards::ShardReader;
+
+#[derive(Default)]
+pub struct UnboundedShardReaderCache {
+    cache: RwLock<HashMap<ShardId, Arc<ShardReader>>>,
+    shards_path: PathBuf,
+}
+
+impl UnboundedShardReaderCache {
+    pub fn new(shards_path: PathBuf) -> Self {
+        Self {
+            cache: RwLock::new(HashMap::new()),
+            shards_path,
+        }
+    }
+
+    fn read(&self) -> RwLockReadGuard<HashMap<ShardId, Arc<ShardReader>>> {
+        self.cache.read().expect("Poisoned lock while reading")
+    }
+
+    fn write(&self) -> RwLockWriteGuard<HashMap<ShardId, Arc<ShardReader>>> {
+        self.cache.write().expect("Poisoned lock while reading")
+    }
+}
+
+impl ReaderShardsProvider for UnboundedShardReaderCache {
+    fn load(&self, id: ShardId) -> NodeResult<()> {
+        let shard_path = disk_structure::shard_path_by_id(&self.shards_path, &id);
+
+        if self.read().contains_key(&id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return Ok(());
+        }
+
+        if !shard_path.is_dir() {
+            return Err(node_error!("Shard {shard_path:?} is not on disk"));
+        }
+        let shard = ShardReader::new(id.clone(), &shard_path).map_err(|error| {
+            node_error!("Shard {shard_path:?} could not be loaded from disk: {error:?}")
+        })?;
+
+        self.write().insert(id, Arc::new(shard));
+        Ok(())
+    }
+
+    fn load_all(&self) -> NodeResult<()> {
+        let mut cache = self.write();
+        let shards_path = self.shards_path.clone();
+        debug!("Recovering shards from {shards_path:?}...");
+        for entry in std::fs::read_dir(&shards_path)? {
+            let entry = entry?;
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            match ShardReader::new(file_name.clone(), &shard_path) {
+                Err(err) => error!("Loading {shard_path:?} raised {err}"),
+                Ok(shard) => {
+                    debug!("Shard loaded: {shard_path:?}");
+                    cache.insert(file_name, Arc::new(shard));
+                }
+            }
+        }
+        Ok(())
+    }
+
+    fn get(&self, id: ShardId) -> Option<Arc<ShardReader>> {
+        self.read().get(&id).map(Arc::clone)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/shards/unbounded_cache_provider/unbounded_writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/mod.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod bfs;
+pub mod reader;
+#[cfg(test)]
+mod tests;
+mod utils;
+pub mod writer;
+
+pub use reader::*;
+pub use writer::*;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/telemetry.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/telemetry.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,120 +1,119 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use nucliadb_core::tracing::{Level, Span};
-use nucliadb_core::{Context, NodeResult};
-use opentelemetry::global;
-use opentelemetry::trace::TraceContextExt;
-use sentry::ClientInitGuard;
-use tracing_opentelemetry::OpenTelemetrySpanExt;
-use tracing_subscriber::filter::{FilterFn, LevelFilter, Targets};
-use tracing_subscriber::layer::SubscriberExt;
-use tracing_subscriber::util::SubscriberInitExt;
-use tracing_subscriber::{Layer, Registry};
-
-use crate::env;
-
-const TRACE_ID: &str = "trace-id";
-
-pub fn init_telemetry() -> NodeResult<ClientInitGuard> {
-    let mut layers = Vec::new();
-
-    let log_levels = env::log_level();
-    let stdout = stdout_layer(log_levels);
-    layers.push(stdout);
-
-    if env::jaeger_enabled() {
-        let span_levels = env::span_levels();
-        let jaeger = jaeger_layer(span_levels)?;
-        layers.push(jaeger);
-    }
-
-    let sentry_guard = setup_sentry(env::get_sentry_env(), env::sentry_url());
-    let sentry = sentry_layer();
-    layers.push(sentry);
-
-    tracing_subscriber::registry()
-        .with(layers)
-        .try_init()
-        .with_context(|| "trying to init tracing")?;
-
-    Ok(sentry_guard)
-}
-
-fn stdout_layer(log_levels: Vec<(String, Level)>) -> Box<dyn Layer<Registry> + Send + Sync> {
-    let format = tracing_subscriber::fmt::format().with_level(true).compact();
-
-    tracing_subscriber::fmt::layer()
-        .event_format(format)
-        .with_filter(Targets::new().with_targets(log_levels))
-        .boxed()
-}
-
-fn jaeger_layer(
-    _span_levels: Vec<(String, Level)>,
-) -> NodeResult<Box<dyn Layer<Registry> + Send + Sync>> {
-    global::set_text_map_propagator(opentelemetry_zipkin::Propagator::new());
-
-    let agent_endpoint = env::jaeger_agent_endp();
-    let tracer = opentelemetry_jaeger::new_pipeline()
-        .with_agent_endpoint(agent_endpoint)
-        .with_service_name("nucliadb_node")
-        .with_auto_split_batch(true)
-        .install_batch(opentelemetry::runtime::Tokio)?;
-
-    // To avoid sending too much information to Jaeger, we filter out all events
-    // (as they are logged to stdout), spans from external instrumented crates
-    // (like tantivy, hyper, tower, mio...) and spans below INFO level (default
-    // span level).
-    let level_filter = LevelFilter::from_level(Level::INFO);
-    let span_filter = FilterFn::new(|metadata| {
-        metadata.is_span()
-            && metadata
-                .file()
-                .filter(|file| file.contains("nucliadb"))
-                .is_some()
-    });
-
-    Ok(tracing_opentelemetry::layer()
-        .with_tracer(tracer)
-        .with_filter(level_filter)
-        .with_filter(span_filter)
-        .boxed())
-}
-
-fn setup_sentry(env: &'static str, sentry_url: String) -> ClientInitGuard {
-    sentry::init((
-        sentry_url,
-        sentry::ClientOptions {
-            release: sentry::release_name!(),
-            environment: Some(env.into()),
-            ..Default::default()
-        },
-    ))
-}
-
-fn sentry_layer() -> Box<dyn Layer<Registry> + Send + Sync> {
-    sentry_tracing::layer().boxed()
-}
-
-pub fn run_with_telemetry<F, R>(current: Span, f: F) -> R
-where F: FnOnce() -> R {
-    let tid = current.context().span().span_context().trace_id();
-    sentry::with_scope(|scope| scope.set_tag(TRACE_ID, tid), || current.in_scope(f))
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::sync::Arc;
+
+use nucliadb_core::tracing::{Level, Span};
+use nucliadb_core::{Context, NodeResult};
+use opentelemetry::global;
+use opentelemetry::trace::TraceContextExt;
+use sentry::ClientInitGuard;
+use tracing_opentelemetry::OpenTelemetrySpanExt;
+use tracing_subscriber::filter::{FilterFn, LevelFilter, Targets};
+use tracing_subscriber::layer::SubscriberExt;
+use tracing_subscriber::util::SubscriberInitExt;
+use tracing_subscriber::{Layer, Registry};
+
+use crate::settings::Settings;
+
+const TRACE_ID: &str = "trace-id";
+
+pub fn init_telemetry(settings: &Arc<Settings>) -> NodeResult<ClientInitGuard> {
+    let mut layers = Vec::new();
+
+    let stdout = stdout_layer(settings);
+    layers.push(stdout);
+
+    if settings.jaeger_enabled() {
+        let jaeger = jaeger_layer(settings)?;
+        layers.push(jaeger);
+    }
+
+    let sentry_guard = setup_sentry(settings.sentry_env(), settings.sentry_url());
+    let sentry = sentry_layer();
+    layers.push(sentry);
+
+    tracing_subscriber::registry()
+        .with(layers)
+        .try_init()
+        .with_context(|| "trying to init tracing")?;
+
+    Ok(sentry_guard)
+}
+
+fn stdout_layer(settings: &Arc<Settings>) -> Box<dyn Layer<Registry> + Send + Sync> {
+    let format = tracing_subscriber::fmt::format().with_level(true).compact();
+
+    let log_levels = settings.log_levels().to_vec();
+    tracing_subscriber::fmt::layer()
+        .event_format(format)
+        .with_filter(Targets::new().with_targets(log_levels))
+        .boxed()
+}
+
+fn jaeger_layer(settings: &Arc<Settings>) -> NodeResult<Box<dyn Layer<Registry> + Send + Sync>> {
+    global::set_text_map_propagator(opentelemetry_zipkin::Propagator::new());
+
+    let agent_endpoint = settings.jaeger_agent_address();
+    let tracer = opentelemetry_jaeger::new_pipeline()
+        .with_agent_endpoint(agent_endpoint)
+        .with_service_name("nucliadb_node")
+        .with_auto_split_batch(true)
+        .install_batch(opentelemetry::runtime::Tokio)?;
+
+    // To avoid sending too much information to Jaeger, we filter out all events
+    // (as they are logged to stdout), spans from external instrumented crates
+    // (like tantivy, hyper, tower, mio...) and spans below INFO level (default
+    // span level).
+    let level_filter = LevelFilter::from_level(Level::INFO);
+    let span_filter = FilterFn::new(|metadata| {
+        metadata.is_span()
+            && metadata
+                .file()
+                .filter(|file| file.contains("nucliadb"))
+                .is_some()
+    });
+
+    Ok(tracing_opentelemetry::layer()
+        .with_tracer(tracer)
+        .with_filter(level_filter)
+        .with_filter(span_filter)
+        .boxed())
+}
+
+fn setup_sentry(env: &'static str, sentry_url: String) -> ClientInitGuard {
+    sentry::init((
+        sentry_url,
+        sentry::ClientOptions {
+            release: sentry::release_name!(),
+            environment: Some(env.into()),
+            ..Default::default()
+        },
+    ))
+}
+
+fn sentry_layer() -> Box<dyn Layer<Registry> + Send + Sync> {
+    sentry_tracing::layer().boxed()
+}
+
+pub fn run_with_telemetry<F, R>(current: Span, f: F) -> R
+where F: FnOnce() -> R {
+    let tid = current.context().span().span_context().trace_id();
+    sentry::with_scope(|scope| scope.set_tag(TRACE_ID, tid), || current.in_scope(f))
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/src/writer/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/src/writer/mod.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,272 +1,274 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod grpc_driver;
-use std::collections::HashMap;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::{
-    DeleteGraphNodes, JoinGraph, NewShardRequest, NewVectorSetRequest, OpStatus, Resource,
-    ResourceId, ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId,
-};
-use nucliadb_core::thread::ThreadPoolBuilder;
-use nucliadb_core::tracing::{self, *};
-use nucliadb_vectors::data_point_provider::Merger as VectorsMerger;
-use uuid::Uuid;
-
-use crate::env;
-use crate::services::writer::ShardWriterService;
-
-#[derive(Debug, Default)]
-pub struct NodeWriterService {
-    pub cache: HashMap<String, ShardWriterService>,
-}
-
-impl NodeWriterService {
-    fn initialize_file_system() -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        let data_path = env::data_path();
-        if !data_path.exists() {
-            return Err(node_error!("{:?} is not created", data_path));
-        }
-        if !shards_path.exists() {
-            std::fs::create_dir(&shards_path)?;
-        }
-        Ok(())
-    }
-    pub fn new() -> NodeResult<Self> {
-        Self::initialize_file_system()?;
-        // We shallow the error if the threadpools were already initialized
-        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
-        let _ = VectorsMerger::install_global().map(std::thread::spawn);
-        Ok(Self::default())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn shutdown(&self) {
-        for (shard_id, shard) in self.cache.iter() {
-            debug!("Stopping shard {}", shard_id);
-            shard.stop();
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shards(&mut self) -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        debug!("Recovering shards from {shards_path:?}...");
-        for entry in std::fs::read_dir(&shards_path)? {
-            let entry = entry?;
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            match ShardWriterService::open(file_name.clone(), &shard_path) {
-                Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
-                Ok(shard) => {
-                    debug!("{shard_path:?}: Shard loaded");
-                    self.cache.insert(file_name, shard);
-                }
-            }
-        }
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shard(&mut self, shard_id: &ShardId) {
-        let shard_name = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id.id);
-        if self.cache.contains_key(&shard_id.id) {
-            debug!("Shard {shard_path:?} is already on memory");
-            return;
-        }
-        if !shard_path.is_dir() {
-            error!("Shard {shard_path:?} is not on disk");
-            return;
-        }
-        match ShardWriterService::open(shard_name, &shard_path) {
-            Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
-            Ok(shard) => {
-                self.cache.insert(shard_id.id.clone(), shard);
-                debug!("{shard_path:?}: Shard loaded");
-            }
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardWriterService> {
-        self.cache.get(&shard_id.id)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_mut_shard(&mut self, shard_id: &ShardId) -> Option<&mut ShardWriterService> {
-        self.cache.get_mut(&shard_id.id)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn new_shard(request: &NewShardRequest) -> NodeResult<ShardCreated> {
-        let shard_id = Uuid::new_v4().to_string();
-        let shard_path = env::shards_path_id(&shard_id);
-        let new_shard = ShardWriterService::new(shard_id.clone(), &shard_path, request)?;
-        Ok(ShardCreated {
-            id: shard_id,
-            document_service: new_shard.document_version() as i32,
-            paragraph_service: new_shard.paragraph_version() as i32,
-            vector_service: new_shard.vector_version() as i32,
-            relation_service: new_shard.relation_version() as i32,
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn delete_shard(&mut self, shard_id: &ShardId) -> NodeResult<()> {
-        if shard_id.id.is_empty() {
-            warn!("Shard id is empty");
-            return Ok(());
-        }
-
-        self.cache.remove(&shard_id.id);
-
-        let shard_path = env::shards_path_id(&shard_id.id);
-        if shard_path.exists() {
-            debug!("Deleting {:?}", shard_path);
-            std::fs::remove_dir_all(shard_path)?;
-        }
-
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn clean_and_upgrade_shard(&mut self, shard_id: &ShardId) -> NodeResult<ShardCleaned> {
-        self.cache.remove(&shard_id.id);
-        let shard_id = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id);
-        let new_shard = ShardWriterService::clean_and_create(shard_id.clone(), &shard_path)?;
-        let shard_data = ShardCleaned {
-            document_service: new_shard.document_version() as i32,
-            paragraph_service: new_shard.paragraph_version() as i32,
-            vector_service: new_shard.vector_version() as i32,
-            relation_service: new_shard.relation_version() as i32,
-        };
-        self.cache.insert(shard_id, new_shard);
-        Ok(shard_data)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn set_resource(
-        &self,
-        shard_id: &ShardId,
-        resource: &Resource,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-
-        shard.set_resource(resource)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn add_vectorset(&self, request: &NewVectorSetRequest) -> NodeResult<Option<OpStatus>> {
-        let Some(setid) = &request.id else {
-            return Err(node_error!("missing vectorset id"));
-        };
-        let Some(shard_id) = &setid.shard else {
-            return Err(node_error!("missing shard id"));
-        };
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.add_vectorset(setid, request.similarity())?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn remove_vectorset(
-        &self,
-        shard_id: &ShardId,
-        setid: &VectorSetId,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.remove_vectorset(setid)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn join_relations_graph(
-        &self,
-        shard_id: &ShardId,
-        graph: &JoinGraph,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.join_relations_graph(graph)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn delete_relation_nodes(
-        &self,
-        shard_id: &ShardId,
-        request: &DeleteGraphNodes,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.delete_relation_nodes(request)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn remove_resource(
-        &self,
-        shard_id: &ShardId,
-        resource: &ResourceId,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.remove_resource(resource)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn gc(&self, shard_id: &ShardId) -> NodeResult<Option<()>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.gc()?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn list_vectorsets(&self, shard_id: &ShardId) -> NodeResult<Option<Vec<String>>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let shard_response = shard.list_vectorsets()?;
-        Ok(Some(shard_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard_ids(&self) -> ShardIds {
-        let ids = self
-            .cache
-            .keys()
-            .cloned()
-            .map(|id| ShardId { id })
-            .collect();
-        ShardIds { ids }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod grpc_driver;
+
+use std::collections::HashMap;
+use std::path::Path;
+
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::{
+    DeleteGraphNodes, JoinGraph, NewShardRequest, NewVectorSetRequest, OpStatus, Resource,
+    ResourceId, ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId,
+};
+use nucliadb_core::thread::ThreadPoolBuilder;
+use nucliadb_core::tracing::{self, *};
+use nucliadb_vectors::data_point_provider::Merger as VectorsMerger;
+use uuid::Uuid;
+
+use crate::services::writer::ShardWriterService;
+use crate::shard_metadata::ShardMetadata;
+use crate::{disk_structure, env};
+
+/// Initialize the index node writer. This function must be called before using
+/// a writer
+pub fn initialize(data_path: &Path, shards_path: &Path) -> NodeResult<()> {
+    if !data_path.exists() {
+        return Err(node_error!(
+            "Data directory ({:?}) should be already created",
+            data_path
+        ));
+    }
+
+    if !shards_path.exists() {
+        std::fs::create_dir(shards_path)?;
+    }
+
+    // We shallow the error if the threadpools were already initialized
+    let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
+    let _ = VectorsMerger::install_global().map(std::thread::spawn);
+
+    Ok(())
+}
+
+#[derive(Debug, Default)]
+pub struct NodeWriterService {
+    pub cache: HashMap<String, ShardWriterService>,
+}
+
+impl NodeWriterService {
+    pub fn new() -> NodeResult<Self> {
+        initialize(&env::data_path(), &env::shards_path())?;
+        Ok(Self::default())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shards(&mut self) -> NodeResult<()> {
+        let shards_path = env::shards_path();
+        debug!("Recovering shards from {shards_path:?}...");
+        for entry in std::fs::read_dir(&shards_path)? {
+            let entry = entry?;
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            match ShardWriterService::open(file_name.clone(), &shard_path) {
+                Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
+                Ok(shard) => {
+                    debug!("{shard_path:?}: Shard loaded");
+                    self.cache.insert(file_name, shard);
+                }
+            }
+        }
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shard(&mut self, shard_id: &ShardId) {
+        let shard_name = shard_id.id.clone();
+        let shard_path = disk_structure::shard_path_by_id(&env::shards_path(), &shard_id.id);
+        if self.cache.contains_key(&shard_id.id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return;
+        }
+        if !shard_path.is_dir() {
+            error!("Shard {shard_path:?} is not on disk");
+            return;
+        }
+        match ShardWriterService::open(shard_name, &shard_path) {
+            Err(err) => error!("Shard {shard_path:?} could not be loaded from disk: {err:?}"),
+            Ok(shard) => {
+                self.cache.insert(shard_id.id.clone(), shard);
+                debug!("{shard_path:?}: Shard loaded");
+            }
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardWriterService> {
+        self.cache.get(&shard_id.id)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn new_shard(request: &NewShardRequest) -> NodeResult<ShardCreated> {
+        let shard_id = Uuid::new_v4().to_string();
+        let shard_path = disk_structure::shard_path_by_id(&env::shards_path(), &shard_id);
+        let new_shard = ShardWriterService::new(
+            shard_id.clone(),
+            &shard_path,
+            ShardMetadata::from(request.to_owned()),
+        )?;
+        Ok(ShardCreated {
+            id: shard_id,
+            document_service: new_shard.document_version() as i32,
+            paragraph_service: new_shard.paragraph_version() as i32,
+            vector_service: new_shard.vector_version() as i32,
+            relation_service: new_shard.relation_version() as i32,
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn delete_shard(&mut self, shard_id: &ShardId) -> NodeResult<()> {
+        if shard_id.id.is_empty() {
+            warn!("Shard id is empty");
+            return Ok(());
+        }
+
+        self.cache.remove(&shard_id.id);
+
+        let shard_path = disk_structure::shard_path_by_id(&env::shards_path(), &shard_id.id);
+        if shard_path.exists() {
+            debug!("Deleting {:?}", shard_path);
+            std::fs::remove_dir_all(shard_path)?;
+        }
+
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn clean_and_upgrade_shard(&mut self, shard_id: &ShardId) -> NodeResult<ShardCleaned> {
+        self.cache.remove(&shard_id.id);
+        let shard_id = shard_id.id.clone();
+        let shard_path = disk_structure::shard_path_by_id(&env::shards_path(), &shard_id);
+        let new_shard = ShardWriterService::clean_and_create(shard_id.clone(), &shard_path)?;
+        let shard_data = ShardCleaned {
+            document_service: new_shard.document_version() as i32,
+            paragraph_service: new_shard.paragraph_version() as i32,
+            vector_service: new_shard.vector_version() as i32,
+            relation_service: new_shard.relation_version() as i32,
+        };
+        self.cache.insert(shard_id, new_shard);
+        Ok(shard_data)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn set_resource(
+        &self,
+        shard_id: &ShardId,
+        resource: &Resource,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+
+        shard.set_resource(resource)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn add_vectorset(&self, request: &NewVectorSetRequest) -> NodeResult<Option<OpStatus>> {
+        let Some(setid) = &request.id else {
+            return Err(node_error!("missing vectorset id"));
+        };
+        let Some(shard_id) = &setid.shard else {
+            return Err(node_error!("missing shard id"));
+        };
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.add_vectorset(setid, request.similarity())?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn remove_vectorset(
+        &self,
+        shard_id: &ShardId,
+        setid: &VectorSetId,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.remove_vectorset(setid)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn join_relations_graph(
+        &self,
+        shard_id: &ShardId,
+        graph: &JoinGraph,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.join_relations_graph(graph)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn delete_relation_nodes(
+        &self,
+        shard_id: &ShardId,
+        request: &DeleteGraphNodes,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.delete_relation_nodes(request)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn remove_resource(
+        &self,
+        shard_id: &ShardId,
+        resource: &ResourceId,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.remove_resource(resource)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn gc(&self, shard_id: &ShardId) -> NodeResult<Option<()>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.gc()?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn list_vectorsets(&self, shard_id: &ShardId) -> NodeResult<Option<Vec<String>>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let shard_response = shard.list_vectorsets()?;
+        Ok(Some(shard_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard_ids(&self) -> ShardIds {
+        let ids = self
+            .cache
+            .keys()
+            .cloned()
+            .map(|id| ShardId { id })
+            .collect();
+        ShardIds { ids }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/common/constants.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/common/constants.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::net::{IpAddr, Ipv4Addr, SocketAddr};
-use std::time::Duration;
-
-use once_cell::sync::Lazy;
-
-const READER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
-const READER_PORT: u16 = 18031;
-pub static READER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(READER_IP, READER_PORT));
-
-const WRITER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
-const WRITER_PORT: u16 = 18030;
-pub static WRITER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(WRITER_IP, WRITER_PORT));
-
-pub const SERVER_STARTUP_TIMEOUT: Duration = Duration::from_secs(5);
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::net::{IpAddr, Ipv4Addr, SocketAddr};
+use std::time::Duration;
+
+use once_cell::sync::Lazy;
+
+const READER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
+const READER_PORT: u16 = 18031;
+pub static READER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(READER_IP, READER_PORT));
+
+const WRITER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
+const WRITER_PORT: u16 = 18030;
+pub static WRITER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(WRITER_IP, WRITER_PORT));
+
+pub const SERVER_STARTUP_TIMEOUT: Duration = Duration::from_secs(5);
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/common/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/common/mod.rs`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-/// Utilities to test NucliaDB node
-
-#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
-mod constants;
-#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
-mod node_services;
-
-pub use constants::*;
-pub use node_services::*;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+/// Utilities to test NucliaDB node
+
+#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
+mod constants;
+#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
+mod node_services;
+
+pub use constants::*;
+pub use node_services::*;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_search_relations.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_search_relations.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,619 +1,619 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod common;
-
-use std::collections::{HashMap, HashSet};
-use std::time::SystemTime;
-
-use common::{node_reader, node_writer, TestNodeWriter};
-use nucliadb_core::protos::op_status::Status;
-use nucliadb_core::protos::prost_types::Timestamp;
-use nucliadb_core::protos::relation::RelationType;
-use nucliadb_core::protos::relation_node::NodeType;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{
-    EntitiesSubgraphRequest, IndexMetadata, NewShardRequest, Relation, RelationEdgeFilter,
-    RelationNode, RelationNodeFilter, RelationPrefixSearchRequest, RelationSearchRequest,
-    RelationSearchResponse, Resource, ResourceId,
-};
-use tonic::Request;
-use uuid::Uuid;
-
-async fn create_knowledge_graph(
-    writer: &mut TestNodeWriter,
-    shard_id: String,
-) -> HashMap<String, RelationNode> {
-    let rid = Uuid::new_v4();
-
-    let mut relation_nodes = HashMap::new();
-    relation_nodes.insert(
-        rid.to_string(),
-        RelationNode {
-            value: rid.to_string(),
-            ntype: NodeType::Resource as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Animal".to_string(),
-        RelationNode {
-            value: "Animal".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Batman".to_string(),
-        RelationNode {
-            value: "Batman".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Becquer".to_string(),
-        RelationNode {
-            value: "Becquer".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Cat".to_string(),
-        RelationNode {
-            value: "Cat".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: "animal".to_string(),
-        },
-    );
-    relation_nodes.insert(
-        "Catwoman".to_string(),
-        RelationNode {
-            value: "Catwoman".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: "superhero".to_string(),
-        },
-    );
-    relation_nodes.insert(
-        "Eric".to_string(),
-        RelationNode {
-            value: "Eric".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Fly".to_string(),
-        RelationNode {
-            value: "Fly".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Gravity".to_string(),
-        RelationNode {
-            value: "Gravity".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Joan Antoni".to_string(),
-        RelationNode {
-            value: "Joan Antoni".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Joker".to_string(),
-        RelationNode {
-            value: "Joker".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Newton".to_string(),
-        RelationNode {
-            value: "Newton".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Physics".to_string(),
-        RelationNode {
-            value: "Physics".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Poetry".to_string(),
-        RelationNode {
-            value: "Poetry".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Swallow".to_string(),
-        RelationNode {
-            value: "Swallow".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-
-    let relation_edges = vec![
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Batman").unwrap().clone()),
-            to: Some(relation_nodes.get("Catwoman").unwrap().clone()),
-            relation_label: "love".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Batman").unwrap().clone()),
-            to: Some(relation_nodes.get("Joker").unwrap().clone()),
-            relation_label: "fight".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joker").unwrap().clone()),
-            to: Some(relation_nodes.get("Physics").unwrap().clone()),
-            relation_label: "enjoy".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Catwoman").unwrap().clone()),
-            to: Some(relation_nodes.get("Cat").unwrap().clone()),
-            relation_label: "imitate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Cat").unwrap().clone()),
-            to: Some(relation_nodes.get("Animal").unwrap().clone()),
-            relation_label: "species".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Newton").unwrap().clone()),
-            to: Some(relation_nodes.get("Physics").unwrap().clone()),
-            relation_label: "study".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Newton").unwrap().clone()),
-            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
-            relation_label: "formulate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Eric").unwrap().clone()),
-            to: Some(relation_nodes.get("Cat").unwrap().clone()),
-            relation_label: "like".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Eric").unwrap().clone()),
-            to: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            relation_label: "collaborate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            to: Some(relation_nodes.get("Eric").unwrap().clone()),
-            relation_label: "collaborate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            to: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            relation_label: "read".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "write".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "like".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::About as i32,
-            source: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            to: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            relation_label: "about".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Other as i32,
-            source: Some(relation_nodes.get(&rid.to_string()).unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "subject".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            to: Some(relation_nodes.get("Animal").unwrap().clone()),
-            relation_label: "species".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            to: Some(relation_nodes.get("Fly").unwrap().clone()),
-            relation_label: "can".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Fly").unwrap().clone()),
-            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
-            relation_label: "defy".to_string(),
-            ..Default::default()
-        },
-    ];
-
-    let now = SystemTime::now()
-        .duration_since(SystemTime::UNIX_EPOCH)
-        .unwrap();
-    let timestamp = Timestamp {
-        seconds: now.as_secs() as i64,
-        nanos: 0,
-    };
-
-    let r = writer
-        .set_resource(Resource {
-            shard_id: shard_id.clone(),
-            resource: Some(ResourceId {
-                shard_id: shard_id.clone(),
-                uuid: rid.to_string(),
-            }),
-            status: ResourceStatus::Processed as i32,
-            relations: relation_edges.clone(),
-            metadata: Some(IndexMetadata {
-                created: Some(timestamp.clone()),
-                modified: Some(timestamp),
-            }),
-            texts: HashMap::new(),
-            ..Default::default()
-        })
-        .await
-        .unwrap();
-
-    assert_eq!(r.get_ref().status(), Status::Ok);
-
-    relation_nodes
-}
-
-#[tokio::test]
-async fn test_search_relations_prefixed() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-    let mut reader = node_reader().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    let nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with empty term. Results are limited
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: String::new(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = &prefix_response.nodes;
-    // TODO: get constants from RelationsReaderService (.../relations/service/reader.rs)
-    assert_eq!(results.len(), nodes.len());
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "cat" term (some results)
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string(), "Catwoman".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "cat" and filters
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                node_filters: vec![RelationNodeFilter {
-                    node_subtype: Some("animal".to_string()),
-                    node_type: NodeType::Entity as i32,
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                node_filters: vec![RelationNodeFilter {
-                    node_subtype: Some("superhero".to_string()),
-                    node_type: NodeType::Entity as i32,
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Catwoman".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with node filters and empty query
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: String::new(),
-                node_filters: vec![RelationNodeFilter {
-                    node_type: NodeType::Entity as i32,
-                    node_subtype: Some("animal".to_string()),
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "zzz" term (empty results)
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "zzz".to_string(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = &prefix_response.nodes;
-    assert!(results.is_empty());
-
-    Ok(())
-}
-
-#[tokio::test]
-async fn test_search_relations_neighbours() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-    let mut reader = node_reader().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    let relation_nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
-
-    fn extract_relations(response: &RelationSearchResponse) -> HashSet<(String, String)> {
-        response
-            .subgraph
-            .iter()
-            .flat_map(|neighbours| neighbours.relations.iter())
-            .flat_map(|node| {
-                [(
-                    node.source.as_ref().unwrap().value.to_owned(),
-                    node.to.as_ref().unwrap().value.to_owned(),
-                )]
-            })
-            .collect::<HashSet<_>>()
-    }
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on existent node
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![relation_nodes.get("Swallow").unwrap().clone()],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([
-        ("Poetry".to_string(), "Swallow".to_string()),
-        ("Swallow".to_string(), "Animal".to_string()),
-        ("Swallow".to_string(), "Fly".to_string()),
-    ]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on multiple existent nodes
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![
-                    relation_nodes.get("Becquer").unwrap().clone(),
-                    relation_nodes.get("Newton").unwrap().clone(),
-                ],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([
-        ("Newton".to_string(), "Physics".to_string()),
-        ("Newton".to_string(), "Gravity".to_string()),
-        ("Becquer".to_string(), "Poetry".to_string()),
-        ("Joan Antoni".to_string(), "Becquer".to_string()),
-    ]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on non existent node
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![RelationNode {
-                    value: "Fake".to_string(),
-                    ntype: NodeType::Entity as i32,
-                    subtype: String::new(),
-                }],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let neighbours = extract_relations(response.get_ref());
-    assert!(neighbours.is_empty());
-
-    // --------------------------------------------------------------
-    // Test: neighbours search with filters
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![relation_nodes.get("Poetry").unwrap().clone()],
-                node_filters: vec![RelationNodeFilter {
-                    node_type: NodeType::Entity as i32,
-                    ..Default::default()
-                }],
-                edge_filters: vec![RelationEdgeFilter {
-                    relation_type: RelationType::About as i32,
-                    ..Default::default()
-                }],
-                depth: Some(1),
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([("Poetry".to_string(), "Swallow".to_string())]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod common;
+
+use std::collections::{HashMap, HashSet};
+use std::time::SystemTime;
+
+use common::{node_reader, node_writer, TestNodeWriter};
+use nucliadb_core::protos::op_status::Status;
+use nucliadb_core::protos::prost_types::Timestamp;
+use nucliadb_core::protos::relation::RelationType;
+use nucliadb_core::protos::relation_node::NodeType;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{
+    EntitiesSubgraphRequest, IndexMetadata, NewShardRequest, Relation, RelationEdgeFilter,
+    RelationNode, RelationNodeFilter, RelationPrefixSearchRequest, RelationSearchRequest,
+    RelationSearchResponse, Resource, ResourceId,
+};
+use tonic::Request;
+use uuid::Uuid;
+
+async fn create_knowledge_graph(
+    writer: &mut TestNodeWriter,
+    shard_id: String,
+) -> HashMap<String, RelationNode> {
+    let rid = Uuid::new_v4();
+
+    let mut relation_nodes = HashMap::new();
+    relation_nodes.insert(
+        rid.to_string(),
+        RelationNode {
+            value: rid.to_string(),
+            ntype: NodeType::Resource as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Animal".to_string(),
+        RelationNode {
+            value: "Animal".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Batman".to_string(),
+        RelationNode {
+            value: "Batman".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Becquer".to_string(),
+        RelationNode {
+            value: "Becquer".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Cat".to_string(),
+        RelationNode {
+            value: "Cat".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: "animal".to_string(),
+        },
+    );
+    relation_nodes.insert(
+        "Catwoman".to_string(),
+        RelationNode {
+            value: "Catwoman".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: "superhero".to_string(),
+        },
+    );
+    relation_nodes.insert(
+        "Eric".to_string(),
+        RelationNode {
+            value: "Eric".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Fly".to_string(),
+        RelationNode {
+            value: "Fly".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Gravity".to_string(),
+        RelationNode {
+            value: "Gravity".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Joan Antoni".to_string(),
+        RelationNode {
+            value: "Joan Antoni".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Joker".to_string(),
+        RelationNode {
+            value: "Joker".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Newton".to_string(),
+        RelationNode {
+            value: "Newton".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Physics".to_string(),
+        RelationNode {
+            value: "Physics".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Poetry".to_string(),
+        RelationNode {
+            value: "Poetry".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Swallow".to_string(),
+        RelationNode {
+            value: "Swallow".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+
+    let relation_edges = vec![
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Batman").unwrap().clone()),
+            to: Some(relation_nodes.get("Catwoman").unwrap().clone()),
+            relation_label: "love".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Batman").unwrap().clone()),
+            to: Some(relation_nodes.get("Joker").unwrap().clone()),
+            relation_label: "fight".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joker").unwrap().clone()),
+            to: Some(relation_nodes.get("Physics").unwrap().clone()),
+            relation_label: "enjoy".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Catwoman").unwrap().clone()),
+            to: Some(relation_nodes.get("Cat").unwrap().clone()),
+            relation_label: "imitate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Cat").unwrap().clone()),
+            to: Some(relation_nodes.get("Animal").unwrap().clone()),
+            relation_label: "species".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Newton").unwrap().clone()),
+            to: Some(relation_nodes.get("Physics").unwrap().clone()),
+            relation_label: "study".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Newton").unwrap().clone()),
+            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
+            relation_label: "formulate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Eric").unwrap().clone()),
+            to: Some(relation_nodes.get("Cat").unwrap().clone()),
+            relation_label: "like".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Eric").unwrap().clone()),
+            to: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            relation_label: "collaborate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            to: Some(relation_nodes.get("Eric").unwrap().clone()),
+            relation_label: "collaborate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            to: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            relation_label: "read".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "write".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "like".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::About as i32,
+            source: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            to: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            relation_label: "about".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Other as i32,
+            source: Some(relation_nodes.get(&rid.to_string()).unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "subject".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            to: Some(relation_nodes.get("Animal").unwrap().clone()),
+            relation_label: "species".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            to: Some(relation_nodes.get("Fly").unwrap().clone()),
+            relation_label: "can".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Fly").unwrap().clone()),
+            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
+            relation_label: "defy".to_string(),
+            ..Default::default()
+        },
+    ];
+
+    let now = SystemTime::now()
+        .duration_since(SystemTime::UNIX_EPOCH)
+        .unwrap();
+    let timestamp = Timestamp {
+        seconds: now.as_secs() as i64,
+        nanos: 0,
+    };
+
+    let r = writer
+        .set_resource(Resource {
+            shard_id: shard_id.clone(),
+            resource: Some(ResourceId {
+                shard_id: shard_id.clone(),
+                uuid: rid.to_string(),
+            }),
+            status: ResourceStatus::Processed as i32,
+            relations: relation_edges.clone(),
+            metadata: Some(IndexMetadata {
+                created: Some(timestamp.clone()),
+                modified: Some(timestamp),
+            }),
+            texts: HashMap::new(),
+            ..Default::default()
+        })
+        .await
+        .unwrap();
+
+    assert_eq!(r.get_ref().status(), Status::Ok);
+
+    relation_nodes
+}
+
+#[tokio::test]
+async fn test_search_relations_prefixed() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+    let mut reader = node_reader().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    let nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with empty term. Results are limited
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: String::new(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = &prefix_response.nodes;
+    // TODO: get constants from RelationsReaderService (.../relations/service/reader.rs)
+    assert_eq!(results.len(), nodes.len());
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "cat" term (some results)
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string(), "Catwoman".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "cat" and filters
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                node_filters: vec![RelationNodeFilter {
+                    node_subtype: Some("animal".to_string()),
+                    node_type: NodeType::Entity as i32,
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                node_filters: vec![RelationNodeFilter {
+                    node_subtype: Some("superhero".to_string()),
+                    node_type: NodeType::Entity as i32,
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Catwoman".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with node filters and empty query
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: String::new(),
+                node_filters: vec![RelationNodeFilter {
+                    node_type: NodeType::Entity as i32,
+                    node_subtype: Some("animal".to_string()),
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "zzz" term (empty results)
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "zzz".to_string(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = &prefix_response.nodes;
+    assert!(results.is_empty());
+
+    Ok(())
+}
+
+#[tokio::test]
+async fn test_search_relations_neighbours() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+    let mut reader = node_reader().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    let relation_nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
+
+    fn extract_relations(response: &RelationSearchResponse) -> HashSet<(String, String)> {
+        response
+            .subgraph
+            .iter()
+            .flat_map(|neighbours| neighbours.relations.iter())
+            .flat_map(|node| {
+                [(
+                    node.source.as_ref().unwrap().value.to_owned(),
+                    node.to.as_ref().unwrap().value.to_owned(),
+                )]
+            })
+            .collect::<HashSet<_>>()
+    }
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on existent node
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![relation_nodes.get("Swallow").unwrap().clone()],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([
+        ("Poetry".to_string(), "Swallow".to_string()),
+        ("Swallow".to_string(), "Animal".to_string()),
+        ("Swallow".to_string(), "Fly".to_string()),
+    ]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on multiple existent nodes
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![
+                    relation_nodes.get("Becquer").unwrap().clone(),
+                    relation_nodes.get("Newton").unwrap().clone(),
+                ],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([
+        ("Newton".to_string(), "Physics".to_string()),
+        ("Newton".to_string(), "Gravity".to_string()),
+        ("Becquer".to_string(), "Poetry".to_string()),
+        ("Joan Antoni".to_string(), "Becquer".to_string()),
+    ]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on non existent node
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![RelationNode {
+                    value: "Fake".to_string(),
+                    ntype: NodeType::Entity as i32,
+                    subtype: String::new(),
+                }],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let neighbours = extract_relations(response.get_ref());
+    assert!(neighbours.is_empty());
+
+    // --------------------------------------------------------------
+    // Test: neighbours search with filters
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![relation_nodes.get("Poetry").unwrap().clone()],
+                node_filters: vec![RelationNodeFilter {
+                    node_type: NodeType::Entity as i32,
+                    ..Default::default()
+                }],
+                edge_filters: vec![RelationEdgeFilter {
+                    relation_type: RelationType::About as i32,
+                    ..Default::default()
+                }],
+                depth: Some(1),
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([("Poetry".to_string(), "Swallow".to_string())]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_search_sorting.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_search_sorting.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,189 +1,187 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod common;
-
-use std::collections::HashMap;
-use std::time::SystemTime;
-
-use common::{node_reader, node_writer, TestNodeReader, TestNodeWriter};
-use nucliadb_core::protos as nucliadb_protos;
-use nucliadb_protos::op_status::Status;
-use nucliadb_protos::prost_types::Timestamp;
-use nucliadb_protos::resource::ResourceStatus;
-use nucliadb_protos::{IndexMetadata, NewShardRequest, Resource, ResourceId, SearchRequest};
-use tonic::Request;
-use uuid::Uuid;
-
-async fn create_dummy_resources(total: u8, writer: &mut TestNodeWriter, shard_id: String) {
-    let resource_creation_delay = std::time::Duration::from_secs(1);
-    for i in 0..total {
-        let rid = Uuid::new_v4();
-        let field = format!("dummy-{i:0>3}");
-
-        let now = SystemTime::now()
-            .duration_since(SystemTime::UNIX_EPOCH)
-            .unwrap();
-        let timestamp = Timestamp {
-            seconds: now.as_secs() as i64,
-            nanos: 0,
-        };
-
-        let labels = vec![format!("/dummy{i:0>3}")];
-        let mut texts = HashMap::new();
-        texts.insert(
-            field,
-            nucliadb_protos::TextInformation {
-                text: format!("Dummy text {i:0>3}"),
-                labels: vec![],
-            },
-        );
-        let result = writer
-            .set_resource(Resource {
-                shard_id: shard_id.clone(),
-                resource: Some(ResourceId {
-                    shard_id: shard_id.clone(),
-                    uuid: rid.to_string(),
-                }),
-                status: ResourceStatus::Processed as i32,
-                metadata: Some(IndexMetadata {
-                    created: Some(timestamp.clone()),
-                    modified: Some(timestamp),
-                }),
-                labels,
-                texts,
-                ..Default::default()
-            })
-            .await
-            .unwrap();
-
-        assert_eq!(result.get_ref().status(), Status::Ok);
-        std::thread::sleep(resource_creation_delay);
-    }
-}
-
-#[tokio::test]
-async fn test_search_sorting() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-    let mut reader = node_reader().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    create_dummy_resources(20, &mut writer, shard_id.clone()).await;
-
-    async fn paginated_search(
-        reader: &mut TestNodeReader,
-        shard_id: String,
-        order: Option<nucliadb_protos::OrderBy>,
-        page_size: i32,
-    ) -> Vec<String> {
-        let mut fields = Vec::new();
-        let mut page = 0;
-        let mut next_page = true;
-
-        while next_page {
-            let response = reader
-                .search(SearchRequest {
-                    shard: shard_id.clone(),
-                    order: order.clone(),
-                    document: true,
-                    page_number: page,
-                    result_per_page: page_size,
-                    ..Default::default()
-                })
-                .await
-                .unwrap();
-
-            let document_response = response.get_ref().document.as_ref().unwrap();
-            fields.extend(document_response.results.iter().cloned().map(|r| r.field));
-
-            next_page = document_response.next_page;
-            page += 1;
-        }
-
-        fields
-    }
-
-    // --------------------------------------------------------------
-    // Test: sort by creation date in ascending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by modified date in ascending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by creation date in descending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    sorted_fields.reverse();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by modified date in descending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    sorted_fields.reverse();
-    assert_eq!(fields, sorted_fields);
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod common;
+
+use std::collections::HashMap;
+use std::time::SystemTime;
+
+use common::{node_reader, node_writer, TestNodeReader, TestNodeWriter};
+use nucliadb_core::protos as nucliadb_protos;
+use nucliadb_protos::op_status::Status;
+use nucliadb_protos::prost_types::Timestamp;
+use nucliadb_protos::resource::ResourceStatus;
+use nucliadb_protos::{IndexMetadata, NewShardRequest, Resource, ResourceId, SearchRequest};
+use tonic::Request;
+use uuid::Uuid;
+
+async fn create_dummy_resources(total: u8, writer: &mut TestNodeWriter, shard_id: String) {
+    for i in 0..total {
+        let rid = Uuid::new_v4();
+        let field = format!("dummy-{i:0>3}");
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64 - (total - i) as i64,
+            nanos: 0,
+        };
+
+        let labels = vec![format!("/dummy{i:0>3}")];
+        let mut texts = HashMap::new();
+        texts.insert(
+            field,
+            nucliadb_protos::TextInformation {
+                text: format!("Dummy text {i:0>3}"),
+                labels: vec![],
+            },
+        );
+        let result = writer
+            .set_resource(Resource {
+                shard_id: shard_id.clone(),
+                resource: Some(ResourceId {
+                    shard_id: shard_id.clone(),
+                    uuid: rid.to_string(),
+                }),
+                status: ResourceStatus::Processed as i32,
+                metadata: Some(IndexMetadata {
+                    created: Some(timestamp.clone()),
+                    modified: Some(timestamp),
+                }),
+                labels,
+                texts,
+                ..Default::default()
+            })
+            .await
+            .unwrap();
+
+        assert_eq!(result.get_ref().status(), Status::Ok);
+    }
+}
+
+#[tokio::test]
+async fn test_search_sorting() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+    let mut reader = node_reader().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    create_dummy_resources(20, &mut writer, shard_id.clone()).await;
+
+    async fn paginated_search(
+        reader: &mut TestNodeReader,
+        shard_id: String,
+        order: Option<nucliadb_protos::OrderBy>,
+        page_size: i32,
+    ) -> Vec<String> {
+        let mut fields = Vec::new();
+        let mut page = 0;
+        let mut next_page = true;
+
+        while next_page {
+            let response = reader
+                .search(SearchRequest {
+                    shard: shard_id.clone(),
+                    order: order.clone(),
+                    document: true,
+                    page_number: page,
+                    result_per_page: page_size,
+                    ..Default::default()
+                })
+                .await
+                .unwrap();
+
+            let document_response = response.get_ref().document.as_ref().unwrap();
+            fields.extend(document_response.results.iter().cloned().map(|r| r.field));
+
+            next_page = document_response.next_page;
+            page += 1;
+        }
+
+        fields
+    }
+
+    // --------------------------------------------------------------
+    // Test: sort by creation date in ascending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by modified date in ascending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by creation date in descending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    sorted_fields.reverse();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by modified date in descending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    sorted_fields.reverse();
+    assert_eq!(fields, sorted_fields);
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_node/tests/test_shards.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_node/tests/test_shards.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod common;
-
-use std::time::SystemTime;
-
-use common::{node_reader, node_writer, TestNodeReader, TestNodeWriter};
-use nucliadb_core::protos::prost_types::Timestamp;
-use nucliadb_core::protos::{
-    EmptyQuery, GetShardRequest, IndexMetadata, NewShardRequest, Resource, ResourceId, ShardId,
-};
-use tonic::Request;
-use uuid::Uuid;
-
-#[tokio::test]
-async fn test_create_shard() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-    let mut reader = node_reader().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    let response = reader
-        .get_shard(Request::new(GetShardRequest {
-            shard_id: Some(ShardId {
-                id: shard_id.to_owned(),
-            }),
-            ..Default::default()
-        }))
-        .await?;
-
-    assert_eq!(shard_id, &response.get_ref().shard_id);
-
-    Ok(())
-}
-
-#[tokio::test]
-async fn test_shard_metadata() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-    let mut reader = node_reader().await;
-
-    async fn create_shard_with_metadata(
-        writer: &mut TestNodeWriter,
-        kbid: String,
-    ) -> Result<String, Box<dyn std::error::Error>> {
-        let shard = writer
-            .new_shard(Request::new(NewShardRequest {
-                kbid,
-                ..Default::default()
-            }))
-            .await?
-            .into_inner();
-        Ok(shard.id)
-    }
-
-    async fn validate_shard_metadata(
-        reader: &mut TestNodeReader,
-        shard_id: String,
-        kbid: String,
-    ) -> Result<(), Box<dyn std::error::Error>> {
-        let shard = reader
-            .get_shard(Request::new(GetShardRequest {
-                shard_id: Some(ShardId { id: shard_id }),
-                ..Default::default()
-            }))
-            .await?
-            .into_inner();
-
-        assert!(shard.metadata.is_some());
-
-        let shard_metadata = shard.metadata.unwrap();
-        assert_eq!(shard_metadata.kbid, kbid);
-
-        Ok(())
-    }
-
-    const KB0: &str = "KB0";
-    const KB1: &str = "KB1";
-    const KB2: &str = "KB2";
-
-    // Used to validate correct creation
-    let shard_0 = create_shard_with_metadata(&mut writer, KB0.to_string()).await?;
-    // Used to check 1 is not overwritting 0
-    let shard_1 = create_shard_with_metadata(&mut writer, KB1.to_string()).await?;
-    // Used to validate correct creation when there are more shards
-    let shard_2 = create_shard_with_metadata(&mut writer, KB2.to_string()).await?;
-
-    validate_shard_metadata(&mut reader, shard_0, KB0.to_string()).await?;
-    validate_shard_metadata(&mut reader, shard_1, KB1.to_string()).await?;
-    validate_shard_metadata(&mut reader, shard_2, KB2.to_string()).await?;
-
-    Ok(())
-}
-
-#[tokio::test]
-async fn test_list_shards() -> Result<(), Box<dyn std::error::Error>> {
-    let mut writer = node_writer().await;
-
-    let request_ids = create_shards(&mut writer, 10).await;
-
-    // XXX We should have a better way to list shards independently of the cache
-    for shard_id in request_ids.iter() {
-        bring_shard_to_cache(&mut writer, shard_id.to_owned()).await;
-    }
-
-    let response = writer
-        .list_shards(Request::new(EmptyQuery {}))
-        .await
-        .expect("Error in list_shards request");
-
-    let response_ids: Vec<String> = response
-        .get_ref()
-        .ids
-        .iter()
-        .map(|s| s.id.clone())
-        .collect();
-
-    assert!(!request_ids.is_empty());
-    assert_eq!(request_ids.len(), response_ids.len());
-    assert!(request_ids
-        .iter()
-        .all(|item| { response_ids.contains(item) }));
-
-    Ok(())
-}
-
-#[tokio::test]
-async fn test_delete_shards() -> anyhow::Result<()> {
-    let mut writer = node_writer().await;
-
-    let response = writer
-        .list_shards(Request::new(EmptyQuery {}))
-        .await
-        .expect("Error in list_shards request");
-
-    assert_eq!(response.get_ref().ids.len(), 0);
-
-    let request_ids = create_shards(&mut writer, 10).await;
-    // XXX We should have a better way to list shards independently of the cache
-    for shard_id in request_ids.iter() {
-        bring_shard_to_cache(&mut writer, shard_id.to_owned()).await;
-    }
-
-    // XXX why are we doing this?
-    for id in request_ids.iter().cloned() {
-        _ = writer
-            .clean_and_upgrade_shard(Request::new(ShardId { id }))
-            .await
-            .expect("Error in new_shard request");
-    }
-
-    for (id, expected) in request_ids.iter().map(|v| (v.clone(), v.clone())) {
-        let response = writer
-            .delete_shard(Request::new(ShardId { id }))
-            .await
-            .expect("Error in delete_shard request");
-        let deleted_id = response.get_ref().id.clone();
-        assert_eq!(deleted_id, expected);
-    }
-
-    let response = writer
-        .list_shards(Request::new(EmptyQuery {}))
-        .await
-        .expect("Error in list_shards request");
-
-    assert_eq!(response.get_ref().ids.len(), 0);
-
-    Ok(())
-}
-
-async fn create_shards(writer: &mut TestNodeWriter, n: usize) -> Vec<String> {
-    let mut shard_ids = Vec::with_capacity(n);
-
-    for _ in 0..n {
-        let response = writer
-            .new_shard(Request::new(NewShardRequest::default()))
-            .await
-            .expect("Error in new_shard request");
-
-        shard_ids.push(response.get_ref().id.clone());
-    }
-
-    shard_ids
-}
-
-async fn bring_shard_to_cache(writer: &mut TestNodeWriter, shard_id: String) {
-    // XXX We use set_resource to ensure shard is being loaded in writer cache
-    let rid = Uuid::new_v4();
-    let now = SystemTime::now()
-        .duration_since(SystemTime::UNIX_EPOCH)
-        .unwrap();
-    let timestamp = Timestamp {
-        seconds: now.as_secs() as i64,
-        nanos: 0,
-    };
-    writer
-        .set_resource(Request::new(Resource {
-            shard_id: shard_id.clone(),
-            resource: Some(ResourceId {
-                shard_id: shard_id.clone(),
-                uuid: rid.to_string(),
-            }),
-            metadata: Some(IndexMetadata {
-                created: Some(timestamp.clone()),
-                modified: Some(timestamp),
-            }),
-            ..Default::default()
-        }))
-        .await
-        .expect("Error in set_resource request");
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod common;
+
+use std::time::SystemTime;
+
+use common::{node_reader, node_writer, TestNodeReader, TestNodeWriter};
+use nucliadb_core::protos::prost_types::Timestamp;
+use nucliadb_core::protos::{
+    EmptyQuery, GetShardRequest, IndexMetadata, NewShardRequest, Resource, ResourceId, ShardId,
+};
+use tonic::Request;
+use uuid::Uuid;
+
+#[tokio::test]
+async fn test_create_shard() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+    let mut reader = node_reader().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    let response = reader
+        .get_shard(Request::new(GetShardRequest {
+            shard_id: Some(ShardId {
+                id: shard_id.to_owned(),
+            }),
+            ..Default::default()
+        }))
+        .await?;
+
+    assert_eq!(shard_id, &response.get_ref().shard_id);
+
+    Ok(())
+}
+
+#[tokio::test]
+async fn test_shard_metadata() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+    let mut reader = node_reader().await;
+
+    async fn create_shard_with_metadata(
+        writer: &mut TestNodeWriter,
+        kbid: String,
+    ) -> Result<String, Box<dyn std::error::Error>> {
+        let shard = writer
+            .new_shard(Request::new(NewShardRequest {
+                kbid,
+                ..Default::default()
+            }))
+            .await?
+            .into_inner();
+        Ok(shard.id)
+    }
+
+    async fn validate_shard_metadata(
+        reader: &mut TestNodeReader,
+        shard_id: String,
+        kbid: String,
+    ) -> Result<(), Box<dyn std::error::Error>> {
+        let shard = reader
+            .get_shard(Request::new(GetShardRequest {
+                shard_id: Some(ShardId { id: shard_id }),
+                ..Default::default()
+            }))
+            .await?
+            .into_inner();
+
+        assert!(shard.metadata.is_some());
+
+        let shard_metadata = shard.metadata.unwrap();
+        assert_eq!(shard_metadata.kbid, kbid);
+
+        Ok(())
+    }
+
+    const KB0: &str = "KB0";
+    const KB1: &str = "KB1";
+    const KB2: &str = "KB2";
+
+    // Used to validate correct creation
+    let shard_0 = create_shard_with_metadata(&mut writer, KB0.to_string()).await?;
+    // Used to check 1 is not overwritting 0
+    let shard_1 = create_shard_with_metadata(&mut writer, KB1.to_string()).await?;
+    // Used to validate correct creation when there are more shards
+    let shard_2 = create_shard_with_metadata(&mut writer, KB2.to_string()).await?;
+
+    validate_shard_metadata(&mut reader, shard_0, KB0.to_string()).await?;
+    validate_shard_metadata(&mut reader, shard_1, KB1.to_string()).await?;
+    validate_shard_metadata(&mut reader, shard_2, KB2.to_string()).await?;
+
+    Ok(())
+}
+
+#[tokio::test]
+async fn test_list_shards() -> Result<(), Box<dyn std::error::Error>> {
+    let mut writer = node_writer().await;
+
+    let request_ids = create_shards(&mut writer, 5).await;
+
+    // XXX We should have a better way to list shards independently of the cache
+    for shard_id in request_ids.iter() {
+        bring_shard_to_cache(&mut writer, shard_id.to_owned()).await;
+    }
+
+    let response = writer
+        .list_shards(Request::new(EmptyQuery {}))
+        .await
+        .expect("Error in list_shards request");
+
+    let response_ids: Vec<String> = response
+        .get_ref()
+        .ids
+        .iter()
+        .map(|s| s.id.clone())
+        .collect();
+
+    assert!(!request_ids.is_empty());
+    assert_eq!(request_ids.len(), response_ids.len());
+    assert!(request_ids
+        .iter()
+        .all(|item| { response_ids.contains(item) }));
+
+    Ok(())
+}
+
+#[tokio::test]
+async fn test_delete_shards() -> anyhow::Result<()> {
+    let mut writer = node_writer().await;
+
+    let response = writer
+        .list_shards(Request::new(EmptyQuery {}))
+        .await
+        .expect("Error in list_shards request");
+
+    assert_eq!(response.get_ref().ids.len(), 0);
+
+    let request_ids = create_shards(&mut writer, 5).await;
+    // XXX We should have a better way to list shards independently of the cache
+    for shard_id in request_ids.iter() {
+        bring_shard_to_cache(&mut writer, shard_id.to_owned()).await;
+    }
+
+    // XXX why are we doing this?
+    for id in request_ids.iter().cloned() {
+        _ = writer
+            .clean_and_upgrade_shard(Request::new(ShardId { id }))
+            .await
+            .expect("Error in new_shard request");
+    }
+
+    for (id, expected) in request_ids.iter().map(|v| (v.clone(), v.clone())) {
+        let response = writer
+            .delete_shard(Request::new(ShardId { id }))
+            .await
+            .expect("Error in delete_shard request");
+        let deleted_id = response.get_ref().id.clone();
+        assert_eq!(deleted_id, expected);
+    }
+
+    let response = writer
+        .list_shards(Request::new(EmptyQuery {}))
+        .await
+        .expect("Error in list_shards request");
+
+    assert_eq!(response.get_ref().ids.len(), 0);
+
+    Ok(())
+}
+
+async fn create_shards(writer: &mut TestNodeWriter, n: usize) -> Vec<String> {
+    let mut shard_ids = Vec::with_capacity(n);
+
+    for _ in 0..n {
+        let response = writer
+            .new_shard(Request::new(NewShardRequest::default()))
+            .await
+            .expect("Error in new_shard request");
+
+        shard_ids.push(response.get_ref().id.clone());
+    }
+
+    shard_ids
+}
+
+async fn bring_shard_to_cache(writer: &mut TestNodeWriter, shard_id: String) {
+    // XXX We use set_resource to ensure shard is being loaded in writer cache
+    let rid = Uuid::new_v4();
+    let now = SystemTime::now()
+        .duration_since(SystemTime::UNIX_EPOCH)
+        .unwrap();
+    let timestamp = Timestamp {
+        seconds: now.as_secs() as i64,
+        nanos: 0,
+    };
+    writer
+        .set_resource(Request::new(Resource {
+            shard_id: shard_id.clone(),
+            resource: Some(ResourceId {
+                shard_id: shard_id.clone(),
+                uuid: rid.to_string(),
+            }),
+            metadata: Some(IndexMetadata {
+                created: Some(timestamp.clone()),
+                modified: Some(timestamp),
+            }),
+            ..Default::default()
+        }))
+        .await
+        .expect("Error in set_resource request");
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/Cargo.toml`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,293 +1,293 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-// Node:
-// -> Layers segment.
-// -> Indexing segment.
-// \Indexing segment:
-// Per layer in the hnsw in reverse order the start of
-// its adjacency list.
-// \Layer segment:
-// -> N: number of connexions.
-// -> List per connexion of T tuples (node, edge), where:
-// -> node: usize, in little endian.
-// -> edge: f32, in little endian.
-// Hnsw:
-// -> Node segment.
-// -> Indexing segment.
-// -> Entry point segment.
-// \Entry point segment:
-// -> Layer, usize in little endian.
-// -> Node, usize in little endian.
-// \Node segment (serialized as explained above).
-// \Indexing segment:
-// Per layer in the hnsw:
-// -> The byte where it ends.
-//
-//
-
-use std::collections::HashMap;
-use std::io;
-
-use super::ops_hnsw::{Hnsw, Layer};
-use super::ram_hnsw::{Edge, EntryPoint, RAMHnsw};
-use super::Address;
-use crate::data_types::usize_utils::*;
-
-const EDGE_LEN: usize = 4;
-const NODE_LEN: usize = USIZE_LEN;
-const CNX_LEN: usize = NODE_LEN + EDGE_LEN;
-
-fn f32_from_le_bytes(buf: &[u8]) -> f32 {
-    let mut temp = [0; 4];
-    temp.copy_from_slice(buf);
-    f32::from_le_bytes(temp)
-}
-
-pub struct DiskLayer<'a> {
-    hnsw: &'a [u8],
-    layer: usize,
-}
-
-impl<'a> Layer for &'a DiskLayer<'a> {
-    type EdgeIt = EdgeIter<'a>;
-    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
-        let node = DiskHnsw::get_node(self.hnsw, node);
-        DiskHnsw::get_out_edges(node, self.layer)
-    }
-}
-
-impl<'a> Layer for DiskLayer<'a> {
-    type EdgeIt = EdgeIter<'a>;
-    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
-        let node = DiskHnsw::get_node(self.hnsw, node);
-        DiskHnsw::get_out_edges(node, self.layer)
-    }
-}
-
-impl<'a> Hnsw for &'a [u8] {
-    type L = DiskLayer<'a>;
-    fn get_entry_point(&self) -> Option<EntryPoint> {
-        DiskHnsw::get_entry_point(self)
-    }
-    fn get_layer(&self, i: usize) -> Self::L {
-        DiskLayer {
-            hnsw: self,
-            layer: i,
-        }
-    }
-}
-
-pub struct EdgeIter<'a> {
-    crnt: usize,
-    buf: &'a [u8],
-}
-impl<'a> Iterator for EdgeIter<'a> {
-    type Item = (Address, Edge);
-    fn next(&mut self) -> Option<Self::Item> {
-        if self.buf.len() == self.crnt {
-            None
-        } else {
-            let buf = self.buf;
-            let mut crnt = self.crnt;
-            let node = usize_from_slice_le(&buf[crnt..(crnt + NODE_LEN)]);
-            crnt += USIZE_LEN;
-            let edge = f32_from_le_bytes(&buf[crnt..(crnt + EDGE_LEN)]);
-            crnt += EDGE_LEN;
-            self.crnt = crnt;
-            Some((Address(node), Edge { dist: edge }))
-        }
-    }
-}
-
-pub struct DiskHnsw;
-impl DiskHnsw {
-    fn serialize_node<W>(
-        mut buf: W,
-        offset: usize,
-        node: usize,
-        hnsw: &RAMHnsw,
-    ) -> io::Result<usize>
-    where
-        W: io::Write,
-    {
-        let node = Address(node);
-        let mut length = offset;
-        let mut indexing = HashMap::new();
-        for layer in 0..hnsw.no_layers() {
-            let no_edges = hnsw.get_layer(layer).no_out_edges(node);
-            indexing.insert(layer, length);
-            buf.write_all(&no_edges.to_le_bytes())?;
-            length += USIZE_LEN;
-            for (cnx, edge) in hnsw.get_layer(layer).get_out_edges(node) {
-                buf.write_all(&cnx.0.to_le_bytes())?;
-                buf.write_all(&edge.dist.to_le_bytes())?;
-                length += CNX_LEN;
-            }
-        }
-        for layer in (0..hnsw.no_layers()).rev() {
-            buf.write_all(&indexing[&layer].to_le_bytes())?;
-        }
-        length += hnsw.no_layers() * USIZE_LEN;
-        buf.flush()?;
-        Ok(length)
-    }
-
-    // node must be serialized using DiskNode, may have trailing bytes at the start.
-    fn get_out_edges(node: &[u8], layer: usize) -> EdgeIter {
-        // layer + 1 since the layers are stored in reverse order.
-        // [l3, l2, l1, l0, end] Since we have the position of end, the layer i is
-        // i + 1 positions to its left.
-        let pos = node.len() - ((layer + 1) * USIZE_LEN);
-        let cnx_start = usize_from_slice_le(&node[pos..(pos + USIZE_LEN)]);
-        let no_cnx = usize_from_slice_le(&node[cnx_start..(cnx_start + USIZE_LEN)]);
-        let cnx_start = cnx_start + USIZE_LEN;
-        let cnx_end = cnx_start + (no_cnx * CNX_LEN);
-        EdgeIter {
-            crnt: 0,
-            buf: &node[cnx_start..cnx_end],
-        }
-    }
-    pub fn serialize_into<W: io::Write>(
-        mut buf: W,
-        no_nodes: usize,
-        hnsw: RAMHnsw,
-    ) -> io::Result<()> {
-        if let Some(entry_point) = hnsw.entry_point {
-            let mut length = 0;
-            let mut nodes_end = vec![];
-            for node in 0..no_nodes {
-                length = DiskHnsw::serialize_node(&mut buf, length, node, &hnsw)?;
-                nodes_end.push(length)
-            }
-            for ends_at in nodes_end.into_iter().rev() {
-                buf.write_all(&ends_at.to_le_bytes())?;
-                length += USIZE_LEN;
-            }
-            let EntryPoint { node, layer } = entry_point;
-            buf.write_all(&layer.to_le_bytes())?;
-            buf.write_all(&node.0.to_le_bytes())?;
-            let _length = length + 2 * USIZE_LEN;
-            buf.flush()?;
-        }
-        Ok(())
-    }
-    // hnsw must be serialized using DiskHnsw, may have trailing bytes at the start.
-    pub fn get_entry_point(hnsw: &[u8]) -> Option<EntryPoint> {
-        if !hnsw.is_empty() {
-            let node_start = hnsw.len() - USIZE_LEN;
-            let layer_start = node_start - USIZE_LEN;
-            let node = usize_from_slice_le(&hnsw[node_start..(node_start + USIZE_LEN)]);
-            let layer = usize_from_slice_le(&hnsw[layer_start..(layer_start + USIZE_LEN)]);
-            Some(EntryPoint {
-                node: Address(node),
-                layer,
-            })
-        } else {
-            None
-        }
-    }
-    // hnsw must be serialized using MHnsw, may have trailing bytes at the start.
-    // The returned node will have trailing bytes at the start.
-    pub fn get_node(hnsw: &[u8], node: usize) -> &[u8] {
-        let indexing_end = hnsw.len() - (2 * USIZE_LEN);
-        // node + 1 since the layers are stored in reverse order.
-        // [n3, n2, n1, n0, end] Since we have the position of end, the node i is
-        // i + 1 positions to its left.
-        let pos = indexing_end - ((node + 1) * USIZE_LEN);
-        let node_end = usize_from_slice_le(&hnsw[pos..(pos + USIZE_LEN)]);
-        &hnsw[..node_end]
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::data_point::ram_hnsw::RAMLayer;
-    fn layer_check<L: Layer>(buf: L, no_nodes: usize, cnx: &[Vec<(Address, Edge)>]) {
-        let no_cnx = vec![];
-        for i in 0..no_nodes {
-            let expected = cnx.get(i).unwrap_or(&no_cnx);
-            let got: Vec<_> = buf.get_out_edges(Address(i)).collect();
-            assert_eq!(expected, &got);
-        }
-    }
-    #[test]
-    fn empty_hnsw() {
-        let hnsw = RAMHnsw::new();
-        let mut buf = vec![];
-        DiskHnsw::serialize_into(&mut buf, 0, hnsw).unwrap();
-        let ep = DiskHnsw::get_entry_point(&buf);
-        assert_eq!(ep, None);
-    }
-
-    #[test]
-    fn hnsw_test() {
-        let no_nodes = 3;
-        let cnx0 = vec![
-            vec![(Address(1), Edge { dist: 1.0 })],
-            vec![(Address(2), Edge { dist: 2.0 })],
-            vec![(Address(3), Edge { dist: 3.0 })],
-        ];
-        let layer0 = RAMLayer {
-            out: cnx0
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let cnx1 = vec![
-            vec![(Address(1), Edge { dist: 4.0 })],
-            vec![(Address(2), Edge { dist: 5.0 })],
-        ];
-        let layer1 = RAMLayer {
-            out: cnx1
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let cnx2 = vec![vec![(Address(1), Edge { dist: 6.0 })]];
-        let layer2 = RAMLayer {
-            out: cnx2
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let entry_point = EntryPoint {
-            node: Address(0),
-            layer: 2,
-        };
-        let mut hnsw = RAMHnsw::new();
-        hnsw.entry_point = Some(entry_point);
-        hnsw.layers = vec![layer0, layer1, layer2];
-        let mut buf = vec![];
-        DiskHnsw::serialize_into(&mut buf, no_nodes, hnsw).unwrap();
-        let ep = DiskHnsw::get_entry_point(&buf).unwrap();
-        assert_eq!(ep, entry_point);
-        let layer0 = buf.as_slice().get_layer(0);
-        layer_check(layer0, no_nodes, &cnx0);
-        let layer1 = buf.as_slice().get_layer(1);
-        layer_check(layer1, no_nodes, &cnx1);
-        let layer2 = buf.as_slice().get_layer(2);
-        layer_check(layer2, no_nodes, &cnx2);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+// Node:
+// -> Layers segment.
+// -> Indexing segment.
+// \Indexing segment:
+// Per layer in the hnsw in reverse order the start of
+// its adjacency list.
+// \Layer segment:
+// -> N: number of connexions.
+// -> List per connexion of T tuples (node, edge), where:
+// -> node: usize, in little endian.
+// -> edge: f32, in little endian.
+// Hnsw:
+// -> Node segment.
+// -> Indexing segment.
+// -> Entry point segment.
+// \Entry point segment:
+// -> Layer, usize in little endian.
+// -> Node, usize in little endian.
+// \Node segment (serialized as explained above).
+// \Indexing segment:
+// Per layer in the hnsw:
+// -> The byte where it ends.
+//
+//
+
+use std::collections::HashMap;
+use std::io;
+
+use super::ops_hnsw::{Hnsw, Layer};
+use super::ram_hnsw::{Edge, EntryPoint, RAMHnsw};
+use super::Address;
+use crate::data_types::usize_utils::*;
+
+const EDGE_LEN: usize = 4;
+const NODE_LEN: usize = USIZE_LEN;
+const CNX_LEN: usize = NODE_LEN + EDGE_LEN;
+
+fn f32_from_le_bytes(buf: &[u8]) -> f32 {
+    let mut temp = [0; 4];
+    temp.copy_from_slice(buf);
+    f32::from_le_bytes(temp)
+}
+
+pub struct DiskLayer<'a> {
+    hnsw: &'a [u8],
+    layer: usize,
+}
+
+impl<'a> Layer for &'a DiskLayer<'a> {
+    type EdgeIt = EdgeIter<'a>;
+    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
+        let node = DiskHnsw::get_node(self.hnsw, node);
+        DiskHnsw::get_out_edges(node, self.layer)
+    }
+}
+
+impl<'a> Layer for DiskLayer<'a> {
+    type EdgeIt = EdgeIter<'a>;
+    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
+        let node = DiskHnsw::get_node(self.hnsw, node);
+        DiskHnsw::get_out_edges(node, self.layer)
+    }
+}
+
+impl<'a> Hnsw for &'a [u8] {
+    type L = DiskLayer<'a>;
+    fn get_entry_point(&self) -> Option<EntryPoint> {
+        DiskHnsw::get_entry_point(self)
+    }
+    fn get_layer(&self, i: usize) -> Self::L {
+        DiskLayer {
+            hnsw: self,
+            layer: i,
+        }
+    }
+}
+
+pub struct EdgeIter<'a> {
+    crnt: usize,
+    buf: &'a [u8],
+}
+impl<'a> Iterator for EdgeIter<'a> {
+    type Item = (Address, Edge);
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.buf.len() == self.crnt {
+            None
+        } else {
+            let buf = self.buf;
+            let mut crnt = self.crnt;
+            let node = usize_from_slice_le(&buf[crnt..(crnt + NODE_LEN)]);
+            crnt += USIZE_LEN;
+            let edge = f32_from_le_bytes(&buf[crnt..(crnt + EDGE_LEN)]);
+            crnt += EDGE_LEN;
+            self.crnt = crnt;
+            Some((Address(node), Edge { dist: edge }))
+        }
+    }
+}
+
+pub struct DiskHnsw;
+impl DiskHnsw {
+    fn serialize_node<W>(
+        mut buf: W,
+        offset: usize,
+        node: usize,
+        hnsw: &RAMHnsw,
+    ) -> io::Result<usize>
+    where
+        W: io::Write,
+    {
+        let node = Address(node);
+        let mut length = offset;
+        let mut indexing = HashMap::new();
+        for layer in 0..hnsw.no_layers() {
+            let no_edges = hnsw.get_layer(layer).no_out_edges(node);
+            indexing.insert(layer, length);
+            buf.write_all(&no_edges.to_le_bytes())?;
+            length += USIZE_LEN;
+            for (cnx, edge) in hnsw.get_layer(layer).get_out_edges(node) {
+                buf.write_all(&cnx.0.to_le_bytes())?;
+                buf.write_all(&edge.dist.to_le_bytes())?;
+                length += CNX_LEN;
+            }
+        }
+        for layer in (0..hnsw.no_layers()).rev() {
+            buf.write_all(&indexing[&layer].to_le_bytes())?;
+        }
+        length += hnsw.no_layers() * USIZE_LEN;
+        buf.flush()?;
+        Ok(length)
+    }
+
+    // node must be serialized using DiskNode, may have trailing bytes at the start.
+    fn get_out_edges(node: &[u8], layer: usize) -> EdgeIter {
+        // layer + 1 since the layers are stored in reverse order.
+        // [l3, l2, l1, l0, end] Since we have the position of end, the layer i is
+        // i + 1 positions to its left.
+        let pos = node.len() - ((layer + 1) * USIZE_LEN);
+        let cnx_start = usize_from_slice_le(&node[pos..(pos + USIZE_LEN)]);
+        let no_cnx = usize_from_slice_le(&node[cnx_start..(cnx_start + USIZE_LEN)]);
+        let cnx_start = cnx_start + USIZE_LEN;
+        let cnx_end = cnx_start + (no_cnx * CNX_LEN);
+        EdgeIter {
+            crnt: 0,
+            buf: &node[cnx_start..cnx_end],
+        }
+    }
+    pub fn serialize_into<W: io::Write>(
+        mut buf: W,
+        no_nodes: usize,
+        hnsw: RAMHnsw,
+    ) -> io::Result<()> {
+        if let Some(entry_point) = hnsw.entry_point {
+            let mut length = 0;
+            let mut nodes_end = vec![];
+            for node in 0..no_nodes {
+                length = DiskHnsw::serialize_node(&mut buf, length, node, &hnsw)?;
+                nodes_end.push(length)
+            }
+            for ends_at in nodes_end.into_iter().rev() {
+                buf.write_all(&ends_at.to_le_bytes())?;
+                length += USIZE_LEN;
+            }
+            let EntryPoint { node, layer } = entry_point;
+            buf.write_all(&layer.to_le_bytes())?;
+            buf.write_all(&node.0.to_le_bytes())?;
+            let _length = length + 2 * USIZE_LEN;
+            buf.flush()?;
+        }
+        Ok(())
+    }
+    // hnsw must be serialized using DiskHnsw, may have trailing bytes at the start.
+    pub fn get_entry_point(hnsw: &[u8]) -> Option<EntryPoint> {
+        if !hnsw.is_empty() {
+            let node_start = hnsw.len() - USIZE_LEN;
+            let layer_start = node_start - USIZE_LEN;
+            let node = usize_from_slice_le(&hnsw[node_start..(node_start + USIZE_LEN)]);
+            let layer = usize_from_slice_le(&hnsw[layer_start..(layer_start + USIZE_LEN)]);
+            Some(EntryPoint {
+                node: Address(node),
+                layer,
+            })
+        } else {
+            None
+        }
+    }
+    // hnsw must be serialized using MHnsw, may have trailing bytes at the start.
+    // The returned node will have trailing bytes at the start.
+    pub fn get_node(hnsw: &[u8], node: usize) -> &[u8] {
+        let indexing_end = hnsw.len() - (2 * USIZE_LEN);
+        // node + 1 since the layers are stored in reverse order.
+        // [n3, n2, n1, n0, end] Since we have the position of end, the node i is
+        // i + 1 positions to its left.
+        let pos = indexing_end - ((node + 1) * USIZE_LEN);
+        let node_end = usize_from_slice_le(&hnsw[pos..(pos + USIZE_LEN)]);
+        &hnsw[..node_end]
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::data_point::ram_hnsw::RAMLayer;
+    fn layer_check<L: Layer>(buf: L, no_nodes: usize, cnx: &[Vec<(Address, Edge)>]) {
+        let no_cnx = vec![];
+        for i in 0..no_nodes {
+            let expected = cnx.get(i).unwrap_or(&no_cnx);
+            let got: Vec<_> = buf.get_out_edges(Address(i)).collect();
+            assert_eq!(expected, &got);
+        }
+    }
+    #[test]
+    fn empty_hnsw() {
+        let hnsw = RAMHnsw::new();
+        let mut buf = vec![];
+        DiskHnsw::serialize_into(&mut buf, 0, hnsw).unwrap();
+        let ep = DiskHnsw::get_entry_point(&buf);
+        assert_eq!(ep, None);
+    }
+
+    #[test]
+    fn hnsw_test() {
+        let no_nodes = 3;
+        let cnx0 = vec![
+            vec![(Address(1), Edge { dist: 1.0 })],
+            vec![(Address(2), Edge { dist: 2.0 })],
+            vec![(Address(3), Edge { dist: 3.0 })],
+        ];
+        let layer0 = RAMLayer {
+            out: cnx0
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let cnx1 = vec![
+            vec![(Address(1), Edge { dist: 4.0 })],
+            vec![(Address(2), Edge { dist: 5.0 })],
+        ];
+        let layer1 = RAMLayer {
+            out: cnx1
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let cnx2 = vec![vec![(Address(1), Edge { dist: 6.0 })]];
+        let layer2 = RAMLayer {
+            out: cnx2
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let entry_point = EntryPoint {
+            node: Address(0),
+            layer: 2,
+        };
+        let mut hnsw = RAMHnsw::new();
+        hnsw.entry_point = Some(entry_point);
+        hnsw.layers = vec![layer0, layer1, layer2];
+        let mut buf = vec![];
+        DiskHnsw::serialize_into(&mut buf, no_nodes, hnsw).unwrap();
+        let ep = DiskHnsw::get_entry_point(&buf).unwrap();
+        assert_eq!(ep, entry_point);
+        let layer0 = buf.as_slice().get_layer(0);
+        layer_check(layer0, no_nodes, &cnx0);
+        let layer1 = buf.as_slice().get_layer(1);
+        layer_check(layer1, no_nodes, &cnx1);
+        let layer2 = buf.as_slice().get_layer(2);
+        layer_check(layer2, no_nodes, &cnx2);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/mod.rs`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,559 +1,559 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod disk_hnsw;
-pub mod node;
-pub mod ops_hnsw;
-pub mod ram_hnsw;
-#[cfg(test)]
-mod tests;
-
-use std::time::SystemTime;
-use std::{fs, io, path};
-
-use disk_hnsw::DiskHnsw;
-use io::{BufWriter, Write};
-use key_value::Slot;
-use memmap2::Mmap;
-use node::Node;
-pub use ops_hnsw::DataRetriever;
-use ops_hnsw::HnswOps;
-use ram_hnsw::RAMHnsw;
-use serde::{Deserialize, Serialize};
-pub use uuid::Uuid as DpId;
-
-use crate::data_types::{key_value, trie, trie_ram, vector, DeleteLog};
-use crate::formula::Formula;
-use crate::VectorR;
-
-mod file_names {
-    pub const NODES: &str = "nodes.kv";
-    pub const HNSW: &str = "index.hnsw";
-    pub const JOURNAL: &str = "journal.json";
-}
-
-pub struct NoDLog;
-impl DeleteLog for NoDLog {
-    fn is_deleted(&self, _: &[u8]) -> bool {
-        false
-    }
-}
-
-#[derive(Default, Debug, Clone, Copy, Serialize, Deserialize)]
-pub enum Similarity {
-    Dot,
-    #[default]
-    Cosine,
-}
-impl Similarity {
-    pub fn compute(&self, x: &[u8], y: &[u8]) -> f32 {
-        match self {
-            Similarity::Cosine => vector::cosine_similarity(x, y),
-            Similarity::Dot => vector::dot_similarity(x, y),
-        }
-    }
-}
-
-#[derive(Clone, Copy, Serialize, Deserialize, Debug)]
-pub struct Journal {
-    uid: DpId,
-    nodes: usize,
-    ctime: SystemTime,
-}
-impl Journal {
-    pub fn id(&self) -> DpId {
-        self.uid
-    }
-    pub fn no_nodes(&self) -> usize {
-        self.nodes
-    }
-    pub fn time(&self) -> SystemTime {
-        self.ctime
-    }
-    pub fn update_time(&mut self, time: SystemTime) {
-        self.ctime = time;
-    }
-}
-
-#[derive(
-    Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize, Default,
-)]
-pub struct Address(usize);
-impl Address {
-    #[cfg(test)]
-    pub const fn dummy() -> Address {
-        Address(0)
-    }
-}
-
-pub struct Retriever<'a, Dlog> {
-    similarity: Similarity,
-    no_nodes: usize,
-    temp: &'a [u8],
-    nodes: &'a Mmap,
-    delete_log: &'a Dlog,
-    min_score: f32,
-}
-impl<'a, Dlog: DeleteLog> Retriever<'a, Dlog> {
-    pub fn new(
-        temp: &'a [u8],
-        nodes: &'a Mmap,
-        delete_log: &'a Dlog,
-        similarity: Similarity,
-        min_score: f32,
-    ) -> Retriever<'a, Dlog> {
-        Retriever {
-            temp,
-            nodes,
-            delete_log,
-            similarity,
-            no_nodes: key_value::get_no_elems(nodes),
-            min_score,
-        }
-    }
-    fn find_node(&self, Address(x): Address) -> &[u8] {
-        if x == self.no_nodes {
-            self.temp
-        } else {
-            key_value::get_value(Node, self.nodes, x)
-        }
-    }
-}
-
-impl<'a, Dlog: DeleteLog> DataRetriever for Retriever<'a, Dlog> {
-    fn get_key(&self, x @ Address(addr): Address) -> &[u8] {
-        if addr == self.no_nodes {
-            &[]
-        } else {
-            let x = self.find_node(x);
-            Node::key(x)
-        }
-    }
-
-    fn get_vector(&self, x @ Address(addr): Address) -> &[u8] {
-        if addr == self.no_nodes {
-            self.temp
-        } else {
-            let x = self.find_node(x);
-            Node::vector(x)
-        }
-    }
-    fn is_deleted(&self, x @ Address(addr): Address) -> bool {
-        if addr == self.no_nodes {
-            false
-        } else {
-            let x = self.find_node(x);
-            let key = Node::key(x);
-            self.delete_log.is_deleted(key)
-        }
-    }
-    fn has_label(&self, Address(x): Address, label: &[u8]) -> bool {
-        if x == self.no_nodes {
-            false
-        } else {
-            let x = key_value::get_value(Node, self.nodes, x);
-            Node::has_label(x, label)
-        }
-    }
-    fn similarity(&self, x @ Address(a0): Address, y @ Address(a1): Address) -> f32 {
-        if a0 == self.no_nodes {
-            let y = self.find_node(y);
-            let y = Node::vector(y);
-            self.similarity.compute(self.temp, y)
-        } else if a1 == self.no_nodes {
-            let x = self.find_node(x);
-            let x = Node::vector(x);
-            self.similarity.compute(self.temp, x)
-        } else {
-            let x = self.find_node(x);
-            let y = self.find_node(y);
-            let x = Node::vector(x);
-            let y = Node::vector(y);
-            self.similarity.compute(x, y)
-        }
-    }
-
-    fn min_score(&self) -> f32 {
-        self.min_score
-    }
-}
-
-#[derive(Clone, Debug)]
-pub struct LabelDictionary(Vec<u8>);
-impl Default for LabelDictionary {
-    fn default() -> Self {
-        LabelDictionary::new(vec![])
-    }
-}
-impl LabelDictionary {
-    pub fn new(mut labels: Vec<String>) -> LabelDictionary {
-        labels.sort();
-        let ram_trie = trie_ram::create_trie(&labels);
-        LabelDictionary(trie::serialize(ram_trie))
-    }
-}
-#[derive(Clone, Debug)]
-pub struct Elem {
-    pub key: Vec<u8>,
-    pub vector: Vec<u8>,
-    pub metadata: Option<Vec<u8>>,
-    pub labels: LabelDictionary,
-}
-impl Elem {
-    pub fn new(
-        key: String,
-        vector: Vec<f32>,
-        labels: LabelDictionary,
-        metadata: Option<Vec<u8>>,
-    ) -> Elem {
-        Elem {
-            labels,
-            metadata,
-            key: key.as_bytes().to_vec(),
-            vector: vector::encode_vector(&vector),
-        }
-    }
-}
-
-impl key_value::KVElem for Elem {
-    fn serialized_len(&self) -> usize {
-        Node::serialized_len(
-            &self.key,
-            &self.vector,
-            &self.labels.0,
-            self.metadata.as_ref(),
-        )
-    }
-    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()> {
-        Node::serialize_into(
-            w,
-            self.key,
-            self.vector,
-            self.labels.0,
-            self.metadata.as_ref(),
-        )
-    }
-}
-
-#[derive(Debug, Clone)]
-pub struct Neighbour {
-    score: f32,
-    node: Vec<u8>,
-}
-impl Eq for Neighbour {}
-impl std::hash::Hash for Neighbour {
-    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
-        self.node.hash(state);
-    }
-}
-impl Ord for Neighbour {
-    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
-        self.node.cmp(&other.node)
-    }
-}
-impl PartialOrd for Neighbour {
-    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
-        self.node.partial_cmp(&other.node)
-    }
-}
-impl PartialEq for Neighbour {
-    fn eq(&self, other: &Self) -> bool {
-        self.node == other.node
-    }
-}
-
-impl Neighbour {
-    #[cfg(test)]
-    pub fn dummy_neighbour(node: &[u8], score: f32) -> Neighbour {
-        Neighbour {
-            score,
-            node: node.to_vec(),
-        }
-    }
-    fn new(Address(addr): Address, data: &[u8], score: f32) -> Neighbour {
-        let node = key_value::get_value(Node, data, addr);
-        let (exact, _) = Node.read_exact(node);
-        Neighbour {
-            score,
-            node: exact.to_vec(),
-        }
-    }
-    pub fn score(&self) -> f32 {
-        self.score
-    }
-    pub fn id(&self) -> &[u8] {
-        Node.get_key(&self.node)
-    }
-    pub fn labels(&self) -> Vec<String> {
-        Node::labels(&self.node)
-    }
-    pub fn metadata(&self) -> Option<&[u8]> {
-        let metadata = Node::metadata(&self.node);
-        if metadata.is_empty() {
-            None
-        } else {
-            Some(metadata)
-        }
-    }
-}
-
-pub struct DataPoint {
-    journal: Journal,
-    nodes: Mmap,
-    index: Mmap,
-}
-
-impl AsRef<DataPoint> for DataPoint {
-    fn as_ref(&self) -> &DataPoint {
-        self
-    }
-}
-
-impl DataPoint {
-    pub fn stored_len(&self) -> Option<u64> {
-        if key_value::get_no_elems(&self.nodes) == 0 {
-            return None;
-        }
-        let node = key_value::get_value(Node, &self.nodes, 0);
-        Some(vector::vector_len(Node::vector(node)))
-    }
-    pub fn get_id(&self) -> DpId {
-        self.journal.uid
-    }
-    pub fn meta(&self) -> Journal {
-        self.journal
-    }
-    pub fn get_keys<Dlog: DeleteLog>(&self, delete_log: &Dlog) -> Vec<String> {
-        key_value::get_keys(Node, &self.nodes)
-            .filter(|k| !delete_log.is_deleted(k))
-            .map(String::from_utf8_lossy)
-            .map(|s| s.to_string())
-            .collect()
-    }
-
-    #[allow(clippy::too_many_arguments)]
-    pub fn search<Dlog: DeleteLog>(
-        &self,
-        delete_log: &Dlog,
-        query: &[f32],
-        filter: &Formula,
-        with_duplicates: bool,
-        results: usize,
-        similarity: Similarity,
-        min_score: f32,
-    ) -> impl Iterator<Item = Neighbour> + '_ {
-        let encoded_query = vector::encode_vector(query);
-        let tracker = Retriever::new(
-            &encoded_query,
-            &self.nodes,
-            delete_log,
-            similarity,
-            min_score,
-        );
-        let ops = HnswOps::new(&tracker);
-        let neighbours = ops.search(
-            Address(self.journal.nodes),
-            self.index.as_ref(),
-            results,
-            filter,
-            with_duplicates,
-        );
-        neighbours
-            .into_iter()
-            .map(|(address, dist)| (Neighbour::new(address, &self.nodes, dist)))
-            .take(results)
-    }
-    pub fn merge<Dlog>(
-        dir: &path::Path,
-        operants: &[(Dlog, DpId)],
-        similarity: Similarity,
-    ) -> VectorR<DataPoint>
-    where
-        Dlog: DeleteLog,
-    {
-        let uid = DpId::new_v4().to_string();
-        let id = dir.join(&uid);
-        fs::create_dir(&id)?;
-        let mut nodes = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::NODES))?;
-        let mut journalf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let mut hnswf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::HNSW))?;
-        let operants = operants
-            .iter()
-            .map(|(dlog, dp_id)| DataPoint::open(dir, *dp_id).map(|v| (dlog, v)))
-            .collect::<VectorR<Vec<_>>>()?;
-        let node_producers = operants
-            .iter()
-            .map(|dp| ((dp.0, Node), dp.1.nodes.as_ref()));
-        {
-            let mut node_buffer = BufWriter::new(&mut nodes);
-            key_value::merge(&mut node_buffer, node_producers.collect())?;
-            node_buffer.flush()?;
-        }
-
-        let nodes = unsafe { Mmap::map(&nodes)? };
-        let no_nodes = key_value::get_no_elems(&nodes);
-        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity, -1.0);
-        let mut ops = HnswOps::new(&tracker);
-        let mut index = RAMHnsw::new();
-        for id in 0..no_nodes {
-            ops.insert(Address(id), &mut index)
-        }
-
-        {
-            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
-            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
-            hnswf_buffer.flush()?;
-        }
-
-        let index = unsafe { Mmap::map(&hnswf)? };
-
-        let journal = Journal {
-            nodes: no_nodes,
-            uid: DpId::parse_str(&uid).unwrap(),
-            ctime: SystemTime::now(),
-        };
-
-        {
-            let mut journalf_buffer = BufWriter::new(&mut journalf);
-            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
-            journalf_buffer.flush()?;
-        }
-
-        // Mark it as a Datapoint in progress, since it needs to be commited.
-
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-    pub fn delete(dir: &path::Path, uid: DpId) -> VectorR<()> {
-        let uid = uid.to_string();
-        let id = dir.join(uid);
-        fs::remove_dir_all(id)?;
-        Ok(())
-    }
-    pub fn open(dir: &path::Path, uid: DpId) -> VectorR<DataPoint> {
-        let uid = uid.to_string();
-        let id = dir.join(uid);
-        let nodes = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::NODES))?;
-        let journal = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let hnswf = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::HNSW))?;
-
-        let nodes = unsafe { Mmap::map(&nodes)? };
-        let index = unsafe { Mmap::map(&hnswf)? };
-        let journal: Journal = serde_json::from_reader(journal)?;
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-    pub fn new(
-        dir: &path::Path,
-        mut elems: Vec<Elem>,
-        time: Option<SystemTime>,
-        similarity: Similarity,
-    ) -> VectorR<DataPoint> {
-        let uid = DpId::new_v4().to_string();
-        let id = dir.join(&uid);
-        fs::create_dir(&id)?;
-        let mut nodesf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::NODES))?;
-        let mut journalf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let mut hnswf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::HNSW))?;
-
-        elems.sort_by(|a, b| a.key.cmp(&b.key));
-        elems.dedup_by(|a, b| a.key.cmp(&b.key).is_eq());
-        {
-            // Serializing nodes on disk
-            // Nodes are stored on disk and mmaped.
-            let mut nodesf_buffer = BufWriter::new(&mut nodesf);
-            key_value::create_key_value(&mut nodesf_buffer, elems)?;
-            nodesf_buffer.flush()?;
-        }
-        let nodes = unsafe { Mmap::map(&nodesf)? };
-        let no_nodes = key_value::get_no_elems(&nodes);
-
-        // Creating the HNSW using the mmaped nodes
-        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity, -1.0);
-        let mut ops = HnswOps::new(&tracker);
-        let mut index = RAMHnsw::new();
-        for id in 0..no_nodes {
-            ops.insert(Address(id), &mut index)
-        }
-
-        {
-            // The HNSW is on RAM
-            // Serializing the HNSW into disk
-            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
-            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
-            hnswf_buffer.flush()?;
-        }
-        let index = unsafe { Mmap::map(&hnswf)? };
-
-        let journal = Journal {
-            nodes: no_nodes,
-            uid: DpId::parse_str(&uid).unwrap(),
-            ctime: time.unwrap_or_else(SystemTime::now),
-        };
-        {
-            // Saving the journal
-            let mut journalf_buffer = BufWriter::new(&mut journalf);
-            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
-            journalf_buffer.flush()?;
-        }
-
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod disk_hnsw;
+pub mod node;
+pub mod ops_hnsw;
+pub mod ram_hnsw;
+#[cfg(test)]
+mod tests;
+
+use std::time::SystemTime;
+use std::{fs, io, path};
+
+use disk_hnsw::DiskHnsw;
+use io::{BufWriter, Write};
+use key_value::Slot;
+use memmap2::Mmap;
+use node::Node;
+pub use ops_hnsw::DataRetriever;
+use ops_hnsw::HnswOps;
+use ram_hnsw::RAMHnsw;
+use serde::{Deserialize, Serialize};
+pub use uuid::Uuid as DpId;
+
+use crate::data_types::{key_value, trie, trie_ram, vector, DeleteLog};
+use crate::formula::Formula;
+use crate::VectorR;
+
+mod file_names {
+    pub const NODES: &str = "nodes.kv";
+    pub const HNSW: &str = "index.hnsw";
+    pub const JOURNAL: &str = "journal.json";
+}
+
+pub struct NoDLog;
+impl DeleteLog for NoDLog {
+    fn is_deleted(&self, _: &[u8]) -> bool {
+        false
+    }
+}
+
+#[derive(Default, Debug, Clone, Copy, Serialize, Deserialize)]
+pub enum Similarity {
+    Dot,
+    #[default]
+    Cosine,
+}
+impl Similarity {
+    pub fn compute(&self, x: &[u8], y: &[u8]) -> f32 {
+        match self {
+            Similarity::Cosine => vector::cosine_similarity(x, y),
+            Similarity::Dot => vector::dot_similarity(x, y),
+        }
+    }
+}
+
+#[derive(Clone, Copy, Serialize, Deserialize, Debug)]
+pub struct Journal {
+    uid: DpId,
+    nodes: usize,
+    ctime: SystemTime,
+}
+impl Journal {
+    pub fn id(&self) -> DpId {
+        self.uid
+    }
+    pub fn no_nodes(&self) -> usize {
+        self.nodes
+    }
+    pub fn time(&self) -> SystemTime {
+        self.ctime
+    }
+    pub fn update_time(&mut self, time: SystemTime) {
+        self.ctime = time;
+    }
+}
+
+#[derive(
+    Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize, Default,
+)]
+pub struct Address(usize);
+impl Address {
+    #[cfg(test)]
+    pub const fn dummy() -> Address {
+        Address(0)
+    }
+}
+
+pub struct Retriever<'a, Dlog> {
+    similarity: Similarity,
+    no_nodes: usize,
+    temp: &'a [u8],
+    nodes: &'a Mmap,
+    delete_log: &'a Dlog,
+    min_score: f32,
+}
+impl<'a, Dlog: DeleteLog> Retriever<'a, Dlog> {
+    pub fn new(
+        temp: &'a [u8],
+        nodes: &'a Mmap,
+        delete_log: &'a Dlog,
+        similarity: Similarity,
+        min_score: f32,
+    ) -> Retriever<'a, Dlog> {
+        Retriever {
+            temp,
+            nodes,
+            delete_log,
+            similarity,
+            no_nodes: key_value::get_no_elems(nodes),
+            min_score,
+        }
+    }
+    fn find_node(&self, Address(x): Address) -> &[u8] {
+        if x == self.no_nodes {
+            self.temp
+        } else {
+            key_value::get_value(Node, self.nodes, x)
+        }
+    }
+}
+
+impl<'a, Dlog: DeleteLog> DataRetriever for Retriever<'a, Dlog> {
+    fn get_key(&self, x @ Address(addr): Address) -> &[u8] {
+        if addr == self.no_nodes {
+            &[]
+        } else {
+            let x = self.find_node(x);
+            Node::key(x)
+        }
+    }
+
+    fn get_vector(&self, x @ Address(addr): Address) -> &[u8] {
+        if addr == self.no_nodes {
+            self.temp
+        } else {
+            let x = self.find_node(x);
+            Node::vector(x)
+        }
+    }
+    fn is_deleted(&self, x @ Address(addr): Address) -> bool {
+        if addr == self.no_nodes {
+            false
+        } else {
+            let x = self.find_node(x);
+            let key = Node::key(x);
+            self.delete_log.is_deleted(key)
+        }
+    }
+    fn has_label(&self, Address(x): Address, label: &[u8]) -> bool {
+        if x == self.no_nodes {
+            false
+        } else {
+            let x = key_value::get_value(Node, self.nodes, x);
+            Node::has_label(x, label)
+        }
+    }
+    fn similarity(&self, x @ Address(a0): Address, y @ Address(a1): Address) -> f32 {
+        if a0 == self.no_nodes {
+            let y = self.find_node(y);
+            let y = Node::vector(y);
+            self.similarity.compute(self.temp, y)
+        } else if a1 == self.no_nodes {
+            let x = self.find_node(x);
+            let x = Node::vector(x);
+            self.similarity.compute(self.temp, x)
+        } else {
+            let x = self.find_node(x);
+            let y = self.find_node(y);
+            let x = Node::vector(x);
+            let y = Node::vector(y);
+            self.similarity.compute(x, y)
+        }
+    }
+
+    fn min_score(&self) -> f32 {
+        self.min_score
+    }
+}
+
+#[derive(Clone, Debug)]
+pub struct LabelDictionary(Vec<u8>);
+impl Default for LabelDictionary {
+    fn default() -> Self {
+        LabelDictionary::new(vec![])
+    }
+}
+impl LabelDictionary {
+    pub fn new(mut labels: Vec<String>) -> LabelDictionary {
+        labels.sort();
+        let ram_trie = trie_ram::create_trie(&labels);
+        LabelDictionary(trie::serialize(ram_trie))
+    }
+}
+#[derive(Clone, Debug)]
+pub struct Elem {
+    pub key: Vec<u8>,
+    pub vector: Vec<u8>,
+    pub metadata: Option<Vec<u8>>,
+    pub labels: LabelDictionary,
+}
+impl Elem {
+    pub fn new(
+        key: String,
+        vector: Vec<f32>,
+        labels: LabelDictionary,
+        metadata: Option<Vec<u8>>,
+    ) -> Elem {
+        Elem {
+            labels,
+            metadata,
+            key: key.as_bytes().to_vec(),
+            vector: vector::encode_vector(&vector),
+        }
+    }
+}
+
+impl key_value::KVElem for Elem {
+    fn serialized_len(&self) -> usize {
+        Node::serialized_len(
+            &self.key,
+            &self.vector,
+            &self.labels.0,
+            self.metadata.as_ref(),
+        )
+    }
+    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()> {
+        Node::serialize_into(
+            w,
+            self.key,
+            self.vector,
+            self.labels.0,
+            self.metadata.as_ref(),
+        )
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct Neighbour {
+    score: f32,
+    node: Vec<u8>,
+}
+impl Eq for Neighbour {}
+impl std::hash::Hash for Neighbour {
+    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
+        self.node.hash(state);
+    }
+}
+impl Ord for Neighbour {
+    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
+        self.node.cmp(&other.node)
+    }
+}
+impl PartialOrd for Neighbour {
+    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
+        self.node.partial_cmp(&other.node)
+    }
+}
+impl PartialEq for Neighbour {
+    fn eq(&self, other: &Self) -> bool {
+        self.node == other.node
+    }
+}
+
+impl Neighbour {
+    #[cfg(test)]
+    pub fn dummy_neighbour(node: &[u8], score: f32) -> Neighbour {
+        Neighbour {
+            score,
+            node: node.to_vec(),
+        }
+    }
+    fn new(Address(addr): Address, data: &[u8], score: f32) -> Neighbour {
+        let node = key_value::get_value(Node, data, addr);
+        let (exact, _) = Node.read_exact(node);
+        Neighbour {
+            score,
+            node: exact.to_vec(),
+        }
+    }
+    pub fn score(&self) -> f32 {
+        self.score
+    }
+    pub fn id(&self) -> &[u8] {
+        Node.get_key(&self.node)
+    }
+    pub fn labels(&self) -> Vec<String> {
+        Node::labels(&self.node)
+    }
+    pub fn metadata(&self) -> Option<&[u8]> {
+        let metadata = Node::metadata(&self.node);
+        if metadata.is_empty() {
+            None
+        } else {
+            Some(metadata)
+        }
+    }
+}
+
+pub struct DataPoint {
+    journal: Journal,
+    nodes: Mmap,
+    index: Mmap,
+}
+
+impl AsRef<DataPoint> for DataPoint {
+    fn as_ref(&self) -> &DataPoint {
+        self
+    }
+}
+
+impl DataPoint {
+    pub fn stored_len(&self) -> Option<u64> {
+        if key_value::get_no_elems(&self.nodes) == 0 {
+            return None;
+        }
+        let node = key_value::get_value(Node, &self.nodes, 0);
+        Some(vector::vector_len(Node::vector(node)))
+    }
+    pub fn get_id(&self) -> DpId {
+        self.journal.uid
+    }
+    pub fn meta(&self) -> Journal {
+        self.journal
+    }
+    pub fn get_keys<Dlog: DeleteLog>(&self, delete_log: &Dlog) -> Vec<String> {
+        key_value::get_keys(Node, &self.nodes)
+            .filter(|k| !delete_log.is_deleted(k))
+            .map(String::from_utf8_lossy)
+            .map(|s| s.to_string())
+            .collect()
+    }
+
+    #[allow(clippy::too_many_arguments)]
+    pub fn search<Dlog: DeleteLog>(
+        &self,
+        delete_log: &Dlog,
+        query: &[f32],
+        filter: &Formula,
+        with_duplicates: bool,
+        results: usize,
+        similarity: Similarity,
+        min_score: f32,
+    ) -> impl Iterator<Item = Neighbour> + '_ {
+        let encoded_query = vector::encode_vector(query);
+        let tracker = Retriever::new(
+            &encoded_query,
+            &self.nodes,
+            delete_log,
+            similarity,
+            min_score,
+        );
+        let ops = HnswOps::new(&tracker);
+        let neighbours = ops.search(
+            Address(self.journal.nodes),
+            self.index.as_ref(),
+            results,
+            filter,
+            with_duplicates,
+        );
+        neighbours
+            .into_iter()
+            .map(|(address, dist)| (Neighbour::new(address, &self.nodes, dist)))
+            .take(results)
+    }
+    pub fn merge<Dlog>(
+        dir: &path::Path,
+        operants: &[(Dlog, DpId)],
+        similarity: Similarity,
+    ) -> VectorR<DataPoint>
+    where
+        Dlog: DeleteLog,
+    {
+        let uid = DpId::new_v4().to_string();
+        let id = dir.join(&uid);
+        fs::create_dir(&id)?;
+        let mut nodes = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::NODES))?;
+        let mut journalf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let mut hnswf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::HNSW))?;
+        let operants = operants
+            .iter()
+            .map(|(dlog, dp_id)| DataPoint::open(dir, *dp_id).map(|v| (dlog, v)))
+            .collect::<VectorR<Vec<_>>>()?;
+        let node_producers = operants
+            .iter()
+            .map(|dp| ((dp.0, Node), dp.1.nodes.as_ref()));
+        {
+            let mut node_buffer = BufWriter::new(&mut nodes);
+            key_value::merge(&mut node_buffer, node_producers.collect())?;
+            node_buffer.flush()?;
+        }
+
+        let nodes = unsafe { Mmap::map(&nodes)? };
+        let no_nodes = key_value::get_no_elems(&nodes);
+        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity, -1.0);
+        let mut ops = HnswOps::new(&tracker);
+        let mut index = RAMHnsw::new();
+        for id in 0..no_nodes {
+            ops.insert(Address(id), &mut index)
+        }
+
+        {
+            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
+            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
+            hnswf_buffer.flush()?;
+        }
+
+        let index = unsafe { Mmap::map(&hnswf)? };
+
+        let journal = Journal {
+            nodes: no_nodes,
+            uid: DpId::parse_str(&uid).unwrap(),
+            ctime: SystemTime::now(),
+        };
+
+        {
+            let mut journalf_buffer = BufWriter::new(&mut journalf);
+            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
+            journalf_buffer.flush()?;
+        }
+
+        // Mark it as a Datapoint in progress, since it needs to be commited.
+
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+    pub fn delete(dir: &path::Path, uid: DpId) -> VectorR<()> {
+        let uid = uid.to_string();
+        let id = dir.join(uid);
+        fs::remove_dir_all(id)?;
+        Ok(())
+    }
+    pub fn open(dir: &path::Path, uid: DpId) -> VectorR<DataPoint> {
+        let uid = uid.to_string();
+        let id = dir.join(uid);
+        let nodes = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::NODES))?;
+        let journal = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let hnswf = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::HNSW))?;
+
+        let nodes = unsafe { Mmap::map(&nodes)? };
+        let index = unsafe { Mmap::map(&hnswf)? };
+        let journal: Journal = serde_json::from_reader(journal)?;
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+    pub fn new(
+        dir: &path::Path,
+        mut elems: Vec<Elem>,
+        time: Option<SystemTime>,
+        similarity: Similarity,
+    ) -> VectorR<DataPoint> {
+        let uid = DpId::new_v4().to_string();
+        let id = dir.join(&uid);
+        fs::create_dir(&id)?;
+        let mut nodesf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::NODES))?;
+        let mut journalf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let mut hnswf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::HNSW))?;
+
+        elems.sort_by(|a, b| a.key.cmp(&b.key));
+        elems.dedup_by(|a, b| a.key.cmp(&b.key).is_eq());
+        {
+            // Serializing nodes on disk
+            // Nodes are stored on disk and mmaped.
+            let mut nodesf_buffer = BufWriter::new(&mut nodesf);
+            key_value::create_key_value(&mut nodesf_buffer, elems)?;
+            nodesf_buffer.flush()?;
+        }
+        let nodes = unsafe { Mmap::map(&nodesf)? };
+        let no_nodes = key_value::get_no_elems(&nodes);
+
+        // Creating the HNSW using the mmaped nodes
+        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity, -1.0);
+        let mut ops = HnswOps::new(&tracker);
+        let mut index = RAMHnsw::new();
+        for id in 0..no_nodes {
+            ops.insert(Address(id), &mut index)
+        }
+
+        {
+            // The HNSW is on RAM
+            // Serializing the HNSW into disk
+            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
+            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
+            hnswf_buffer.flush()?;
+        }
+        let index = unsafe { Mmap::map(&hnswf)? };
+
+        let journal = Journal {
+            nodes: no_nodes,
+            uid: DpId::parse_str(&uid).unwrap(),
+            ctime: time.unwrap_or_else(SystemTime::now),
+        };
+        {
+            // Saving the journal
+            let mut journalf_buffer = BufWriter::new(&mut journalf);
+            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
+            journalf_buffer.flush()?;
+        }
+
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/node.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/node.rs`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,284 +1,284 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io;
-
-use crate::data_types::key_value::Slot;
-use crate::data_types::trie;
-use crate::data_types::usize_utils::*;
-
-// Nodes are the main element of the system. The following data is stored inside them:
-// -> vector: Vec<u8> used for building a hnsw index with them (is a serialized Vec<f32>).
-// -> key: String assigned by the user to identify the node.
-// Once a node is persisted by the system it will be dentified by a pointer. This pointer represents
-// the start of the serialized node in the file (or other element) it is serialized into. Therefore
-// nodes are used by the system in their serialized form, which is:
-// len: number of bytes representing this node. (usize in little endian)
-// vector_start: byte where the vector segment starts. (usize in little endian)
-// key_start: byte where the key segment starts. (usize in little endian)
-// label_start: byte where the label segment starts. (usize in little endian)
-// [Block of metadata, may be empty]: Everything between the header and the content is consider metadata.
-// vector segment:
-// - len: size of the segment. (usize in little endian)
-// - value: the vector.
-// string segment:
-// - len: size of the segment. (usize in little endian)
-// - value: the serialized string.
-// label segment: trie
-
-const LEN: (usize, usize) = (0, USIZE_LEN);
-const VECTOR_START: (usize, usize) = (LEN.1, LEN.1 + USIZE_LEN);
-const KEY_START: (usize, usize) = (VECTOR_START.1, VECTOR_START.1 + USIZE_LEN);
-const LABEL_START: (usize, usize) = (KEY_START.1, KEY_START.1 + USIZE_LEN);
-const HEADER_LEN: usize = 4 * USIZE_LEN;
-
-#[derive(Clone, Copy)]
-pub struct Node;
-impl Node {
-    pub fn serialized_len<S, V, T, M>(key: S, vector: V, trie: T, metadata: Option<M>) -> usize
-    where
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let skey = key.as_ref();
-        let svector = vector.as_ref();
-        let strie = trie.as_ref();
-        let svector_len = svector.len() + USIZE_LEN;
-        let skey_len = skey.len() + USIZE_LEN;
-        let slabels_len = strie.len();
-        let metadata_len = metadata.map(|m| m.as_ref().len()).unwrap_or_default();
-        HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len
-    }
-    pub fn serialize<S, V, T, M>(key: S, vector: V, labels: T, metadata: Option<M>) -> Vec<u8>
-    where
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let mut buf = vec![];
-        Node::serialize_into(&mut buf, key, vector, labels, metadata).unwrap();
-        buf
-    }
-    // labels must be sorted.
-    pub fn serialize_into<W, S, V, T, M>(
-        mut w: W,
-        key: S,
-        vector: V,
-        trie: T,
-        metadata: Option<M>,
-    ) -> io::Result<()>
-    where
-        W: io::Write,
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let skey = key.as_ref();
-        let svector = vector.as_ref();
-        let strie = trie.as_ref();
-
-        // Reading lens
-        let svector_len = svector.len() + USIZE_LEN;
-        let skey_len = skey.len() + USIZE_LEN;
-        let slabels_len = strie.len();
-        let metadata_len = metadata
-            .as_ref()
-            .map(|m| m.as_ref().len())
-            .unwrap_or_default();
-
-        // Pointer computations
-        let len = HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len;
-        let vector_start = HEADER_LEN + metadata_len;
-        let key_start = vector_start + svector_len;
-        let labels_start = key_start + skey_len;
-
-        // Write pointers
-        w.write_all(&len.to_le_bytes())?;
-        w.write_all(&vector_start.to_le_bytes())?;
-        w.write_all(&key_start.to_le_bytes())?;
-        w.write_all(&labels_start.to_le_bytes())?;
-        // Metadata segment
-        metadata.map_or(Ok(()), |m| w.write_all(m.as_ref()))?;
-        // Values
-        w.write_all(&svector.len().to_le_bytes())?;
-        w.write_all(svector)?;
-        w.write_all(&skey.len().to_le_bytes())?;
-        w.write_all(skey)?;
-        w.write_all(strie)?;
-        w.flush()
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn metadata(x: &[u8]) -> &[u8] {
-        // The metadata starts just after the header ends.
-        let metadata_start = LABEL_START.1;
-        // The metadata ends when the vector segment starts.
-        let metadata_end = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
-        &x[metadata_start..metadata_end]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    // This function will decompress the trie data structure that contains the
-    // labels. Use only if you need all the labels.
-    pub fn labels(x: &[u8]) -> Vec<String> {
-        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
-        trie::decompress(&x[xlabel_ptr..])
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn key(x: &[u8]) -> &[u8] {
-        let xkey_ptr = usize_from_slice_le(&x[KEY_START.0..KEY_START.1]);
-        let xkey_len = usize_from_slice_le(&x[xkey_ptr..(xkey_ptr + USIZE_LEN)]);
-        let xkey_start = xkey_ptr + USIZE_LEN;
-        &x[xkey_start..(xkey_start + xkey_len)]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn vector(x: &[u8]) -> &[u8] {
-        let xvec_ptr = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
-        let xvec_len = usize_from_slice_le(&x[xvec_ptr..(xvec_ptr + USIZE_LEN)]);
-        let xvec_start = xvec_ptr + USIZE_LEN;
-        &x[xvec_start..(xvec_start + xvec_len)]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn has_label(x: &[u8], label: &[u8]) -> bool {
-        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
-        trie::has_word(&x[xlabel_ptr..], label)
-    }
-}
-impl Slot for Node {
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
-        let xkey = self.get_key(x);
-        xkey.cmp(key)
-    }
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-        Self::key(x)
-    }
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
-        let len = usize_from_slice_le(&x[LEN.0..LEN.1]);
-        (&x[0..len], &x[len..])
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::data_types::{trie_ram, vector};
-    lazy_static::lazy_static! {
-        static ref NO_LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&NO_LABELS));
-        static ref LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&LABELS));
-    }
-    const NO_LABELS: [&[u8]; 0] = [];
-    const LABELS: [&[u8]; 3] = [b"L1", b"L2", b"L3"];
-    const NO_METADATA: Option<&[u8]> = None;
-
-    #[test]
-    fn create_test() {
-        let key = b"NODE1";
-        let vector = vector::encode_vector(&[12.; 1000]);
-        let mut buf = Vec::new();
-        Node::serialize_into(&mut buf, key, &vector, NO_LABELS_TRIE.clone(), NO_METADATA).unwrap();
-        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
-        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
-        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
-        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
-        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
-        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
-        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
-        let metadata = Node::metadata(&buf);
-        assert_eq!(metadata.len(), 0);
-        assert_eq!(len, buf.len());
-        assert_eq!(vector_len, vector.len());
-        assert_eq!(key_len, key.len());
-        assert_eq!(&buf[svector], &vector);
-        assert_eq!(&buf[skey], key.as_slice());
-        assert_eq!(Node::vector(&buf), &vector);
-        assert_eq!(Node::key(&buf), key);
-
-        let key = b"NODE2";
-        let metadata = b"THIS ARE THE METADATA CONTENTS";
-        let vector = vector::encode_vector(&[13.; 1000]);
-        let mut buf = Vec::new();
-        Node::serialize_into(&mut buf, key, &vector, LABELS_TRIE.clone(), Some(metadata)).unwrap();
-        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
-        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
-        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
-        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
-        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
-        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
-        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
-        let smetadata = Node::metadata(&buf);
-        assert_eq!(smetadata, metadata.as_slice());
-        assert_eq!(len, buf.len());
-        assert_eq!(vector_len, vector.len());
-        assert_eq!(key_len, key.len());
-        assert_eq!(&buf[svector], &vector);
-        assert_eq!(&buf[skey], key.as_slice());
-        assert_eq!(Node::vector(&buf), &vector);
-        assert_eq!(Node::key(&buf), key);
-        assert!(LABELS.iter().all(|l| Node::has_label(&buf, l)));
-    }
-
-    #[test]
-    fn look_up_test() {
-        let mut buf = Vec::new();
-        let key1 = b"NODE1";
-        let metadata1 = b"The node 1 has metadata";
-        let vector1 = vector::encode_vector(&[12.; 1000]);
-        let node1 = buf.len();
-        Node::serialize_into(
-            &mut buf,
-            key1,
-            &vector1,
-            NO_LABELS_TRIE.clone(),
-            Some(&metadata1),
-        )
-        .unwrap();
-        let key2 = b"NODE2";
-        let metadata2 = b"Tuns out node 2 also has metadata";
-        let vector2 = vector::encode_vector(&[15.; 1000]);
-        let node2 = buf.len();
-        Node::serialize_into(
-            &mut buf,
-            key2,
-            &vector2,
-            NO_LABELS_TRIE.clone(),
-            Some(&metadata2),
-        )
-        .unwrap();
-        assert_eq!(Node::key(&buf[node1..]), key1);
-        assert_eq!(Node::key(&buf[node2..]), key2);
-        assert_eq!(Node::vector(&buf[node1..]), vector1);
-        assert_eq!(Node::vector(&buf[node2..]), vector2);
-        assert!(Node.keep_in_merge(&buf[node1..]));
-        assert!(Node.keep_in_merge(&buf[node2..]));
-        assert_eq!(Node.cmp_slot(&buf[node1..], &buf[node2..]), key1.cmp(key2));
-        assert_eq!(Node::metadata(&buf[node1..]), metadata1);
-        assert_eq!(Node::metadata(&buf[node2..]), metadata2);
-        assert_eq!(
-            Node.read_exact(&buf[node1..]),
-            (&buf[node1..node2], &buf[node2..])
-        );
-        assert_eq!(
-            Node.read_exact(&buf[node2..]),
-            (&buf[node2..], [].as_slice())
-        );
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io;
+
+use crate::data_types::key_value::Slot;
+use crate::data_types::trie;
+use crate::data_types::usize_utils::*;
+
+// Nodes are the main element of the system. The following data is stored inside them:
+// -> vector: Vec<u8> used for building a hnsw index with them (is a serialized Vec<f32>).
+// -> key: String assigned by the user to identify the node.
+// Once a node is persisted by the system it will be dentified by a pointer. This pointer represents
+// the start of the serialized node in the file (or other element) it is serialized into. Therefore
+// nodes are used by the system in their serialized form, which is:
+// len: number of bytes representing this node. (usize in little endian)
+// vector_start: byte where the vector segment starts. (usize in little endian)
+// key_start: byte where the key segment starts. (usize in little endian)
+// label_start: byte where the label segment starts. (usize in little endian)
+// [Block of metadata, may be empty]: Everything between the header and the content is consider metadata.
+// vector segment:
+// - len: size of the segment. (usize in little endian)
+// - value: the vector.
+// string segment:
+// - len: size of the segment. (usize in little endian)
+// - value: the serialized string.
+// label segment: trie
+
+const LEN: (usize, usize) = (0, USIZE_LEN);
+const VECTOR_START: (usize, usize) = (LEN.1, LEN.1 + USIZE_LEN);
+const KEY_START: (usize, usize) = (VECTOR_START.1, VECTOR_START.1 + USIZE_LEN);
+const LABEL_START: (usize, usize) = (KEY_START.1, KEY_START.1 + USIZE_LEN);
+const HEADER_LEN: usize = 4 * USIZE_LEN;
+
+#[derive(Clone, Copy)]
+pub struct Node;
+impl Node {
+    pub fn serialized_len<S, V, T, M>(key: S, vector: V, trie: T, metadata: Option<M>) -> usize
+    where
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let skey = key.as_ref();
+        let svector = vector.as_ref();
+        let strie = trie.as_ref();
+        let svector_len = svector.len() + USIZE_LEN;
+        let skey_len = skey.len() + USIZE_LEN;
+        let slabels_len = strie.len();
+        let metadata_len = metadata.map(|m| m.as_ref().len()).unwrap_or_default();
+        HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len
+    }
+    pub fn serialize<S, V, T, M>(key: S, vector: V, labels: T, metadata: Option<M>) -> Vec<u8>
+    where
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let mut buf = vec![];
+        Node::serialize_into(&mut buf, key, vector, labels, metadata).unwrap();
+        buf
+    }
+    // labels must be sorted.
+    pub fn serialize_into<W, S, V, T, M>(
+        mut w: W,
+        key: S,
+        vector: V,
+        trie: T,
+        metadata: Option<M>,
+    ) -> io::Result<()>
+    where
+        W: io::Write,
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let skey = key.as_ref();
+        let svector = vector.as_ref();
+        let strie = trie.as_ref();
+
+        // Reading lens
+        let svector_len = svector.len() + USIZE_LEN;
+        let skey_len = skey.len() + USIZE_LEN;
+        let slabels_len = strie.len();
+        let metadata_len = metadata
+            .as_ref()
+            .map(|m| m.as_ref().len())
+            .unwrap_or_default();
+
+        // Pointer computations
+        let len = HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len;
+        let vector_start = HEADER_LEN + metadata_len;
+        let key_start = vector_start + svector_len;
+        let labels_start = key_start + skey_len;
+
+        // Write pointers
+        w.write_all(&len.to_le_bytes())?;
+        w.write_all(&vector_start.to_le_bytes())?;
+        w.write_all(&key_start.to_le_bytes())?;
+        w.write_all(&labels_start.to_le_bytes())?;
+        // Metadata segment
+        metadata.map_or(Ok(()), |m| w.write_all(m.as_ref()))?;
+        // Values
+        w.write_all(&svector.len().to_le_bytes())?;
+        w.write_all(svector)?;
+        w.write_all(&skey.len().to_le_bytes())?;
+        w.write_all(skey)?;
+        w.write_all(strie)?;
+        w.flush()
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn metadata(x: &[u8]) -> &[u8] {
+        // The metadata starts just after the header ends.
+        let metadata_start = LABEL_START.1;
+        // The metadata ends when the vector segment starts.
+        let metadata_end = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
+        &x[metadata_start..metadata_end]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    // This function will decompress the trie data structure that contains the
+    // labels. Use only if you need all the labels.
+    pub fn labels(x: &[u8]) -> Vec<String> {
+        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
+        trie::decompress(&x[xlabel_ptr..])
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn key(x: &[u8]) -> &[u8] {
+        let xkey_ptr = usize_from_slice_le(&x[KEY_START.0..KEY_START.1]);
+        let xkey_len = usize_from_slice_le(&x[xkey_ptr..(xkey_ptr + USIZE_LEN)]);
+        let xkey_start = xkey_ptr + USIZE_LEN;
+        &x[xkey_start..(xkey_start + xkey_len)]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn vector(x: &[u8]) -> &[u8] {
+        let xvec_ptr = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
+        let xvec_len = usize_from_slice_le(&x[xvec_ptr..(xvec_ptr + USIZE_LEN)]);
+        let xvec_start = xvec_ptr + USIZE_LEN;
+        &x[xvec_start..(xvec_start + xvec_len)]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn has_label(x: &[u8], label: &[u8]) -> bool {
+        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
+        trie::has_word(&x[xlabel_ptr..], label)
+    }
+}
+impl Slot for Node {
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
+        let xkey = self.get_key(x);
+        xkey.cmp(key)
+    }
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+        Self::key(x)
+    }
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
+        let len = usize_from_slice_le(&x[LEN.0..LEN.1]);
+        (&x[0..len], &x[len..])
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::data_types::{trie_ram, vector};
+    lazy_static::lazy_static! {
+        static ref NO_LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&NO_LABELS));
+        static ref LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&LABELS));
+    }
+    const NO_LABELS: [&[u8]; 0] = [];
+    const LABELS: [&[u8]; 3] = [b"L1", b"L2", b"L3"];
+    const NO_METADATA: Option<&[u8]> = None;
+
+    #[test]
+    fn create_test() {
+        let key = b"NODE1";
+        let vector = vector::encode_vector(&[12.; 1000]);
+        let mut buf = Vec::new();
+        Node::serialize_into(&mut buf, key, &vector, NO_LABELS_TRIE.clone(), NO_METADATA).unwrap();
+        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
+        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
+        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
+        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
+        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
+        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
+        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
+        let metadata = Node::metadata(&buf);
+        assert_eq!(metadata.len(), 0);
+        assert_eq!(len, buf.len());
+        assert_eq!(vector_len, vector.len());
+        assert_eq!(key_len, key.len());
+        assert_eq!(&buf[svector], &vector);
+        assert_eq!(&buf[skey], key.as_slice());
+        assert_eq!(Node::vector(&buf), &vector);
+        assert_eq!(Node::key(&buf), key);
+
+        let key = b"NODE2";
+        let metadata = b"THIS ARE THE METADATA CONTENTS";
+        let vector = vector::encode_vector(&[13.; 1000]);
+        let mut buf = Vec::new();
+        Node::serialize_into(&mut buf, key, &vector, LABELS_TRIE.clone(), Some(metadata)).unwrap();
+        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
+        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
+        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
+        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
+        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
+        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
+        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
+        let smetadata = Node::metadata(&buf);
+        assert_eq!(smetadata, metadata.as_slice());
+        assert_eq!(len, buf.len());
+        assert_eq!(vector_len, vector.len());
+        assert_eq!(key_len, key.len());
+        assert_eq!(&buf[svector], &vector);
+        assert_eq!(&buf[skey], key.as_slice());
+        assert_eq!(Node::vector(&buf), &vector);
+        assert_eq!(Node::key(&buf), key);
+        assert!(LABELS.iter().all(|l| Node::has_label(&buf, l)));
+    }
+
+    #[test]
+    fn look_up_test() {
+        let mut buf = Vec::new();
+        let key1 = b"NODE1";
+        let metadata1 = b"The node 1 has metadata";
+        let vector1 = vector::encode_vector(&[12.; 1000]);
+        let node1 = buf.len();
+        Node::serialize_into(
+            &mut buf,
+            key1,
+            &vector1,
+            NO_LABELS_TRIE.clone(),
+            Some(&metadata1),
+        )
+        .unwrap();
+        let key2 = b"NODE2";
+        let metadata2 = b"Tuns out node 2 also has metadata";
+        let vector2 = vector::encode_vector(&[15.; 1000]);
+        let node2 = buf.len();
+        Node::serialize_into(
+            &mut buf,
+            key2,
+            &vector2,
+            NO_LABELS_TRIE.clone(),
+            Some(&metadata2),
+        )
+        .unwrap();
+        assert_eq!(Node::key(&buf[node1..]), key1);
+        assert_eq!(Node::key(&buf[node2..]), key2);
+        assert_eq!(Node::vector(&buf[node1..]), vector1);
+        assert_eq!(Node::vector(&buf[node2..]), vector2);
+        assert!(Node.keep_in_merge(&buf[node1..]));
+        assert!(Node.keep_in_merge(&buf[node2..]));
+        assert_eq!(Node.cmp_slot(&buf[node1..], &buf[node2..]), key1.cmp(key2));
+        assert_eq!(Node::metadata(&buf[node1..]), metadata1);
+        assert_eq!(Node::metadata(&buf[node2..]), metadata2);
+        assert_eq!(
+            Node.read_exact(&buf[node1..]),
+            (&buf[node1..node2], &buf[node2..])
+        );
+        assert_eq!(
+            Node.read_exact(&buf[node2..]),
+            (&buf[node2..], [].as_slice())
+        );
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashMap;
-
-use ops_hnsw::{Hnsw, Layer};
-use serde::{Deserialize, Serialize};
-
-use super::*;
-
-const NO_EDGES: [(Address, Edge); 0] = [];
-
-#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
-pub struct EntryPoint {
-    pub node: Address,
-    pub layer: usize,
-}
-
-#[derive(Clone, Copy, Debug, PartialEq, PartialOrd, Serialize, Deserialize)]
-pub struct Edge {
-    pub dist: f32,
-}
-
-#[derive(Default, Clone)]
-pub struct RAMLayer {
-    pub out: HashMap<Address, Vec<(Address, Edge)>>,
-}
-
-impl RAMLayer {
-    fn out_edges(&self, node: Address) -> std::iter::Copied<std::slice::Iter<'_, (Address, Edge)>> {
-        self.out
-            .get(&node)
-            .map_or_else(|| NO_EDGES.iter().copied(), |out| out.iter().copied())
-    }
-    pub fn new() -> RAMLayer {
-        RAMLayer::default()
-    }
-    pub fn add_node(&mut self, node: Address) {
-        self.out.entry(node).or_insert_with(Vec::new);
-    }
-    pub fn add_edge(&mut self, from: Address, edge: Edge, to: Address) {
-        if let Some(edges) = self.out.get_mut(&from) {
-            edges.push((to, edge))
-        }
-    }
-    pub fn take_out_edges(&mut self, x: Address) -> Vec<(Address, Edge)> {
-        self.out.get_mut(&x).map(std::mem::take).unwrap_or_default()
-    }
-    pub fn no_out_edges(&self, node: Address) -> usize {
-        self.out.get(&node).map_or(0, |v| v.len())
-    }
-    pub fn first(&self) -> Option<Address> {
-        self.out.keys().next().cloned()
-    }
-    pub fn is_empty(&self) -> bool {
-        self.out.len() == 0
-    }
-    #[cfg(test)]
-    #[allow(unused)]
-    pub fn no_nodes(&self) -> usize {
-        self.out.len()
-    }
-}
-
-#[derive(Default, Clone)]
-pub struct RAMHnsw {
-    pub entry_point: Option<EntryPoint>,
-    pub layers: Vec<RAMLayer>,
-}
-impl RAMHnsw {
-    pub fn new() -> RAMHnsw {
-        Self::default()
-    }
-    pub fn increase_layers_with(&mut self, x: Address, level: usize) -> &mut Self {
-        while self.layers.len() <= level {
-            let mut new_layer = RAMLayer::new();
-            new_layer.add_node(x);
-            self.layers.push(new_layer);
-        }
-        self
-    }
-    pub fn remove_empty_layers(&mut self) -> &mut Self {
-        while self.layers.last().map(|l| l.is_empty()).unwrap_or_default() {
-            self.layers.pop();
-        }
-        self
-    }
-    pub fn update_entry_point(&mut self) -> &mut Self {
-        self.remove_empty_layers();
-        self.entry_point = self
-            .layers
-            .iter()
-            .enumerate()
-            .last()
-            .and_then(|(index, l)| l.first().map(|node| (node, index)))
-            .map(|(node, layer)| EntryPoint { node, layer });
-        self
-    }
-    pub fn no_layers(&self) -> usize {
-        self.layers.len()
-    }
-}
-
-impl<'a> Layer for &'a RAMLayer {
-    type EdgeIt = std::iter::Copied<std::slice::Iter<'a, (Address, Edge)>>;
-    fn get_out_edges(&self, node: Address) -> Self::EdgeIt {
-        self.out_edges(node)
-    }
-}
-
-impl<'a> Hnsw for &'a RAMHnsw {
-    type L = &'a RAMLayer;
-    fn get_entry_point(&self) -> Option<EntryPoint> {
-        self.entry_point
-    }
-    fn get_layer(&self, i: usize) -> Self::L {
-        &self.layers[i]
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashMap;
+
+use ops_hnsw::{Hnsw, Layer};
+use serde::{Deserialize, Serialize};
+
+use super::*;
+
+const NO_EDGES: [(Address, Edge); 0] = [];
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
+pub struct EntryPoint {
+    pub node: Address,
+    pub layer: usize,
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, PartialOrd, Serialize, Deserialize)]
+pub struct Edge {
+    pub dist: f32,
+}
+
+#[derive(Default, Clone)]
+pub struct RAMLayer {
+    pub out: HashMap<Address, Vec<(Address, Edge)>>,
+}
+
+impl RAMLayer {
+    fn out_edges(&self, node: Address) -> std::iter::Copied<std::slice::Iter<'_, (Address, Edge)>> {
+        self.out
+            .get(&node)
+            .map_or_else(|| NO_EDGES.iter().copied(), |out| out.iter().copied())
+    }
+    pub fn new() -> RAMLayer {
+        RAMLayer::default()
+    }
+    pub fn add_node(&mut self, node: Address) {
+        self.out.entry(node).or_insert_with(Vec::new);
+    }
+    pub fn add_edge(&mut self, from: Address, edge: Edge, to: Address) {
+        if let Some(edges) = self.out.get_mut(&from) {
+            edges.push((to, edge))
+        }
+    }
+    pub fn take_out_edges(&mut self, x: Address) -> Vec<(Address, Edge)> {
+        self.out.get_mut(&x).map(std::mem::take).unwrap_or_default()
+    }
+    pub fn no_out_edges(&self, node: Address) -> usize {
+        self.out.get(&node).map_or(0, |v| v.len())
+    }
+    pub fn first(&self) -> Option<Address> {
+        self.out.keys().next().cloned()
+    }
+    pub fn is_empty(&self) -> bool {
+        self.out.len() == 0
+    }
+    #[cfg(test)]
+    #[allow(unused)]
+    pub fn no_nodes(&self) -> usize {
+        self.out.len()
+    }
+}
+
+#[derive(Default, Clone)]
+pub struct RAMHnsw {
+    pub entry_point: Option<EntryPoint>,
+    pub layers: Vec<RAMLayer>,
+}
+impl RAMHnsw {
+    pub fn new() -> RAMHnsw {
+        Self::default()
+    }
+    pub fn increase_layers_with(&mut self, x: Address, level: usize) -> &mut Self {
+        while self.layers.len() <= level {
+            let mut new_layer = RAMLayer::new();
+            new_layer.add_node(x);
+            self.layers.push(new_layer);
+        }
+        self
+    }
+    pub fn remove_empty_layers(&mut self) -> &mut Self {
+        while self.layers.last().map(|l| l.is_empty()).unwrap_or_default() {
+            self.layers.pop();
+        }
+        self
+    }
+    pub fn update_entry_point(&mut self) -> &mut Self {
+        self.remove_empty_layers();
+        self.entry_point = self
+            .layers
+            .iter()
+            .enumerate()
+            .last()
+            .and_then(|(index, l)| l.first().map(|node| (node, index)))
+            .map(|(node, layer)| EntryPoint { node, layer });
+        self
+    }
+    pub fn no_layers(&self) -> usize {
+        self.layers.len()
+    }
+}
+
+impl<'a> Layer for &'a RAMLayer {
+    type EdgeIt = std::iter::Copied<std::slice::Iter<'a, (Address, Edge)>>;
+    fn get_out_edges(&self, node: Address) -> Self::EdgeIt {
+        self.out_edges(node)
+    }
+}
+
+impl<'a> Hnsw for &'a RAMHnsw {
+    type L = &'a RAMLayer;
+    fn get_entry_point(&self) -> Option<EntryPoint> {
+        self.entry_point
+    }
+    fn get_layer(&self, i: usize) -> Self::L {
+        &self.layers[i]
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,128 +1,123 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::path::PathBuf;
-use std::sync::MutexGuard;
-use std::time::Duration;
-
-use nucliadb_core::fs_state;
-use nucliadb_core::tracing::*;
-
-use super::merger::{MergeQuery, MergeRequest};
-use super::work_flag::MergerWriterSync;
-use super::State;
-use crate::data_point::{DataPoint, DpId, Similarity};
-use crate::data_point_provider::merger;
-use crate::VectorR;
-
-const SLEEP_TIME: Duration = Duration::from_millis(100);
-pub(crate) struct Worker {
-    location: PathBuf,
-    work_flag: MergerWriterSync,
-    similarity: Similarity,
-}
-impl MergeQuery for Worker {
-    fn do_work(&self) -> VectorR<()> {
-        self.work()
-    }
-}
-impl Worker {
-    pub(crate) fn request(
-        location: PathBuf,
-        work_flag: MergerWriterSync,
-        similarity: Similarity,
-    ) -> MergeRequest {
-        Box::new(Worker {
-            similarity,
-            location,
-            work_flag,
-        })
-    }
-    fn merge_report<It>(&self, old: It, new: DpId) -> String
-    where It: Iterator<Item = DpId> {
-        use std::fmt::Write;
-        let mut msg = String::new();
-        for (id, dp_id) in old.enumerate() {
-            writeln!(msg, "  ({id}) {dp_id}").unwrap();
-        }
-        write!(msg, "==> {new}").unwrap();
-        msg
-    }
-    fn notify_merger(&self) {
-        let worker = Worker::request(
-            self.location.clone(),
-            self.work_flag.clone(),
-            self.similarity,
-        );
-        merger::send_merge_request(worker);
-    }
-    fn try_to_work_or_delay(&self) -> MutexGuard<'_, ()> {
-        loop {
-            match self.work_flag.try_to_start_working() {
-                Ok(lock) => break lock,
-                Err(_) => {
-                    info!("Merge delayed at: {:?}", self.location);
-                    std::thread::sleep(SLEEP_TIME);
-                }
-            }
-        }
-    }
-    fn work(&self) -> VectorR<()> {
-        let work_flag = self.try_to_work_or_delay();
-
-        let subscriber = self.location.as_path();
-        info!("{subscriber:?} is ready to perform a merge");
-        let lock = fs_state::shared_lock(subscriber)?;
-        let state: State = fs_state::load_state(&lock)?;
-        std::mem::drop(lock);
-
-        let Some(work) = state.current_work_unit().map(|work|
-            work
-            .iter()
-            .rev()
-            .map(|journal| (state.delete_log(*journal), journal.id()))
-            .collect::<Vec<_>>()
-        ) else { return Ok(());};
-        let new_dp = DataPoint::merge(subscriber, &work, self.similarity)?;
-        let ids: Vec<_> = work.into_iter().map(|(_, v)| v).collect();
-        std::mem::drop(state);
-
-        let report = self.merge_report(ids.iter().copied(), new_dp.meta().id());
-
-        let lock = fs_state::exclusive_lock(subscriber)?;
-        let mut state: State = fs_state::load_state(&lock)?;
-        let creates_work = state.replace_work_unit(new_dp);
-        fs_state::persist_state(&lock, &state)?;
-        std::mem::drop(lock);
-        info!("Merge on {subscriber:?}:\n{report}");
-        if creates_work {
-            self.notify_merger();
-        }
-
-        info!("Removing deprecated datapoints");
-        ids.into_iter()
-            .map(|dp| (subscriber, dp, DataPoint::delete(subscriber, dp)))
-            .filter(|(.., r)| r.is_err())
-            .for_each(|(s, id, ..)| info!("Error while deleting {s:?}/{id}"));
-        std::mem::drop(work_flag);
-
-        info!("Merge request completed");
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::path::PathBuf;
+use std::sync::MutexGuard;
+use std::time::Duration;
+
+use nucliadb_core::fs_state;
+use nucliadb_core::tracing::*;
+
+use super::merger::{MergeQuery, MergeRequest};
+use super::work_flag::MergerWriterSync;
+use super::State;
+use crate::data_point::{DataPoint, DpId, Similarity};
+use crate::data_point_provider::merger;
+use crate::VectorR;
+
+const SLEEP_TIME: Duration = Duration::from_millis(100);
+pub(crate) struct Worker {
+    location: PathBuf,
+    work_flag: MergerWriterSync,
+    similarity: Similarity,
+}
+impl MergeQuery for Worker {
+    fn do_work(&self) -> VectorR<()> {
+        self.work()
+    }
+}
+impl Worker {
+    pub(crate) fn request(
+        location: PathBuf,
+        work_flag: MergerWriterSync,
+        similarity: Similarity,
+    ) -> MergeRequest {
+        Box::new(Worker {
+            similarity,
+            location,
+            work_flag,
+        })
+    }
+    fn merge_report<It>(&self, old: It, new: DpId) -> String
+    where It: Iterator<Item = DpId> {
+        use std::fmt::Write;
+        let mut msg = String::new();
+        for (id, dp_id) in old.enumerate() {
+            writeln!(msg, "  ({id}) {dp_id}").unwrap();
+        }
+        write!(msg, "==> {new}").unwrap();
+        msg
+    }
+    fn notify_merger(&self) {
+        let worker = Worker::request(
+            self.location.clone(),
+            self.work_flag.clone(),
+            self.similarity,
+        );
+        merger::send_merge_request(worker);
+    }
+    fn try_to_work_or_delay(&self) -> MutexGuard<'_, ()> {
+        loop {
+            match self.work_flag.try_to_start_working() {
+                Ok(lock) => break lock,
+                Err(_) => {
+                    info!("Merge delayed at: {:?}", self.location);
+                    std::thread::sleep(SLEEP_TIME);
+                }
+            }
+        }
+    }
+    fn work(&self) -> VectorR<()> {
+        let work_flag = self.try_to_work_or_delay();
+
+        let subscriber = self.location.as_path();
+        info!("{subscriber:?} is ready to perform a merge");
+        let lock = fs_state::shared_lock(subscriber)?;
+        let state: State = fs_state::load_state(&lock)?;
+        std::mem::drop(lock);
+
+        let Some(work) = state.current_work_unit().map(|work| {
+            work.iter()
+                .rev()
+                .map(|journal| (state.delete_log(*journal), journal.id()))
+                .collect::<Vec<_>>()
+        }) else {
+            return Ok(());
+        };
+        let new_dp = DataPoint::merge(subscriber, &work, self.similarity)?;
+        let ids: Vec<_> = work.into_iter().map(|(_, v)| v).collect();
+        std::mem::drop(state);
+
+        let report = self.merge_report(ids.iter().copied(), new_dp.meta().id());
+
+        let lock = fs_state::exclusive_lock(subscriber)?;
+        let mut state: State = fs_state::load_state(&lock)?;
+        let creates_work = state.replace_work_unit(new_dp);
+        std::mem::drop(work_flag);
+
+        fs_state::persist_state(&lock, &state)?;
+        std::mem::drop(lock);
+        info!("Merge on {subscriber:?}:\n{report}");
+        if creates_work {
+            self.notify_merger();
+        }
+        info!("Merge request completed");
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::sync::mpsc::{self, Receiver, Sender};
-use std::sync::Once;
-
-use nucliadb_core::tracing;
-
-use crate::{VectorErr, VectorR};
-
-pub type MergeRequest = Box<dyn MergeQuery>;
-pub type MergeTxn = Sender<MergeRequest>;
-
-pub trait MergeQuery: Send {
-    fn do_work(&self) -> VectorR<()>;
-}
-
-#[derive(Clone)]
-struct MergerHandle(MergeTxn);
-impl MergerHandle {
-    pub fn send(&self, request: MergeRequest) {
-        let Err(e) = self.0.send(request) else { return };
-        tracing::info!("Error sending merge request, {e}");
-    }
-}
-
-static mut MERGER_NOTIFIER: Option<MergerHandle> = None;
-static MERGER_NOTIFIER_SET: Once = Once::new();
-
-pub fn send_merge_request(request: MergeRequest) {
-    // It is always safe to read from MERGER_NOTIFIER since
-    // it can only be writen through MERGER_NOTIFIER_SET and is not exposed in the public interface.
-    // MERGER_NOTIFIER_SET is protected by the type Once so we avoid concurrency problems.
-    match unsafe { &MERGER_NOTIFIER } {
-        Some(merger) => merger.send(request),
-        None => tracing::warn!("Merge requests are being sent without a merger intalled"),
-    }
-}
-
-pub struct Merger {
-    rtxn: Receiver<MergeRequest>,
-}
-
-impl Merger {
-    pub fn install_global() -> VectorR<impl FnOnce()> {
-        let mut status = Err(VectorErr::MergerAlreadyInitialized);
-        MERGER_NOTIFIER_SET.call_once(|| unsafe {
-            let (stxn, rtxn) = mpsc::channel();
-            let handler = MergerHandle(stxn);
-            // It is safe to initialize MERGER_NOTIFIER
-            // since the setter can only be called once.
-            MERGER_NOTIFIER = Some(handler);
-            status = Ok(|| Merger { rtxn }.run());
-        });
-        status
-    }
-    fn run(self) {
-        loop {
-            match self.rtxn.recv() {
-                Err(err) => tracing::info!("channel error {}", err),
-                Ok(query) => match query.do_work() {
-                    Ok(()) => (),
-                    Err(err) => tracing::info!("error merging: {err}"),
-                },
-            }
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::sync::mpsc::{self, Receiver, Sender};
+use std::sync::Once;
+
+use nucliadb_core::tracing;
+
+use crate::{VectorErr, VectorR};
+
+pub type MergeRequest = Box<dyn MergeQuery>;
+pub type MergeTxn = Sender<MergeRequest>;
+
+pub trait MergeQuery: Send {
+    fn do_work(&self) -> VectorR<()>;
+}
+
+#[derive(Clone)]
+struct MergerHandle(MergeTxn);
+impl MergerHandle {
+    pub fn send(&self, request: MergeRequest) {
+        let Err(e) = self.0.send(request) else { return };
+        tracing::info!("Error sending merge request, {e}");
+    }
+}
+
+static mut MERGER_NOTIFIER: Option<MergerHandle> = None;
+static MERGER_NOTIFIER_SET: Once = Once::new();
+
+pub fn send_merge_request(request: MergeRequest) {
+    // It is always safe to read from MERGER_NOTIFIER since
+    // it can only be writen through MERGER_NOTIFIER_SET and is not exposed in the public interface.
+    // MERGER_NOTIFIER_SET is protected by the type Once so we avoid concurrency problems.
+    match unsafe { &MERGER_NOTIFIER } {
+        Some(merger) => merger.send(request),
+        None => tracing::warn!("Merge requests are being sent without a merger intalled"),
+    }
+}
+
+pub struct Merger {
+    rtxn: Receiver<MergeRequest>,
+}
+
+impl Merger {
+    pub fn install_global() -> VectorR<impl FnOnce()> {
+        let mut status = Err(VectorErr::MergerAlreadyInitialized);
+        MERGER_NOTIFIER_SET.call_once(|| unsafe {
+            let (stxn, rtxn) = mpsc::channel();
+            let handler = MergerHandle(stxn);
+            // It is safe to initialize MERGER_NOTIFIER
+            // since the setter can only be called once.
+            MERGER_NOTIFIER = Some(handler);
+            status = Ok(|| Merger { rtxn }.run());
+        });
+        status
+    }
+    fn run(self) {
+        loop {
+            match self.rtxn.recv() {
+                Err(err) => tracing::info!("channel error {}", err),
+                Ok(query) => match query.do_work() {
+                    Ok(()) => (),
+                    Err(err) => tracing::info!("error merging: {err}"),
+                },
+            }
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs`

 * *Files 21% similar despite different names*

```diff
@@ -1,282 +1,292 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod merge_worker;
-mod merger;
-mod state;
-mod work_flag;
-use std::fs::File;
-use std::io::{BufReader, BufWriter, Write};
-use std::mem;
-use std::path::{Path, PathBuf};
-use std::sync::{RwLock, RwLockReadGuard, RwLockWriteGuard};
-use std::time::SystemTime;
-
-pub use merger::Merger;
-use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
-use nucliadb_core::tracing::*;
-use serde::{Deserialize, Serialize};
-use state::*;
-use work_flag::MergerWriterSync;
-
-pub use crate::data_point::Neighbour;
-use crate::data_point::{DataPoint, DpId, Similarity};
-use crate::data_point_provider::merge_worker::Worker;
-use crate::formula::Formula;
-use crate::{VectorErr, VectorR};
-pub type TemporalMark = SystemTime;
-
-const METADATA: &str = "metadata.json";
-
-pub trait SearchRequest {
-    fn get_query(&self) -> &[f32];
-    fn get_filter(&self) -> &Formula;
-    fn no_results(&self) -> usize;
-    fn with_duplicates(&self) -> bool;
-    fn min_score(&self) -> f32;
-}
-
-#[derive(Clone, Copy, Debug)]
-pub enum IndexCheck {
-    None,
-    Sanity,
-}
-
-#[derive(Debug, Default, Serialize, Deserialize)]
-pub struct IndexMetadata {
-    #[serde(default)]
-    pub similarity: Similarity,
-}
-impl IndexMetadata {
-    pub fn write(&self, path: &Path) -> VectorR<()> {
-        let mut writer = BufWriter::new(File::create(path.join(METADATA))?);
-        serde_json::to_writer(&mut writer, self)?;
-        Ok(writer.flush()?)
-    }
-    pub fn open(path: &Path) -> VectorR<Option<IndexMetadata>> {
-        let path = &path.join(METADATA);
-        if !path.is_file() {
-            return Ok(None);
-        }
-        let mut reader = BufReader::new(File::open(path)?);
-        Ok(Some(serde_json::from_reader(&mut reader)?))
-    }
-}
-
-pub struct Index {
-    metadata: IndexMetadata,
-    work_flag: MergerWriterSync,
-    state: RwLock<State>,
-    date: RwLock<Version>,
-    location: PathBuf,
-    dimension_used: Option<u64>,
-}
-impl Index {
-    fn read_state(&self) -> RwLockReadGuard<'_, State> {
-        self.state.read().unwrap_or_else(|e| e.into_inner())
-    }
-    fn write_state(&self) -> RwLockWriteGuard<'_, State> {
-        self.state.write().unwrap_or_else(|e| e.into_inner())
-    }
-    fn read_date(&self) -> RwLockReadGuard<'_, Version> {
-        self.date.read().unwrap_or_else(|e| e.into_inner())
-    }
-    fn write_date(&self) -> RwLockWriteGuard<'_, Version> {
-        self.date.write().unwrap_or_else(|e| e.into_inner())
-    }
-    fn update(&self, lock: &Lock) -> VectorR<()> {
-        let disk_v = fs_state::crnt_version(lock)?;
-        let date = self.read_date();
-        if disk_v > *date {
-            mem::drop(date);
-            let new_state = fs_state::load_state(lock)?;
-            let mut state = self.write_state();
-            let mut date = self.write_date();
-            *state = new_state;
-            *date = disk_v;
-            mem::drop(date);
-            mem::drop(state);
-        }
-        Ok(())
-    }
-    fn notify_merger(&self) {
-        let worker = Worker::request(
-            self.location.clone(),
-            self.work_flag.clone(),
-            self.metadata.similarity,
-        );
-        merger::send_merge_request(worker);
-    }
-    pub fn open(path: &Path, with_check: IndexCheck) -> VectorR<Index> {
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        let dimension_used = state.stored_len(path)?;
-        let metadata = IndexMetadata::open(path)?.map(Ok).unwrap_or_else(|| {
-            // Old indexes may not have this file so in that case the
-            // metadata file they should have is created.
-            let metadata = IndexMetadata::default();
-            metadata.write(path).map(|_| metadata)
-        })?;
-        let index = Index {
-            metadata,
-            dimension_used,
-            work_flag: MergerWriterSync::new(),
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        if let IndexCheck::Sanity = with_check {
-            let mut state = index.write_state();
-            let merge_work = state.work_stack_len();
-            (0..merge_work).for_each(|_| index.notify_merger());
-        }
-        Ok(index)
-    }
-    pub fn new(path: &Path, metadata: IndexMetadata) -> VectorR<Index> {
-        std::fs::create_dir(path)?;
-        fs_state::initialize_disk(path, State::new)?;
-        metadata.write(path)?;
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        let index = Index {
-            metadata,
-            dimension_used: None,
-            work_flag: MergerWriterSync::new(),
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        Ok(index)
-    }
-    pub fn delete(&self, prefix: impl AsRef<str>, temporal_mark: SystemTime, _: &ELock) {
-        let mut state = self.write_state();
-        state.remove(prefix.as_ref(), temporal_mark);
-    }
-    pub fn get_keys(&self, _: &Lock) -> VectorR<Vec<String>> {
-        self.read_state().keys(&self.location)
-    }
-    pub fn search(&self, request: &dyn SearchRequest, _: &Lock) -> VectorR<Vec<Neighbour>> {
-        let state = self.read_state();
-        let given_len = request.get_query().len() as u64;
-        match self.dimension_used {
-            Some(expected) if expected != given_len => Err(VectorErr::InconsistentDimensions),
-            None => Ok(Vec::with_capacity(0)),
-            Some(_) => state.search(&self.location, request, self.metadata.similarity),
-        }
-    }
-    pub fn no_nodes(&self, _: &Lock) -> usize {
-        self.read_state().no_nodes()
-    }
-    pub fn collect_garbage(&self, _: &Lock) -> VectorR<()> {
-        use std::collections::HashSet;
-        let work_flag = self.work_flag.try_to_start_working()?;
-        let state = self.read_state();
-        let in_use_dp: HashSet<_> = state.dpid_iter().collect();
-        for dir_entry in std::fs::read_dir(&self.location)? {
-            let entry = dir_entry?;
-            let path = entry.path();
-            let name = entry.file_name().to_string_lossy().to_string();
-            if path.is_file() {
-                continue;
-            }
-            let Ok(dpid) = DpId::parse_str(&name) else {
-                info!("Unknown item {path:?} found");
-                continue;
-            };
-            if !in_use_dp.contains(&dpid) {
-                info!("found garbage {name}");
-                let Err(err)  = DataPoint::delete(&self.location, dpid) else { continue };
-                warn!("{name} is garbage and could not be deleted because of {err}");
-            }
-        }
-        std::mem::drop(work_flag);
-        Ok(())
-    }
-    pub fn add(&mut self, dp: DataPoint, _: &ELock) -> VectorR<()> {
-        let mut state = self.write_state();
-        let Some(new_dp_vector_len) = dp.stored_len() else {
-            return Ok(());
-        };
-        let Some(state_vector_len) = self.dimension_used else {
-            // There is not a len in the state, therefore adding the datapoint can not
-            // create a merging requirement.
-            let dp_dimension = dp.stored_len();
-            let _ = state.add(dp);
-            std::mem::drop(state);
-            self.dimension_used = dp_dimension;
-            return Ok(());
-        };
-        if state_vector_len != new_dp_vector_len {
-            return Err(VectorErr::InconsistentDimensions);
-        }
-        if state.add(dp) {
-            self.notify_merger()
-        }
-        Ok(())
-    }
-    pub fn commit(&self, lock: ELock) -> VectorR<()> {
-        let state = self.read_state();
-        let mut date = self.write_date();
-        fs_state::persist_state::<State>(&lock, &state)?;
-        *date = fs_state::crnt_version(&lock)?;
-        Ok(())
-    }
-    pub fn get_elock(&self) -> VectorR<ELock> {
-        let lock = fs_state::exclusive_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_slock(&self) -> VectorR<SLock> {
-        let lock = fs_state::shared_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn location(&self) -> &Path {
-        &self.location
-    }
-    pub fn metadata(&self) -> &IndexMetadata {
-        &self.metadata
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use nucliadb_core::NodeResult;
-
-    use super::*;
-    use crate::data_point::Similarity;
-    #[test]
-    fn garbage_collection_test() -> NodeResult<()> {
-        let dir = tempfile::tempdir()?;
-        let vectors_path = dir.path().join("vectors");
-        let index = Index::new(&vectors_path, IndexMetadata::default())?;
-        let empty_no_entries = std::fs::read_dir(&vectors_path)?.count();
-        for _ in 0..10 {
-            DataPoint::new(&vectors_path, vec![], None, Similarity::Cosine).unwrap();
-        }
-        let lock = index.get_slock()?;
-        index.collect_garbage(&lock)?;
-        let no_entries = std::fs::read_dir(&vectors_path)?.count();
-        assert_eq!(no_entries, empty_no_entries);
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod merge_worker;
+mod merger;
+mod state;
+mod work_flag;
+use std::fs::File;
+use std::io::{BufReader, BufWriter, Write};
+use std::mem;
+use std::path::{Path, PathBuf};
+use std::sync::{RwLock, RwLockReadGuard, RwLockWriteGuard};
+use std::time::SystemTime;
+
+pub use merger::Merger;
+use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
+use nucliadb_core::tracing::*;
+use serde::{Deserialize, Serialize};
+use state::*;
+use work_flag::MergerWriterSync;
+
+pub use crate::data_point::Neighbour;
+use crate::data_point::{DataPoint, DpId, Similarity};
+use crate::data_point_provider::merge_worker::Worker;
+use crate::formula::Formula;
+use crate::{VectorErr, VectorR};
+pub type TemporalMark = SystemTime;
+
+const METADATA: &str = "metadata.json";
+
+pub trait SearchRequest {
+    fn get_query(&self) -> &[f32];
+    fn get_filter(&self) -> &Formula;
+    fn no_results(&self) -> usize;
+    fn with_duplicates(&self) -> bool;
+    fn min_score(&self) -> f32;
+}
+
+#[derive(Clone, Copy, Debug)]
+pub enum IndexCheck {
+    None,
+    Sanity,
+}
+
+#[derive(Debug, Default, Serialize, Deserialize)]
+pub struct IndexMetadata {
+    #[serde(default)]
+    pub similarity: Similarity,
+}
+impl IndexMetadata {
+    pub fn write(&self, path: &Path) -> VectorR<()> {
+        let mut writer = BufWriter::new(File::create(path.join(METADATA))?);
+        serde_json::to_writer(&mut writer, self)?;
+        Ok(writer.flush()?)
+    }
+    pub fn open(path: &Path) -> VectorR<Option<IndexMetadata>> {
+        let path = &path.join(METADATA);
+        if !path.is_file() {
+            return Ok(None);
+        }
+        let mut reader = BufReader::new(File::open(path)?);
+        Ok(Some(serde_json::from_reader(&mut reader)?))
+    }
+}
+
+pub struct Index {
+    metadata: IndexMetadata,
+    work_flag: MergerWriterSync,
+    state: RwLock<State>,
+    date: RwLock<Version>,
+    location: PathBuf,
+    dimension: RwLock<Option<u64>>,
+}
+impl Index {
+    fn get_dimension(&self) -> Option<u64> {
+        *self.dimension.read().unwrap_or_else(|e| e.into_inner())
+    }
+    fn set_dimension(&self, dimension: Option<u64>) {
+        *self.dimension.write().unwrap_or_else(|e| e.into_inner()) = dimension;
+    }
+    fn read_state(&self) -> RwLockReadGuard<'_, State> {
+        self.state.read().unwrap_or_else(|e| e.into_inner())
+    }
+    fn write_state(&self) -> RwLockWriteGuard<'_, State> {
+        self.state.write().unwrap_or_else(|e| e.into_inner())
+    }
+    fn read_date(&self) -> RwLockReadGuard<'_, Version> {
+        self.date.read().unwrap_or_else(|e| e.into_inner())
+    }
+    fn write_date(&self) -> RwLockWriteGuard<'_, Version> {
+        self.date.write().unwrap_or_else(|e| e.into_inner())
+    }
+    fn update(&self, lock: &Lock) -> VectorR<()> {
+        let location = self.location();
+        let disk_v = fs_state::crnt_version(lock)?;
+        let date = self.read_date();
+        if disk_v > *date {
+            mem::drop(date);
+            let new_state: State = fs_state::load_state(lock)?;
+            let new_dimension = new_state.stored_len(location)?;
+            let mut state = self.write_state();
+            let mut date = self.write_date();
+            *state = new_state;
+            *date = disk_v;
+            mem::drop(date);
+            mem::drop(state);
+            self.set_dimension(new_dimension);
+        }
+        Ok(())
+    }
+    fn notify_merger(&self) {
+        let worker = Worker::request(
+            self.location.clone(),
+            self.work_flag.clone(),
+            self.metadata.similarity,
+        );
+        merger::send_merge_request(worker);
+    }
+    pub fn open(path: &Path, with_check: IndexCheck) -> VectorR<Index> {
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        let dimension_used = state.stored_len(path)?;
+        let metadata = IndexMetadata::open(path)?.map(Ok).unwrap_or_else(|| {
+            // Old indexes may not have this file so in that case the
+            // metadata file they should have is created.
+            let metadata = IndexMetadata::default();
+            metadata.write(path).map(|_| metadata)
+        })?;
+        let index = Index {
+            metadata,
+            work_flag: MergerWriterSync::new(),
+            dimension: RwLock::new(dimension_used),
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        if let IndexCheck::Sanity = with_check {
+            let mut state = index.write_state();
+            let merge_work = state.work_stack_len();
+            (0..merge_work).for_each(|_| index.notify_merger());
+        }
+        Ok(index)
+    }
+    pub fn new(path: &Path, metadata: IndexMetadata) -> VectorR<Index> {
+        std::fs::create_dir(path)?;
+        fs_state::initialize_disk(path, State::new)?;
+        metadata.write(path)?;
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        let index = Index {
+            metadata,
+            work_flag: MergerWriterSync::new(),
+            dimension: RwLock::new(None),
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        Ok(index)
+    }
+    pub fn delete(&self, prefix: impl AsRef<str>, temporal_mark: SystemTime, _: &ELock) {
+        let mut state = self.write_state();
+        state.remove(prefix.as_ref(), temporal_mark);
+    }
+    pub fn get_keys(&self, _: &Lock) -> VectorR<Vec<String>> {
+        self.read_state().keys(&self.location)
+    }
+    pub fn search(&self, request: &dyn SearchRequest, _: &Lock) -> VectorR<Vec<Neighbour>> {
+        let state = self.read_state();
+        let given_len = request.get_query().len() as u64;
+        match self.get_dimension() {
+            Some(expected) if expected != given_len => Err(VectorErr::InconsistentDimensions),
+            None => Ok(Vec::with_capacity(0)),
+            Some(_) => state.search(&self.location, request, self.metadata.similarity),
+        }
+    }
+    pub fn no_nodes(&self, _: &Lock) -> usize {
+        self.read_state().no_nodes()
+    }
+    pub fn collect_garbage(&self, _: &ELock) -> VectorR<()> {
+        use std::collections::HashSet;
+        let work_flag = self.work_flag.try_to_start_working()?;
+        let state = self.read_state();
+        let in_use_dp: HashSet<_> = state.dpid_iter().collect();
+        for dir_entry in std::fs::read_dir(&self.location)? {
+            let entry = dir_entry?;
+            let path = entry.path();
+            let name = entry.file_name().to_string_lossy().to_string();
+            if path.is_file() {
+                continue;
+            }
+            let Ok(dpid) = DpId::parse_str(&name) else {
+                info!("Unknown item {path:?} found");
+                continue;
+            };
+            if !in_use_dp.contains(&dpid) {
+                info!("found garbage {name}");
+                let Err(err) = DataPoint::delete(&self.location, dpid) else {
+                    continue;
+                };
+                warn!("{name} is garbage and could not be deleted because of {err}");
+            }
+        }
+        std::mem::drop(work_flag);
+        Ok(())
+    }
+    pub fn add(&mut self, dp: DataPoint, _: &ELock) -> VectorR<()> {
+        let mut state = self.write_state();
+        let Some(new_dp_vector_len) = dp.stored_len() else {
+            return Ok(());
+        };
+        let Some(state_vector_len) = self.get_dimension() else {
+            // There is not a len in the state, therefore adding the datapoint can not
+            // create a merging requirement.
+            self.set_dimension(dp.stored_len());
+            let _ = state.add(dp);
+            std::mem::drop(state);
+            return Ok(());
+        };
+        if state_vector_len != new_dp_vector_len {
+            return Err(VectorErr::InconsistentDimensions);
+        }
+        if state.add(dp) {
+            self.notify_merger()
+        }
+        Ok(())
+    }
+    pub fn commit(&self, lock: ELock) -> VectorR<()> {
+        let state = self.read_state();
+        let mut date = self.write_date();
+        fs_state::persist_state::<State>(&lock, &state)?;
+        *date = fs_state::crnt_version(&lock)?;
+        Ok(())
+    }
+    pub fn get_elock(&self) -> VectorR<ELock> {
+        let lock = fs_state::exclusive_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_slock(&self) -> VectorR<SLock> {
+        let lock = fs_state::shared_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn location(&self) -> &Path {
+        &self.location
+    }
+    pub fn metadata(&self) -> &IndexMetadata {
+        &self.metadata
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use nucliadb_core::NodeResult;
+
+    use super::*;
+    use crate::data_point::Similarity;
+    #[test]
+    fn garbage_collection_test() -> NodeResult<()> {
+        let dir = tempfile::tempdir()?;
+        let vectors_path = dir.path().join("vectors");
+        let index = Index::new(&vectors_path, IndexMetadata::default())?;
+        let empty_no_entries = std::fs::read_dir(&vectors_path)?.count();
+        for _ in 0..10 {
+            DataPoint::new(&vectors_path, vec![], None, Similarity::Cosine).unwrap();
+        }
+        let lock = index.get_elock()?;
+        index.collect_garbage(&lock)?;
+        let no_entries = std::fs::read_dir(&vectors_path)?.count();
+        assert_eq!(no_entries, empty_no_entries);
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,372 +1,374 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::cmp::Ordering;
-use std::collections::{HashMap, LinkedList};
-use std::mem;
-use std::path::{Path, PathBuf};
-use std::time::SystemTime;
-
-use serde::{Deserialize, Serialize};
-
-use super::{SearchRequest, VectorR};
-use crate::data_point::{DataPoint, DpId, Journal, Neighbour, Similarity};
-use crate::data_types::dtrie_ram::DTrie;
-use crate::data_types::DeleteLog;
-const BUFFER_CAP: usize = 5;
-
-#[derive(Serialize, Deserialize)]
-struct WorkUnit {
-    pub age: SystemTime,
-    pub load: Vec<Journal>,
-}
-impl Default for WorkUnit {
-    fn default() -> Self {
-        WorkUnit::new()
-    }
-}
-impl WorkUnit {
-    pub fn new() -> WorkUnit {
-        WorkUnit {
-            age: SystemTime::now(),
-            load: vec![],
-        }
-    }
-    pub fn add_unit(&mut self, dp: Journal) {
-        self.load.push(dp);
-    }
-    pub fn size(&self) -> usize {
-        self.load.len()
-    }
-}
-
-#[derive(Clone, Copy)]
-struct TimeSensitiveDLog<'a> {
-    dlog: &'a DTrie,
-    time: SystemTime,
-}
-impl<'a> DeleteLog for TimeSensitiveDLog<'a> {
-    fn is_deleted(&self, key: &[u8]) -> bool {
-        self.dlog
-            .get(key)
-            .map(|t| t > self.time)
-            .unwrap_or_default()
-    }
-}
-
-// Fixed-sized sorted collection
-struct Fssc {
-    size: usize,
-    buff: HashMap<Neighbour, f32>,
-}
-impl From<Fssc> for Vec<Neighbour> {
-    fn from(fssv: Fssc) -> Self {
-        let mut result: Vec<_> = fssv.buff.into_iter().map(|i| i.0).collect();
-        result.sort_by(|a, b| b.score().partial_cmp(&a.score()).unwrap_or(Ordering::Less));
-        result
-    }
-}
-impl Fssc {
-    fn is_full(&self) -> bool {
-        self.buff.len() == self.size
-    }
-    fn new(size: usize) -> Fssc {
-        Fssc {
-            size,
-            buff: HashMap::with_capacity(size),
-        }
-    }
-    fn add(&mut self, candidate: Neighbour) {
-        let score = candidate.score();
-        if self.is_full() {
-            let smallest_bigger = self
-                .buff
-                .iter()
-                .map(|(key, score)| (key.clone(), *score))
-                .filter(|(_, v)| score > *v)
-                .min_by(|(_, v0), (_, v1)| v0.partial_cmp(v1).unwrap())
-                .map(|(key, _)| key);
-            if let Some(key) = smallest_bigger {
-                self.buff.remove(&key);
-                self.buff.insert(candidate, score);
-            }
-        } else {
-            self.buff.insert(candidate, score);
-        }
-    }
-}
-
-#[derive(Serialize, Deserialize)]
-pub struct State {
-    // Deprecated, location must be passed as an argument.
-    // WARNING: Can not use serde::skip nor move this field due to a bug in serde.
-    #[allow(unused)]
-    #[deprecated]
-    location: PathBuf,
-
-    // Total number of nodes stored. Some
-    // may be marked as deleted but are waiting
-    // for a merge to be fully removed.
-    no_nodes: usize,
-
-    // Current work unit
-    current: WorkUnit,
-
-    // Trie containing the deleted keys and the
-    // time when they were deleted
-    delete_log: DTrie,
-
-    // Already closed WorkUnits waiting to be merged
-    work_stack: LinkedList<WorkUnit>,
-
-    // This field is deprecated and is only
-    // used for old states. Always use
-    // the data_point journal for time references
-    data_points: HashMap<DpId, SystemTime>,
-
-    // Deprecated field, not all vector clusters are
-    // identified by a resource.
-    #[serde(skip)]
-    #[allow(unused)]
-    #[deprecated]
-    resources: HashMap<String, usize>,
-}
-impl State {
-    fn data_point_iterator(&self) -> impl Iterator<Item = &Journal> {
-        self.work_stack
-            .iter()
-            .flat_map(|u| u.load.iter())
-            .chain(self.current.load.iter())
-    }
-    fn close_work_unit(&mut self) {
-        let prev = mem::replace(&mut self.current, WorkUnit::new());
-        self.work_stack.push_front(prev);
-    }
-    fn creation_time(&self, journal: Journal) -> SystemTime {
-        self.data_points
-            // if data_points contains a value for the id,
-            // this data point is older than the refactor.
-            .get(&journal.id())
-            .cloned()
-            // In the case the data_point was created
-            // after the refactor, no entry for it appears.
-            // Is safe to use the journal time.
-            .unwrap_or_else(|| journal.time())
-    }
-    #[allow(deprecated)]
-    pub fn new() -> State {
-        State {
-            location: PathBuf::default(),
-            no_nodes: usize::default(),
-            current: WorkUnit::default(),
-            delete_log: DTrie::default(),
-            work_stack: LinkedList::default(),
-            data_points: HashMap::default(),
-            resources: HashMap::default(),
-        }
-    }
-    pub fn search(
-        &self,
-        location: &Path,
-        request: &dyn SearchRequest,
-        similarity: Similarity,
-    ) -> VectorR<Vec<Neighbour>> {
-        let query = request.get_query();
-        let filter = request.get_filter();
-        let with_duplicates = request.with_duplicates();
-        let no_results = request.no_results();
-        let min_score = request.min_score();
-        let mut ffsv = Fssc::new(request.no_results());
-        for journal in self.data_point_iterator().copied() {
-            let delete_log = self.delete_log(journal);
-            let data_point = DataPoint::open(location, journal.id())?;
-            data_point
-                .search(
-                    &delete_log,
-                    query,
-                    filter,
-                    with_duplicates,
-                    no_results,
-                    similarity,
-                    min_score,
-                )
-                .for_each(|candidate| ffsv.add(candidate));
-        }
-        Ok(ffsv.into())
-    }
-    pub fn remove(&mut self, id: &str, deleted_since: SystemTime) {
-        self.delete_log.insert(id.as_bytes(), deleted_since);
-    }
-    #[must_use]
-    pub fn add(&mut self, dp: DataPoint) -> bool {
-        let meta = dp.meta();
-        self.no_nodes += meta.no_nodes();
-        self.current.add_unit(meta);
-        if self.current.size() == BUFFER_CAP {
-            self.close_work_unit();
-        }
-        self.current.size() == 0
-    }
-    #[must_use]
-    pub fn replace_work_unit(&mut self, new: DataPoint) -> bool {
-        let Some(unit) = self.work_stack.pop_back() else { return false };
-        let age_cap = self
-            .work_stack
-            .back()
-            .and_then(|v| v.load.last().map(|l| l.time()));
-        if let Some(age_cap) = age_cap {
-            self.delete_log.prune(age_cap);
-        }
-        unit.load.iter().cloned().for_each(|dp| {
-            // The data_point may be older that the refactor
-            self.data_points.remove(&dp.id());
-            self.no_nodes -= dp.no_nodes();
-        });
-        self.add(new)
-    }
-    pub fn dpid_iter(&self) -> impl Iterator<Item = DpId> + '_ {
-        self.data_point_iterator()
-            .copied()
-            .map(|journal| journal.id())
-    }
-    pub fn keys(&self, location: &Path) -> VectorR<Vec<String>> {
-        let mut keys = vec![];
-        for journal in self.data_point_iterator().copied() {
-            let delete_log = self.delete_log(journal);
-            let dp_id = journal.id();
-            let data_point = DataPoint::open(location, dp_id)?;
-            let mut results = data_point.get_keys(&delete_log);
-            keys.append(&mut results);
-        }
-        Ok(keys)
-    }
-    pub fn delete_log(&self, journal: Journal) -> impl DeleteLog + '_ {
-        TimeSensitiveDLog {
-            time: self.creation_time(journal),
-            dlog: &self.delete_log,
-        }
-    }
-    pub fn no_nodes(&self) -> usize {
-        self.no_nodes
-    }
-    pub fn work_stack_len(&mut self) -> usize {
-        self.work_stack.len()
-    }
-    pub fn current_work_unit(&self) -> Option<&[Journal]> {
-        self.work_stack.back().map(|wu| wu.load.as_slice())
-    }
-    pub fn stored_len(&self, location: &Path) -> VectorR<Option<u64>> {
-        let Some(journal) = self.data_point_iterator().next() else {
-            return Ok(None);
-        };
-        let data_point = DataPoint::open(location, journal.id())?;
-        Ok(data_point.stored_len())
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use std::path::Path;
-
-    use rand::random;
-    use uuid::Uuid;
-
-    use super::*;
-    use crate::data_point::{Elem, LabelDictionary, Similarity};
-    #[test]
-    fn fssv_test() {
-        let values: &[Neighbour] = &[
-            Neighbour::dummy_neighbour(b"k0", 4.0),
-            Neighbour::dummy_neighbour(b"k1", 3.0),
-            Neighbour::dummy_neighbour(b"k2", 2.0),
-            Neighbour::dummy_neighbour(b"k3", 1.0),
-            Neighbour::dummy_neighbour(b"k4", 0.0),
-        ];
-
-        let mut fssv = Fssc::new(2);
-        values.iter().for_each(|i| fssv.add(i.clone()));
-        let result: Vec<_> = fssv.into();
-        assert_eq!(result[0], values[0]);
-        assert_eq!(result[0].score(), values[0].score());
-        assert_eq!(result[1], values[1]);
-        assert_eq!(result[1].score(), values[1].score());
-    }
-
-    struct DataPointProducer<'a> {
-        dimension: usize,
-        path: &'a Path,
-    }
-    impl<'a> DataPointProducer<'a> {
-        pub fn new(path: &'a Path) -> DataPointProducer<'a> {
-            DataPointProducer {
-                dimension: 12,
-                path,
-            }
-        }
-    }
-    impl<'a> Iterator for DataPointProducer<'a> {
-        type Item = DataPoint;
-        fn next(&mut self) -> Option<Self::Item> {
-            let no_vectors = random::<usize>() % 20;
-            let mut elems = vec![];
-            for _ in 0..no_vectors {
-                let key = Uuid::new_v4().to_string();
-                let labels = LabelDictionary::new(vec![]);
-                let vector = (0..self.dimension)
-                    .map(|_| random::<f32>())
-                    .collect::<Vec<_>>();
-                elems.push(Elem::new(key, vector, labels, None));
-            }
-            Some(DataPoint::new(self.path, elems, None, Similarity::Cosine).unwrap())
-        }
-    }
-
-    #[test]
-    fn state_test() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let mut state = State::new();
-        let no_nodes = DataPointProducer::new(dir.path())
-            .take(5)
-            .map(|dp| {
-                let no_nodes = dp.meta().no_nodes();
-                let _ = state.add(dp);
-                no_nodes
-            })
-            .sum::<usize>();
-        assert_eq!(state.no_nodes(), no_nodes);
-        assert_eq!(state.work_stack.len(), 1);
-        assert_eq!(state.current.size(), 0);
-        let work = state.current_work_unit().unwrap();
-        let work = work
-            .iter()
-            .map(|j| (state.delete_log(*j), j.id()))
-            .collect::<Vec<_>>();
-        let new = DataPoint::merge(dir.path(), &work, Similarity::Cosine).unwrap();
-        std::mem::drop(work);
-        let _ = state.replace_work_unit(new);
-        assert!(state.current_work_unit().is_none());
-        assert_eq!(state.work_stack.len(), 0);
-        assert_eq!(state.current.size(), 1);
-        assert_eq!(state.no_nodes(), no_nodes);
-        assert_eq!(state.work_stack.len(), 0);
-        assert_eq!(state.current.size(), 1);
-        assert_eq!(state.no_nodes(), no_nodes);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::cmp::Ordering;
+use std::collections::{HashMap, LinkedList};
+use std::mem;
+use std::path::{Path, PathBuf};
+use std::time::SystemTime;
+
+use serde::{Deserialize, Serialize};
+
+use super::{SearchRequest, VectorR};
+use crate::data_point::{DataPoint, DpId, Journal, Neighbour, Similarity};
+use crate::data_types::dtrie_ram::DTrie;
+use crate::data_types::DeleteLog;
+const BUFFER_CAP: usize = 5;
+
+#[derive(Serialize, Deserialize)]
+struct WorkUnit {
+    pub age: SystemTime,
+    pub load: Vec<Journal>,
+}
+impl Default for WorkUnit {
+    fn default() -> Self {
+        WorkUnit::new()
+    }
+}
+impl WorkUnit {
+    pub fn new() -> WorkUnit {
+        WorkUnit {
+            age: SystemTime::now(),
+            load: vec![],
+        }
+    }
+    pub fn add_unit(&mut self, dp: Journal) {
+        self.load.push(dp);
+    }
+    pub fn size(&self) -> usize {
+        self.load.len()
+    }
+}
+
+#[derive(Clone, Copy)]
+struct TimeSensitiveDLog<'a> {
+    dlog: &'a DTrie,
+    time: SystemTime,
+}
+impl<'a> DeleteLog for TimeSensitiveDLog<'a> {
+    fn is_deleted(&self, key: &[u8]) -> bool {
+        self.dlog
+            .get(key)
+            .map(|t| t > self.time)
+            .unwrap_or_default()
+    }
+}
+
+// Fixed-sized sorted collection
+struct Fssc {
+    size: usize,
+    buff: HashMap<Neighbour, f32>,
+}
+impl From<Fssc> for Vec<Neighbour> {
+    fn from(fssv: Fssc) -> Self {
+        let mut result: Vec<_> = fssv.buff.into_iter().map(|i| i.0).collect();
+        result.sort_by(|a, b| b.score().partial_cmp(&a.score()).unwrap_or(Ordering::Less));
+        result
+    }
+}
+impl Fssc {
+    fn is_full(&self) -> bool {
+        self.buff.len() == self.size
+    }
+    fn new(size: usize) -> Fssc {
+        Fssc {
+            size,
+            buff: HashMap::with_capacity(size),
+        }
+    }
+    fn add(&mut self, candidate: Neighbour) {
+        let score = candidate.score();
+        if self.is_full() {
+            let smallest_bigger = self
+                .buff
+                .iter()
+                .map(|(key, score)| (key.clone(), *score))
+                .filter(|(_, v)| score > *v)
+                .min_by(|(_, v0), (_, v1)| v0.partial_cmp(v1).unwrap())
+                .map(|(key, _)| key);
+            if let Some(key) = smallest_bigger {
+                self.buff.remove(&key);
+                self.buff.insert(candidate, score);
+            }
+        } else {
+            self.buff.insert(candidate, score);
+        }
+    }
+}
+
+#[derive(Serialize, Deserialize)]
+pub struct State {
+    // Deprecated, location must be passed as an argument.
+    // WARNING: Can not use serde::skip nor move this field due to a bug in serde.
+    #[allow(unused)]
+    #[deprecated]
+    location: PathBuf,
+
+    // Total number of nodes stored. Some
+    // may be marked as deleted but are waiting
+    // for a merge to be fully removed.
+    no_nodes: usize,
+
+    // Current work unit
+    current: WorkUnit,
+
+    // Trie containing the deleted keys and the
+    // time when they were deleted
+    delete_log: DTrie,
+
+    // Already closed WorkUnits waiting to be merged
+    work_stack: LinkedList<WorkUnit>,
+
+    // This field is deprecated and is only
+    // used for old states. Always use
+    // the data_point journal for time references
+    data_points: HashMap<DpId, SystemTime>,
+
+    // Deprecated field, not all vector clusters are
+    // identified by a resource.
+    #[serde(skip)]
+    #[allow(unused)]
+    #[deprecated]
+    resources: HashMap<String, usize>,
+}
+impl State {
+    fn data_point_iterator(&self) -> impl Iterator<Item = &Journal> {
+        self.work_stack
+            .iter()
+            .flat_map(|u| u.load.iter())
+            .chain(self.current.load.iter())
+    }
+    fn close_work_unit(&mut self) {
+        let prev = mem::replace(&mut self.current, WorkUnit::new());
+        self.work_stack.push_front(prev);
+    }
+    fn creation_time(&self, journal: Journal) -> SystemTime {
+        self.data_points
+            // if data_points contains a value for the id,
+            // this data point is older than the refactor.
+            .get(&journal.id())
+            .cloned()
+            // In the case the data_point was created
+            // after the refactor, no entry for it appears.
+            // Is safe to use the journal time.
+            .unwrap_or_else(|| journal.time())
+    }
+    #[allow(deprecated)]
+    pub fn new() -> State {
+        State {
+            location: PathBuf::default(),
+            no_nodes: usize::default(),
+            current: WorkUnit::default(),
+            delete_log: DTrie::default(),
+            work_stack: LinkedList::default(),
+            data_points: HashMap::default(),
+            resources: HashMap::default(),
+        }
+    }
+    pub fn search(
+        &self,
+        location: &Path,
+        request: &dyn SearchRequest,
+        similarity: Similarity,
+    ) -> VectorR<Vec<Neighbour>> {
+        let query = request.get_query();
+        let filter = request.get_filter();
+        let with_duplicates = request.with_duplicates();
+        let no_results = request.no_results();
+        let min_score = request.min_score();
+        let mut ffsv = Fssc::new(request.no_results());
+        for journal in self.data_point_iterator().copied() {
+            let delete_log = self.delete_log(journal);
+            let data_point = DataPoint::open(location, journal.id())?;
+            data_point
+                .search(
+                    &delete_log,
+                    query,
+                    filter,
+                    with_duplicates,
+                    no_results,
+                    similarity,
+                    min_score,
+                )
+                .for_each(|candidate| ffsv.add(candidate));
+        }
+        Ok(ffsv.into())
+    }
+    pub fn remove(&mut self, id: &str, deleted_since: SystemTime) {
+        self.delete_log.insert(id.as_bytes(), deleted_since);
+    }
+    #[must_use]
+    pub fn add(&mut self, dp: DataPoint) -> bool {
+        let meta = dp.meta();
+        self.no_nodes += meta.no_nodes();
+        self.current.add_unit(meta);
+        if self.current.size() == BUFFER_CAP {
+            self.close_work_unit();
+        }
+        self.current.size() == 0
+    }
+    #[must_use]
+    pub fn replace_work_unit(&mut self, new: DataPoint) -> bool {
+        let Some(unit) = self.work_stack.pop_back() else {
+            return false;
+        };
+        let age_cap = self
+            .work_stack
+            .back()
+            .and_then(|v| v.load.last().map(|l| l.time()));
+        if let Some(age_cap) = age_cap {
+            self.delete_log.prune(age_cap);
+        }
+        unit.load.iter().cloned().for_each(|dp| {
+            // The data_point may be older that the refactor
+            self.data_points.remove(&dp.id());
+            self.no_nodes -= dp.no_nodes();
+        });
+        self.add(new)
+    }
+    pub fn dpid_iter(&self) -> impl Iterator<Item = DpId> + '_ {
+        self.data_point_iterator()
+            .copied()
+            .map(|journal| journal.id())
+    }
+    pub fn keys(&self, location: &Path) -> VectorR<Vec<String>> {
+        let mut keys = vec![];
+        for journal in self.data_point_iterator().copied() {
+            let delete_log = self.delete_log(journal);
+            let dp_id = journal.id();
+            let data_point = DataPoint::open(location, dp_id)?;
+            let mut results = data_point.get_keys(&delete_log);
+            keys.append(&mut results);
+        }
+        Ok(keys)
+    }
+    pub fn delete_log(&self, journal: Journal) -> impl DeleteLog + '_ {
+        TimeSensitiveDLog {
+            time: self.creation_time(journal),
+            dlog: &self.delete_log,
+        }
+    }
+    pub fn no_nodes(&self) -> usize {
+        self.no_nodes
+    }
+    pub fn work_stack_len(&mut self) -> usize {
+        self.work_stack.len()
+    }
+    pub fn current_work_unit(&self) -> Option<&[Journal]> {
+        self.work_stack.back().map(|wu| wu.load.as_slice())
+    }
+    pub fn stored_len(&self, location: &Path) -> VectorR<Option<u64>> {
+        let Some(journal) = self.data_point_iterator().next() else {
+            return Ok(None);
+        };
+        let data_point = DataPoint::open(location, journal.id())?;
+        Ok(data_point.stored_len())
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use std::path::Path;
+
+    use rand::random;
+    use uuid::Uuid;
+
+    use super::*;
+    use crate::data_point::{Elem, LabelDictionary, Similarity};
+    #[test]
+    fn fssv_test() {
+        let values: &[Neighbour] = &[
+            Neighbour::dummy_neighbour(b"k0", 4.0),
+            Neighbour::dummy_neighbour(b"k1", 3.0),
+            Neighbour::dummy_neighbour(b"k2", 2.0),
+            Neighbour::dummy_neighbour(b"k3", 1.0),
+            Neighbour::dummy_neighbour(b"k4", 0.0),
+        ];
+
+        let mut fssv = Fssc::new(2);
+        values.iter().for_each(|i| fssv.add(i.clone()));
+        let result: Vec<_> = fssv.into();
+        assert_eq!(result[0], values[0]);
+        assert_eq!(result[0].score(), values[0].score());
+        assert_eq!(result[1], values[1]);
+        assert_eq!(result[1].score(), values[1].score());
+    }
+
+    struct DataPointProducer<'a> {
+        dimension: usize,
+        path: &'a Path,
+    }
+    impl<'a> DataPointProducer<'a> {
+        pub fn new(path: &'a Path) -> DataPointProducer<'a> {
+            DataPointProducer {
+                dimension: 12,
+                path,
+            }
+        }
+    }
+    impl<'a> Iterator for DataPointProducer<'a> {
+        type Item = DataPoint;
+        fn next(&mut self) -> Option<Self::Item> {
+            let no_vectors = random::<usize>() % 20;
+            let mut elems = vec![];
+            for _ in 0..no_vectors {
+                let key = Uuid::new_v4().to_string();
+                let labels = LabelDictionary::new(vec![]);
+                let vector = (0..self.dimension)
+                    .map(|_| random::<f32>())
+                    .collect::<Vec<_>>();
+                elems.push(Elem::new(key, vector, labels, None));
+            }
+            Some(DataPoint::new(self.path, elems, None, Similarity::Cosine).unwrap())
+        }
+    }
+
+    #[test]
+    fn state_test() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let mut state = State::new();
+        let no_nodes = DataPointProducer::new(dir.path())
+            .take(5)
+            .map(|dp| {
+                let no_nodes = dp.meta().no_nodes();
+                let _ = state.add(dp);
+                no_nodes
+            })
+            .sum::<usize>();
+        assert_eq!(state.no_nodes(), no_nodes);
+        assert_eq!(state.work_stack.len(), 1);
+        assert_eq!(state.current.size(), 0);
+        let work = state.current_work_unit().unwrap();
+        let work = work
+            .iter()
+            .map(|j| (state.delete_log(*j), j.id()))
+            .collect::<Vec<_>>();
+        let new = DataPoint::merge(dir.path(), &work, Similarity::Cosine).unwrap();
+        std::mem::drop(work);
+        let _ = state.replace_work_unit(new);
+        assert!(state.current_work_unit().is_none());
+        assert_eq!(state.work_stack.len(), 0);
+        assert_eq!(state.current.size(), 1);
+        assert_eq!(state.no_nodes(), no_nodes);
+        assert_eq!(state.work_stack.len(), 0);
+        assert_eq!(state.current.size(), 1);
+        assert_eq!(state.no_nodes(), no_nodes);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/texts.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,38 +1,53 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::sync::{Arc, Mutex, MutexGuard, TryLockError};
-
-use crate::{VectorErr, VectorR};
-
-#[derive(Clone)]
-pub struct MergerWriterSync(Arc<Mutex<()>>);
-impl MergerWriterSync {
-    pub fn new() -> MergerWriterSync {
-        MergerWriterSync(Arc::new(Mutex::new(())))
-    }
-    pub fn try_to_start_working(&self) -> VectorR<MutexGuard<'_, ()>> {
-        match self.0.try_lock() {
-            Ok(lock) => Ok(lock),
-            Err(TryLockError::Poisoned(poisoned)) => Ok(poisoned.into_inner()),
-            Err(TryLockError::WouldBlock) => Err(VectorErr::WorkDelayed),
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use crate::prelude::*;
+use crate::protos::*;
+
+pub type TextsReaderPointer = Arc<dyn FieldReader>;
+pub type TextsWriterPointer = Arc<RwLock<dyn FieldWriter>>;
+pub struct TextConfig {
+    pub path: PathBuf,
+}
+
+pub struct DocumentIterator(Box<dyn Iterator<Item = DocumentItem> + Send>);
+impl DocumentIterator {
+    pub fn new<I>(inner: I) -> DocumentIterator
+    where I: Iterator<Item = DocumentItem> + Send + 'static {
+        DocumentIterator(Box::new(inner))
+    }
+}
+impl Iterator for DocumentIterator {
+    type Item = DocumentItem;
+    fn next(&mut self) -> Option<Self::Item> {
+        self.0.next()
+    }
+}
+
+pub trait FieldReader:
+    ReaderChild<Request = DocumentSearchRequest, Response = DocumentSearchResponse>
+{
+    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait FieldWriter: WriterChild {}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,185 +1,189 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashMap;
-use std::time::SystemTime;
-
-use serde::{Deserialize, Serialize};
-
-#[derive(Default, Clone, Serialize, Deserialize)]
-pub struct DTrie {
-    value: Option<SystemTime>,
-    go_table: HashMap<u8, Box<DTrie>>,
-}
-impl DTrie {
-    fn inner_get(&self, key: &[u8], current: Option<SystemTime>) -> Option<SystemTime> {
-        let current = std::cmp::max(current, self.value);
-        let [head, tail @ ..] = key else { return current };
-        let Some(node) = self.go_table.get(head) else { return current};
-        node.inner_get(tail, current)
-    }
-    fn inner_prune(&mut self, time: SystemTime) -> bool {
-        self.value = self.value.filter(|v| *v > time);
-        self.go_table = std::mem::take(&mut self.go_table)
-            .into_iter()
-            .map(|(k, mut v)| (v.inner_prune(time), k, v))
-            .filter(|v| !v.0)
-            .map(|v| (v.1, v.2))
-            .collect();
-        self.value.is_none() && self.go_table.is_empty()
-    }
-    pub fn new() -> DTrie {
-        DTrie::default()
-    }
-    pub fn insert(&mut self, key: &[u8], value: SystemTime) {
-        match key {
-            [] => {
-                self.value = Some(value);
-                self.go_table.clear();
-            }
-            [head, tail @ ..] => {
-                self.go_table
-                    .entry(*head)
-                    .or_insert_with(|| Box::new(DTrie::new()))
-                    .as_mut()
-                    .insert(tail, value);
-            }
-        }
-    }
-    pub fn get(&self, key: &[u8]) -> Option<SystemTime> {
-        self.inner_get(key, None)
-    }
-    pub fn prune(&mut self, time: SystemTime) {
-        self.inner_prune(time);
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::time::Duration;
-
-    use super::*;
-
-    const KEY: &str = "key";
-    const N0: &str = "key_0";
-    const N1: &str = "key_1";
-    const N2: &str = "key_2";
-
-    #[test]
-    fn insert_search() {
-        let tplus0 = SystemTime::now();
-        let tplus1 = tplus0 + Duration::from_secs(1);
-        let tplus2 = tplus0 + Duration::from_secs(2);
-        let tplus3 = tplus0 + Duration::from_secs(3);
-
-        // Time matches the prefix order
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-
-        // Prefixes overwrite previous values
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N2.as_bytes(), tplus0);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus0));
-    }
-    #[test]
-    fn prune() {
-        let tplus0 = SystemTime::now();
-        let tplus1 = tplus0 + Duration::from_secs(1);
-        let tplus2 = tplus0 + Duration::from_secs(2);
-        let tplus3 = tplus0 + Duration::from_secs(3);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus0);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus1);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus2);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), None);
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus3);
-        assert_eq!(trie.get(N2.as_bytes()), None);
-        assert_eq!(trie.get(N1.as_bytes()), None);
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashMap;
+use std::time::SystemTime;
+
+use serde::{Deserialize, Serialize};
+
+#[derive(Default, Clone, Serialize, Deserialize)]
+pub struct DTrie {
+    value: Option<SystemTime>,
+    go_table: HashMap<u8, Box<DTrie>>,
+}
+impl DTrie {
+    fn inner_get(&self, key: &[u8], current: Option<SystemTime>) -> Option<SystemTime> {
+        let current = std::cmp::max(current, self.value);
+        let [head, tail @ ..] = key else {
+            return current;
+        };
+        let Some(node) = self.go_table.get(head) else {
+            return current;
+        };
+        node.inner_get(tail, current)
+    }
+    fn inner_prune(&mut self, time: SystemTime) -> bool {
+        self.value = self.value.filter(|v| *v > time);
+        self.go_table = std::mem::take(&mut self.go_table)
+            .into_iter()
+            .map(|(k, mut v)| (v.inner_prune(time), k, v))
+            .filter(|v| !v.0)
+            .map(|v| (v.1, v.2))
+            .collect();
+        self.value.is_none() && self.go_table.is_empty()
+    }
+    pub fn new() -> DTrie {
+        DTrie::default()
+    }
+    pub fn insert(&mut self, key: &[u8], value: SystemTime) {
+        match key {
+            [] => {
+                self.value = Some(value);
+                self.go_table.clear();
+            }
+            [head, tail @ ..] => {
+                self.go_table
+                    .entry(*head)
+                    .or_insert_with(|| Box::new(DTrie::new()))
+                    .as_mut()
+                    .insert(tail, value);
+            }
+        }
+    }
+    pub fn get(&self, key: &[u8]) -> Option<SystemTime> {
+        self.inner_get(key, None)
+    }
+    pub fn prune(&mut self, time: SystemTime) {
+        self.inner_prune(time);
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::time::Duration;
+
+    use super::*;
+
+    const KEY: &str = "key";
+    const N0: &str = "key_0";
+    const N1: &str = "key_1";
+    const N2: &str = "key_2";
+
+    #[test]
+    fn insert_search() {
+        let tplus0 = SystemTime::now();
+        let tplus1 = tplus0 + Duration::from_secs(1);
+        let tplus2 = tplus0 + Duration::from_secs(2);
+        let tplus3 = tplus0 + Duration::from_secs(3);
+
+        // Time matches the prefix order
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+
+        // Prefixes overwrite previous values
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N2.as_bytes(), tplus0);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus0));
+    }
+    #[test]
+    fn prune() {
+        let tplus0 = SystemTime::now();
+        let tplus1 = tplus0 + Duration::from_secs(1);
+        let tplus2 = tplus0 + Duration::from_secs(2);
+        let tplus3 = tplus0 + Duration::from_secs(3);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus0);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus1);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus2);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), None);
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus3);
+        assert_eq!(trie.get(N2.as_bytes()), None);
+        assert_eq!(trie.get(N1.as_bytes()), None);
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,384 +1,384 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::cmp::Ordering;
-use std::io::{self, Seek, SeekFrom, Write};
-
-use super::usize_utils::*;
-
-// A key-value store schema.
-// The data is arrange in a way such that the following operations can be performed:
-// -> Given a key, find its value. O(log(n))
-// -> Given an id, find its value. O(1).
-// N = Address size in bytes.
-// Serialization schema:
-//
-// [ number of values: N bytes in little endian.   ] --> Header
-// [ sequence of pointers sorted by the keys.      ]
-// [ slots: in sequence.                           ]
-//
-// A pointer in idx is the start of a value, stored N bytes in little endian)
-// It might not be obvious why the list of pointers is needed. It is because the slots may be of
-// variable length, which would render a binary search imposible. Idx elements do have a fixed
-// length (N bytes).
-
-const N: usize = USIZE_LEN;
-const HEADER_LEN: usize = N;
-const POINTER_LEN: usize = N;
-pub type Pointer = usize;
-pub type HeaderE = usize;
-
-// Given an index i, the start of its element on idx is:
-//              [i*IDXE_LEN + HEADER_LEN]
-pub fn get_pointer(x: &[u8], i: usize) -> Pointer {
-    let start = (i * POINTER_LEN) + HEADER_LEN;
-    let end = start + POINTER_LEN;
-    let mut buff = [0; POINTER_LEN];
-    buff.copy_from_slice(&x[start..end]);
-    usize::from_le_bytes(buff)
-}
-
-// O(1)
-pub fn get_no_elems(x: &[u8]) -> HeaderE {
-    usize_from_slice_le(&x[..HEADER_LEN])
-}
-
-pub trait Slot {
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8];
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering;
-    // The function should split x at i, being i the index where
-    // x[0],..,x[i] is a valid representation of a slot value.
-    // i is ensured to exists.
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]);
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn keep_in_merge(&self, _: &[u8]) -> bool {
-        true
-    }
-    // Is a mistake to assume that x and y do not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid Slot.
-    // Is safe to assume that there is an j such that y[0],.., y[k] is a valid Slot.
-    fn cmp_slot(&self, x: &[u8], y: &[u8]) -> Ordering {
-        let y_key = self.get_key(y);
-        self.cmp_keys(x, y_key)
-    }
-}
-
-pub trait KVElem {
-    fn serialized_len(&self) -> usize;
-    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()>;
-}
-
-impl<T: AsRef<[u8]>> KVElem for T {
-    fn serialized_len(&self) -> usize {
-        self.as_ref().len()
-    }
-    fn serialize_into<W: io::Write>(self, mut w: W) -> io::Result<()> {
-        w.write_all(self.as_ref())
-    }
-}
-
-#[allow(unused)]
-pub fn new_key_value<S>(slots: Vec<S>) -> Vec<u8>
-where S: KVElem {
-    let mut buf = vec![];
-    create_key_value(&mut buf, slots).unwrap();
-    buf
-}
-// Slots are expected to be sorted by their key.
-pub fn create_key_value<S, W>(mut at: W, slots: Vec<S>) -> io::Result<()>
-where
-    S: KVElem,
-    W: Write,
-{
-    let no_values: [u8; POINTER_LEN] = slots.len().to_le_bytes();
-    at.write_all(&no_values)?;
-    slots.iter().try_fold::<_, _, io::Result<_>>(
-        HEADER_LEN + (slots.len() * POINTER_LEN),
-        |pos, slot| {
-            let pbytes: [u8; POINTER_LEN] = pos.to_le_bytes();
-            at.write_all(&pbytes)?;
-            Ok(pos + slot.serialized_len())
-        },
-    )?;
-    slots
-        .into_iter()
-        .try_for_each(|slot| slot.serialize_into(&mut at))
-}
-
-// O(log n) where n is the number of slots in src.
-#[allow(unused)]
-pub fn search_by_key<S: Slot>(interface: S, src: &[u8], key: &[u8]) -> Option<usize> {
-    let number_of_values = get_no_elems(src);
-    let mut start = 0;
-    let mut end = number_of_values;
-    let mut found = None;
-    while start < end && found.is_none() {
-        let m = start + ((end - start) / 2);
-        let slot_start = get_pointer(src, m);
-        let slot = &src[slot_start..];
-        match interface.cmp_keys(slot, key) {
-            Ordering::Equal => {
-                found = Some(m);
-            }
-            Ordering::Less => {
-                start = m + 1;
-            }
-            Ordering::Greater => {
-                end = m;
-            }
-        }
-    }
-    found
-}
-
-// O(1)
-pub fn get_value<S: Slot>(interface: S, src: &[u8], id: usize) -> &[u8] {
-    let pointer = get_pointer(src, id);
-    interface.read_exact(&src[pointer..]).0
-}
-
-// Returns all the keys stored at the serialized key-value 'x'
-// O(1)
-pub fn get_keys<'a, S: Slot + Copy + 'a>(
-    interface: S,
-    x: &'a [u8],
-) -> impl Iterator<Item = &'a [u8]> {
-    (0..get_no_elems(x))
-        .map(move |i| get_value(interface, x, i))
-        .map(move |v| interface.get_key(v))
-}
-
-fn transfer_elem<S, R>(
-    interface: S,
-    at: &mut R,
-    from: &[u8],
-    id: Pointer,
-    writen_elems: usize,
-    crnt_length: usize,
-) -> io::Result<usize>
-where
-    S: Slot,
-    R: Write + Seek,
-{
-    let idx_slot = (HEADER_LEN + (writen_elems * POINTER_LEN)) as u64;
-    let value = get_value::<S>(interface, from, id);
-    at.seek(SeekFrom::Start(idx_slot))?;
-    at.write_all(&crnt_length.to_le_bytes())?;
-    at.seek(SeekFrom::Start(crnt_length as u64))?;
-    at.write_all(value)?;
-    Ok(crnt_length + value.len())
-}
-fn get_metrics<S: Slot>(interface: S, source: &[u8]) -> (usize, usize) {
-    let len = get_no_elems(source);
-    let mut value_space = 0;
-    let mut no_elems = 0;
-    for id in 0..len {
-        let ptr = get_pointer(source, id);
-        let (elem, _) = interface.read_exact(&source[ptr..]);
-        if interface.keep_in_merge(elem) {
-            value_space += elem.len();
-            no_elems += 1;
-        }
-    }
-    (no_elems, value_space)
-}
-
-// Merge algorithm for n key-value stores.
-// WARNING: In case of keys duplicatied keys it favors the contents of the first slot.
-// Returns the number of elements merged into the file.
-pub fn merge<S, R>(recepient: &mut R, producers: Vec<(S, &[u8])>) -> io::Result<usize>
-where
-    S: Slot + Copy,
-    R: Write + Seek,
-{
-    let lens = producers
-        .iter()
-        .copied()
-        .map(|(_, data)| data)
-        .map(get_no_elems)
-        .collect::<Vec<_>>();
-
-    // The number of elements that will remain at the merged file
-    // needs to be computed so the space is reserved.
-    let (no_elems, value_space) = producers
-        .iter()
-        .copied()
-        .map(|(interface, p)| get_metrics::<S>(interface, p))
-        .fold((0, 0), |(ne, vs), (ne_p, vs_p)| (ne + ne_p, vs + vs_p));
-
-    // Reserve space
-    let total_space = HEADER_LEN + (POINTER_LEN * no_elems) + value_space;
-    for _ in 0..total_space {
-        recepient.write_all(&[0])?;
-    }
-
-    // Merge loop
-    let mut writen_elems = 0;
-    let mut crnt_length = HEADER_LEN + (POINTER_LEN * no_elems);
-    let mut ids = vec![0usize; producers.len()];
-
-    while ids
-        .iter()
-        .copied()
-        .zip(lens.iter().copied())
-        .any(|(id, len)| id < len)
-    {
-        let min_data = producers
-            .iter()
-            .copied()
-            .zip(ids.iter().copied())
-            .zip(lens.iter().copied())
-            .filter(|((_, x_id), x_len)| *x_id < *x_len)
-            .map(|((x, x_id), _)| (x, x_id, get_pointer(x.1, x_id)))
-            .filter(|((interface, x), _, x_ptr)| interface.keep_in_merge(&x[*x_ptr..]))
-            .min_by(|(x, _, x_ptr), (y, _, y_ptr)| x.0.cmp_slot(&x.1[*x_ptr..], &y.1[*y_ptr..]));
-        producers
-            .iter()
-            .copied()
-            .zip(ids.iter_mut())
-            .zip(lens.iter().copied())
-            .filter(|((_, x_id), x_len)| **x_id < *x_len)
-            .map(|((x, x_id), _)| (x, get_pointer(x.1, *x_id), x_id))
-            .for_each(|((interface, x), x_ptr, x_id)| {
-                let is_equal = min_data.map(|(min, _, min_ptr)| {
-                    interface.cmp_slot(&x[x_ptr..], &min.1[min_ptr..]).is_eq()
-                });
-                if !interface.keep_in_merge(&x[x_ptr..]) {
-                    *x_id += 1;
-                } else if is_equal.unwrap_or_default() {
-                    *x_id += 1
-                }
-            });
-        if let Some(((interface, min), min_id, _)) = min_data {
-            crnt_length =
-                transfer_elem(interface, recepient, min, min_id, writen_elems, crnt_length)?;
-            writen_elems += 1;
-        }
-    }
-    // Write the number of elements
-    recepient.seek(SeekFrom::Start(0))?;
-    recepient.write_all(&writen_elems.to_le_bytes())?;
-    recepient.seek(SeekFrom::Start(0)).unwrap();
-    recepient.flush()?;
-    Ok(writen_elems)
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    // u32 numbers in big-endian (so cmp is faster)
-    #[derive(Clone, Copy)]
-    pub struct TElem;
-    impl Slot for TElem {
-        fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-            &x[0..4]
-        }
-        fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering {
-            x[0..4].cmp(&key[0..4])
-        }
-        fn read_exact<'a>(&self, x: &'a [u8]) -> (&'a [u8], &'a [u8]) {
-            x.split_at(4)
-        }
-    }
-
-    fn store_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
-        let no_values = get_no_elems(buf);
-        assert_eq!(no_values, expected.len());
-        for (i, item) in expected.iter().enumerate() {
-            let value_ptr = get_pointer(buf, i);
-            let (exact, _) = TElem.read_exact(&buf[value_ptr..]);
-            assert_eq!(exact, item.as_ref());
-        }
-    }
-
-    fn retrieval_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
-        let interface = TElem;
-        let mut expected_keys = vec![];
-        for i in 0..expected.len() {
-            let id =
-                search_by_key(interface, buf, interface.get_key(expected[i].as_ref())).unwrap();
-            let value = get_value(interface, buf, id);
-            let key = interface.get_key(value);
-            let (head, tail) = interface.read_exact(value);
-            expected_keys.push(key);
-            assert_eq!(id, i);
-            assert_eq!(value, head);
-            assert_eq!(tail, &[] as &[u8]);
-            assert_eq!(value, expected[id].as_ref());
-        }
-        let got_keys = get_keys(interface, buf).collect::<Vec<_>>();
-        assert_eq!(expected_keys, got_keys);
-    }
-    #[test]
-    fn store_test() {
-        let elems: [u32; 5] = [0, 1, 2, 3, 4];
-        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
-        let mut buf = Vec::new();
-        create_key_value(&mut buf, encoded.clone()).unwrap();
-        store_checks(&encoded, &buf);
-    }
-    #[test]
-    fn retrieval_test() {
-        let elems: [u32; 5] = [0, 1, 2, 3, 4];
-        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
-        let mut buf = Vec::new();
-        create_key_value(&mut buf, encoded.clone()).unwrap();
-        store_checks(&encoded, &buf);
-        retrieval_checks(&encoded, &buf);
-    }
-    #[test]
-    fn merge_test() {
-        use std::io::Read;
-        let v0: Vec<_> = [0u32, 2, 4].iter().map(|x| x.to_be_bytes()).collect();
-        let v1: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
-        let v2: Vec<_> = [8u32, 9, 10, 11].iter().map(|x| x.to_be_bytes()).collect();
-        let v3: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
-        let expected: Vec<_> = [0u32, 1, 2, 4, 5, 7, 8, 9, 10, 11]
-            .iter()
-            .map(|x| x.to_be_bytes())
-            .collect();
-        let mut v0_store = vec![];
-        let mut v1_store = vec![];
-        let mut v2_store = vec![];
-        let mut v3_store = vec![];
-        create_key_value(&mut v0_store, v0).unwrap();
-        create_key_value(&mut v1_store, v1).unwrap();
-        create_key_value(&mut v2_store, v2).unwrap();
-        create_key_value(&mut v3_store, v3).unwrap();
-        let mut file = tempfile::tempfile().unwrap();
-        let elems: Vec<_> = [&v0_store, &v1_store, &v2_store, &v3_store]
-            .into_iter()
-            .map(|e| (TElem, e.as_slice()))
-            .collect();
-        merge(&mut file, elems).unwrap();
-        let mut buf = vec![];
-        file.read_to_end(&mut buf).unwrap();
-        store_checks(&expected, &buf);
-        retrieval_checks(&expected, &buf);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::cmp::Ordering;
+use std::io::{self, Seek, SeekFrom, Write};
+
+use super::usize_utils::*;
+
+// A key-value store schema.
+// The data is arrange in a way such that the following operations can be performed:
+// -> Given a key, find its value. O(log(n))
+// -> Given an id, find its value. O(1).
+// N = Address size in bytes.
+// Serialization schema:
+//
+// [ number of values: N bytes in little endian.   ] --> Header
+// [ sequence of pointers sorted by the keys.      ]
+// [ slots: in sequence.                           ]
+//
+// A pointer in idx is the start of a value, stored N bytes in little endian)
+// It might not be obvious why the list of pointers is needed. It is because the slots may be of
+// variable length, which would render a binary search imposible. Idx elements do have a fixed
+// length (N bytes).
+
+const N: usize = USIZE_LEN;
+const HEADER_LEN: usize = N;
+const POINTER_LEN: usize = N;
+pub type Pointer = usize;
+pub type HeaderE = usize;
+
+// Given an index i, the start of its element on idx is:
+//              [i*IDXE_LEN + HEADER_LEN]
+pub fn get_pointer(x: &[u8], i: usize) -> Pointer {
+    let start = (i * POINTER_LEN) + HEADER_LEN;
+    let end = start + POINTER_LEN;
+    let mut buff = [0; POINTER_LEN];
+    buff.copy_from_slice(&x[start..end]);
+    usize::from_le_bytes(buff)
+}
+
+// O(1)
+pub fn get_no_elems(x: &[u8]) -> HeaderE {
+    usize_from_slice_le(&x[..HEADER_LEN])
+}
+
+pub trait Slot {
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8];
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering;
+    // The function should split x at i, being i the index where
+    // x[0],..,x[i] is a valid representation of a slot value.
+    // i is ensured to exists.
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]);
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn keep_in_merge(&self, _: &[u8]) -> bool {
+        true
+    }
+    // Is a mistake to assume that x and y do not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid Slot.
+    // Is safe to assume that there is an j such that y[0],.., y[k] is a valid Slot.
+    fn cmp_slot(&self, x: &[u8], y: &[u8]) -> Ordering {
+        let y_key = self.get_key(y);
+        self.cmp_keys(x, y_key)
+    }
+}
+
+pub trait KVElem {
+    fn serialized_len(&self) -> usize;
+    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()>;
+}
+
+impl<T: AsRef<[u8]>> KVElem for T {
+    fn serialized_len(&self) -> usize {
+        self.as_ref().len()
+    }
+    fn serialize_into<W: io::Write>(self, mut w: W) -> io::Result<()> {
+        w.write_all(self.as_ref())
+    }
+}
+
+#[allow(unused)]
+pub fn new_key_value<S>(slots: Vec<S>) -> Vec<u8>
+where S: KVElem {
+    let mut buf = vec![];
+    create_key_value(&mut buf, slots).unwrap();
+    buf
+}
+// Slots are expected to be sorted by their key.
+pub fn create_key_value<S, W>(mut at: W, slots: Vec<S>) -> io::Result<()>
+where
+    S: KVElem,
+    W: Write,
+{
+    let no_values: [u8; POINTER_LEN] = slots.len().to_le_bytes();
+    at.write_all(&no_values)?;
+    slots.iter().try_fold::<_, _, io::Result<_>>(
+        HEADER_LEN + (slots.len() * POINTER_LEN),
+        |pos, slot| {
+            let pbytes: [u8; POINTER_LEN] = pos.to_le_bytes();
+            at.write_all(&pbytes)?;
+            Ok(pos + slot.serialized_len())
+        },
+    )?;
+    slots
+        .into_iter()
+        .try_for_each(|slot| slot.serialize_into(&mut at))
+}
+
+// O(log n) where n is the number of slots in src.
+#[allow(unused)]
+pub fn search_by_key<S: Slot>(interface: S, src: &[u8], key: &[u8]) -> Option<usize> {
+    let number_of_values = get_no_elems(src);
+    let mut start = 0;
+    let mut end = number_of_values;
+    let mut found = None;
+    while start < end && found.is_none() {
+        let m = start + ((end - start) / 2);
+        let slot_start = get_pointer(src, m);
+        let slot = &src[slot_start..];
+        match interface.cmp_keys(slot, key) {
+            Ordering::Equal => {
+                found = Some(m);
+            }
+            Ordering::Less => {
+                start = m + 1;
+            }
+            Ordering::Greater => {
+                end = m;
+            }
+        }
+    }
+    found
+}
+
+// O(1)
+pub fn get_value<S: Slot>(interface: S, src: &[u8], id: usize) -> &[u8] {
+    let pointer = get_pointer(src, id);
+    interface.read_exact(&src[pointer..]).0
+}
+
+// Returns all the keys stored at the serialized key-value 'x'
+// O(1)
+pub fn get_keys<'a, S: Slot + Copy + 'a>(
+    interface: S,
+    x: &'a [u8],
+) -> impl Iterator<Item = &'a [u8]> {
+    (0..get_no_elems(x))
+        .map(move |i| get_value(interface, x, i))
+        .map(move |v| interface.get_key(v))
+}
+
+fn transfer_elem<S, R>(
+    interface: S,
+    at: &mut R,
+    from: &[u8],
+    id: Pointer,
+    writen_elems: usize,
+    crnt_length: usize,
+) -> io::Result<usize>
+where
+    S: Slot,
+    R: Write + Seek,
+{
+    let idx_slot = (HEADER_LEN + (writen_elems * POINTER_LEN)) as u64;
+    let value = get_value::<S>(interface, from, id);
+    at.seek(SeekFrom::Start(idx_slot))?;
+    at.write_all(&crnt_length.to_le_bytes())?;
+    at.seek(SeekFrom::Start(crnt_length as u64))?;
+    at.write_all(value)?;
+    Ok(crnt_length + value.len())
+}
+fn get_metrics<S: Slot>(interface: S, source: &[u8]) -> (usize, usize) {
+    let len = get_no_elems(source);
+    let mut value_space = 0;
+    let mut no_elems = 0;
+    for id in 0..len {
+        let ptr = get_pointer(source, id);
+        let (elem, _) = interface.read_exact(&source[ptr..]);
+        if interface.keep_in_merge(elem) {
+            value_space += elem.len();
+            no_elems += 1;
+        }
+    }
+    (no_elems, value_space)
+}
+
+// Merge algorithm for n key-value stores.
+// WARNING: In case of keys duplicatied keys it favors the contents of the first slot.
+// Returns the number of elements merged into the file.
+pub fn merge<S, R>(recepient: &mut R, producers: Vec<(S, &[u8])>) -> io::Result<usize>
+where
+    S: Slot + Copy,
+    R: Write + Seek,
+{
+    let lens = producers
+        .iter()
+        .copied()
+        .map(|(_, data)| data)
+        .map(get_no_elems)
+        .collect::<Vec<_>>();
+
+    // The number of elements that will remain at the merged file
+    // needs to be computed so the space is reserved.
+    let (no_elems, value_space) = producers
+        .iter()
+        .copied()
+        .map(|(interface, p)| get_metrics::<S>(interface, p))
+        .fold((0, 0), |(ne, vs), (ne_p, vs_p)| (ne + ne_p, vs + vs_p));
+
+    // Reserve space
+    let total_space = HEADER_LEN + (POINTER_LEN * no_elems) + value_space;
+    for _ in 0..total_space {
+        recepient.write_all(&[0])?;
+    }
+
+    // Merge loop
+    let mut writen_elems = 0;
+    let mut crnt_length = HEADER_LEN + (POINTER_LEN * no_elems);
+    let mut ids = vec![0usize; producers.len()];
+
+    while ids
+        .iter()
+        .copied()
+        .zip(lens.iter().copied())
+        .any(|(id, len)| id < len)
+    {
+        let min_data = producers
+            .iter()
+            .copied()
+            .zip(ids.iter().copied())
+            .zip(lens.iter().copied())
+            .filter(|((_, x_id), x_len)| *x_id < *x_len)
+            .map(|((x, x_id), _)| (x, x_id, get_pointer(x.1, x_id)))
+            .filter(|((interface, x), _, x_ptr)| interface.keep_in_merge(&x[*x_ptr..]))
+            .min_by(|(x, _, x_ptr), (y, _, y_ptr)| x.0.cmp_slot(&x.1[*x_ptr..], &y.1[*y_ptr..]));
+        producers
+            .iter()
+            .copied()
+            .zip(ids.iter_mut())
+            .zip(lens.iter().copied())
+            .filter(|((_, x_id), x_len)| **x_id < *x_len)
+            .map(|((x, x_id), _)| (x, get_pointer(x.1, *x_id), x_id))
+            .for_each(|((interface, x), x_ptr, x_id)| {
+                let is_equal = min_data.map(|(min, _, min_ptr)| {
+                    interface.cmp_slot(&x[x_ptr..], &min.1[min_ptr..]).is_eq()
+                });
+                if !interface.keep_in_merge(&x[x_ptr..]) {
+                    *x_id += 1;
+                } else if is_equal.unwrap_or_default() {
+                    *x_id += 1
+                }
+            });
+        if let Some(((interface, min), min_id, _)) = min_data {
+            crnt_length =
+                transfer_elem(interface, recepient, min, min_id, writen_elems, crnt_length)?;
+            writen_elems += 1;
+        }
+    }
+    // Write the number of elements
+    recepient.seek(SeekFrom::Start(0))?;
+    recepient.write_all(&writen_elems.to_le_bytes())?;
+    recepient.seek(SeekFrom::Start(0)).unwrap();
+    recepient.flush()?;
+    Ok(writen_elems)
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    // u32 numbers in big-endian (so cmp is faster)
+    #[derive(Clone, Copy)]
+    pub struct TElem;
+    impl Slot for TElem {
+        fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+            &x[0..4]
+        }
+        fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering {
+            x[0..4].cmp(&key[0..4])
+        }
+        fn read_exact<'a>(&self, x: &'a [u8]) -> (&'a [u8], &'a [u8]) {
+            x.split_at(4)
+        }
+    }
+
+    fn store_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
+        let no_values = get_no_elems(buf);
+        assert_eq!(no_values, expected.len());
+        for (i, item) in expected.iter().enumerate() {
+            let value_ptr = get_pointer(buf, i);
+            let (exact, _) = TElem.read_exact(&buf[value_ptr..]);
+            assert_eq!(exact, item.as_ref());
+        }
+    }
+
+    fn retrieval_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
+        let interface = TElem;
+        let mut expected_keys = vec![];
+        for i in 0..expected.len() {
+            let id =
+                search_by_key(interface, buf, interface.get_key(expected[i].as_ref())).unwrap();
+            let value = get_value(interface, buf, id);
+            let key = interface.get_key(value);
+            let (head, tail) = interface.read_exact(value);
+            expected_keys.push(key);
+            assert_eq!(id, i);
+            assert_eq!(value, head);
+            assert_eq!(tail, &[] as &[u8]);
+            assert_eq!(value, expected[id].as_ref());
+        }
+        let got_keys = get_keys(interface, buf).collect::<Vec<_>>();
+        assert_eq!(expected_keys, got_keys);
+    }
+    #[test]
+    fn store_test() {
+        let elems: [u32; 5] = [0, 1, 2, 3, 4];
+        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
+        let mut buf = Vec::new();
+        create_key_value(&mut buf, encoded.clone()).unwrap();
+        store_checks(&encoded, &buf);
+    }
+    #[test]
+    fn retrieval_test() {
+        let elems: [u32; 5] = [0, 1, 2, 3, 4];
+        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
+        let mut buf = Vec::new();
+        create_key_value(&mut buf, encoded.clone()).unwrap();
+        store_checks(&encoded, &buf);
+        retrieval_checks(&encoded, &buf);
+    }
+    #[test]
+    fn merge_test() {
+        use std::io::Read;
+        let v0: Vec<_> = [0u32, 2, 4].iter().map(|x| x.to_be_bytes()).collect();
+        let v1: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
+        let v2: Vec<_> = [8u32, 9, 10, 11].iter().map(|x| x.to_be_bytes()).collect();
+        let v3: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
+        let expected: Vec<_> = [0u32, 1, 2, 4, 5, 7, 8, 9, 10, 11]
+            .iter()
+            .map(|x| x.to_be_bytes())
+            .collect();
+        let mut v0_store = vec![];
+        let mut v1_store = vec![];
+        let mut v2_store = vec![];
+        let mut v3_store = vec![];
+        create_key_value(&mut v0_store, v0).unwrap();
+        create_key_value(&mut v1_store, v1).unwrap();
+        create_key_value(&mut v2_store, v2).unwrap();
+        create_key_value(&mut v3_store, v3).unwrap();
+        let mut file = tempfile::tempfile().unwrap();
+        let elems: Vec<_> = [&v0_store, &v1_store, &v2_store, &v3_store]
+            .into_iter()
+            .map(|e| (TElem, e.as_slice()))
+            .collect();
+        merge(&mut file, elems).unwrap();
+        let mut buf = vec![];
+        file.read_to_end(&mut buf).unwrap();
+        store_checks(&expected, &buf);
+        retrieval_checks(&expected, &buf);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/mod.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod dtrie_ram;
-pub mod key_value;
-pub mod trie;
-pub mod trie_ram;
-pub mod vector;
-
-pub mod usize_utils {
-    pub const USIZE_LEN: usize = (usize::BITS / 8) as usize;
-    pub fn usize_from_slice_le(v: &[u8]) -> usize {
-        let mut buff = [0; USIZE_LEN];
-        buff.copy_from_slice(v);
-        usize::from_le_bytes(buff)
-    }
-}
-
-pub trait DeleteLog: std::marker::Sync {
-    fn is_deleted(&self, _: &[u8]) -> bool;
-}
-
-impl<'a, D: DeleteLog> DeleteLog for &'a D {
-    fn is_deleted(&self, x: &[u8]) -> bool {
-        D::is_deleted(self, x)
-    }
-}
-
-impl DeleteLog for dtrie_ram::DTrie {
-    fn is_deleted(&self, key: &[u8]) -> bool {
-        self.get(key).is_some()
-    }
-}
-
-impl<Dl: DeleteLog, S: key_value::Slot> key_value::Slot for (Dl, S) {
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-        self.1.get_key(x)
-    }
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
-        self.1.cmp_keys(x, key)
-    }
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
-        self.1.read_exact(x)
-    }
-    fn keep_in_merge(&self, x: &[u8]) -> bool {
-        let key = self.1.get_key(x);
-        !self.0.is_deleted(key)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod dtrie_ram;
+pub mod key_value;
+pub mod trie;
+pub mod trie_ram;
+pub mod vector;
+
+pub mod usize_utils {
+    pub const USIZE_LEN: usize = (usize::BITS / 8) as usize;
+    pub fn usize_from_slice_le(v: &[u8]) -> usize {
+        let mut buff = [0; USIZE_LEN];
+        buff.copy_from_slice(v);
+        usize::from_le_bytes(buff)
+    }
+}
+
+pub trait DeleteLog: std::marker::Sync {
+    fn is_deleted(&self, _: &[u8]) -> bool;
+}
+
+impl<'a, D: DeleteLog> DeleteLog for &'a D {
+    fn is_deleted(&self, x: &[u8]) -> bool {
+        D::is_deleted(self, x)
+    }
+}
+
+impl DeleteLog for dtrie_ram::DTrie {
+    fn is_deleted(&self, key: &[u8]) -> bool {
+        self.get(key).is_some()
+    }
+}
+
+impl<Dl: DeleteLog, S: key_value::Slot> key_value::Slot for (Dl, S) {
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+        self.1.get_key(x)
+    }
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
+        self.1.cmp_keys(x, key)
+    }
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
+        self.1.read_exact(x)
+    }
+    fn keep_in_merge(&self, x: &[u8]) -> bool {
+        let key = self.1.get_key(x);
+        !self.0.is_deleted(key)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/trie.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/trie.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,180 +1,180 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io;
-
-use super::trie_ram::*;
-use super::usize_utils::*;
-
-const ADJ_HEADER: usize = 1 + USIZE_LEN;
-const EDGE_LEN: usize = 1 + USIZE_LEN;
-
-const IS_FINAL: (usize, usize) = (0, 1);
-const LENGTH: (usize, usize) = (IS_FINAL.1, IS_FINAL.1 + USIZE_LEN);
-const TABLE: usize = LENGTH.1;
-
-fn get_node_ptr(trie: &[u8], node: usize) -> usize {
-    let start = trie.len() - ((node + 1) * USIZE_LEN);
-    usize_from_slice_le(&trie[start..(start + USIZE_LEN)])
-}
-
-// A serialized trie is an value section and a index section and the length.
-// -> len: usize little endian
-// Value section:
-// -> Is final: 1 byte, if the value stored is 1 then is final.
-// -> no_connexions:  usize little endian.
-// per connexion:
-// -> 1 byte for the edge label
-// -> 1 usize little endian for the targeted node.
-// Index section: 1 usize in little per node in order, containing the
-// address of the value section.
-pub fn serialize_into<W: io::Write>(mut buf: W, trie: Trie) -> io::Result<()> {
-    use std::collections::HashMap;
-    let no_nodes = trie.len();
-    let len = serialized_len(&trie);
-    let mut indexing = HashMap::new();
-    let mut byte_offset = 0;
-    buf.write_all(&len.to_le_bytes())?;
-    byte_offset += USIZE_LEN;
-    for (node, (is_final, adjacency)) in trie.into_iter().enumerate() {
-        indexing.insert(node, byte_offset);
-        buf.write_all(&[u8::from(is_final)])?;
-        buf.write_all(&adjacency.len().to_le_bytes())?;
-        byte_offset += ADJ_HEADER;
-        for (edge, node) in adjacency {
-            buf.write_all(&edge.to_le_bytes())?;
-            buf.write_all(&node.to_le_bytes())?;
-            byte_offset += EDGE_LEN;
-        }
-    }
-    for node in (0..no_nodes).rev() {
-        let is_in = indexing[&node];
-        buf.write_all(&is_in.to_le_bytes())?;
-        byte_offset += USIZE_LEN;
-    }
-    buf.flush()
-}
-pub fn serialized_len(trie: &Trie) -> usize {
-    USIZE_LEN
-        + trie
-            .iter()
-            .map(|(_, table)| EDGE_LEN * table.len())
-            .map(|table_len| table_len + ADJ_HEADER)
-            .map(|value| value + USIZE_LEN)
-            .sum::<usize>()
-}
-pub fn serialize(trie: Trie) -> Vec<u8> {
-    let mut buf = vec![];
-    serialize_into(&mut buf, trie).unwrap();
-    buf
-}
-pub fn has_word(trie: &[u8], word: &[u8]) -> bool {
-    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
-    search(&trie[0..len], 0, word)
-}
-
-pub fn decompress(trie: &[u8]) -> Vec<String> {
-    let mut collector = vec![];
-    let mut current = vec![];
-    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
-    decompress_labels(&trie[0..len], 0, &mut collector, &mut current);
-    collector
-}
-
-fn decompress_labels(trie: &[u8], node: usize, collector: &mut Vec<String>, current: &mut Vec<u8>) {
-    let node_ptr = get_node_ptr(trie, node);
-    if trie[node_ptr] == 1 {
-        let label = String::from_utf8_lossy(current).to_string();
-        collector.push(label);
-    }
-    let offset = &trie[node_ptr..];
-    let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
-    let adjacency = &offset[TABLE..];
-    let mut i = 0;
-    while i < length {
-        let position = i * EDGE_LEN;
-        let number_s = position + 1;
-        let number_e = number_s + USIZE_LEN;
-        let new_byte = adjacency[position];
-        let new_node = usize_from_slice_le(&adjacency[number_s..number_e]);
-        current.push(new_byte);
-        decompress_labels(trie, new_node, collector, current);
-        current.pop();
-        i += 1;
-    }
-}
-
-fn search(trie: &[u8], node: usize, word: &[u8]) -> bool {
-    let node_ptr = get_node_ptr(trie, node);
-    match word {
-        [] => trie[node_ptr] == 1,
-        [head, tail @ ..] => {
-            let offset = &trie[node_ptr..];
-            let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
-            let adjacency = &offset[TABLE..];
-            let mut i = 0;
-            let mut goes_to = None;
-            while i < length && goes_to.is_none() {
-                let position = i * EDGE_LEN;
-                if *head == adjacency[position] {
-                    let number_s = position + 1;
-                    let number_e = number_s + USIZE_LEN;
-                    goes_to = Some(usize_from_slice_le(&adjacency[number_s..number_e]));
-                }
-                i += 1;
-            }
-            match goes_to {
-                Some(new_node) => search(trie, new_node, tail),
-                None => false,
-            }
-        }
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    #[test]
-    fn create_and_search_test() {
-        let dictionary = [
-            b"WORD1".as_slice(),
-            b"WORD2".as_slice(),
-            b"WORD3".as_slice(),
-            b"ORD1".as_slice(),
-            b"BAD".as_slice(),
-            b"GOOD".as_slice(),
-        ];
-        let not_in_dictionary = [
-            b"WO1D1".as_slice(),
-            b"LORD".as_slice(),
-            b"BAF".as_slice(),
-            b"WOR".as_slice(),
-        ];
-
-        let trie = create_trie(&dictionary);
-        let trie = serialize(trie);
-        let labels = super::decompress(&trie);
-        assert!(dictionary.iter().all(|w| has_word(&trie, w)));
-        assert!(not_in_dictionary.iter().all(|w| !has_word(&trie, w)));
-
-        assert_eq!(labels.len(), dictionary.len());
-        assert!(labels.iter().all(|w| dictionary.contains(&w.as_bytes())));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io;
+
+use super::trie_ram::*;
+use super::usize_utils::*;
+
+const ADJ_HEADER: usize = 1 + USIZE_LEN;
+const EDGE_LEN: usize = 1 + USIZE_LEN;
+
+const IS_FINAL: (usize, usize) = (0, 1);
+const LENGTH: (usize, usize) = (IS_FINAL.1, IS_FINAL.1 + USIZE_LEN);
+const TABLE: usize = LENGTH.1;
+
+fn get_node_ptr(trie: &[u8], node: usize) -> usize {
+    let start = trie.len() - ((node + 1) * USIZE_LEN);
+    usize_from_slice_le(&trie[start..(start + USIZE_LEN)])
+}
+
+// A serialized trie is an value section and a index section and the length.
+// -> len: usize little endian
+// Value section:
+// -> Is final: 1 byte, if the value stored is 1 then is final.
+// -> no_connexions:  usize little endian.
+// per connexion:
+// -> 1 byte for the edge label
+// -> 1 usize little endian for the targeted node.
+// Index section: 1 usize in little per node in order, containing the
+// address of the value section.
+pub fn serialize_into<W: io::Write>(mut buf: W, trie: Trie) -> io::Result<()> {
+    use std::collections::HashMap;
+    let no_nodes = trie.len();
+    let len = serialized_len(&trie);
+    let mut indexing = HashMap::new();
+    let mut byte_offset = 0;
+    buf.write_all(&len.to_le_bytes())?;
+    byte_offset += USIZE_LEN;
+    for (node, (is_final, adjacency)) in trie.into_iter().enumerate() {
+        indexing.insert(node, byte_offset);
+        buf.write_all(&[u8::from(is_final)])?;
+        buf.write_all(&adjacency.len().to_le_bytes())?;
+        byte_offset += ADJ_HEADER;
+        for (edge, node) in adjacency {
+            buf.write_all(&edge.to_le_bytes())?;
+            buf.write_all(&node.to_le_bytes())?;
+            byte_offset += EDGE_LEN;
+        }
+    }
+    for node in (0..no_nodes).rev() {
+        let is_in = indexing[&node];
+        buf.write_all(&is_in.to_le_bytes())?;
+        byte_offset += USIZE_LEN;
+    }
+    buf.flush()
+}
+pub fn serialized_len(trie: &Trie) -> usize {
+    USIZE_LEN
+        + trie
+            .iter()
+            .map(|(_, table)| EDGE_LEN * table.len())
+            .map(|table_len| table_len + ADJ_HEADER)
+            .map(|value| value + USIZE_LEN)
+            .sum::<usize>()
+}
+pub fn serialize(trie: Trie) -> Vec<u8> {
+    let mut buf = vec![];
+    serialize_into(&mut buf, trie).unwrap();
+    buf
+}
+pub fn has_word(trie: &[u8], word: &[u8]) -> bool {
+    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
+    search(&trie[0..len], 0, word)
+}
+
+pub fn decompress(trie: &[u8]) -> Vec<String> {
+    let mut collector = vec![];
+    let mut current = vec![];
+    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
+    decompress_labels(&trie[0..len], 0, &mut collector, &mut current);
+    collector
+}
+
+fn decompress_labels(trie: &[u8], node: usize, collector: &mut Vec<String>, current: &mut Vec<u8>) {
+    let node_ptr = get_node_ptr(trie, node);
+    if trie[node_ptr] == 1 {
+        let label = String::from_utf8_lossy(current).to_string();
+        collector.push(label);
+    }
+    let offset = &trie[node_ptr..];
+    let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
+    let adjacency = &offset[TABLE..];
+    let mut i = 0;
+    while i < length {
+        let position = i * EDGE_LEN;
+        let number_s = position + 1;
+        let number_e = number_s + USIZE_LEN;
+        let new_byte = adjacency[position];
+        let new_node = usize_from_slice_le(&adjacency[number_s..number_e]);
+        current.push(new_byte);
+        decompress_labels(trie, new_node, collector, current);
+        current.pop();
+        i += 1;
+    }
+}
+
+fn search(trie: &[u8], node: usize, word: &[u8]) -> bool {
+    let node_ptr = get_node_ptr(trie, node);
+    match word {
+        [] => trie[node_ptr] == 1,
+        [head, tail @ ..] => {
+            let offset = &trie[node_ptr..];
+            let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
+            let adjacency = &offset[TABLE..];
+            let mut i = 0;
+            let mut goes_to = None;
+            while i < length && goes_to.is_none() {
+                let position = i * EDGE_LEN;
+                if *head == adjacency[position] {
+                    let number_s = position + 1;
+                    let number_e = number_s + USIZE_LEN;
+                    goes_to = Some(usize_from_slice_le(&adjacency[number_s..number_e]));
+                }
+                i += 1;
+            }
+            match goes_to {
+                Some(new_node) => search(trie, new_node, tail),
+                None => false,
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    #[test]
+    fn create_and_search_test() {
+        let dictionary = [
+            b"WORD1".as_slice(),
+            b"WORD2".as_slice(),
+            b"WORD3".as_slice(),
+            b"ORD1".as_slice(),
+            b"BAD".as_slice(),
+            b"GOOD".as_slice(),
+        ];
+        let not_in_dictionary = [
+            b"WO1D1".as_slice(),
+            b"LORD".as_slice(),
+            b"BAF".as_slice(),
+            b"WOR".as_slice(),
+        ];
+
+        let trie = create_trie(&dictionary);
+        let trie = serialize(trie);
+        let labels = super::decompress(&trie);
+        assert!(dictionary.iter().all(|w| has_word(&trie, w)));
+        assert!(not_in_dictionary.iter().all(|w| !has_word(&trie, w)));
+
+        assert_eq!(labels.len(), dictionary.len());
+        assert!(labels.iter().all(|w| dictionary.contains(&w.as_bytes())));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/data_types/vector.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/data_types/vector.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io::{Read, Write};
-type Len = u64;
-type Unit = f32;
-type Dist = f32;
-
-fn encode_length(mut buff: Vec<u8>, vec: &[Unit]) -> Vec<u8> {
-    let len = vec.len() as Len;
-    buff.write_all(&len.to_le_bytes()).unwrap();
-    buff.flush().unwrap();
-    buff
-}
-fn encode_unit(mut buff: Vec<u8>, unit: Unit) -> Vec<u8> {
-    buff.write_all(&unit.to_le_bytes()).unwrap();
-    buff.flush().unwrap();
-    buff
-}
-
-pub fn vector_len(mut x: &[u8]) -> u64 {
-    let mut buff_x = [0; 8];
-    x.read_exact(&mut buff_x).unwrap();
-    Len::from_le_bytes(buff_x)
-}
-
-pub fn cosine_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
-    let mut buff_x = [0; 8];
-    let mut buff_y = [0; 8];
-    x.read_exact(&mut buff_x).unwrap();
-    y.read_exact(&mut buff_y).unwrap();
-    let len_x = Len::from_le_bytes(buff_x);
-    let len_y = Len::from_le_bytes(buff_y);
-    assert_eq!(len_x, len_y);
-    let len = len_x;
-    let mut buff_x = [0; 4];
-    let mut buff_y = [0; 4];
-    let mut sum = 0.0;
-    let mut dem_x = 0.0;
-    let mut dem_y = 0.0;
-    for _ in 0..len {
-        x.read_exact(&mut buff_x).unwrap();
-        y.read_exact(&mut buff_y).unwrap();
-        let x_value = Unit::from_le_bytes(buff_x);
-        let y_value = Unit::from_le_bytes(buff_y);
-        sum += x_value * y_value;
-        dem_x += x_value * x_value;
-        dem_y += y_value * y_value;
-    }
-    sum / (f32::sqrt(dem_x) * f32::sqrt(dem_y))
-}
-
-pub fn dot_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
-    let mut buff_x = [0; 8];
-    let mut buff_y = [0; 8];
-    x.read_exact(&mut buff_x).unwrap();
-    y.read_exact(&mut buff_y).unwrap();
-    let len_x = Len::from_le_bytes(buff_x);
-    let len_y = Len::from_le_bytes(buff_y);
-    assert_eq!(len_x, len_y);
-    let len = len_x;
-    let mut buff_x = [0; 4];
-    let mut buff_y = [0; 4];
-    let mut sum = 0.0;
-    for _ in 0..len {
-        x.read_exact(&mut buff_x).unwrap();
-        y.read_exact(&mut buff_y).unwrap();
-        let x_value = Unit::from_le_bytes(buff_x);
-        let y_value = Unit::from_le_bytes(buff_y);
-        sum += x_value * y_value;
-    }
-    sum
-}
-
-pub fn encode_vector(vec: &[Unit]) -> Vec<u8> {
-    vec.iter()
-        .cloned()
-        .fold(encode_length(vec![], vec), encode_unit)
-}
-
-#[cfg(test)]
-mod test {
-    use super::*;
-    fn naive_cosine_similatiry(a: &[f32], b: &[f32]) -> f32 {
-        let ab: f32 = a
-            .iter()
-            .cloned()
-            .zip(b.iter().cloned())
-            .map(|(a, b)| a * b)
-            .sum();
-        let aa: f32 = a.iter().cloned().map(|a| a * a).sum();
-        let bb: f32 = b.iter().cloned().map(|b| b * b).sum();
-        ab / (f32::sqrt(aa) * f32::sqrt(bb))
-    }
-
-    fn naive_dot_similatiry(a: &[f32], b: &[f32]) -> f32 {
-        a.iter()
-            .cloned()
-            .zip(b.iter().cloned())
-            .map(|(a, b)| a * b)
-            .sum()
-    }
-
-    #[test]
-    fn cosine_test() {
-        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
-        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
-        let v0_r = encode_vector(&v0);
-        let v1_r = encode_vector(&v1);
-        assert_eq!(
-            naive_cosine_similatiry(&v0, &v1),
-            cosine_similarity(&v0_r, &v1_r)
-        );
-        assert_eq!(
-            naive_cosine_similatiry(&v0, &v0),
-            cosine_similarity(&v0_r, &v0_r)
-        );
-    }
-
-    #[test]
-    fn dot_test() {
-        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
-        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
-        let v0_r = encode_vector(&v0);
-        let v1_r = encode_vector(&v1);
-        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
-        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io::{Read, Write};
+type Len = u64;
+type Unit = f32;
+type Dist = f32;
+
+fn encode_length(mut buff: Vec<u8>, vec: &[Unit]) -> Vec<u8> {
+    let len = vec.len() as Len;
+    buff.write_all(&len.to_le_bytes()).unwrap();
+    buff.flush().unwrap();
+    buff
+}
+fn encode_unit(mut buff: Vec<u8>, unit: Unit) -> Vec<u8> {
+    buff.write_all(&unit.to_le_bytes()).unwrap();
+    buff.flush().unwrap();
+    buff
+}
+
+pub fn vector_len(mut x: &[u8]) -> u64 {
+    let mut buff_x = [0; 8];
+    x.read_exact(&mut buff_x).unwrap();
+    Len::from_le_bytes(buff_x)
+}
+
+pub fn cosine_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
+    let mut buff_x = [0; 8];
+    let mut buff_y = [0; 8];
+    x.read_exact(&mut buff_x).unwrap();
+    y.read_exact(&mut buff_y).unwrap();
+    let len_x = Len::from_le_bytes(buff_x);
+    let len_y = Len::from_le_bytes(buff_y);
+    assert_eq!(len_x, len_y);
+    let len = len_x;
+    let mut buff_x = [0; 4];
+    let mut buff_y = [0; 4];
+    let mut sum = 0.0;
+    let mut dem_x = 0.0;
+    let mut dem_y = 0.0;
+    for _ in 0..len {
+        x.read_exact(&mut buff_x).unwrap();
+        y.read_exact(&mut buff_y).unwrap();
+        let x_value = Unit::from_le_bytes(buff_x);
+        let y_value = Unit::from_le_bytes(buff_y);
+        sum += x_value * y_value;
+        dem_x += x_value * x_value;
+        dem_y += y_value * y_value;
+    }
+    sum / (f32::sqrt(dem_x) * f32::sqrt(dem_y))
+}
+
+pub fn dot_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
+    let mut buff_x = [0; 8];
+    let mut buff_y = [0; 8];
+    x.read_exact(&mut buff_x).unwrap();
+    y.read_exact(&mut buff_y).unwrap();
+    let len_x = Len::from_le_bytes(buff_x);
+    let len_y = Len::from_le_bytes(buff_y);
+    assert_eq!(len_x, len_y);
+    let len = len_x;
+    let mut buff_x = [0; 4];
+    let mut buff_y = [0; 4];
+    let mut sum = 0.0;
+    for _ in 0..len {
+        x.read_exact(&mut buff_x).unwrap();
+        y.read_exact(&mut buff_y).unwrap();
+        let x_value = Unit::from_le_bytes(buff_x);
+        let y_value = Unit::from_le_bytes(buff_y);
+        sum += x_value * y_value;
+    }
+    sum
+}
+
+pub fn encode_vector(vec: &[Unit]) -> Vec<u8> {
+    vec.iter()
+        .cloned()
+        .fold(encode_length(vec![], vec), encode_unit)
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+    fn naive_cosine_similatiry(a: &[f32], b: &[f32]) -> f32 {
+        let ab: f32 = a
+            .iter()
+            .cloned()
+            .zip(b.iter().cloned())
+            .map(|(a, b)| a * b)
+            .sum();
+        let aa: f32 = a.iter().cloned().map(|a| a * a).sum();
+        let bb: f32 = b.iter().cloned().map(|b| b * b).sum();
+        ab / (f32::sqrt(aa) * f32::sqrt(bb))
+    }
+
+    fn naive_dot_similatiry(a: &[f32], b: &[f32]) -> f32 {
+        a.iter()
+            .cloned()
+            .zip(b.iter().cloned())
+            .map(|(a, b)| a * b)
+            .sum()
+    }
+
+    #[test]
+    fn cosine_test() {
+        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
+        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
+        let v0_r = encode_vector(&v0);
+        let v1_r = encode_vector(&v1);
+        assert_eq!(
+            naive_cosine_similatiry(&v0, &v1),
+            cosine_similarity(&v0_r, &v1_r)
+        );
+        assert_eq!(
+            naive_cosine_similatiry(&v0, &v0),
+            cosine_similarity(&v0_r, &v0_r)
+        );
+    }
+
+    #[test]
+    fn dot_test() {
+        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
+        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
+        let v0_r = encode_vector(&v0);
+        let v1_r = encode_vector(&v1);
+        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
+        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/indexset/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/indexset/mod.rs`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,114 +1,114 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-mod state;
-use std::path::{Path, PathBuf};
-use std::sync::RwLock;
-
-use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
-use state::State;
-
-use crate::data_point::Similarity;
-use crate::data_point_provider::{Index, IndexCheck};
-use crate::VectorR;
-pub trait IndexKeyCollector {
-    fn add_key(&mut self, key: String);
-}
-
-pub struct IndexSet {
-    state: RwLock<State>,
-    date: RwLock<Version>,
-    location: PathBuf,
-}
-impl IndexSet {
-    pub fn new(path: &Path, with_check: IndexCheck) -> VectorR<IndexSet> {
-        if !path.exists() {
-            std::fs::create_dir(path)?;
-        }
-        fs_state::initialize_disk(path, || State::new(path.to_path_buf()))?;
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        if let IndexCheck::Sanity = with_check {
-            state.do_sanity_checks()?;
-        }
-        let index = IndexSet {
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        Ok(index)
-    }
-    pub fn remove_index(&mut self, index: &str, _: &ELock) -> VectorR<()> {
-        let mut write = self.state.write().unwrap();
-        write.remove_index(index)
-    }
-    pub fn get_or_create<'a, S>(
-        &'a mut self,
-        index: S,
-        similarity: Similarity,
-        _: &ELock,
-    ) -> VectorR<Index>
-    where
-        S: Into<std::borrow::Cow<'a, str>>,
-    {
-        let mut write = self.state.write().unwrap();
-        write.get_or_create(index, similarity)
-    }
-    fn update(&self, lock: &fs_state::Lock) -> VectorR<()> {
-        let disk_v = fs_state::crnt_version(lock)?;
-        let date = *self.date.read().unwrap();
-        if disk_v > date {
-            let new_state = fs_state::load_state(lock)?;
-            let mut state = self.state.write().unwrap();
-            let mut date = self.date.write().unwrap();
-            *state = new_state;
-            *date = disk_v;
-        }
-        Ok(())
-    }
-    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C, _: &Lock) {
-        let read = self.state.read().unwrap();
-        read.index_keys(c);
-    }
-    pub fn get(&self, index: &str, _: &Lock) -> VectorR<Option<Index>> {
-        let read = self.state.read().unwrap();
-        read.get(index)
-    }
-    pub fn get_elock(&self) -> VectorR<ELock> {
-        let lock = fs_state::exclusive_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_slock(&self) -> VectorR<SLock> {
-        let lock = fs_state::shared_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_location(&self) -> &Path {
-        &self.location
-    }
-    pub fn commit(&self, lock: ELock) -> VectorR<()> {
-        let state = self.state.read().unwrap();
-        let mut date = self.date.write().unwrap();
-        fs_state::persist_state::<State>(&lock, &state)?;
-        *date = fs_state::crnt_version(&lock)?;
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+mod state;
+use std::path::{Path, PathBuf};
+use std::sync::RwLock;
+
+use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
+use state::State;
+
+use crate::data_point::Similarity;
+use crate::data_point_provider::{Index, IndexCheck};
+use crate::VectorR;
+pub trait IndexKeyCollector {
+    fn add_key(&mut self, key: String);
+}
+
+pub struct IndexSet {
+    state: RwLock<State>,
+    date: RwLock<Version>,
+    location: PathBuf,
+}
+impl IndexSet {
+    pub fn new(path: &Path, with_check: IndexCheck) -> VectorR<IndexSet> {
+        if !path.exists() {
+            std::fs::create_dir(path)?;
+        }
+        fs_state::initialize_disk(path, || State::new(path.to_path_buf()))?;
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        if let IndexCheck::Sanity = with_check {
+            state.do_sanity_checks()?;
+        }
+        let index = IndexSet {
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        Ok(index)
+    }
+    pub fn remove_index(&mut self, index: &str, _: &ELock) -> VectorR<()> {
+        let mut write = self.state.write().unwrap();
+        write.remove_index(index)
+    }
+    pub fn get_or_create<'a, S>(
+        &'a mut self,
+        index: S,
+        similarity: Similarity,
+        _: &ELock,
+    ) -> VectorR<Index>
+    where
+        S: Into<std::borrow::Cow<'a, str>>,
+    {
+        let mut write = self.state.write().unwrap();
+        write.get_or_create(index, similarity)
+    }
+    fn update(&self, lock: &fs_state::Lock) -> VectorR<()> {
+        let disk_v = fs_state::crnt_version(lock)?;
+        let date = *self.date.read().unwrap();
+        if disk_v > date {
+            let new_state = fs_state::load_state(lock)?;
+            let mut state = self.state.write().unwrap();
+            let mut date = self.date.write().unwrap();
+            *state = new_state;
+            *date = disk_v;
+        }
+        Ok(())
+    }
+    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C, _: &Lock) {
+        let read = self.state.read().unwrap();
+        read.index_keys(c);
+    }
+    pub fn get(&self, index: &str, _: &Lock) -> VectorR<Option<Index>> {
+        let read = self.state.read().unwrap();
+        read.get(index)
+    }
+    pub fn get_elock(&self) -> VectorR<ELock> {
+        let lock = fs_state::exclusive_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_slock(&self) -> VectorR<SLock> {
+        let lock = fs_state::shared_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_location(&self) -> &Path {
+        &self.location
+    }
+    pub fn commit(&self, lock: ELock) -> VectorR<()> {
+        let state = self.state.read().unwrap();
+        let mut date = self.date.write().unwrap();
+        fs_state::persist_state::<State>(&lock, &state)?;
+        *date = fs_state::crnt_version(&lock)?;
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/indexset/state.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/indexset/state.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,119 +1,119 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::borrow::Cow;
-use std::collections::HashSet;
-use std::path::PathBuf;
-
-use serde::{Deserialize, Serialize};
-
-use super::IndexKeyCollector;
-use crate::data_point::Similarity;
-use crate::data_point_provider::{Index, IndexCheck, IndexMetadata};
-use crate::VectorR;
-#[derive(Serialize, Deserialize)]
-pub struct State {
-    location: PathBuf,
-    indexes: HashSet<String>,
-}
-impl State {
-    pub fn new(at: PathBuf) -> State {
-        State {
-            location: at,
-            indexes: HashSet::default(),
-        }
-    }
-    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C) {
-        self.indexes.iter().cloned().for_each(|s| c.add_key(s));
-    }
-    pub fn do_sanity_checks(&self) -> VectorR<()> {
-        for index in &self.indexes {
-            let index_path = self.location.join(index);
-            Index::open(&index_path, IndexCheck::Sanity)?;
-        }
-        Ok(())
-    }
-    pub fn remove_index(&mut self, index: &str) -> VectorR<()> {
-        if self.indexes.remove(index) {
-            let index_path = self.location.join(index);
-            std::fs::remove_dir_all(index_path)?;
-        }
-        Ok(())
-    }
-    pub fn get(&self, index: &str) -> VectorR<Option<Index>> {
-        if self.indexes.contains(index) {
-            let location = self.location.join(index);
-            Some(Index::open(&location, IndexCheck::None)).transpose()
-        } else {
-            Ok(None)
-        }
-    }
-    pub fn get_or_create<'a, S>(&mut self, index: S, similarity: Similarity) -> VectorR<Index>
-    where S: Into<Cow<'a, str>> {
-        let index: Cow<_> = index.into();
-        if self.indexes.contains(index.as_ref()) {
-            let index = index.as_ref();
-            let location = self.location.join(index);
-            Index::open(&location, IndexCheck::None)
-        } else {
-            let index = index.to_string();
-            let location = self.location.join(&index);
-            self.indexes.insert(index);
-            Index::new(&location, IndexMetadata { similarity })
-        }
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use tempfile::TempDir;
-
-    use super::*;
-    #[test]
-    fn basic_functionality_test() {
-        let dir = TempDir::new().unwrap();
-        let mut vectorset = State::new(dir.path().to_path_buf());
-        let _index1 = vectorset
-            .get_or_create("Index1".to_string(), Similarity::Cosine)
-            .unwrap();
-        let _index2 = vectorset
-            .get_or_create("Index2".to_string(), Similarity::Cosine)
-            .unwrap();
-        let _index3 = vectorset
-            .get_or_create("Index3".to_string(), Similarity::Cosine)
-            .unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_some());
-        assert!(vectorset.get("Index2").unwrap().is_some());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        assert!(vectorset.get("Index4").unwrap().is_none());
-        vectorset.do_sanity_checks().unwrap();
-        vectorset.remove_index("Index1").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_some());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        vectorset.remove_index("Index2").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_none());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        vectorset.remove_index("Index3").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_none());
-        assert!(vectorset.get("Index3").unwrap().is_none());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::borrow::Cow;
+use std::collections::HashSet;
+use std::path::PathBuf;
+
+use serde::{Deserialize, Serialize};
+
+use super::IndexKeyCollector;
+use crate::data_point::Similarity;
+use crate::data_point_provider::{Index, IndexCheck, IndexMetadata};
+use crate::VectorR;
+#[derive(Serialize, Deserialize)]
+pub struct State {
+    location: PathBuf,
+    indexes: HashSet<String>,
+}
+impl State {
+    pub fn new(at: PathBuf) -> State {
+        State {
+            location: at,
+            indexes: HashSet::default(),
+        }
+    }
+    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C) {
+        self.indexes.iter().cloned().for_each(|s| c.add_key(s));
+    }
+    pub fn do_sanity_checks(&self) -> VectorR<()> {
+        for index in &self.indexes {
+            let index_path = self.location.join(index);
+            Index::open(&index_path, IndexCheck::Sanity)?;
+        }
+        Ok(())
+    }
+    pub fn remove_index(&mut self, index: &str) -> VectorR<()> {
+        if self.indexes.remove(index) {
+            let index_path = self.location.join(index);
+            std::fs::remove_dir_all(index_path)?;
+        }
+        Ok(())
+    }
+    pub fn get(&self, index: &str) -> VectorR<Option<Index>> {
+        if self.indexes.contains(index) {
+            let location = self.location.join(index);
+            Some(Index::open(&location, IndexCheck::None)).transpose()
+        } else {
+            Ok(None)
+        }
+    }
+    pub fn get_or_create<'a, S>(&mut self, index: S, similarity: Similarity) -> VectorR<Index>
+    where S: Into<Cow<'a, str>> {
+        let index: Cow<_> = index.into();
+        if self.indexes.contains(index.as_ref()) {
+            let index = index.as_ref();
+            let location = self.location.join(index);
+            Index::open(&location, IndexCheck::None)
+        } else {
+            let index = index.to_string();
+            let location = self.location.join(&index);
+            self.indexes.insert(index);
+            Index::new(&location, IndexMetadata { similarity })
+        }
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use tempfile::TempDir;
+
+    use super::*;
+    #[test]
+    fn basic_functionality_test() {
+        let dir = TempDir::new().unwrap();
+        let mut vectorset = State::new(dir.path().to_path_buf());
+        let _index1 = vectorset
+            .get_or_create("Index1".to_string(), Similarity::Cosine)
+            .unwrap();
+        let _index2 = vectorset
+            .get_or_create("Index2".to_string(), Similarity::Cosine)
+            .unwrap();
+        let _index3 = vectorset
+            .get_or_create("Index3".to_string(), Similarity::Cosine)
+            .unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_some());
+        assert!(vectorset.get("Index2").unwrap().is_some());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        assert!(vectorset.get("Index4").unwrap().is_none());
+        vectorset.do_sanity_checks().unwrap();
+        vectorset.remove_index("Index1").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_some());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        vectorset.remove_index("Index2").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_none());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        vectorset.remove_index("Index3").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_none());
+        assert!(vectorset.get("Index3").unwrap().is_none());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/lib.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/lib.rs`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod data_point;
-pub mod data_point_provider;
-mod data_types;
-pub mod formula;
-pub mod indexset;
-pub mod service;
-
-use thiserror::Error;
-#[derive(Debug, Error)]
-pub enum VectorErr {
-    #[error("json error: {0}")]
-    SJ(#[from] serde_json::Error),
-    #[error("IO error: {0}")]
-    IoErr(#[from] std::io::Error),
-    #[error("Error in fs: {0}")]
-    FsError(#[from] nucliadb_core::fs_state::FsError),
-    #[error("Garbage collection delayed")]
-    WorkDelayed,
-    #[error("Several writers are open at the same time ")]
-    MultipleWriters,
-    #[error("Merger is already initialized")]
-    MergerAlreadyInitialized,
-    #[error("Can not merge zero datapoints")]
-    EmptyMerge,
-    #[error("Inconsistent dimensions")]
-    InconsistentDimensions,
-}
-
-pub type VectorR<O> = Result<O, VectorErr>;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod data_point;
+pub mod data_point_provider;
+mod data_types;
+pub mod formula;
+pub mod indexset;
+pub mod service;
+
+use thiserror::Error;
+#[derive(Debug, Error)]
+pub enum VectorErr {
+    #[error("json error: {0}")]
+    SJ(#[from] serde_json::Error),
+    #[error("IO error: {0}")]
+    IoErr(#[from] std::io::Error),
+    #[error("Error in fs: {0}")]
+    FsError(#[from] nucliadb_core::fs_state::FsError),
+    #[error("Garbage collection delayed")]
+    WorkDelayed,
+    #[error("Several writers are open at the same time ")]
+    MultipleWriters,
+    #[error("Merger is already initialized")]
+    MergerAlreadyInitialized,
+    #[error("Can not merge zero datapoints")]
+    EmptyMerge,
+    #[error("Inconsistent dimensions")]
+    InconsistentDimensions,
+}
+
+pub type VectorR<O> = Result<O, VectorErr>;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/mod.rs` & `nucliadb_node_binding-0.8.0/pyproject.toml`

 * *Files 25% similar despite different names*

```diff
@@ -1,37 +1,31 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod reader;
-pub mod writer;
-
-use nucliadb_core::protos::VectorSimilarity as GrpcSimilarity;
-pub use reader::*;
-pub use writer::*;
-
-use crate::data_point::Similarity;
-
-impl From<GrpcSimilarity> for Similarity {
-    fn from(value: GrpcSimilarity) -> Self {
-        match value {
-            GrpcSimilarity::Cosine => Similarity::Cosine,
-            GrpcSimilarity::Dot => Similarity::Dot,
-        }
-    }
-}
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+[build-system]
+requires = ["maturin>=0.12,<0.13"]
+build-backend = "maturin"
+
+[project]
+name = "nucliadb_node_binding"
+requires-python = ">=3.6"
+classifiers = [
+    "Programming Language :: Rust",
+    "Programming Language :: Python :: Implementation :: CPython",
+    "Programming Language :: Python :: Implementation :: PyPy",
+]
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/reader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/service/reader.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,377 +1,367 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::fmt::Debug;
-use std::time::SystemTime;
-
-use nucliadb_core::metrics;
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::prost::Message;
-use nucliadb_core::protos::{
-    DocumentScored, DocumentVectorIdentifier, SentenceMetadata, VectorSearchRequest,
-    VectorSearchResponse,
-};
-use nucliadb_core::tracing::{self, *};
-
-use crate::data_point_provider::*;
-use crate::formula::{AtomClause, CompoundClause, Formula};
-use crate::indexset::IndexSet;
-
-impl<'a> SearchRequest for (usize, &'a VectorSearchRequest, Formula) {
-    fn with_duplicates(&self) -> bool {
-        self.1.with_duplicates
-    }
-    fn get_filter(&self) -> &Formula {
-        &self.2
-    }
-    fn get_query(&self) -> &[f32] {
-        &self.1.vector
-    }
-    fn no_results(&self) -> usize {
-        self.0
-    }
-    fn min_score(&self) -> f32 {
-        self.1.min_score
-    }
-}
-
-pub struct VectorReaderService {
-    index: Index,
-    indexset: IndexSet,
-}
-impl Debug for VectorReaderService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("VectorReaderService").finish()
-    }
-}
-
-impl VectorReader for VectorReaderService {
-    #[tracing::instrument(skip_all)]
-    fn count(&self, vectorset: &str) -> NodeResult<usize> {
-        let time = SystemTime::now();
-
-        let indexet_slock = self.indexset.get_slock()?;
-        if vectorset.is_empty() {
-            debug!("Id for the vectorset is empty");
-            let index_slock = self.index.get_slock()?;
-            let no_nodes = self.index.no_nodes(&index_slock);
-            std::mem::drop(index_slock);
-
-            let metrics = metrics::get_metrics();
-            let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-            let metric = request_time::RequestTimeKey::vectors("count".to_string());
-            metrics.record_request_time(metric, took);
-            debug!("Ending at {took} ms");
-
-            Ok(no_nodes)
-        } else if let Some(index) = self.indexset.get(vectorset, &indexet_slock)? {
-            debug!("Counting nodes for {vectorset}");
-            let lock = index.get_slock()?;
-            let no_nodes = index.no_nodes(&lock);
-            std::mem::drop(lock);
-
-            let metrics = metrics::get_metrics();
-            let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-            let metric = request_time::RequestTimeKey::vectors("count".to_string());
-            metrics.record_request_time(metric, took);
-            debug!("Ending at {took} ms");
-
-            Ok(no_nodes)
-        } else {
-            debug!("There was not a set called {vectorset}");
-            Ok(0)
-        }
-    }
-}
-impl ReaderChild for VectorReaderService {
-    type Request = VectorSearchRequest;
-    type Response = VectorSearchResponse;
-    fn stop(&self) -> NodeResult<()> {
-        debug!("Stopping vector reader Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
-        let time = SystemTime::now();
-
-        let id = Some(&request.id);
-        let offset = request.result_per_page * request.page_number;
-        let total_to_get = offset + request.result_per_page;
-        let offset = offset as usize;
-        let total_to_get = total_to_get as usize;
-        let indexet_slock = self.indexset.get_slock()?;
-        let index_slock = self.index.get_slock()?;
-
-        let key_filters = request
-            .key_filters
-            .iter()
-            .cloned()
-            .map(AtomClause::key_prefix);
-        let mut formula = Formula::new();
-        request
-            .tags
-            .iter()
-            .cloned()
-            .map(AtomClause::label)
-            .for_each(|c| formula.extend(c));
-        if key_filters.len() > 0 {
-            formula.extend(CompoundClause::new(1, key_filters.collect()));
-        }
-
-        let search_request = (total_to_get, request, formula);
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Searching: starts at {v} ms");
-        }
-        let result = if request.vector_set.is_empty() {
-            debug!("{id:?} - No vectorset specified, searching in the main index");
-            self.index.search(&search_request, &index_slock)?
-        } else if let Some(index) = self.indexset.get(&request.vector_set, &indexet_slock)? {
-            debug!(
-                "{id:?} - vectorset specified and found, searching on {}",
-                request.vector_set
-            );
-            let lock = index.get_slock()?;
-            index.search(&search_request, &lock)?
-        } else {
-            debug!(
-                "{id:?} - A was vectorset specified, but not found. {} is not a vectorset",
-                request.vector_set
-            );
-            vec![]
-        };
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Searching: ends at {v} ms");
-        }
-
-        std::mem::drop(indexet_slock);
-        std::mem::drop(index_slock);
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating results: starts at {v} ms");
-        }
-        let documents = result
-            .into_iter()
-            .enumerate()
-            .filter(|(idx, _)| *idx >= offset)
-            .map(|(_, v)| v)
-            .flat_map(DocumentScored::try_from)
-            .collect::<Vec<_>>();
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating results: ends at {v} ms");
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("search".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?} - Ending at {took} ms");
-
-        Ok(VectorSearchResponse {
-            documents,
-            page_number: request.page_number,
-            result_per_page: request.result_per_page,
-        })
-    }
-    #[tracing::instrument(skip_all)]
-    fn stored_ids(&self) -> NodeResult<Vec<String>> {
-        let time = SystemTime::now();
-        let lock = self.index.get_slock().unwrap();
-        let result = self.index.get_keys(&lock)?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Ending at {v} ms")
-        }
-        Ok(result)
-    }
-    fn reload(&self) {}
-}
-
-impl TryFrom<Neighbour> for DocumentScored {
-    type Error = String;
-    fn try_from(neighbour: Neighbour) -> Result<Self, Self::Error> {
-        let id = std::str::from_utf8(neighbour.id());
-        let metadata = neighbour.metadata().map(SentenceMetadata::decode);
-        let labels = neighbour.labels();
-        let Ok(id) = id.map(|i| i.to_string())else {
-            return Err("Id could not be decoded".to_string())
-        };
-        let Ok(metadata) = metadata.transpose() else {
-            return Err("The metadata could not be decoded".to_string());
-        };
-        Ok(DocumentScored {
-            labels,
-            metadata,
-            doc_id: Some(DocumentVectorIdentifier { id }),
-            score: neighbour.score(),
-        })
-    }
-}
-impl VectorReaderService {
-    #[tracing::instrument(skip_all)]
-    pub fn start(config: &VectorConfig) -> NodeResult<Self> {
-        if !config.path.exists() {
-            return Err(node_error!("Invalid path {:?}", config.path));
-        }
-        if !config.vectorset.exists() {
-            return Err(node_error!("Invalid path {:?}", config.vectorset));
-        }
-        Ok(VectorReaderService {
-            index: Index::open(&config.path, IndexCheck::None)?,
-            indexset: IndexSet::new(&config.vectorset, IndexCheck::None)?,
-        })
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashMap;
-
-    use nucliadb_core::protos::resource::ResourceStatus;
-    use nucliadb_core::protos::{
-        IndexParagraph, IndexParagraphs, Resource, ResourceId, VectorSentence, VectorSimilarity,
-    };
-    use tempfile::TempDir;
-
-    use super::*;
-    use crate::service::writer::VectorWriterService;
-
-    #[test]
-    fn test_new_vector_reader() {
-        let dir = TempDir::new().unwrap();
-        let vsc = VectorConfig {
-            similarity: Some(VectorSimilarity::Cosine),
-            path: dir.path().join("vectors"),
-            vectorset: dir.path().join("vectorset"),
-        };
-        let raw_sentences = [
-            ("DOC/KEY/1/1".to_string(), vec![1.0, 3.0, 4.0]),
-            ("DOC/KEY/1/2".to_string(), vec![2.0, 4.0, 5.0]),
-            ("DOC/KEY/1/3".to_string(), vec![3.0, 5.0, 6.0]),
-            ("DOC/KEY/1/4".to_string(), vec![3.0, 5.0, 6.0]),
-        ];
-        let resource_id = ResourceId {
-            shard_id: "DOC".to_string(),
-            uuid: "DOC/KEY".to_string(),
-        };
-
-        let mut sentences = HashMap::new();
-        for (key, vector) in raw_sentences {
-            let vector = VectorSentence {
-                vector,
-                ..Default::default()
-            };
-            sentences.insert(key, vector);
-        }
-        let paragraph = IndexParagraph {
-            start: 0,
-            end: 0,
-            sentences,
-            field: "".to_string(),
-            labels: vec!["1".to_string()],
-            index: 3,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let paragraphs = IndexParagraphs {
-            paragraphs: HashMap::from([("DOC/KEY/1".to_string(), paragraph)]),
-        };
-        let resource = Resource {
-            resource: Some(resource_id),
-            metadata: None,
-            texts: HashMap::with_capacity(0),
-            status: ResourceStatus::Processed as i32,
-            labels: vec!["2".to_string()],
-            paragraphs: HashMap::from([("DOC/KEY".to_string(), paragraphs)]),
-            paragraphs_to_delete: vec![],
-            sentences_to_delete: vec![],
-            relations_to_delete: vec![],
-            relations: vec![],
-            vectors: HashMap::default(),
-            vectors_to_delete: HashMap::default(),
-            shard_id: "DOC".to_string(),
-        };
-        // insert - delete - insert sequence
-        let mut writer = VectorWriterService::start(&vsc).unwrap();
-        let res = writer.set_resource(&resource);
-        assert!(res.is_ok());
-        writer.stop().unwrap();
-        let reader = VectorReaderService::start(&vsc).unwrap();
-        let request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: "".to_string(),
-            vector: vec![4.0, 6.0, 7.0],
-            tags: vec!["1".to_string()],
-            page_number: 0,
-            result_per_page: 20,
-            reload: false,
-            with_duplicates: true,
-            ..Default::default()
-        };
-        let result = reader.search(&request).unwrap();
-        assert_eq!(result.documents.len(), 4);
-
-        let request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: "".to_string(),
-            vector: vec![4.0, 6.0, 7.0],
-            tags: vec!["1".to_string()],
-            page_number: 0,
-            result_per_page: 20,
-            reload: false,
-            with_duplicates: false,
-            ..Default::default()
-        };
-        let result = reader.search(&request).unwrap();
-        let no_nodes = reader.count("").unwrap();
-        assert_eq!(no_nodes, 4);
-        assert_eq!(result.documents.len(), 3);
-
-        // Check that min_score works
-        let request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: "".to_string(),
-            vector: vec![4.0, 6.0, 7.0],
-            tags: vec!["1".to_string()],
-            page_number: 0,
-            result_per_page: 20,
-            reload: false,
-            with_duplicates: false,
-            min_score: 900.0,
-            ..Default::default()
-        };
-        let result = reader.search(&request).unwrap();
-        let no_nodes = reader.count("").unwrap();
-        assert_eq!(no_nodes, 4);
-        assert_eq!(result.documents.len(), 0);
-
-        let bad_request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: "".to_string(),
-            vector: vec![4.0, 6.0],
-            tags: vec!["1".to_string()],
-            page_number: 0,
-            result_per_page: 20,
-            reload: false,
-            with_duplicates: false,
-            ..Default::default()
-        };
-        assert!(reader.search(&bad_request).is_err());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::fmt::Debug;
+use std::time::SystemTime;
+
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::prost::Message;
+use nucliadb_core::protos::{
+    DocumentScored, DocumentVectorIdentifier, SentenceMetadata, VectorSearchRequest,
+    VectorSearchResponse,
+};
+use nucliadb_core::tracing::{self, *};
+
+use crate::data_point_provider::*;
+use crate::formula::{AtomClause, CompoundClause, Formula};
+use crate::indexset::IndexSet;
+
+impl<'a> SearchRequest for (usize, &'a VectorSearchRequest, Formula) {
+    fn with_duplicates(&self) -> bool {
+        self.1.with_duplicates
+    }
+    fn get_filter(&self) -> &Formula {
+        &self.2
+    }
+    fn get_query(&self) -> &[f32] {
+        &self.1.vector
+    }
+    fn no_results(&self) -> usize {
+        self.0
+    }
+    fn min_score(&self) -> f32 {
+        self.1.min_score
+    }
+}
+
+pub struct VectorReaderService {
+    index: Index,
+    indexset: IndexSet,
+}
+impl Debug for VectorReaderService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("VectorReaderService").finish()
+    }
+}
+
+impl VectorReader for VectorReaderService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self, vectorset: &str) -> NodeResult<usize> {
+        let time = SystemTime::now();
+
+        let indexet_slock = self.indexset.get_slock()?;
+        if vectorset.is_empty() {
+            debug!("Id for the vectorset is empty");
+            let index_slock = self.index.get_slock()?;
+            let no_nodes = self.index.no_nodes(&index_slock);
+            std::mem::drop(index_slock);
+
+            let metrics = metrics::get_metrics();
+            let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+            let metric = request_time::RequestTimeKey::vectors("count".to_string());
+            metrics.record_request_time(metric, took);
+            debug!("Ending at {took} ms");
+
+            Ok(no_nodes)
+        } else if let Some(index) = self.indexset.get(vectorset, &indexet_slock)? {
+            debug!("Counting nodes for {vectorset}");
+            let lock = index.get_slock()?;
+            let no_nodes = index.no_nodes(&lock);
+            std::mem::drop(lock);
+
+            let metrics = metrics::get_metrics();
+            let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+            let metric = request_time::RequestTimeKey::vectors("count".to_string());
+            metrics.record_request_time(metric, took);
+            debug!("Ending at {took} ms");
+
+            Ok(no_nodes)
+        } else {
+            debug!("There was not a set called {vectorset}");
+            Ok(0)
+        }
+    }
+}
+impl ReaderChild for VectorReaderService {
+    type Request = VectorSearchRequest;
+    type Response = VectorSearchResponse;
+    #[tracing::instrument(skip_all)]
+    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
+        let time = SystemTime::now();
+
+        let id = Some(&request.id);
+        let offset = request.result_per_page * request.page_number;
+        let total_to_get = offset + request.result_per_page;
+        let offset = offset as usize;
+        let total_to_get = total_to_get as usize;
+        let indexet_slock = self.indexset.get_slock()?;
+        let index_slock = self.index.get_slock()?;
+
+        let key_filters = request
+            .key_filters
+            .iter()
+            .cloned()
+            .map(AtomClause::key_prefix);
+        let mut formula = Formula::new();
+        request
+            .tags
+            .iter()
+            .cloned()
+            .map(AtomClause::label)
+            .for_each(|c| formula.extend(c));
+        if key_filters.len() > 0 {
+            formula.extend(CompoundClause::new(1, key_filters.collect()));
+        }
+
+        let search_request = (total_to_get, request, formula);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Searching: starts at {v} ms");
+        }
+        let result = if request.vector_set.is_empty() {
+            debug!("{id:?} - No vectorset specified, searching in the main index");
+            self.index.search(&search_request, &index_slock)?
+        } else if let Some(index) = self.indexset.get(&request.vector_set, &indexet_slock)? {
+            debug!(
+                "{id:?} - vectorset specified and found, searching on {}",
+                request.vector_set
+            );
+            let lock = index.get_slock()?;
+            index.search(&search_request, &lock)?
+        } else {
+            debug!(
+                "{id:?} - A was vectorset specified, but not found. {} is not a vectorset",
+                request.vector_set
+            );
+            vec![]
+        };
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Searching: ends at {v} ms");
+        }
+
+        std::mem::drop(indexet_slock);
+        std::mem::drop(index_slock);
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating results: starts at {v} ms");
+        }
+        let documents = result
+            .into_iter()
+            .enumerate()
+            .filter(|(idx, _)| *idx >= offset)
+            .map(|(_, v)| v)
+            .flat_map(DocumentScored::try_from)
+            .collect::<Vec<_>>();
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating results: ends at {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("search".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took} ms");
+
+        Ok(VectorSearchResponse {
+            documents,
+            page_number: request.page_number,
+            result_per_page: request.result_per_page,
+        })
+    }
+    #[tracing::instrument(skip_all)]
+    fn stored_ids(&self) -> NodeResult<Vec<String>> {
+        let time = SystemTime::now();
+        let lock = self.index.get_slock().unwrap();
+        let result = self.index.get_keys(&lock)?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Ending at {v} ms")
+        }
+        Ok(result)
+    }
+}
+
+impl TryFrom<Neighbour> for DocumentScored {
+    type Error = String;
+    fn try_from(neighbour: Neighbour) -> Result<Self, Self::Error> {
+        let id = std::str::from_utf8(neighbour.id());
+        let metadata = neighbour.metadata().map(SentenceMetadata::decode);
+        let labels = neighbour.labels();
+        let Ok(id) = id.map(|i| i.to_string()) else {
+            return Err("Id could not be decoded".to_string());
+        };
+        let Ok(metadata) = metadata.transpose() else {
+            return Err("The metadata could not be decoded".to_string());
+        };
+        Ok(DocumentScored {
+            labels,
+            metadata,
+            doc_id: Some(DocumentVectorIdentifier { id }),
+            score: neighbour.score(),
+        })
+    }
+}
+impl VectorReaderService {
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &VectorConfig) -> NodeResult<Self> {
+        if !config.path.exists() {
+            return Err(node_error!("Invalid path {:?}", config.path));
+        }
+        if !config.vectorset.exists() {
+            return Err(node_error!("Invalid path {:?}", config.vectorset));
+        }
+        Ok(VectorReaderService {
+            index: Index::open(&config.path, IndexCheck::None)?,
+            indexset: IndexSet::new(&config.vectorset, IndexCheck::None)?,
+        })
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashMap;
+
+    use nucliadb_core::protos::resource::ResourceStatus;
+    use nucliadb_core::protos::{
+        IndexParagraph, IndexParagraphs, Resource, ResourceId, VectorSentence, VectorSimilarity,
+    };
+    use tempfile::TempDir;
+
+    use super::*;
+    use crate::service::writer::VectorWriterService;
+
+    #[test]
+    fn test_new_vector_reader() {
+        let dir = TempDir::new().unwrap();
+        let vsc = VectorConfig {
+            similarity: Some(VectorSimilarity::Cosine),
+            path: dir.path().join("vectors"),
+            vectorset: dir.path().join("vectorset"),
+        };
+        let raw_sentences = [
+            ("DOC/KEY/1/1".to_string(), vec![1.0, 3.0, 4.0]),
+            ("DOC/KEY/1/2".to_string(), vec![2.0, 4.0, 5.0]),
+            ("DOC/KEY/1/3".to_string(), vec![3.0, 5.0, 6.0]),
+            ("DOC/KEY/1/4".to_string(), vec![3.0, 5.0, 6.0]),
+        ];
+        let resource_id = ResourceId {
+            shard_id: "DOC".to_string(),
+            uuid: "DOC/KEY".to_string(),
+        };
+
+        let mut sentences = HashMap::new();
+        for (key, vector) in raw_sentences {
+            let vector = VectorSentence {
+                vector,
+                ..Default::default()
+            };
+            sentences.insert(key, vector);
+        }
+        let paragraph = IndexParagraph {
+            start: 0,
+            end: 0,
+            sentences,
+            field: "".to_string(),
+            labels: vec!["1".to_string()],
+            index: 3,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let paragraphs = IndexParagraphs {
+            paragraphs: HashMap::from([("DOC/KEY/1".to_string(), paragraph)]),
+        };
+        let resource = Resource {
+            resource: Some(resource_id),
+            metadata: None,
+            texts: HashMap::with_capacity(0),
+            status: ResourceStatus::Processed as i32,
+            labels: vec!["2".to_string()],
+            paragraphs: HashMap::from([("DOC/KEY".to_string(), paragraphs)]),
+            paragraphs_to_delete: vec![],
+            sentences_to_delete: vec![],
+            relations_to_delete: vec![],
+            relations: vec![],
+            vectors: HashMap::default(),
+            vectors_to_delete: HashMap::default(),
+            shard_id: "DOC".to_string(),
+        };
+        // insert - delete - insert sequence
+        let mut writer = VectorWriterService::start(&vsc).unwrap();
+        let res = writer.set_resource(&resource);
+        assert!(res.is_ok());
+        let reader = VectorReaderService::start(&vsc).unwrap();
+        let request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: "".to_string(),
+            vector: vec![4.0, 6.0, 7.0],
+            tags: vec!["1".to_string()],
+            page_number: 0,
+            result_per_page: 20,
+            with_duplicates: true,
+            ..Default::default()
+        };
+        let result = reader.search(&request).unwrap();
+        assert_eq!(result.documents.len(), 4);
+
+        let request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: "".to_string(),
+            vector: vec![4.0, 6.0, 7.0],
+            tags: vec!["1".to_string()],
+            page_number: 0,
+            result_per_page: 20,
+            with_duplicates: false,
+            ..Default::default()
+        };
+        let result = reader.search(&request).unwrap();
+        let no_nodes = reader.count("").unwrap();
+        assert_eq!(no_nodes, 4);
+        assert_eq!(result.documents.len(), 3);
+
+        // Check that min_score works
+        let request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: "".to_string(),
+            vector: vec![4.0, 6.0, 7.0],
+            tags: vec!["1".to_string()],
+            page_number: 0,
+            result_per_page: 20,
+            with_duplicates: false,
+            min_score: 900.0,
+            ..Default::default()
+        };
+        let result = reader.search(&request).unwrap();
+        let no_nodes = reader.count("").unwrap();
+        assert_eq!(no_nodes, 4);
+        assert_eq!(result.documents.len(), 0);
+
+        let bad_request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: "".to_string(),
+            vector: vec![4.0, 6.0],
+            tags: vec!["1".to_string()],
+            page_number: 0,
+            result_per_page: 20,
+            with_duplicates: false,
+            ..Default::default()
+        };
+        assert!(reader.search(&bad_request).is_err());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_vectors/src/service/writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_vectors/src/service/writer.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,582 +1,575 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-use std::collections::HashMap;
-use std::fmt::Debug;
-use std::time::SystemTime;
-
-use data_point::{Elem, LabelDictionary};
-use nucliadb_core::metrics;
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::prost::Message;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{Resource, ResourceId, VectorSetId, VectorSimilarity};
-use nucliadb_core::tracing::{self, *};
-
-use crate::data_point::DataPoint;
-use crate::data_point_provider::*;
-use crate::indexset::{IndexKeyCollector, IndexSet};
-use crate::{data_point, VectorErr};
-
-impl IndexKeyCollector for Vec<String> {
-    fn add_key(&mut self, key: String) {
-        self.push(key);
-    }
-}
-
-pub struct VectorWriterService {
-    index: Index,
-    indexset: IndexSet,
-}
-
-impl Debug for VectorWriterService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("VectorWriterService").finish()
-    }
-}
-
-impl VectorWriter for VectorWriterService {
-    #[tracing::instrument(skip_all)]
-    fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
-        let time = SystemTime::now();
-
-        let id: Option<String> = None;
-        let mut collector = Vec::new();
-        let indexset_slock = self.indexset.get_slock()?;
-        self.indexset.index_keys(&mut collector, &indexset_slock);
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("list_vectorsets".to_string());
-        metrics.record_request_time(metric, took);
-
-        debug!("{id:?} - Ending at {took} ms");
-        Ok(collector)
-    }
-    #[tracing::instrument(skip_all)]
-    fn add_vectorset(
-        &mut self,
-        setid: &VectorSetId,
-        similarity: VectorSimilarity,
-    ) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = setid.shard.as_ref().map(|s| &s.id);
-        let set = &setid.vectorset;
-        let indexid = setid.vectorset.as_str();
-        let similarity = similarity.into();
-        let indexset_elock = self.indexset.get_elock()?;
-        self.indexset
-            .get_or_create(indexid, similarity, &indexset_elock)?;
-        self.indexset.commit(indexset_elock)?;
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("add_vectorset".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?}/{set} - Ending at {took} ms");
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn remove_vectorset(&mut self, setid: &VectorSetId) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = setid.shard.as_ref().map(|s| &s.id);
-        let set = &setid.vectorset;
-        let indexid = &setid.vectorset;
-        let indexset_elock = self.indexset.get_elock()?;
-        self.indexset.remove_index(indexid, &indexset_elock)?;
-        self.indexset.commit(indexset_elock)?;
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("remove_vectorset".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?}/{set} - Ending at {took} ms");
-
-        Ok(())
-    }
-}
-impl WriterChild for VectorWriterService {
-    #[tracing::instrument(skip_all)]
-    fn stop(&mut self) -> NodeResult<()> {
-        debug!("Stopping vector writer Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn count(&self) -> NodeResult<usize> {
-        let time = SystemTime::now();
-
-        let id: Option<String> = None;
-        let lock = self.index.get_slock()?;
-        let no_nodes = self.index.no_nodes(&lock);
-        std::mem::drop(lock);
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("count".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?} - Ending at {took} ms");
-
-        Ok(no_nodes)
-    }
-    #[tracing::instrument(skip_all)]
-    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = Some(&resource_id.shard_id);
-        let temporal_mark = TemporalMark::now();
-        let lock = self.index.get_elock()?;
-        self.index.delete(&resource_id.uuid, temporal_mark, &lock);
-        self.index.commit(lock)?;
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("delete_resource".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?} - Ending at {took} ms");
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = resource.resource.as_ref().map(|i| &i.shard_id);
-        debug!("{id:?} - Updating main index");
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating elements for the main index: starts {v} ms");
-        }
-
-        let temporal_mark = TemporalMark::now();
-        let mut lengths: HashMap<usize, Vec<_>> = HashMap::new();
-        let mut elems = Vec::new();
-        if resource.status != ResourceStatus::Delete as i32 {
-            for (paragraph_field, paragraph) in resource.paragraphs.iter() {
-                let field = &[paragraph_field.clone()];
-                for index in paragraph.paragraphs.values() {
-                    let labels = LabelDictionary::new(
-                        resource
-                            .labels
-                            .iter()
-                            .chain(index.labels.iter())
-                            .chain(field.iter())
-                            .cloned()
-                            .collect(),
-                    );
-                    for (key, sentence) in index.sentences.iter().clone() {
-                        let key = key.to_string();
-                        let labels = labels.clone();
-                        let vector = sentence.vector.clone();
-                        let metadata = sentence.metadata.as_ref().map(|m| m.encode_to_vec());
-                        let bucket = lengths.entry(vector.len()).or_default();
-                        elems.push(Elem::new(key, vector, labels, metadata));
-                        bucket.push(paragraph_field);
-                    }
-                }
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating elements for the main index: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Datapoint creation: starts {v} ms");
-        }
-
-        if lengths.len() > 1 {
-            return Ok(tracing::error!("{}", self.dimensions_report(lengths)));
-        }
-
-        let new_dp = if !elems.is_empty() {
-            Some(DataPoint::new(
-                self.index.location(),
-                elems,
-                Some(temporal_mark),
-                self.index.metadata().similarity,
-            )?)
-        } else {
-            None
-        };
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Datapoint creation: ends {v} ms");
-        }
-
-        let lock = self.index.get_elock()?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Processing Sentences to delete: starts {v} ms");
-        }
-        for to_delete in &resource.sentences_to_delete {
-            self.index.delete(to_delete, temporal_mark, &lock)
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Processing Sentences to delete: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Indexing datapoint: starts {v} ms");
-        }
-        match new_dp.map(|i| self.index.add(i, &lock)).unwrap_or(Ok(())) {
-            Ok(_) => self.index.commit(lock)?,
-            Err(e) => tracing::error!("{id:?}/default could insert vectors: {e:?}"),
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Indexing datapoint: ends {v} ms");
-        }
-
-        // Updating existing indexes
-        // Perform delete operations over the vector set
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Delete requests for indexes in the set: starts {v} ms");
-        }
-        let indexset_slock = self.indexset.get_slock()?;
-        let index_iter = resource.vectors_to_delete.iter().flat_map(|(k, v)| {
-            self.indexset
-                .get(k, &indexset_slock)
-                .transpose()
-                .map(|i| (v, i))
-        });
-        for (vectorlist, index) in index_iter {
-            let index = index?;
-            let index_lock = index.get_elock()?;
-            vectorlist.vectors.iter().for_each(|vector| {
-                index.delete(vector, temporal_mark, &index_lock);
-            });
-            index.commit(index_lock)?;
-        }
-        std::mem::drop(indexset_slock);
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Delete requests for indexes in the set: ends {v} ms");
-        }
-
-        // Perform add operations over the vector set
-        // New indexes may be created.
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating and geting indexes in the set: starts {v} ms");
-        }
-        let indexset_elock = self.indexset.get_elock()?;
-        let indexes = resource
-            .vectors
-            .keys()
-            .map(|k| (k, self.indexset.get(k, &indexset_elock)))
-            .map(|(key, index)| index.map(|index| (key, index)))
-            .collect::<Result<HashMap<_, _>, _>>()?;
-        self.indexset.commit(indexset_elock)?;
-
-        // Inner indexes are updated
-        for (index_key, mut index) in indexes.into_iter().flat_map(|i| i.1.map(|j| (i.0, j))) {
-            let mut lengths: HashMap<usize, Vec<_>> = HashMap::new();
-            let mut elems = vec![];
-            for (vectorset, user_vector) in resource.vectors[index_key].vectors.iter() {
-                let key = vectorset.clone();
-                let vector = user_vector.vector.clone();
-                let bucket = lengths.entry(vector.len()).or_default();
-                let labels = LabelDictionary::new(user_vector.labels.clone());
-                elems.push(Elem::new(key, vector, labels, None));
-                bucket.push(vectorset);
-            }
-            if lengths.len() > 1 {
-                tracing::error!("vectorsets report: {}", self.dimensions_report(lengths));
-            } else if !elems.is_empty() {
-                let similarity = index.metadata().similarity;
-                let location = index.location();
-                let new_dp = DataPoint::new(location, elems, Some(temporal_mark), similarity)?;
-                let lock = index.get_elock()?;
-                match index.add(new_dp, &lock) {
-                    Ok(_) => index.commit(lock)?,
-                    Err(e) => tracing::error!("Could not insert at {id:?}/{index_key}: {e:?}"),
-                }
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating and geting indexes in the set: ends {v} ms");
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("set_resource".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?} - Ending at {took} ms");
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn garbage_collection(&mut self) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        self.collect_garbage_for(&self.index)?;
-        let indexset_slock = self.indexset.get_slock()?;
-        let mut index_keys = vec![];
-        self.indexset.index_keys(&mut index_keys, &indexset_slock);
-        for index_key in index_keys {
-            let Some(index) = self.indexset.get(&index_key, &indexset_slock)? else {
-                return Err(node_error!("Unknown state for {index_key}"));
-            };
-            self.collect_garbage_for(&index)?;
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::vectors("garbage_collection".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("Garbage collection {took} ms");
-
-        Ok(())
-    }
-}
-
-impl VectorWriterService {
-    fn dimensions_report<'a>(&'a self, dimensions: HashMap<usize, Vec<&'a String>>) -> String {
-        let mut report = String::new();
-        for (dimension, bucket) in dimensions {
-            let partial = format!("{dimension} : {bucket:?}\n");
-            report.push_str(&partial);
-        }
-        report.pop();
-        report
-    }
-    fn collect_garbage_for(&self, index: &Index) -> NodeResult<()> {
-        debug!("Collecting garbage for index: {:?}", index.location());
-        let slock = index.get_slock()?;
-        match index.collect_garbage(&slock) {
-            Ok(_) => debug!("Garbage collected for main index"),
-            Err(VectorErr::WorkDelayed) => debug!("Garbage collection delayed"),
-            Err(e) => Err(e)?,
-        }
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn start(config: &VectorConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            match VectorWriterService::new(config) {
-                Err(e) if path.exists() => {
-                    std::fs::remove_dir(path)?;
-                    Err(e)
-                }
-                Err(e) => Err(e),
-                Ok(v) => Ok(v),
-            }
-        } else {
-            VectorWriterService::open(config)
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn new(config: &VectorConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        let indexset = std::path::Path::new(&config.vectorset);
-        if path.exists() {
-            Err(node_error!("Shard does exist".to_string()))
-        } else {
-            let Some(similarity) = config.similarity.map(|i| i.into()) else {
-                return Err(node_error!("A similarity must be specified"));
-            };
-            Ok(VectorWriterService {
-                index: Index::new(path, IndexMetadata { similarity })?,
-                indexset: IndexSet::new(indexset, IndexCheck::None)?,
-            })
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn open(config: &VectorConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        let indexset = std::path::Path::new(&config.vectorset);
-        if !path.exists() {
-            Err(node_error!("Shard does not exist".to_string()))
-        } else {
-            Ok(VectorWriterService {
-                index: Index::open(path, IndexCheck::Sanity)?,
-                indexset: IndexSet::new(indexset, IndexCheck::Sanity)?,
-            })
-        }
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::{HashMap, HashSet};
-
-    use nucliadb_core::protos::resource::ResourceStatus;
-    use nucliadb_core::protos::{
-        IndexParagraph, IndexParagraphs, Resource, ResourceId, UserVector, UserVectors,
-        VectorSearchRequest, VectorSentence, VectorSimilarity,
-    };
-    use tempfile::TempDir;
-
-    use super::*;
-    use crate::service::reader::VectorReaderService;
-    fn create_vector_set(
-        writer: &mut VectorWriterService,
-        set_name: String,
-    ) -> (String, UserVectors) {
-        let label = format!("{set_name}/label");
-        let key = format!("{set_name}/key");
-        let vector = vec![1.0, 3.0, 4.0];
-        let data = UserVector {
-            vector,
-            labels: vec![label],
-            ..Default::default()
-        };
-        let set = UserVectors {
-            vectors: HashMap::from([(key, data)]),
-        };
-        let id = VectorSetId {
-            shard: None,
-            vectorset: set_name.clone(),
-        };
-        writer.add_vectorset(&id, VectorSimilarity::Cosine).unwrap();
-        (set_name, set)
-    }
-    #[test]
-    fn test_vectorset_functionality() {
-        let dir = TempDir::new().unwrap();
-        let vsc = VectorConfig {
-            similarity: Some(VectorSimilarity::Cosine),
-            path: dir.path().join("vectors"),
-            vectorset: dir.path().join("vectorsets"),
-        };
-
-        let mut writer = VectorWriterService::start(&vsc).unwrap();
-        let indexes: HashMap<_, _> = (0..10)
-            .map(|i| create_vector_set(&mut writer, i.to_string()))
-            .collect();
-        let keys: HashSet<_> = indexes.keys().cloned().collect();
-        let resource = Resource {
-            vectors: indexes,
-            ..Default::default()
-        };
-        writer.set_resource(&resource).unwrap();
-        let index_keys: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
-        assert_eq!(index_keys, keys);
-
-        let reader = VectorReaderService::start(&vsc).unwrap();
-        let mut request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: "4".to_string(),
-            vector: vec![4.0, 6.0, 7.0],
-            tags: vec!["4/label".to_string()],
-            page_number: 0,
-            result_per_page: 20,
-            reload: false,
-            with_duplicates: false,
-            ..Default::default()
-        };
-        let results = reader.search(&request).unwrap();
-        let id = results.documents[0].doc_id.clone().unwrap().id;
-        assert_eq!(results.documents.len(), 1);
-        assert_eq!(id, "4/key");
-
-        // Same set, but no label match
-        request.tags = vec!["5/label".to_string()];
-        let results = reader.search(&request).unwrap();
-        assert_eq!(results.documents.len(), 0);
-
-        // Invalid set
-        request.vector_set = "not a set".to_string();
-        let results = reader.search(&request).unwrap();
-        assert_eq!(results.documents.len(), 0);
-
-        // Remove set 4
-        let id = VectorSetId {
-            vectorset: "4".to_string(),
-            ..Default::default()
-        };
-        let mut index_keys: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
-        writer.remove_vectorset(&id).unwrap();
-        let index_keysp: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
-        index_keys.remove("4");
-        assert!(!index_keysp.contains("4"));
-        assert_eq!(index_keys, index_keysp);
-
-        // Now vectorset 4 is no longer available
-        request.vector_set = "4".to_string();
-        request.tags = vec!["4/label".to_string()];
-        let results = reader.search(&request).unwrap();
-        assert_eq!(results.documents.len(), 0);
-    }
-
-    #[test]
-    fn test_new_vector_writer() {
-        let dir = TempDir::new().unwrap();
-        let vsc = VectorConfig {
-            similarity: Some(VectorSimilarity::Cosine),
-            path: dir.path().join("vectors"),
-            vectorset: dir.path().join("vectorset"),
-        };
-        let raw_sentences = [
-            ("DOC/KEY/1/1".to_string(), vec![1.0, 3.0, 4.0]),
-            ("DOC/KEY/1/2".to_string(), vec![2.0, 4.0, 5.0]),
-            ("DOC/KEY/1/3".to_string(), vec![3.0, 5.0, 6.0]),
-        ];
-        let resource_id = ResourceId {
-            shard_id: "DOC".to_string(),
-            uuid: "DOC/KEY".to_string(),
-        };
-
-        let mut sentences = HashMap::new();
-        for (key, vector) in raw_sentences {
-            let vector = VectorSentence {
-                vector,
-                ..Default::default()
-            };
-            sentences.insert(key, vector);
-        }
-        let paragraph = IndexParagraph {
-            start: 0,
-            end: 0,
-            sentences,
-            field: "".to_string(),
-            labels: vec!["1".to_string(), "2".to_string(), "3".to_string()],
-            index: 3,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let paragraphs = IndexParagraphs {
-            paragraphs: HashMap::from([("DOC/KEY/1".to_string(), paragraph)]),
-        };
-        let resource = Resource {
-            resource: Some(resource_id.clone()),
-            metadata: None,
-            texts: HashMap::with_capacity(0),
-            status: ResourceStatus::Processed as i32,
-            labels: vec!["FULL".to_string()],
-            paragraphs: HashMap::from([("DOC/KEY".to_string(), paragraphs)]),
-            paragraphs_to_delete: vec![],
-            sentences_to_delete: vec![],
-            relations_to_delete: vec![],
-            relations: vec![],
-            vectors: HashMap::default(),
-            vectors_to_delete: HashMap::default(),
-            shard_id: "DOC".to_string(),
-        };
-        // insert - delete - insert sequence
-        let mut writer = VectorWriterService::start(&vsc).unwrap();
-        let res = writer.set_resource(&resource);
-        assert!(res.is_ok());
-        let res = writer.delete_resource(&resource_id);
-        assert!(res.is_ok());
-        let res = writer.set_resource(&resource);
-        assert!(res.is_ok());
-        writer.stop().unwrap();
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+use std::collections::HashMap;
+use std::fmt::Debug;
+use std::time::SystemTime;
+
+use data_point::{Elem, LabelDictionary};
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::prost::Message;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{Resource, ResourceId, VectorSetId, VectorSimilarity};
+use nucliadb_core::tracing::{self, *};
+
+use crate::data_point::DataPoint;
+use crate::data_point_provider::*;
+use crate::indexset::{IndexKeyCollector, IndexSet};
+use crate::{data_point, VectorErr};
+
+impl IndexKeyCollector for Vec<String> {
+    fn add_key(&mut self, key: String) {
+        self.push(key);
+    }
+}
+
+pub struct VectorWriterService {
+    index: Index,
+    indexset: IndexSet,
+}
+
+impl Debug for VectorWriterService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("VectorWriterService").finish()
+    }
+}
+
+impl VectorWriter for VectorWriterService {
+    #[tracing::instrument(skip_all)]
+    fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
+        let time = SystemTime::now();
+
+        let id: Option<String> = None;
+        let mut collector = Vec::new();
+        let indexset_slock = self.indexset.get_slock()?;
+        self.indexset.index_keys(&mut collector, &indexset_slock);
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("list_vectorsets".to_string());
+        metrics.record_request_time(metric, took);
+
+        debug!("{id:?} - Ending at {took} ms");
+        Ok(collector)
+    }
+    #[tracing::instrument(skip_all)]
+    fn add_vectorset(
+        &mut self,
+        setid: &VectorSetId,
+        similarity: VectorSimilarity,
+    ) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = setid.shard.as_ref().map(|s| &s.id);
+        let set = &setid.vectorset;
+        let indexid = setid.vectorset.as_str();
+        let similarity = similarity.into();
+        let indexset_elock = self.indexset.get_elock()?;
+        self.indexset
+            .get_or_create(indexid, similarity, &indexset_elock)?;
+        self.indexset.commit(indexset_elock)?;
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("add_vectorset".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?}/{set} - Ending at {took} ms");
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn remove_vectorset(&mut self, setid: &VectorSetId) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = setid.shard.as_ref().map(|s| &s.id);
+        let set = &setid.vectorset;
+        let indexid = &setid.vectorset;
+        let indexset_elock = self.indexset.get_elock()?;
+        self.indexset.remove_index(indexid, &indexset_elock)?;
+        self.indexset.commit(indexset_elock)?;
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("remove_vectorset".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?}/{set} - Ending at {took} ms");
+
+        Ok(())
+    }
+}
+impl WriterChild for VectorWriterService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let time = SystemTime::now();
+
+        let id: Option<String> = None;
+        let lock = self.index.get_slock()?;
+        let no_nodes = self.index.no_nodes(&lock);
+        std::mem::drop(lock);
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("count".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took} ms");
+
+        Ok(no_nodes)
+    }
+    #[tracing::instrument(skip_all)]
+    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = Some(&resource_id.shard_id);
+        let temporal_mark = TemporalMark::now();
+        let lock = self.index.get_elock()?;
+        self.index.delete(&resource_id.uuid, temporal_mark, &lock);
+        self.index.commit(lock)?;
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("delete_resource".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took} ms");
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = resource.resource.as_ref().map(|i| &i.shard_id);
+        debug!("{id:?} - Updating main index");
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating elements for the main index: starts {v} ms");
+        }
+
+        let temporal_mark = TemporalMark::now();
+        let mut lengths: HashMap<usize, Vec<_>> = HashMap::new();
+        let mut elems = Vec::new();
+        if resource.status != ResourceStatus::Delete as i32 {
+            for (paragraph_field, paragraph) in resource.paragraphs.iter() {
+                let field = &[paragraph_field.clone()];
+                for index in paragraph.paragraphs.values() {
+                    let labels = LabelDictionary::new(
+                        resource
+                            .labels
+                            .iter()
+                            .chain(index.labels.iter())
+                            .chain(field.iter())
+                            .cloned()
+                            .collect(),
+                    );
+                    for (key, sentence) in index.sentences.iter().clone() {
+                        let key = key.to_string();
+                        let labels = labels.clone();
+                        let vector = sentence.vector.clone();
+                        let metadata = sentence.metadata.as_ref().map(|m| m.encode_to_vec());
+                        let bucket = lengths.entry(vector.len()).or_default();
+                        elems.push(Elem::new(key, vector, labels, metadata));
+                        bucket.push(paragraph_field);
+                    }
+                }
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating elements for the main index: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Datapoint creation: starts {v} ms");
+        }
+
+        if lengths.len() > 1 {
+            return Ok(tracing::error!("{}", self.dimensions_report(lengths)));
+        }
+
+        let new_dp = if !elems.is_empty() {
+            Some(DataPoint::new(
+                self.index.location(),
+                elems,
+                Some(temporal_mark),
+                self.index.metadata().similarity,
+            )?)
+        } else {
+            None
+        };
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Datapoint creation: ends {v} ms");
+        }
+
+        let lock = self.index.get_elock()?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Processing Sentences to delete: starts {v} ms");
+        }
+        for to_delete in &resource.sentences_to_delete {
+            self.index.delete(to_delete, temporal_mark, &lock)
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Processing Sentences to delete: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Indexing datapoint: starts {v} ms");
+        }
+        match new_dp.map(|i| self.index.add(i, &lock)).unwrap_or(Ok(())) {
+            Ok(_) => self.index.commit(lock)?,
+            Err(e) => tracing::error!("{id:?}/default could insert vectors: {e:?}"),
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Indexing datapoint: ends {v} ms");
+        }
+
+        // Updating existing indexes
+        // Perform delete operations over the vector set
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete requests for indexes in the set: starts {v} ms");
+        }
+        let indexset_slock = self.indexset.get_slock()?;
+        let index_iter = resource.vectors_to_delete.iter().flat_map(|(k, v)| {
+            self.indexset
+                .get(k, &indexset_slock)
+                .transpose()
+                .map(|i| (v, i))
+        });
+        for (vectorlist, index) in index_iter {
+            let index = index?;
+            let index_lock = index.get_elock()?;
+            vectorlist.vectors.iter().for_each(|vector| {
+                index.delete(vector, temporal_mark, &index_lock);
+            });
+            index.commit(index_lock)?;
+        }
+        std::mem::drop(indexset_slock);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete requests for indexes in the set: ends {v} ms");
+        }
+
+        // Perform add operations over the vector set
+        // New indexes may be created.
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating and geting indexes in the set: starts {v} ms");
+        }
+        let indexset_elock = self.indexset.get_elock()?;
+        let indexes = resource
+            .vectors
+            .keys()
+            .map(|k| (k, self.indexset.get(k, &indexset_elock)))
+            .map(|(key, index)| index.map(|index| (key, index)))
+            .collect::<Result<HashMap<_, _>, _>>()?;
+        self.indexset.commit(indexset_elock)?;
+
+        // Inner indexes are updated
+        for (index_key, mut index) in indexes.into_iter().flat_map(|i| i.1.map(|j| (i.0, j))) {
+            let mut lengths: HashMap<usize, Vec<_>> = HashMap::new();
+            let mut elems = vec![];
+            for (vectorset, user_vector) in resource.vectors[index_key].vectors.iter() {
+                let key = vectorset.clone();
+                let vector = user_vector.vector.clone();
+                let bucket = lengths.entry(vector.len()).or_default();
+                let labels = LabelDictionary::new(user_vector.labels.clone());
+                elems.push(Elem::new(key, vector, labels, None));
+                bucket.push(vectorset);
+            }
+            if lengths.len() > 1 {
+                tracing::error!("vectorsets report: {}", self.dimensions_report(lengths));
+            } else if !elems.is_empty() {
+                let similarity = index.metadata().similarity;
+                let location = index.location();
+                let new_dp = DataPoint::new(location, elems, Some(temporal_mark), similarity)?;
+                let lock = index.get_elock()?;
+                match index.add(new_dp, &lock) {
+                    Ok(_) => index.commit(lock)?,
+                    Err(e) => tracing::error!("Could not insert at {id:?}/{index_key}: {e:?}"),
+                }
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating and geting indexes in the set: ends {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("set_resource".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took} ms");
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn garbage_collection(&mut self) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        self.collect_garbage_for(&self.index)?;
+        let indexset_slock = self.indexset.get_slock()?;
+        let mut index_keys = vec![];
+        self.indexset.index_keys(&mut index_keys, &indexset_slock);
+        for index_key in index_keys {
+            let Some(index) = self.indexset.get(&index_key, &indexset_slock)? else {
+                return Err(node_error!("Unknown state for {index_key}"));
+            };
+            self.collect_garbage_for(&index)?;
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::vectors("garbage_collection".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("Garbage collection {took} ms");
+
+        Ok(())
+    }
+}
+
+impl VectorWriterService {
+    fn dimensions_report<'a>(&'a self, dimensions: HashMap<usize, Vec<&'a String>>) -> String {
+        let mut report = String::new();
+        for (dimension, bucket) in dimensions {
+            let partial = format!("{dimension} : {bucket:?}\n");
+            report.push_str(&partial);
+        }
+        report.pop();
+        report
+    }
+    fn collect_garbage_for(&self, index: &Index) -> NodeResult<()> {
+        debug!("Collecting garbage for index: {:?}", index.location());
+        let lock = index.get_elock()?;
+        match index.collect_garbage(&lock) {
+            Ok(_) => debug!("Garbage collected for main index"),
+            Err(VectorErr::WorkDelayed) => debug!("Garbage collection delayed"),
+            Err(e) => Err(e)?,
+        }
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &VectorConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            match VectorWriterService::new(config) {
+                Err(e) if path.exists() => {
+                    std::fs::remove_dir(path)?;
+                    Err(e)
+                }
+                Err(e) => Err(e),
+                Ok(v) => Ok(v),
+            }
+        } else {
+            VectorWriterService::open(config)
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn new(config: &VectorConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        let indexset = std::path::Path::new(&config.vectorset);
+        if path.exists() {
+            Err(node_error!("Shard does exist".to_string()))
+        } else {
+            let Some(similarity) = config.similarity.map(|i| i.into()) else {
+                return Err(node_error!("A similarity must be specified"));
+            };
+            Ok(VectorWriterService {
+                index: Index::new(path, IndexMetadata { similarity })?,
+                indexset: IndexSet::new(indexset, IndexCheck::None)?,
+            })
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn open(config: &VectorConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        let indexset = std::path::Path::new(&config.vectorset);
+        if !path.exists() {
+            Err(node_error!("Shard does not exist".to_string()))
+        } else {
+            Ok(VectorWriterService {
+                index: Index::open(path, IndexCheck::Sanity)?,
+                indexset: IndexSet::new(indexset, IndexCheck::Sanity)?,
+            })
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::{HashMap, HashSet};
+
+    use nucliadb_core::protos::resource::ResourceStatus;
+    use nucliadb_core::protos::{
+        IndexParagraph, IndexParagraphs, Resource, ResourceId, UserVector, UserVectors,
+        VectorSearchRequest, VectorSentence, VectorSimilarity,
+    };
+    use tempfile::TempDir;
+
+    use super::*;
+    use crate::service::reader::VectorReaderService;
+    fn create_vector_set(
+        writer: &mut VectorWriterService,
+        set_name: String,
+    ) -> (String, UserVectors) {
+        let label = format!("{set_name}/label");
+        let key = format!("{set_name}/key");
+        let vector = vec![1.0, 3.0, 4.0];
+        let data = UserVector {
+            vector,
+            labels: vec![label],
+            ..Default::default()
+        };
+        let set = UserVectors {
+            vectors: HashMap::from([(key, data)]),
+        };
+        let id = VectorSetId {
+            shard: None,
+            vectorset: set_name.clone(),
+        };
+        writer.add_vectorset(&id, VectorSimilarity::Cosine).unwrap();
+        (set_name, set)
+    }
+    #[test]
+    fn test_vectorset_functionality() {
+        let dir = TempDir::new().unwrap();
+        let vsc = VectorConfig {
+            similarity: Some(VectorSimilarity::Cosine),
+            path: dir.path().join("vectors"),
+            vectorset: dir.path().join("vectorsets"),
+        };
+
+        let mut writer = VectorWriterService::start(&vsc).expect("Error starting vector writer");
+        let indexes: HashMap<_, _> = (0..10)
+            .map(|i| create_vector_set(&mut writer, i.to_string()))
+            .collect();
+        let keys: HashSet<_> = indexes.keys().cloned().collect();
+        let resource = Resource {
+            vectors: indexes,
+            ..Default::default()
+        };
+        writer.set_resource(&resource).unwrap();
+        let index_keys: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
+        assert_eq!(index_keys, keys);
+
+        let reader = VectorReaderService::start(&vsc).unwrap();
+        let mut request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: "4".to_string(),
+            vector: vec![4.0, 6.0, 7.0],
+            tags: vec!["4/label".to_string()],
+            page_number: 0,
+            result_per_page: 20,
+            with_duplicates: false,
+            ..Default::default()
+        };
+        let results = reader.search(&request).unwrap();
+        let id = results.documents[0].doc_id.clone().unwrap().id;
+        assert_eq!(results.documents.len(), 1);
+        assert_eq!(id, "4/key");
+
+        // Same set, but no label match
+        request.tags = vec!["5/label".to_string()];
+        let results = reader.search(&request).unwrap();
+        assert_eq!(results.documents.len(), 0);
+
+        // Invalid set
+        request.vector_set = "not a set".to_string();
+        let results = reader.search(&request).unwrap();
+        assert_eq!(results.documents.len(), 0);
+
+        // Remove set 4
+        let id = VectorSetId {
+            vectorset: "4".to_string(),
+            ..Default::default()
+        };
+        let mut index_keys: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
+        writer.remove_vectorset(&id).unwrap();
+        let index_keysp: HashSet<_> = writer.list_vectorsets().unwrap().into_iter().collect();
+        index_keys.remove("4");
+        assert!(!index_keysp.contains("4"));
+        assert_eq!(index_keys, index_keysp);
+
+        // Now vectorset 4 is no longer available
+        request.vector_set = "4".to_string();
+        request.tags = vec!["4/label".to_string()];
+        let results = reader.search(&request).unwrap();
+        assert_eq!(results.documents.len(), 0);
+    }
+
+    #[test]
+    fn test_new_vector_writer() {
+        let dir = TempDir::new().unwrap();
+        let vsc = VectorConfig {
+            similarity: Some(VectorSimilarity::Cosine),
+            path: dir.path().join("vectors"),
+            vectorset: dir.path().join("vectorset"),
+        };
+        let raw_sentences = [
+            ("DOC/KEY/1/1".to_string(), vec![1.0, 3.0, 4.0]),
+            ("DOC/KEY/1/2".to_string(), vec![2.0, 4.0, 5.0]),
+            ("DOC/KEY/1/3".to_string(), vec![3.0, 5.0, 6.0]),
+        ];
+        let resource_id = ResourceId {
+            shard_id: "DOC".to_string(),
+            uuid: "DOC/KEY".to_string(),
+        };
+
+        let mut sentences = HashMap::new();
+        for (key, vector) in raw_sentences {
+            let vector = VectorSentence {
+                vector,
+                ..Default::default()
+            };
+            sentences.insert(key, vector);
+        }
+        let paragraph = IndexParagraph {
+            start: 0,
+            end: 0,
+            sentences,
+            field: "".to_string(),
+            labels: vec!["1".to_string(), "2".to_string(), "3".to_string()],
+            index: 3,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let paragraphs = IndexParagraphs {
+            paragraphs: HashMap::from([("DOC/KEY/1".to_string(), paragraph)]),
+        };
+        let resource = Resource {
+            resource: Some(resource_id.clone()),
+            metadata: None,
+            texts: HashMap::with_capacity(0),
+            status: ResourceStatus::Processed as i32,
+            labels: vec!["FULL".to_string()],
+            paragraphs: HashMap::from([("DOC/KEY".to_string(), paragraphs)]),
+            paragraphs_to_delete: vec![],
+            sentences_to_delete: vec![],
+            relations_to_delete: vec![],
+            relations: vec![],
+            vectors: HashMap::default(),
+            vectors_to_delete: HashMap::default(),
+            shard_id: "DOC".to_string(),
+        };
+        // insert - delete - insert sequence
+        let mut writer = VectorWriterService::start(&vsc).unwrap();
+        let res = writer.set_resource(&resource);
+        assert!(res.is_ok());
+        let res = writer.delete_resource(&resource_id);
+        assert!(res.is_ok());
+        let res = writer.set_resource(&resource);
+        assert!(res.is_ok());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/Cargo.toml`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-[package]
-name = "nucliadb_cluster"
-version = "0.1.0"
-authors = ['Bosutech S.L.']
-edition = "2021"
-license = "AGPL-3.0-or-later"
-description = "nucliadb cluster membership"
-repository = "https://github.com/stashify/nucliadb"
-homepage = "https://nuclia.com/"
-documentation = "https://nuclia.com/"
-
-[[bin]]
-name = "cluster_manager"
-path = "src/bin/manager.rs"
-
-[dependencies]
-anyhow = "1.0"
-async-trait = "0.1"
-serde = { version = "1.0", features = ["derive"] }
-serde_json = "1.0.81"
-thiserror = "1.0"
-tokio = { version = "1.7", features = ["full"] }
-tokio-stream = { version = "0.1.6", features = ["sync"] }
-tracing = "0.1"
-uuid = { version = "1.1", features = ["v4"] }
-log = "0.4.14"
-env_logger = "0.9.0"
-chitchat = "0.5.0"
-dockertest = "0.3.0"
-bytes = "1.1.0"
-strum = { version = "0.24.1", features = ["derive"] }
-clap = { version = "4.0.29", features = ["derive", "env"] }
-parse_duration = "2.1.1"
-derive_builder = "0.12.0"
-derive_more = { version = "0.99.17", default-features = false, features = ["display", "deref", "deref_mut"] }
-futures = "0.3.25"
-reqwest = { version = "0.11.17", default-features = true,  features = ["json"] }
-
-[dev-dependencies]
-serde_test = "1.0.150"
-serial_test = "0.10.0"
-
-[[test]]
-name = "integration"
-path = "tests/integration.rs"
+[package]
+name = "nucliadb_cluster"
+version = "0.1.0"
+authors = ['Bosutech S.L.']
+edition = "2021"
+license = "AGPL-3.0-or-later"
+description = "nucliadb cluster membership"
+repository = "https://github.com/stashify/nucliadb"
+homepage = "https://nuclia.com/"
+documentation = "https://nuclia.com/"
+
+[[bin]]
+name = "cluster_manager"
+path = "src/bin/manager.rs"
+
+[dependencies]
+anyhow = "1.0"
+async-trait = "0.1"
+serde = { version = "1.0", features = ["derive"] }
+serde_json = "1.0.81"
+thiserror = "1.0"
+tokio = { version = "1.7", features = ["full"] }
+tokio-stream = { version = "0.1.6", features = ["sync"] }
+tracing = "0.1"
+uuid = { version = "1.1", features = ["v4"] }
+log = "0.4.14"
+env_logger = "0.9.0"
+chitchat = "0.5.0"
+dockertest = "0.3.0"
+bytes = "1.1.0"
+strum = { version = "0.24.1", features = ["derive"] }
+clap = { version = "4.0.29", features = ["derive", "env"] }
+parse_duration = "2.1.1"
+derive_builder = "0.12.0"
+derive_more = { version = "0.99.17", default-features = false, features = ["display", "deref", "deref_mut"] }
+futures = "0.3.25"
+reqwest = { version = "0.11.17", default-features = true,  features = ["json"] }
+
+[dev-dependencies]
+serde_test = "1.0.150"
+serial_test = "0.10.0"
+
+[[test]]
+name = "integration"
+path = "tests/integration.rs"
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/LICENSE-AGPL` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/LICENSE-AGPL`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-Copyright (C) 2021 Bosutech, S.L.
-
-Copyright for portions of project nucliadb-cluster are held by Quickwit Inc as part of project `quickwit` (2021)
-
-All other copyright for project nucliadb-cluster are held by Bosutech S.L., 2021.
-
-AGPL:
-This program is free software: you can redistribute it and/or modify
-it under the terms of the GNU Affero General Public License as
-published by the Free Software Foundation, either version 3 of the
-License, or (at your option) any later version.
-
-This program is distributed in the hope that it will be useful,
-but WITHOUT ANY WARRANTY; without even the implied warranty of
-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-GNU Affero General Public License for more details.
-
-You should have received a copy of the GNU Affero General Public License
-along with this program. If not, see <http://www.gnu.org/licenses/>.
+Copyright (C) 2021 Bosutech, S.L.
+
+Copyright for portions of project nucliadb-cluster are held by Quickwit Inc as part of project `quickwit` (2021)
+
+All other copyright for project nucliadb-cluster are held by Bosutech S.L., 2021.
+
+AGPL:
+This program is free software: you can redistribute it and/or modify
+it under the terms of the GNU Affero General Public License as
+published by the Free Software Foundation, either version 3 of the
+License, or (at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+GNU Affero General Public License for more details.
+
+You should have received a copy of the GNU Affero General Public License
+along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/src/node.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/src/node.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,482 +1,482 @@
-use std::collections::HashMap;
-use std::fmt;
-use std::net::SocketAddr;
-use std::sync::Arc;
-use std::time::Duration;
-
-use anyhow::anyhow;
-use chitchat::transport::UdpTransport;
-use chitchat::{ChitchatConfig, ChitchatHandle, FailureDetectorConfig, NodeId};
-use derive_builder::Builder;
-use futures::{stream, Stream, StreamExt};
-use serde::{Deserialize, Serialize};
-use strum::{Display as EnumDisplay, EnumString};
-use uuid::Uuid;
-
-use crate::error::Error;
-use crate::key::Key;
-use crate::register::Register;
-
-const NODE_KIND_KEY: Key<NodeType> = Key::new("## nucliadb reserved ## node_type");
-const REGISTER_KEY: Key<Register> = Key::new("## nucliadb reserved ## register");
-
-/// Retrieve the information about a given list of cluster nodes.
-pub async fn cluster_snapshot(nodes: Vec<NodeHandle>) -> Vec<NodeSnapshot> {
-    stream::iter(nodes)
-        .then(|node| async move { node.snapshot().await })
-        .collect()
-        .await
-}
-
-#[non_exhaustive]
-#[derive(
-    Debug, Copy, Clone, PartialEq, Eq, Hash, EnumString, EnumDisplay, Serialize, Deserialize,
-)]
-pub enum NodeType {
-    Io,
-    Search,
-    Ingest,
-    Train,
-    Unknown,
-}
-
-/// Represents a single node instance in a distributed cluster based on scuttlebut protocol.
-///
-/// # Examples
-/// ```rust,no_run
-/// # #[tokio::main]
-/// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
-/// use nucliadb_cluster::{Node, NodeType};
-///
-/// let node = Node::builder()
-///     .register_as(NodeType::Ingest)
-///     .on_local_network("0.0.0.0:4000".parse()?)
-///     .build()?;
-///
-/// // Holds that handle to keep the node alive.
-/// let node = node.start().await?;
-///
-/// # Ok(())
-/// # }
-/// ```
-#[derive(Debug, Builder)]
-#[builder(pattern = "owned", setter(prefix = "with", strip_option, into))]
-pub struct Node {
-    /// The node identifier.
-    #[builder(default = "Uuid::new_v4().to_string()")]
-    id: String,
-    /// The type of the node.
-    #[builder(setter(name = "register_as"))]
-    r#type: NodeType,
-    /// The cluster identifier to join.
-    #[builder(
-        setter(name = "join_cluster"),
-        default = "Node::DEFAULT_CLUSTER_ID.to_string()"
-    )]
-    cluster_id: String,
-    /// A list of known nodes to connect with in order to join the cluster.
-    #[builder(default)]
-    seed_nodes: Vec<String>,
-    /// The address the node will listen to.
-    /// Note that address can be the same as the `public_address` if the node
-    /// joins a local cluster.
-    #[builder(setter(custom))]
-    listen_address: SocketAddr,
-    /// The public address to communicate with the node.
-    #[builder(setter(custom))]
-    public_address: SocketAddr,
-    /// The interval between each node update.
-    /// An update consists to send/receive the node state to/from other cluster nodes.
-    #[builder(default = "Node::DEFAULT_UPDATE_INTERVAL")]
-    update_interval: Duration,
-    /// The initial node state.
-    #[builder(setter(custom), default)]
-    initial_state: Vec<(String, String)>,
-}
-
-impl NodeBuilder {
-    /// Configures the node to join a local cluster network.
-    pub fn on_local_network(mut self, address: SocketAddr) -> Self {
-        self.listen_address = Some(address);
-        self.public_address = Some(address);
-
-        self
-    }
-
-    /// Configures the node to join a public cluster network.
-    pub fn on_public_network(
-        mut self,
-        public_address: SocketAddr,
-        listen_address: SocketAddr,
-    ) -> Self {
-        self.listen_address = Some(listen_address);
-        self.public_address = Some(public_address);
-
-        self
-    }
-
-    /// Inserts the given key/value to the initial node state.
-    ///
-    /// Note that, in case of key duplication, only the last one will be kept in the state.
-    pub fn insert_to_initial_state<T: ToString>(mut self, key: Key<T>, value: T) -> Self {
-        self.initial_state
-            .get_or_insert_with(Vec::default)
-            .push((key.to_string(), value.to_string()));
-
-        self
-    }
-}
-
-impl Node {
-    const DEFAULT_CLUSTER_ID: &str = "nucliadb-cluster";
-    const DEFAULT_UPDATE_INTERVAL: Duration = Duration::from_millis(100);
-
-    /// Returns a builder to create a custom `Node`.
-    pub fn builder() -> NodeBuilder {
-        NodeBuilder::default()
-    }
-
-    /// Starts the node and join the cluster.
-    ///
-    /// The handle returning by this method must be kept in order to keep alive the node
-    /// and retrieve information about the cluster.
-    ///
-    /// # Errors
-    /// This method may fails in the node does not succeed to join the cluster.
-    pub async fn start(mut self) -> Result<NodeHandle, Error> {
-        let node_id = NodeId::new(self.id, self.public_address);
-
-        let configuration = ChitchatConfig {
-            node_id: node_id.clone(),
-            cluster_id: self.cluster_id,
-            gossip_interval: self.update_interval,
-            listen_addr: self.listen_address,
-            seed_nodes: self.seed_nodes,
-            failure_detector_config: FailureDetectorConfig::default(),
-            is_ready_predicate: None,
-        };
-
-        let register = Register::with_keys(
-            self.initial_state
-                .iter()
-                .cloned()
-                .map(|(key, _)| key)
-                .collect(),
-        );
-
-        self.initial_state
-            .push((REGISTER_KEY.to_string(), register.to_string()));
-        self.initial_state
-            .push((NODE_KIND_KEY.to_string(), self.r#type.to_string()));
-
-        let handle = chitchat::spawn_chitchat(configuration, self.initial_state, &UdpTransport)
-            .await
-            .map_err(Error::Start)?;
-
-        Ok(NodeHandle {
-            node_id,
-            handle: Arc::new(handle),
-        })
-    }
-}
-
-/// A handle of the started node.
-///
-/// This type permits to watch the cluster joined by the node and retrieve
-/// information about the cluster nodes as well.
-#[must_use]
-#[derive(Clone)]
-pub struct NodeHandle {
-    node_id: NodeId,
-    handle: Arc<ChitchatHandle>,
-}
-
-impl fmt::Debug for NodeHandle {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        f.debug_struct("NodeHandle")
-            .field("node_id", &self.node_id)
-            .finish()
-    }
-}
-
-impl NodeHandle {
-    /// Returns the node identifier.
-    pub fn id(&self) -> &str {
-        &self.node_id.id
-    }
-
-    /// Update the node state with the given key/value.
-    ///
-    /// Note that the new node state will be propagated only on the next cluster update.
-    pub async fn update_state<T: ToString>(&self, key: Key<T>, value: T) {
-        let chitchat = self.handle.chitchat();
-        let mut chitchat = chitchat.lock().await;
-
-        let state = chitchat.self_node_state();
-        let mut register: Register = state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
-
-        register.insert(key.to_string());
-
-        state.set(REGISTER_KEY, register);
-        state.set(key, value);
-    }
-
-    /// Shuts the node down by notifying and quitting the cluster.
-    pub async fn shutdown(self) -> Result<(), Error> {
-        let handle =
-            Arc::try_unwrap(self.handle).map_err(|_| Error::Shutdown(anyhow!("node is in use")))?;
-
-        handle.shutdown().await.map_err(Error::Shutdown)
-    }
-
-    /// Returns a stream over any changes in the cluster.
-    pub async fn cluster_watcher(&self) -> impl Stream<Item = Vec<NodeHandle>> + 'static {
-        let handle = Arc::clone(&self.handle);
-        let node_id = self.node_id.clone();
-
-        self.handle
-            .chitchat()
-            .lock()
-            .await
-            .ready_nodes_watcher()
-            .map(move |nodes| {
-                nodes
-                    .into_iter()
-                    // `ready_nodes_watcher` does not contains this node so we add it
-                    // to have the complete list of live nodes.
-                    .chain([node_id.clone()])
-                    .map(|node_id| NodeHandle {
-                        node_id,
-                        handle: Arc::clone(&handle),
-                    })
-                    .collect()
-            })
-    }
-
-    /// Returns the current list of live nodes in the cluster.
-    pub async fn live_nodes(&self) -> Vec<NodeHandle> {
-        self.handle
-            .chitchat()
-            .lock()
-            .await
-            .live_nodes()
-            .cloned()
-            // `live_nodes` does not contains this node so we add it
-            // to have the complete list of live nodes.
-            .chain([self.node_id.clone()])
-            .map(|node_id| NodeHandle {
-                node_id,
-                handle: Arc::clone(&self.handle),
-            })
-            .collect()
-    }
-
-    /// Returns a node snapshot.
-    pub async fn snapshot(&self) -> NodeSnapshot {
-        self.handle
-            .with_chitchat(|chitchat| {
-                let state = chitchat.node_state(&self.node_id).unwrap();
-                let register: Register =
-                    state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
-
-                let r#type = state
-                    .get(NODE_KIND_KEY.as_str())
-                    .and_then(|s| s.parse().ok())
-                    .unwrap_or(NodeType::Unknown);
-
-                let state = register
-                    .iter()
-                    .filter_map(|key| {
-                        state
-                            .get(key)
-                            .map(|value| (key.to_string(), value.to_string()))
-                    })
-                    .collect();
-
-                NodeSnapshot {
-                    id: self.node_id.id.to_string(),
-                    address: self.node_id.gossip_public_address,
-                    is_self: chitchat.self_node_id() == &self.node_id,
-                    r#type,
-                    state,
-                }
-            })
-            .await
-    }
-}
-
-/// A snapshot of a cluster node with its state and meta information.
-#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
-pub struct NodeSnapshot {
-    id: String,
-    r#type: NodeType,
-    address: SocketAddr,
-    is_self: bool,
-    #[serde(flatten)]
-    state: HashMap<String, String>,
-}
-
-#[cfg(test)]
-mod tests {
-    use serde_test::{assert_tokens, Token};
-
-    use super::*;
-
-    #[test]
-    fn it_ser_de_node_type() {
-        let data = [
-            (NodeType::Io, "Io"),
-            (NodeType::Search, "Search"),
-            (NodeType::Ingest, "Ingest"),
-            (NodeType::Train, "Train"),
-            (NodeType::Unknown, "Unknown"),
-        ];
-
-        for (node, variant) in data {
-            assert_tokens(
-                &node,
-                &[Token::UnitVariant {
-                    name: "NodeType",
-                    variant,
-                }],
-            );
-        }
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_registers_user_keys_in_state() -> Result<(), Box<dyn std::error::Error>> {
-        const A_KEY: Key<u32> = Key::new("a-key");
-        const B_KEY: Key<f32> = Key::new("b-key");
-        const C_KEY: Key<char> = Key::new("c-key");
-
-        let node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(A_KEY, 42)
-            .insert_to_initial_state(B_KEY, 0.0)
-            .build()?;
-
-        let node = node.start().await?;
-
-        let retrieve_registered_keys = || async {
-            node.handle
-                .chitchat()
-                .lock()
-                .await
-                .self_node_state()
-                .get(REGISTER_KEY.as_str())
-                .map(Register::from)
-        };
-
-        assert_eq!(
-            retrieve_registered_keys().await,
-            Some(Register::with_keys(
-                ["a-key".to_string(), "b-key".to_string()]
-                    .into_iter()
-                    .collect()
-            ))
-        );
-
-        node.update_state(C_KEY, 'a').await;
-
-        assert_eq!(
-            retrieve_registered_keys().await,
-            Some(Register::with_keys(
-                [
-                    "a-key".to_string(),
-                    "b-key".to_string(),
-                    "c-key".to_string()
-                ]
-                .into_iter()
-                .collect()
-            ))
-        );
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_builds_node_snapshot() -> Result<(), Box<dyn std::error::Error>> {
-        const LOAD_SCORE_KEY: Key<f32> = Key::new("load-score");
-
-        let node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
-            .build()?;
-
-        let node = node.start().await?;
-        let snapshot = node.snapshot().await;
-
-        assert_eq!(
-            snapshot,
-            NodeSnapshot {
-                id: node.node_id.id,
-                r#type: NodeType::Io,
-                address: node.node_id.gossip_public_address,
-                is_self: true,
-                state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
-                    .into_iter()
-                    .collect(),
-            }
-        );
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_builds_cluster_snapshot() -> Result<(), Box<dyn std::error::Error>> {
-        const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
-
-        let first_node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
-            .build()?;
-
-        let first_node = first_node.start().await?;
-
-        let second_node = Node::builder()
-            .on_local_network("0.0.0.0:8081".parse()?)
-            .register_as(NodeType::Ingest)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 40.0)
-            .with_seed_nodes(vec!["0.0.0.0:8080".to_string()])
-            .build()?;
-
-        let second_node = second_node.start().await?;
-
-        tokio::time::sleep(Node::DEFAULT_UPDATE_INTERVAL * 2).await;
-
-        let live_nodes = first_node.live_nodes().await;
-        let snapshots = cluster_snapshot(live_nodes).await;
-
-        assert_eq!(
-            snapshots,
-            vec![
-                NodeSnapshot {
-                    id: second_node.node_id.id,
-                    r#type: NodeType::Ingest,
-                    address: second_node.node_id.gossip_public_address,
-                    is_self: false,
-                    state: [(LOAD_SCORE_KEY.to_string(), 40.to_string())]
-                        .into_iter()
-                        .collect(),
-                },
-                NodeSnapshot {
-                    id: first_node.node_id.id,
-                    r#type: NodeType::Io,
-                    address: first_node.node_id.gossip_public_address,
-                    is_self: true,
-                    state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
-                        .into_iter()
-                        .collect(),
-                },
-            ]
-        );
-
-        Ok(())
-    }
-}
+use std::collections::HashMap;
+use std::fmt;
+use std::net::SocketAddr;
+use std::sync::Arc;
+use std::time::Duration;
+
+use anyhow::anyhow;
+use chitchat::transport::UdpTransport;
+use chitchat::{ChitchatConfig, ChitchatHandle, FailureDetectorConfig, NodeId};
+use derive_builder::Builder;
+use futures::{stream, Stream, StreamExt};
+use serde::{Deserialize, Serialize};
+use strum::{Display as EnumDisplay, EnumString};
+use uuid::Uuid;
+
+use crate::error::Error;
+use crate::key::Key;
+use crate::register::Register;
+
+const NODE_KIND_KEY: Key<NodeType> = Key::new("## nucliadb reserved ## node_type");
+const REGISTER_KEY: Key<Register> = Key::new("## nucliadb reserved ## register");
+
+/// Retrieve the information about a given list of cluster nodes.
+pub async fn cluster_snapshot(nodes: Vec<NodeHandle>) -> Vec<NodeSnapshot> {
+    stream::iter(nodes)
+        .then(|node| async move { node.snapshot().await })
+        .collect()
+        .await
+}
+
+#[non_exhaustive]
+#[derive(
+    Debug, Copy, Clone, PartialEq, Eq, Hash, EnumString, EnumDisplay, Serialize, Deserialize,
+)]
+pub enum NodeType {
+    Io,
+    Search,
+    Ingest,
+    Train,
+    Unknown,
+}
+
+/// Represents a single node instance in a distributed cluster based on scuttlebut protocol.
+///
+/// # Examples
+/// ```rust,no_run
+/// # #[tokio::main]
+/// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
+/// use nucliadb_cluster::{Node, NodeType};
+///
+/// let node = Node::builder()
+///     .register_as(NodeType::Ingest)
+///     .on_local_network("0.0.0.0:4000".parse()?)
+///     .build()?;
+///
+/// // Holds that handle to keep the node alive.
+/// let node = node.start().await?;
+///
+/// # Ok(())
+/// # }
+/// ```
+#[derive(Debug, Builder)]
+#[builder(pattern = "owned", setter(prefix = "with", strip_option, into))]
+pub struct Node {
+    /// The node identifier.
+    #[builder(default = "Uuid::new_v4().to_string()")]
+    id: String,
+    /// The type of the node.
+    #[builder(setter(name = "register_as"))]
+    r#type: NodeType,
+    /// The cluster identifier to join.
+    #[builder(
+        setter(name = "join_cluster"),
+        default = "Node::DEFAULT_CLUSTER_ID.to_string()"
+    )]
+    cluster_id: String,
+    /// A list of known nodes to connect with in order to join the cluster.
+    #[builder(default)]
+    seed_nodes: Vec<String>,
+    /// The address the node will listen to.
+    /// Note that address can be the same as the `public_address` if the node
+    /// joins a local cluster.
+    #[builder(setter(custom))]
+    listen_address: SocketAddr,
+    /// The public address to communicate with the node.
+    #[builder(setter(custom))]
+    public_address: SocketAddr,
+    /// The interval between each node update.
+    /// An update consists to send/receive the node state to/from other cluster nodes.
+    #[builder(default = "Node::DEFAULT_UPDATE_INTERVAL")]
+    update_interval: Duration,
+    /// The initial node state.
+    #[builder(setter(custom), default)]
+    initial_state: Vec<(String, String)>,
+}
+
+impl NodeBuilder {
+    /// Configures the node to join a local cluster network.
+    pub fn on_local_network(mut self, address: SocketAddr) -> Self {
+        self.listen_address = Some(address);
+        self.public_address = Some(address);
+
+        self
+    }
+
+    /// Configures the node to join a public cluster network.
+    pub fn on_public_network(
+        mut self,
+        public_address: SocketAddr,
+        listen_address: SocketAddr,
+    ) -> Self {
+        self.listen_address = Some(listen_address);
+        self.public_address = Some(public_address);
+
+        self
+    }
+
+    /// Inserts the given key/value to the initial node state.
+    ///
+    /// Note that, in case of key duplication, only the last one will be kept in the state.
+    pub fn insert_to_initial_state<T: ToString>(mut self, key: Key<T>, value: T) -> Self {
+        self.initial_state
+            .get_or_insert_with(Vec::default)
+            .push((key.to_string(), value.to_string()));
+
+        self
+    }
+}
+
+impl Node {
+    const DEFAULT_CLUSTER_ID: &str = "nucliadb-cluster";
+    const DEFAULT_UPDATE_INTERVAL: Duration = Duration::from_millis(100);
+
+    /// Returns a builder to create a custom `Node`.
+    pub fn builder() -> NodeBuilder {
+        NodeBuilder::default()
+    }
+
+    /// Starts the node and join the cluster.
+    ///
+    /// The handle returning by this method must be kept in order to keep alive the node
+    /// and retrieve information about the cluster.
+    ///
+    /// # Errors
+    /// This method may fails in the node does not succeed to join the cluster.
+    pub async fn start(mut self) -> Result<NodeHandle, Error> {
+        let node_id = NodeId::new(self.id, self.public_address);
+
+        let configuration = ChitchatConfig {
+            node_id: node_id.clone(),
+            cluster_id: self.cluster_id,
+            gossip_interval: self.update_interval,
+            listen_addr: self.listen_address,
+            seed_nodes: self.seed_nodes,
+            failure_detector_config: FailureDetectorConfig::default(),
+            is_ready_predicate: None,
+        };
+
+        let register = Register::with_keys(
+            self.initial_state
+                .iter()
+                .cloned()
+                .map(|(key, _)| key)
+                .collect(),
+        );
+
+        self.initial_state
+            .push((REGISTER_KEY.to_string(), register.to_string()));
+        self.initial_state
+            .push((NODE_KIND_KEY.to_string(), self.r#type.to_string()));
+
+        let handle = chitchat::spawn_chitchat(configuration, self.initial_state, &UdpTransport)
+            .await
+            .map_err(Error::Start)?;
+
+        Ok(NodeHandle {
+            node_id,
+            handle: Arc::new(handle),
+        })
+    }
+}
+
+/// A handle of the started node.
+///
+/// This type permits to watch the cluster joined by the node and retrieve
+/// information about the cluster nodes as well.
+#[must_use]
+#[derive(Clone)]
+pub struct NodeHandle {
+    node_id: NodeId,
+    handle: Arc<ChitchatHandle>,
+}
+
+impl fmt::Debug for NodeHandle {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        f.debug_struct("NodeHandle")
+            .field("node_id", &self.node_id)
+            .finish()
+    }
+}
+
+impl NodeHandle {
+    /// Returns the node identifier.
+    pub fn id(&self) -> &str {
+        &self.node_id.id
+    }
+
+    /// Update the node state with the given key/value.
+    ///
+    /// Note that the new node state will be propagated only on the next cluster update.
+    pub async fn update_state<T: ToString>(&self, key: Key<T>, value: T) {
+        let chitchat = self.handle.chitchat();
+        let mut chitchat = chitchat.lock().await;
+
+        let state = chitchat.self_node_state();
+        let mut register: Register = state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
+
+        register.insert(key.to_string());
+
+        state.set(REGISTER_KEY, register);
+        state.set(key, value);
+    }
+
+    /// Shuts the node down by notifying and quitting the cluster.
+    pub async fn shutdown(self) -> Result<(), Error> {
+        let handle =
+            Arc::try_unwrap(self.handle).map_err(|_| Error::Shutdown(anyhow!("node is in use")))?;
+
+        handle.shutdown().await.map_err(Error::Shutdown)
+    }
+
+    /// Returns a stream over any changes in the cluster.
+    pub async fn cluster_watcher(&self) -> impl Stream<Item = Vec<NodeHandle>> + 'static {
+        let handle = Arc::clone(&self.handle);
+        let node_id = self.node_id.clone();
+
+        self.handle
+            .chitchat()
+            .lock()
+            .await
+            .ready_nodes_watcher()
+            .map(move |nodes| {
+                nodes
+                    .into_iter()
+                    // `ready_nodes_watcher` does not contains this node so we add it
+                    // to have the complete list of live nodes.
+                    .chain([node_id.clone()])
+                    .map(|node_id| NodeHandle {
+                        node_id,
+                        handle: Arc::clone(&handle),
+                    })
+                    .collect()
+            })
+    }
+
+    /// Returns the current list of live nodes in the cluster.
+    pub async fn live_nodes(&self) -> Vec<NodeHandle> {
+        self.handle
+            .chitchat()
+            .lock()
+            .await
+            .live_nodes()
+            .cloned()
+            // `live_nodes` does not contains this node so we add it
+            // to have the complete list of live nodes.
+            .chain([self.node_id.clone()])
+            .map(|node_id| NodeHandle {
+                node_id,
+                handle: Arc::clone(&self.handle),
+            })
+            .collect()
+    }
+
+    /// Returns a node snapshot.
+    pub async fn snapshot(&self) -> NodeSnapshot {
+        self.handle
+            .with_chitchat(|chitchat| {
+                let state = chitchat.node_state(&self.node_id).unwrap();
+                let register: Register =
+                    state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
+
+                let r#type = state
+                    .get(NODE_KIND_KEY.as_str())
+                    .and_then(|s| s.parse().ok())
+                    .unwrap_or(NodeType::Unknown);
+
+                let state = register
+                    .iter()
+                    .filter_map(|key| {
+                        state
+                            .get(key)
+                            .map(|value| (key.to_string(), value.to_string()))
+                    })
+                    .collect();
+
+                NodeSnapshot {
+                    id: self.node_id.id.to_string(),
+                    address: self.node_id.gossip_public_address,
+                    is_self: chitchat.self_node_id() == &self.node_id,
+                    r#type,
+                    state,
+                }
+            })
+            .await
+    }
+}
+
+/// A snapshot of a cluster node with its state and meta information.
+#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
+pub struct NodeSnapshot {
+    id: String,
+    r#type: NodeType,
+    address: SocketAddr,
+    is_self: bool,
+    #[serde(flatten)]
+    state: HashMap<String, String>,
+}
+
+#[cfg(test)]
+mod tests {
+    use serde_test::{assert_tokens, Token};
+
+    use super::*;
+
+    #[test]
+    fn it_ser_de_node_type() {
+        let data = [
+            (NodeType::Io, "Io"),
+            (NodeType::Search, "Search"),
+            (NodeType::Ingest, "Ingest"),
+            (NodeType::Train, "Train"),
+            (NodeType::Unknown, "Unknown"),
+        ];
+
+        for (node, variant) in data {
+            assert_tokens(
+                &node,
+                &[Token::UnitVariant {
+                    name: "NodeType",
+                    variant,
+                }],
+            );
+        }
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_registers_user_keys_in_state() -> Result<(), Box<dyn std::error::Error>> {
+        const A_KEY: Key<u32> = Key::new("a-key");
+        const B_KEY: Key<f32> = Key::new("b-key");
+        const C_KEY: Key<char> = Key::new("c-key");
+
+        let node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(A_KEY, 42)
+            .insert_to_initial_state(B_KEY, 0.0)
+            .build()?;
+
+        let node = node.start().await?;
+
+        let retrieve_registered_keys = || async {
+            node.handle
+                .chitchat()
+                .lock()
+                .await
+                .self_node_state()
+                .get(REGISTER_KEY.as_str())
+                .map(Register::from)
+        };
+
+        assert_eq!(
+            retrieve_registered_keys().await,
+            Some(Register::with_keys(
+                ["a-key".to_string(), "b-key".to_string()]
+                    .into_iter()
+                    .collect()
+            ))
+        );
+
+        node.update_state(C_KEY, 'a').await;
+
+        assert_eq!(
+            retrieve_registered_keys().await,
+            Some(Register::with_keys(
+                [
+                    "a-key".to_string(),
+                    "b-key".to_string(),
+                    "c-key".to_string()
+                ]
+                .into_iter()
+                .collect()
+            ))
+        );
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_builds_node_snapshot() -> Result<(), Box<dyn std::error::Error>> {
+        const LOAD_SCORE_KEY: Key<f32> = Key::new("load-score");
+
+        let node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
+            .build()?;
+
+        let node = node.start().await?;
+        let snapshot = node.snapshot().await;
+
+        assert_eq!(
+            snapshot,
+            NodeSnapshot {
+                id: node.node_id.id,
+                r#type: NodeType::Io,
+                address: node.node_id.gossip_public_address,
+                is_self: true,
+                state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
+                    .into_iter()
+                    .collect(),
+            }
+        );
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_builds_cluster_snapshot() -> Result<(), Box<dyn std::error::Error>> {
+        const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
+
+        let first_node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
+            .build()?;
+
+        let first_node = first_node.start().await?;
+
+        let second_node = Node::builder()
+            .on_local_network("0.0.0.0:8081".parse()?)
+            .register_as(NodeType::Ingest)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 40.0)
+            .with_seed_nodes(vec!["0.0.0.0:8080".to_string()])
+            .build()?;
+
+        let second_node = second_node.start().await?;
+
+        tokio::time::sleep(Node::DEFAULT_UPDATE_INTERVAL * 2).await;
+
+        let live_nodes = first_node.live_nodes().await;
+        let snapshots = cluster_snapshot(live_nodes).await;
+
+        assert_eq!(
+            snapshots,
+            vec![
+                NodeSnapshot {
+                    id: second_node.node_id.id,
+                    r#type: NodeType::Ingest,
+                    address: second_node.node_id.gossip_public_address,
+                    is_self: false,
+                    state: [(LOAD_SCORE_KEY.to_string(), 40.to_string())]
+                        .into_iter()
+                        .collect(),
+                },
+                NodeSnapshot {
+                    id: first_node.node_id.id,
+                    r#type: NodeType::Io,
+                    address: first_node.node_id.gossip_public_address,
+                    is_self: true,
+                    state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
+                        .into_iter()
+                        .collect(),
+                },
+            ]
+        );
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/tests/cluster_reader.py` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/tests/cluster_reader.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-#just for local tests
-from nucliadb_cluster_rust import Member
-import asyncio, socket
-import json
-from codecs import StreamReader, StreamWriter
-from typing import Optional, List
-
-class UnSockReader:
-    socket_reader: Optional[asyncio.Task] = None
-    
-    def __init__(self, path):
-        self.sock_path = path
-        self.socket_reader = None
-    
-    async def handle_connection(self, reader: StreamReader, _):
-        print("Handle new connection via unix socket")
-        while True:
-            try:
-                update_readed = await reader.read(512)
-                if len(update_readed) == 0:
-                    print("connection closed by cluster manager")
-                    break
-                json_data: List[Member] = json.loads(update_readed.decode("utf8").replace("'", '"'))
-                print(json.dumps(json_data, indent=2, sort_keys=True))
-            except IOError as e:
-                print(f"error while reading from unix socket: {e}")
-
-    def run_server(self):
-        self.socket_reader = asyncio.start_unix_server(self.handle_connection, self.sock_path)
-        asyncio.create_task(self.socket_reader, name="socket_reader")
-        loop = asyncio.get_event_loop()
-        loop.run_forever()
-
-if __name__ == "__main__":
-    reader = UnSockReader("/tmp/rust_python.sock")
+#just for local tests
+from nucliadb_cluster_rust import Member
+import asyncio, socket
+import json
+from codecs import StreamReader, StreamWriter
+from typing import Optional, List
+
+class UnSockReader:
+    socket_reader: Optional[asyncio.Task] = None
+    
+    def __init__(self, path):
+        self.sock_path = path
+        self.socket_reader = None
+    
+    async def handle_connection(self, reader: StreamReader, _):
+        print("Handle new connection via unix socket")
+        while True:
+            try:
+                update_readed = await reader.read(512)
+                if len(update_readed) == 0:
+                    print("connection closed by cluster manager")
+                    break
+                json_data: List[Member] = json.loads(update_readed.decode("utf8").replace("'", '"'))
+                print(json.dumps(json_data, indent=2, sort_keys=True))
+            except IOError as e:
+                print(f"error while reading from unix socket: {e}")
+
+    def run_server(self):
+        self.socket_reader = asyncio.start_unix_server(self.handle_connection, self.sock_path)
+        asyncio.create_task(self.socket_reader, name="socket_reader")
+        loop = asyncio.get_event_loop()
+        loop.run_forever()
+
+if __name__ == "__main__":
+    reader = UnSockReader("/tmp/rust_python.sock")
     reader.run_server()
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_cluster/tests/integration.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_cluster/tests/integration.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-use std::net::{IpAddr, Ipv4Addr, SocketAddr, TcpListener};
-use std::str::FromStr;
-use std::sync::atomic::{AtomicUsize, Ordering};
-use std::time::Duration;
-use std::{env, io};
-
-use bytes::BytesMut;
-use dockertest::{Composition, DockerTest, StartPolicy};
-use log::error;
-use nucliadb_cluster::{Node, NodeHandle, NodeSnapshot, NodeType};
-use tokio::net::UnixStream;
-use tokio::sync::mpsc;
-use tokio_stream::StreamExt;
-use uuid::Uuid;
-
-const SEED_NODE: &str = "0.0.0.0:4040";
-const UPDATE_INTERVAL: Duration = Duration::from_millis(250);
-
-pub async fn create_seed_node() -> anyhow::Result<NodeHandle> {
-    let node = Node::builder()
-        .register_as(NodeType::Io)
-        .on_local_network(SocketAddr::from_str(SEED_NODE).unwrap())
-        .with_seed_nodes(vec![SEED_NODE.to_string()])
-        .with_update_interval(UPDATE_INTERVAL)
-        .build()?;
-
-    let node = node.start().await?;
-
-    Ok(node)
-}
-
-pub fn find_available_port() -> anyhow::Result<u16> {
-    let socket = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1)), 0);
-    let listener = TcpListener::bind(socket)?;
-    let port = listener.local_addr()?.port();
-
-    Ok(port)
-}
-
-pub async fn create_node_for_test_with_id(
-    id: String,
-    seed_node: String,
-) -> anyhow::Result<NodeHandle> {
-    let port = find_available_port()?;
-
-    eprintln!("port: {port}");
-
-    let node = Node::builder()
-        .with_id(id)
-        .register_as(NodeType::Io)
-        .on_local_network(SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), port))
-        .with_seed_nodes(vec![seed_node])
-        .with_update_interval(UPDATE_INTERVAL)
-        .build()?;
-
-    let node = node.start().await?;
-
-    Ok(node)
-}
-
-/// Creates a local cluster listening on a random port.
-pub async fn create_node_for_test(seed_node: String) -> anyhow::Result<NodeHandle> {
-    let peer_uuid = Uuid::new_v4().to_string();
-    let node = create_node_for_test_with_id(peer_uuid, seed_node).await?;
-
-    Ok(node)
-}
-
-pub fn setup_logging_for_tests() {
-    use std::sync::Once;
-    static INIT: Once = Once::new();
-    INIT.call_once(|| {
-        env_logger::builder().format_timestamp(None).init();
-    });
-}
-
-#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
-#[serial_test::serial("cluster")]
-async fn test_cluster_single_node() {
-    setup_logging_for_tests();
-    // create seed node
-    let node = create_seed_node().await.unwrap();
-
-    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
-
-    let live_nodes = node.live_nodes().await;
-
-    assert_eq!(live_nodes.len(), 1);
-}
-
-#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
-#[serial_test::serial("cluster")]
-async fn test_cluster_two_nodes() {
-    setup_logging_for_tests();
-    // create seed node
-    let first_node = create_seed_node().await.unwrap();
-    let mut cluster_watcher = first_node.cluster_watcher().await;
-
-    // add node to cluster
-    let second_node = create_node_for_test(SEED_NODE.to_string()).await.unwrap();
-
-    // allow nodes start and communicate
-    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
-
-    match tokio::time::timeout(UPDATE_INTERVAL, cluster_watcher.next()).await {
-        Ok(Some(live_nodes)) => {
-            let live_nodes = live_nodes
-                .into_iter()
-                .map(|node| node.id().to_string())
-                .collect::<Vec<_>>();
-
-            assert_eq!(live_nodes.len(), 2);
-            assert!(live_nodes.contains(&first_node.id().to_string()));
-            assert!(live_nodes.contains(&second_node.id().to_string()));
-        }
-        Ok(None) => {
-            panic!("no changes in cluster");
-        }
-        Err(e) => {
-            panic!("timeout while waiting cluster changes: {e}");
-        }
-    }
-}
-
-enum NodeOperation {
-    Add,
-    Delete,
-}
-
-struct TestClusterState {
-    nodes: AtomicUsize,
-}
-
-impl TestClusterState {
-    fn new() -> Self {
-        Self {
-            nodes: AtomicUsize::new(0),
-        }
-    }
-
-    pub(crate) fn change_state(&self, ops: &NodeOperation) {
-        match ops {
-            NodeOperation::Add => self.nodes.fetch_add(1, Ordering::AcqRel),
-            NodeOperation::Delete => self.nodes.fetch_sub(1, Ordering::AcqRel),
-        };
-    }
-
-    pub(crate) fn nodes(&self) -> usize {
-        self.nodes.load(Ordering::Relaxed)
-    }
-}
-
-#[ignore = "ignore"]
-#[allow(unused_assignments)]
-#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
-async fn test_integration_3_nodes_with_monitor() {
-    let registry = env::var("IMAGE_REPOSITORY").unwrap();
-    let sock_path = env::var("SOCKET_PATH").unwrap();
-    let mut docker = DockerTest::new();
-
-    let unix_stream = UnixStream::connect(sock_path.clone()).await.unwrap();
-    let (tx, mut rx) = mpsc::channel::<()>(2);
-    let state = TestClusterState::new();
-    let mut operation = NodeOperation::Add;
-
-    tokio::spawn(async move {
-        loop {
-            if unix_stream.readable().await.is_ok() {
-                let mut buffer = BytesMut::with_capacity(512);
-                match unix_stream.try_read(&mut buffer) {
-                    Ok(bytes_read) => {
-                        assert_ne!(
-                            bytes_read, 0,
-                            "0 bytes read from socket. Connection closed by writer"
-                        );
-                        let update = serde_json::from_slice::<Vec<NodeSnapshot>>(&buffer).unwrap();
-                        state.change_state(&operation);
-                        assert_eq!(update.len(), state.nodes());
-                        if state.nodes() == 2 {
-                            tx.send(()).await.unwrap();
-                            operation = NodeOperation::Delete;
-                            break; // TODO: when dockertest crate will implement stop/pause delete
-                                   // this break
-                        }
-                    }
-                    Err(ref e) if e.kind() == io::ErrorKind::WouldBlock => continue,
-                    Err(e) => {
-                        error!("error during reading from unix socket: {e}");
-                        break;
-                    }
-                }
-            }
-        }
-    });
-
-    let mut watcher_node = Composition::with_repository(registry.clone())
-        .with_container_name("node_with_watcher")
-        .with_start_policy(StartPolicy::Strict);
-    watcher_node.bind_mount(sock_path.clone(), sock_path.clone());
-
-    let node2 = Composition::with_repository(registry.clone())
-        .with_container_name("node2")
-        .with_start_policy(StartPolicy::Strict);
-
-    let node3 = Composition::with_repository(registry)
-        .with_container_name("node3")
-        .with_start_policy(StartPolicy::Strict);
-
-    docker.add_composition(watcher_node);
-    docker.add_composition(node2);
-    docker.add_composition(node3);
-    docker.run(|_ops| async move {
-        // wait until all containers started and all nodes will be in cluster
-        assert_eq!(Some(()), rx.recv().await);
-
-        // TODO: when dockertest crate will implement stop/pause for containers
-        // sequentially turn off the nodes from the cluster
-    });
-}
+use std::net::{IpAddr, Ipv4Addr, SocketAddr, TcpListener};
+use std::str::FromStr;
+use std::sync::atomic::{AtomicUsize, Ordering};
+use std::time::Duration;
+use std::{env, io};
+
+use bytes::BytesMut;
+use dockertest::{Composition, DockerTest, StartPolicy};
+use log::error;
+use nucliadb_cluster::{Node, NodeHandle, NodeSnapshot, NodeType};
+use tokio::net::UnixStream;
+use tokio::sync::mpsc;
+use tokio_stream::StreamExt;
+use uuid::Uuid;
+
+const SEED_NODE: &str = "0.0.0.0:4040";
+const UPDATE_INTERVAL: Duration = Duration::from_millis(250);
+
+pub async fn create_seed_node() -> anyhow::Result<NodeHandle> {
+    let node = Node::builder()
+        .register_as(NodeType::Io)
+        .on_local_network(SocketAddr::from_str(SEED_NODE).unwrap())
+        .with_seed_nodes(vec![SEED_NODE.to_string()])
+        .with_update_interval(UPDATE_INTERVAL)
+        .build()?;
+
+    let node = node.start().await?;
+
+    Ok(node)
+}
+
+pub fn find_available_port() -> anyhow::Result<u16> {
+    let socket = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1)), 0);
+    let listener = TcpListener::bind(socket)?;
+    let port = listener.local_addr()?.port();
+
+    Ok(port)
+}
+
+pub async fn create_node_for_test_with_id(
+    id: String,
+    seed_node: String,
+) -> anyhow::Result<NodeHandle> {
+    let port = find_available_port()?;
+
+    eprintln!("port: {port}");
+
+    let node = Node::builder()
+        .with_id(id)
+        .register_as(NodeType::Io)
+        .on_local_network(SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), port))
+        .with_seed_nodes(vec![seed_node])
+        .with_update_interval(UPDATE_INTERVAL)
+        .build()?;
+
+    let node = node.start().await?;
+
+    Ok(node)
+}
+
+/// Creates a local cluster listening on a random port.
+pub async fn create_node_for_test(seed_node: String) -> anyhow::Result<NodeHandle> {
+    let peer_uuid = Uuid::new_v4().to_string();
+    let node = create_node_for_test_with_id(peer_uuid, seed_node).await?;
+
+    Ok(node)
+}
+
+pub fn setup_logging_for_tests() {
+    use std::sync::Once;
+    static INIT: Once = Once::new();
+    INIT.call_once(|| {
+        env_logger::builder().format_timestamp(None).init();
+    });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+#[serial_test::serial("cluster")]
+async fn test_cluster_single_node() {
+    setup_logging_for_tests();
+    // create seed node
+    let node = create_seed_node().await.unwrap();
+
+    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
+
+    let live_nodes = node.live_nodes().await;
+
+    assert_eq!(live_nodes.len(), 1);
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
+#[serial_test::serial("cluster")]
+async fn test_cluster_two_nodes() {
+    setup_logging_for_tests();
+    // create seed node
+    let first_node = create_seed_node().await.unwrap();
+    let mut cluster_watcher = first_node.cluster_watcher().await;
+
+    // add node to cluster
+    let second_node = create_node_for_test(SEED_NODE.to_string()).await.unwrap();
+
+    // allow nodes start and communicate
+    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
+
+    match tokio::time::timeout(UPDATE_INTERVAL, cluster_watcher.next()).await {
+        Ok(Some(live_nodes)) => {
+            let live_nodes = live_nodes
+                .into_iter()
+                .map(|node| node.id().to_string())
+                .collect::<Vec<_>>();
+
+            assert_eq!(live_nodes.len(), 2);
+            assert!(live_nodes.contains(&first_node.id().to_string()));
+            assert!(live_nodes.contains(&second_node.id().to_string()));
+        }
+        Ok(None) => {
+            panic!("no changes in cluster");
+        }
+        Err(e) => {
+            panic!("timeout while waiting cluster changes: {e}");
+        }
+    }
+}
+
+enum NodeOperation {
+    Add,
+    Delete,
+}
+
+struct TestClusterState {
+    nodes: AtomicUsize,
+}
+
+impl TestClusterState {
+    fn new() -> Self {
+        Self {
+            nodes: AtomicUsize::new(0),
+        }
+    }
+
+    pub(crate) fn change_state(&self, ops: &NodeOperation) {
+        match ops {
+            NodeOperation::Add => self.nodes.fetch_add(1, Ordering::AcqRel),
+            NodeOperation::Delete => self.nodes.fetch_sub(1, Ordering::AcqRel),
+        };
+    }
+
+    pub(crate) fn nodes(&self) -> usize {
+        self.nodes.load(Ordering::Relaxed)
+    }
+}
+
+#[ignore = "ignore"]
+#[allow(unused_assignments)]
+#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
+async fn test_integration_3_nodes_with_monitor() {
+    let registry = env::var("IMAGE_REPOSITORY").unwrap();
+    let sock_path = env::var("SOCKET_PATH").unwrap();
+    let mut docker = DockerTest::new();
+
+    let unix_stream = UnixStream::connect(sock_path.clone()).await.unwrap();
+    let (tx, mut rx) = mpsc::channel::<()>(2);
+    let state = TestClusterState::new();
+    let mut operation = NodeOperation::Add;
+
+    tokio::spawn(async move {
+        loop {
+            if unix_stream.readable().await.is_ok() {
+                let mut buffer = BytesMut::with_capacity(512);
+                match unix_stream.try_read(&mut buffer) {
+                    Ok(bytes_read) => {
+                        assert_ne!(
+                            bytes_read, 0,
+                            "0 bytes read from socket. Connection closed by writer"
+                        );
+                        let update = serde_json::from_slice::<Vec<NodeSnapshot>>(&buffer).unwrap();
+                        state.change_state(&operation);
+                        assert_eq!(update.len(), state.nodes());
+                        if state.nodes() == 2 {
+                            tx.send(()).await.unwrap();
+                            operation = NodeOperation::Delete;
+                            break; // TODO: when dockertest crate will implement stop/pause delete
+                                   // this break
+                        }
+                    }
+                    Err(ref e) if e.kind() == io::ErrorKind::WouldBlock => continue,
+                    Err(e) => {
+                        error!("error during reading from unix socket: {e}");
+                        break;
+                    }
+                }
+            }
+        }
+    });
+
+    let mut watcher_node = Composition::with_repository(registry.clone())
+        .with_container_name("node_with_watcher")
+        .with_start_policy(StartPolicy::Strict);
+    watcher_node.bind_mount(sock_path.clone(), sock_path.clone());
+
+    let node2 = Composition::with_repository(registry.clone())
+        .with_container_name("node2")
+        .with_start_policy(StartPolicy::Strict);
+
+    let node3 = Composition::with_repository(registry)
+        .with_container_name("node3")
+        .with_start_policy(StartPolicy::Strict);
+
+    docker.add_composition(watcher_node);
+    docker.add_composition(node2);
+    docker.add_composition(node3);
+    docker.run(|_ops| async move {
+        // wait until all containers started and all nodes will be in cluster
+        assert_eq!(Some(()), rx.recv().await);
+
+        // TODO: when dockertest crate will implement stop/pause for containers
+        // sequentially turn off the nodes from the cluster
+    });
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/Cargo.toml`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/fs_state.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/fs_state.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,212 +1,212 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs::{File, OpenOptions};
-use std::io;
-use std::io::{BufReader, BufWriter, Write};
-use std::path::{Path, PathBuf};
-use std::time::SystemTime;
-
-use fs2::FileExt;
-use serde::de::DeserializeOwned;
-use serde::Serialize;
-use thiserror::Error;
-
-pub type FsResult<O> = std::result::Result<O, FsError>;
-
-#[derive(Debug, Error)]
-pub enum FsError {
-    #[error("Serialization error: {0}")]
-    ParsingError(#[from] bincode::Error),
-    #[error("IO error: {0}")]
-    IoError(#[from] std::io::Error),
-}
-
-mod names {
-    pub const LOCK: &str = "lk.lock";
-    pub const STATE: &str = "state.bincode";
-    pub const TEMP: &str = "temp_state.bincode";
-}
-
-#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
-pub struct Version(SystemTime);
-
-fn write_state<S>(path: &Path, state: &S) -> FsResult<()>
-where S: Serialize {
-    let temporal_path = path.join(names::TEMP);
-    let state_path = path.join(names::STATE);
-    let mut file = BufWriter::new(
-        OpenOptions::new()
-            .create(true)
-            .write(true)
-            .truncate(true)
-            .open(&temporal_path)?,
-    );
-    bincode::serialize_into(&mut file, state)?;
-    file.flush()?;
-    std::fs::rename(&temporal_path, state_path)?;
-    Ok(())
-}
-
-fn read_state<S>(path: &Path) -> FsResult<S>
-where S: DeserializeOwned {
-    let mut file = BufReader::new(
-        OpenOptions::new()
-            .read(true)
-            .open(path.join(names::STATE))?,
-    );
-    Ok(bincode::deserialize_from(&mut file)?)
-}
-
-pub fn initialize_disk<S, F>(path: &Path, with: F) -> FsResult<()>
-where
-    F: Fn() -> S,
-    S: Serialize,
-{
-    if !path.join(names::STATE).is_file() {
-        write_state(path, &with())?;
-    }
-    Ok(())
-}
-
-pub fn exclusive_lock(path: &Path) -> FsResult<ELock> {
-    Ok(ELock::new(path)?)
-}
-pub fn shared_lock(path: &Path) -> FsResult<SLock> {
-    Ok(SLock::new(path)?)
-}
-
-pub fn persist_state<S>(lock: &ELock, state: &S) -> FsResult<()>
-where S: Serialize {
-    write_state(lock.as_ref(), state)
-}
-
-pub fn load_state<S>(lock: &Lock) -> FsResult<S>
-where S: DeserializeOwned {
-    read_state(lock.as_ref())
-}
-pub fn crnt_version(lock: &Lock) -> FsResult<Version> {
-    let meta = std::fs::metadata(lock.path.join(names::STATE))?;
-    Ok(Version(meta.modified()?))
-}
-
-/// A Lock that may be exclusive or shared
-/// Useful when the code would work in either case.
-pub struct Lock {
-    path: PathBuf,
-    #[allow(unused)]
-    lock: File,
-}
-impl Lock {
-    fn open_lock(path: &Path) -> io::Result<File> {
-        let file = OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(path.join(names::LOCK))?;
-        Ok(file)
-    }
-    fn exclusive(path: &Path) -> io::Result<Lock> {
-        let path = path.to_path_buf();
-        let lock = Lock::open_lock(&path)?;
-        lock.lock_exclusive()?;
-        Ok(Lock { lock, path })
-    }
-    fn shared(path: &Path) -> io::Result<Lock> {
-        let path = path.to_path_buf();
-        let lock = Lock::open_lock(&path)?;
-        lock.lock_shared()?;
-        Ok(Lock { lock, path })
-    }
-}
-impl AsRef<Path> for Lock {
-    fn as_ref(&self) -> &Path {
-        &self.path
-    }
-}
-
-/// A exclusive lock
-pub struct ELock(Lock);
-impl ELock {
-    pub(super) fn new(path: &Path) -> io::Result<ELock> {
-        Lock::exclusive(path).map(ELock)
-    }
-}
-impl std::ops::Deref for ELock {
-    type Target = Lock;
-    fn deref(&self) -> &Self::Target {
-        &self.0
-    }
-}
-impl AsRef<Path> for ELock {
-    fn as_ref(&self) -> &Path {
-        self.0.as_ref()
-    }
-}
-
-/// A shared lock
-pub struct SLock(Lock);
-impl SLock {
-    pub fn new(path: &Path) -> io::Result<SLock> {
-        Lock::shared(path).map(SLock)
-    }
-}
-impl std::ops::Deref for SLock {
-    type Target = Lock;
-    fn deref(&self) -> &Self::Target {
-        &self.0
-    }
-}
-impl AsRef<Path> for SLock {
-    fn as_ref(&self) -> &Path {
-        self.0.as_ref()
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use tempfile::TempDir;
-
-    use super::*;
-
-    #[derive(Serialize, serde::Deserialize, Default)]
-    struct State {
-        n: usize,
-    }
-
-    #[test]
-    fn test() {
-        let dir = TempDir::new().unwrap();
-        initialize_disk(dir.path(), State::default).unwrap();
-        let lock = exclusive_lock(dir.path()).unwrap();
-        assert!(dir.path().join(names::STATE).is_file());
-        assert!(dir.path().join(names::LOCK).is_file());
-        let v0 = crnt_version(&lock).unwrap();
-        std::mem::drop(lock);
-        let lock = exclusive_lock(dir.path()).unwrap();
-        assert!(dir.path().join(names::STATE).is_file());
-        assert!(dir.path().join(names::LOCK).is_file());
-        assert_eq!(v0, crnt_version(&lock).unwrap());
-        std::thread::sleep(std::time::Duration::from_millis(100));
-        write_state(dir.path(), &State::default()).unwrap();
-        let new_version = crnt_version(&lock).unwrap();
-        assert!(v0 < new_version);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs::{File, OpenOptions};
+use std::io;
+use std::io::{BufReader, BufWriter, Write};
+use std::path::{Path, PathBuf};
+use std::time::SystemTime;
+
+use fs2::FileExt;
+use serde::de::DeserializeOwned;
+use serde::Serialize;
+use thiserror::Error;
+
+pub type FsResult<O> = std::result::Result<O, FsError>;
+
+#[derive(Debug, Error)]
+pub enum FsError {
+    #[error("Serialization error: {0}")]
+    ParsingError(#[from] bincode::Error),
+    #[error("IO error: {0}")]
+    IoError(#[from] std::io::Error),
+}
+
+mod names {
+    pub const LOCK: &str = "lk.lock";
+    pub const STATE: &str = "state.bincode";
+    pub const TEMP: &str = "temp_state.bincode";
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
+pub struct Version(SystemTime);
+
+fn write_state<S>(path: &Path, state: &S) -> FsResult<()>
+where S: Serialize {
+    let temporal_path = path.join(names::TEMP);
+    let state_path = path.join(names::STATE);
+    let mut file = BufWriter::new(
+        OpenOptions::new()
+            .create(true)
+            .write(true)
+            .truncate(true)
+            .open(&temporal_path)?,
+    );
+    bincode::serialize_into(&mut file, state)?;
+    file.flush()?;
+    std::fs::rename(&temporal_path, state_path)?;
+    Ok(())
+}
+
+fn read_state<S>(path: &Path) -> FsResult<S>
+where S: DeserializeOwned {
+    let mut file = BufReader::new(
+        OpenOptions::new()
+            .read(true)
+            .open(path.join(names::STATE))?,
+    );
+    Ok(bincode::deserialize_from(&mut file)?)
+}
+
+pub fn initialize_disk<S, F>(path: &Path, with: F) -> FsResult<()>
+where
+    F: Fn() -> S,
+    S: Serialize,
+{
+    if !path.join(names::STATE).is_file() {
+        write_state(path, &with())?;
+    }
+    Ok(())
+}
+
+pub fn exclusive_lock(path: &Path) -> FsResult<ELock> {
+    Ok(ELock::new(path)?)
+}
+pub fn shared_lock(path: &Path) -> FsResult<SLock> {
+    Ok(SLock::new(path)?)
+}
+
+pub fn persist_state<S>(lock: &ELock, state: &S) -> FsResult<()>
+where S: Serialize {
+    write_state(lock.as_ref(), state)
+}
+
+pub fn load_state<S>(lock: &Lock) -> FsResult<S>
+where S: DeserializeOwned {
+    read_state(lock.as_ref())
+}
+pub fn crnt_version(lock: &Lock) -> FsResult<Version> {
+    let meta = std::fs::metadata(lock.path.join(names::STATE))?;
+    Ok(Version(meta.modified()?))
+}
+
+/// A Lock that may be exclusive or shared
+/// Useful when the code would work in either case.
+pub struct Lock {
+    path: PathBuf,
+    #[allow(unused)]
+    lock: File,
+}
+impl Lock {
+    fn open_lock(path: &Path) -> io::Result<File> {
+        let file = OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(path.join(names::LOCK))?;
+        Ok(file)
+    }
+    fn exclusive(path: &Path) -> io::Result<Lock> {
+        let path = path.to_path_buf();
+        let lock = Lock::open_lock(&path)?;
+        lock.lock_exclusive()?;
+        Ok(Lock { lock, path })
+    }
+    fn shared(path: &Path) -> io::Result<Lock> {
+        let path = path.to_path_buf();
+        let lock = Lock::open_lock(&path)?;
+        lock.lock_shared()?;
+        Ok(Lock { lock, path })
+    }
+}
+impl AsRef<Path> for Lock {
+    fn as_ref(&self) -> &Path {
+        &self.path
+    }
+}
+
+/// A exclusive lock
+pub struct ELock(Lock);
+impl ELock {
+    pub(super) fn new(path: &Path) -> io::Result<ELock> {
+        Lock::exclusive(path).map(ELock)
+    }
+}
+impl std::ops::Deref for ELock {
+    type Target = Lock;
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+impl AsRef<Path> for ELock {
+    fn as_ref(&self) -> &Path {
+        self.0.as_ref()
+    }
+}
+
+/// A shared lock
+pub struct SLock(Lock);
+impl SLock {
+    pub fn new(path: &Path) -> io::Result<SLock> {
+        Lock::shared(path).map(SLock)
+    }
+}
+impl std::ops::Deref for SLock {
+    type Target = Lock;
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+impl AsRef<Path> for SLock {
+    fn as_ref(&self) -> &Path {
+        self.0.as_ref()
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use tempfile::TempDir;
+
+    use super::*;
+
+    #[derive(Serialize, serde::Deserialize, Default)]
+    struct State {
+        n: usize,
+    }
+
+    #[test]
+    fn test() {
+        let dir = TempDir::new().unwrap();
+        initialize_disk(dir.path(), State::default).unwrap();
+        let lock = exclusive_lock(dir.path()).unwrap();
+        assert!(dir.path().join(names::STATE).is_file());
+        assert!(dir.path().join(names::LOCK).is_file());
+        let v0 = crnt_version(&lock).unwrap();
+        std::mem::drop(lock);
+        let lock = exclusive_lock(dir.path()).unwrap();
+        assert!(dir.path().join(names::STATE).is_file());
+        assert!(dir.path().join(names::LOCK).is_file());
+        assert_eq!(v0, crnt_version(&lock).unwrap());
+        std::thread::sleep(std::time::Duration::from_millis(100));
+        write_state(dir.path(), &State::default()).unwrap();
+        let new_version = crnt_version(&lock).unwrap();
+        assert!(v0 < new_version);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/lib.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/lib.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,130 +1,127 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod fs_state;
-pub mod metrics;
-pub mod paragraphs;
-pub mod relations;
-pub mod texts;
-pub mod vectors;
-use std::sync::{Arc, RwLock, RwLockReadGuard, RwLockWriteGuard};
-
-pub use anyhow::{anyhow as node_error, Context, Error};
-use nucliadb_protos::noderesources::{Resource, ResourceId};
-pub type NodeResult<O> = anyhow::Result<O>;
-
-pub fn paragraph_write(
-    x: &paragraphs::ParagraphsWriterPointer,
-) -> RwLockWriteGuard<'_, dyn paragraphs::ParagraphWriter + 'static> {
-    x.write().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn text_write(
-    x: &texts::TextsWriterPointer,
-) -> RwLockWriteGuard<'_, dyn texts::FieldWriter + 'static> {
-    x.write().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn vector_write(
-    x: &vectors::VectorsWriterPointer,
-) -> RwLockWriteGuard<'_, dyn vectors::VectorWriter + 'static> {
-    x.write().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn relation_write(
-    x: &relations::RelationsWriterPointer,
-) -> RwLockWriteGuard<'_, dyn relations::RelationWriter + 'static> {
-    x.write().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn paragraph_read(
-    x: &paragraphs::ParagraphsWriterPointer,
-) -> RwLockReadGuard<'_, dyn paragraphs::ParagraphWriter + 'static> {
-    x.read().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn text_read(
-    x: &texts::TextsWriterPointer,
-) -> RwLockReadGuard<'_, dyn texts::FieldWriter + 'static> {
-    x.read().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn vector_read(
-    x: &vectors::VectorsWriterPointer,
-) -> RwLockReadGuard<'_, dyn vectors::VectorWriter + 'static> {
-    x.read().unwrap_or_else(|l| l.into_inner())
-}
-
-pub fn relation_read(
-    x: &relations::RelationsWriterPointer,
-) -> RwLockReadGuard<'_, dyn relations::RelationWriter + 'static> {
-    x.read().unwrap_or_else(|l| l.into_inner())
-}
-
-pub mod protos {
-    pub use nucliadb_protos::prelude::*;
-    pub use {prost, prost_types};
-}
-
-pub mod tracing {
-    pub use tracing::*;
-}
-
-pub mod thread {
-    pub use rayon::prelude::*;
-    pub use rayon::*;
-}
-
-pub mod prelude {
-    pub use crate::paragraphs::{self, *};
-    pub use crate::relations::{self, *};
-    pub use crate::texts::{self, *};
-    pub use crate::vectors::{self, *};
-    pub use crate::{
-        encapsulate_reader, encapsulate_writer, node_error, paragraph_read, paragraph_write,
-        relation_read, relation_write, text_read, text_write, vector_read, vector_write, Context,
-        NodeResult, ReaderChild, WriterChild,
-    };
-}
-
-pub fn encapsulate_reader<T>(reader: T) -> Arc<T> {
-    Arc::new(reader)
-}
-
-pub fn encapsulate_writer<T>(writer: T) -> Arc<RwLock<T>> {
-    Arc::new(RwLock::new(writer))
-}
-
-pub trait WriterChild: std::fmt::Debug + Send + Sync {
-    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()>;
-    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()>;
-    fn garbage_collection(&mut self) -> NodeResult<()>;
-    fn stop(&mut self) -> NodeResult<()>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait ReaderChild: std::fmt::Debug + Send + Sync {
-    type Request;
-    type Response;
-    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response>;
-    fn reload(&self);
-    fn stored_ids(&self) -> NodeResult<Vec<String>>;
-    fn stop(&self) -> NodeResult<()>;
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod fs_state;
+pub mod metrics;
+pub mod paragraphs;
+pub mod relations;
+pub mod texts;
+pub mod vectors;
+use std::sync::{Arc, RwLock, RwLockReadGuard, RwLockWriteGuard};
+
+pub use anyhow::{anyhow as node_error, Context, Error};
+use nucliadb_protos::noderesources::{Resource, ResourceId};
+pub type NodeResult<O> = anyhow::Result<O>;
+
+pub fn paragraph_write(
+    x: &paragraphs::ParagraphsWriterPointer,
+) -> RwLockWriteGuard<'_, dyn paragraphs::ParagraphWriter + 'static> {
+    x.write().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn text_write(
+    x: &texts::TextsWriterPointer,
+) -> RwLockWriteGuard<'_, dyn texts::FieldWriter + 'static> {
+    x.write().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn vector_write(
+    x: &vectors::VectorsWriterPointer,
+) -> RwLockWriteGuard<'_, dyn vectors::VectorWriter + 'static> {
+    x.write().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn relation_write(
+    x: &relations::RelationsWriterPointer,
+) -> RwLockWriteGuard<'_, dyn relations::RelationWriter + 'static> {
+    x.write().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn paragraph_read(
+    x: &paragraphs::ParagraphsWriterPointer,
+) -> RwLockReadGuard<'_, dyn paragraphs::ParagraphWriter + 'static> {
+    x.read().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn text_read(
+    x: &texts::TextsWriterPointer,
+) -> RwLockReadGuard<'_, dyn texts::FieldWriter + 'static> {
+    x.read().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn vector_read(
+    x: &vectors::VectorsWriterPointer,
+) -> RwLockReadGuard<'_, dyn vectors::VectorWriter + 'static> {
+    x.read().unwrap_or_else(|l| l.into_inner())
+}
+
+pub fn relation_read(
+    x: &relations::RelationsWriterPointer,
+) -> RwLockReadGuard<'_, dyn relations::RelationWriter + 'static> {
+    x.read().unwrap_or_else(|l| l.into_inner())
+}
+
+pub mod protos {
+    pub use nucliadb_protos::prelude::*;
+    pub use {prost, prost_types};
+}
+
+pub mod tracing {
+    pub use tracing::*;
+}
+
+pub mod thread {
+    pub use rayon::prelude::*;
+    pub use rayon::*;
+}
+
+pub mod prelude {
+    pub use crate::paragraphs::{self, *};
+    pub use crate::relations::{self, *};
+    pub use crate::texts::{self, *};
+    pub use crate::vectors::{self, *};
+    pub use crate::{
+        encapsulate_reader, encapsulate_writer, node_error, paragraph_read, paragraph_write,
+        relation_read, relation_write, text_read, text_write, vector_read, vector_write, Context,
+        NodeResult, ReaderChild, WriterChild,
+    };
+}
+
+pub fn encapsulate_reader<T>(reader: T) -> Arc<T> {
+    Arc::new(reader)
+}
+
+pub fn encapsulate_writer<T>(writer: T) -> Arc<RwLock<T>> {
+    Arc::new(RwLock::new(writer))
+}
+
+pub trait WriterChild: std::fmt::Debug + Send + Sync {
+    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()>;
+    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()>;
+    fn garbage_collection(&mut self) -> NodeResult<()>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait ReaderChild: std::fmt::Debug + Send + Sync {
+    type Request;
+    type Response;
+    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response>;
+    fn stored_ids(&self) -> NodeResult<Vec<String>>;
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/console.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/console.rs`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::fmt::Debug;
-
-use crate::metrics::meters::Meter;
-use crate::metrics::metric::request_time;
-use crate::{tracing, NodeResult};
-
-pub struct ConsoleMeter;
-
-impl ConsoleMeter {
-    fn record<Metric: Debug, Value: Debug>(&self, metric: Metric, value: Value) {
-        tracing::debug!("{metric:?} : {value:?}")
-    }
-}
-
-impl Meter for ConsoleMeter {
-    fn export(&self) -> NodeResult<String> {
-        Ok(Default::default())
-    }
-
-    fn record_request_time(
-        &self,
-        metric: request_time::RequestTimeKey,
-        value: request_time::RequestTimeValue,
-    ) {
-        self.record(metric, value)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::fmt::Debug;
+
+use crate::metrics::meters::Meter;
+use crate::metrics::metric::request_time;
+use crate::{tracing, NodeResult};
+
+pub struct ConsoleMeter;
+
+impl ConsoleMeter {
+    fn record<Metric: Debug, Value: Debug>(&self, metric: Metric, value: Value) {
+        tracing::debug!("{metric:?} : {value:?}")
+    }
+}
+
+impl Meter for ConsoleMeter {
+    fn export(&self) -> NodeResult<String> {
+        Ok(Default::default())
+    }
+
+    fn record_request_time(
+        &self,
+        metric: request_time::RequestTimeKey,
+        value: request_time::RequestTimeValue,
+    ) {
+        self.record(metric, value)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/noop.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,35 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-mod console;
-mod noop;
-mod prometheus;
-
-pub use console::ConsoleMeter;
-pub use noop::NoOpMeter;
-pub use prometheus::PrometheusMeter;
-
-use crate::metrics::metric::request_time;
-use crate::metrics::task_monitor::{Monitor, TaskId};
-use crate::NodeResult;
-
-pub trait Meter: Send + Sync {
-    fn record_request_time(
-        &self,
-        metric: request_time::RequestTimeKey,
-        value: request_time::RequestTimeValue,
-    );
-
-    fn export(&self) -> NodeResult<String>;
-
-    fn task_monitor(&self, _task_id: TaskId) -> Option<Monitor> {
-        None
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use crate::metrics::meters::Meter;
+use crate::metrics::metric::request_time;
+use crate::NodeResult;
+
+pub struct NoOpMeter;
+impl Meter for NoOpMeter {
+    fn export(&self) -> NodeResult<String> {
+        Ok(Default::default())
+    }
+    fn record_request_time(
+        &self,
+        _: request_time::RequestTimeKey,
+        _: request_time::RequestTimeValue,
+    ) {
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/noop.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/lib.rs`

 * *Files 27% similar despite different names*

```diff
@@ -1,35 +1,27 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use crate::metrics::meters::Meter;
-use crate::metrics::metric::request_time;
-use crate::NodeResult;
-
-pub struct NoOpMeter;
-impl Meter for NoOpMeter {
-    fn export(&self) -> NodeResult<String> {
-        Ok(Default::default())
-    }
-    fn record_request_time(
-        &self,
-        _: request_time::RequestTimeKey,
-        _: request_time::RequestTimeValue,
-    ) {
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod fuzzy_query;
+pub mod reader;
+pub mod schema;
+pub mod search_query;
+pub mod search_response;
+pub(crate) mod stop_words;
+pub mod writer;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/meters/prometheus.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/meters/prometheus.rs`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,88 +1,88 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use prometheus_client::encoding;
-use prometheus_client::registry::Registry;
-
-use crate::metrics::meters::Meter;
-use crate::metrics::metric::tokio_tasks::TaskLabels;
-use crate::metrics::metric::{request_time, tokio_tasks};
-use crate::metrics::task_monitor::{Monitor, MultiTaskMonitor, TaskId};
-use crate::NodeResult;
-
-pub struct PrometheusMeter {
-    registry: Registry,
-    request_time_metric: request_time::RequestTimeMetric,
-    tokio_task_metrics: tokio_tasks::TokioTaskMetrics,
-    tasks_monitor: MultiTaskMonitor,
-}
-
-impl Default for PrometheusMeter {
-    fn default() -> Self {
-        Self::new()
-    }
-}
-
-impl Meter for PrometheusMeter {
-    fn export(&self) -> NodeResult<String> {
-        self.tasks_monitor
-            .export_all()
-            .for_each(|(task_id, metrics)| {
-                let labels = TaskLabels { request: task_id };
-                self.tokio_task_metrics.collect(labels, metrics.to_owned());
-            });
-
-        let mut buf = String::new();
-        encoding::text::encode(&mut buf, &self.registry)?;
-        Ok(buf)
-    }
-
-    fn record_request_time(
-        &self,
-        metric: request_time::RequestTimeKey,
-        value: request_time::RequestTimeValue,
-    ) {
-        self.request_time_metric
-            .get_or_create(&metric)
-            .observe(value);
-    }
-
-    fn task_monitor(&self, task_id: TaskId) -> Option<Monitor> {
-        Some(self.tasks_monitor.task_monitor(task_id))
-    }
-}
-
-impl PrometheusMeter {
-    pub fn new() -> PrometheusMeter {
-        let mut registry = Registry::default();
-
-        // This must be done for every metric
-        let request_time_metric = request_time::register_request_time(&mut registry);
-        let tokio_task_metrics = tokio_tasks::register_tokio_task_metrics(&mut registry);
-
-        let tasks_monitor = MultiTaskMonitor::new();
-
-        PrometheusMeter {
-            registry,
-            request_time_metric,
-            tokio_task_metrics,
-            tasks_monitor,
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use prometheus_client::encoding;
+use prometheus_client::registry::Registry;
+
+use crate::metrics::meters::Meter;
+use crate::metrics::metric::tokio_tasks::TaskLabels;
+use crate::metrics::metric::{request_time, tokio_tasks};
+use crate::metrics::task_monitor::{Monitor, MultiTaskMonitor, TaskId};
+use crate::NodeResult;
+
+pub struct PrometheusMeter {
+    registry: Registry,
+    request_time_metric: request_time::RequestTimeMetric,
+    tokio_task_metrics: tokio_tasks::TokioTaskMetrics,
+    tasks_monitor: MultiTaskMonitor,
+}
+
+impl Default for PrometheusMeter {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
+impl Meter for PrometheusMeter {
+    fn export(&self) -> NodeResult<String> {
+        self.tasks_monitor
+            .export_all()
+            .for_each(|(task_id, metrics)| {
+                let labels = TaskLabels { request: task_id };
+                self.tokio_task_metrics.collect(labels, metrics.to_owned());
+            });
+
+        let mut buf = String::new();
+        encoding::text::encode(&mut buf, &self.registry)?;
+        Ok(buf)
+    }
+
+    fn record_request_time(
+        &self,
+        metric: request_time::RequestTimeKey,
+        value: request_time::RequestTimeValue,
+    ) {
+        self.request_time_metric
+            .get_or_create(&metric)
+            .observe(value);
+    }
+
+    fn task_monitor(&self, task_id: TaskId) -> Option<Monitor> {
+        Some(self.tasks_monitor.task_monitor(task_id))
+    }
+}
+
+impl PrometheusMeter {
+    pub fn new() -> PrometheusMeter {
+        let mut registry = Registry::default();
+
+        // This must be done for every metric
+        let request_time_metric = request_time::register_request_time(&mut registry);
+        let tokio_task_metrics = tokio_tasks::register_tokio_task_metrics(&mut registry);
+
+        let tasks_monitor = MultiTaskMonitor::new();
+
+        PrometheusMeter {
+            registry,
+            request_time_metric,
+            tokio_task_metrics,
+            tasks_monitor,
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/metric/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/metric/mod.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-/// Every metric must be define in its own module, which must fulfill the following requirements:
-/// - The name of the module must be the name of the name of the metric.
-/// - If the metric is called SomeName, then there must be a type 'SomeNameMetric' describing such
-///   metric.
-/// - If the metric is called SomeName, a function 'register_some_name' must be defined and its job
-///   is to recive a registry, register there the metric and return such metric.
-/// - If the metric is called SomeName, a struct 'SomeNameKey' must be defined.
-/// - If the metric is called SomeName, a struct 'SomeNameValue' must be defined.
-pub mod request_time;
-pub mod tokio_tasks;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+/// Every metric must be define in its own module, which must fulfill the following requirements:
+/// - The name of the module must be the name of the name of the metric.
+/// - If the metric is called SomeName, then there must be a type 'SomeNameMetric' describing such
+///   metric.
+/// - If the metric is called SomeName, a function 'register_some_name' must be defined and its job
+///   is to recive a registry, register there the metric and return such metric.
+/// - If the metric is called SomeName, a struct 'SomeNameKey' must be defined.
+/// - If the metric is called SomeName, a struct 'SomeNameValue' must be defined.
+pub mod request_time;
+pub mod tokio_tasks;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/metric/tokio_tasks.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/metric/tokio_tasks.rs`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,389 +1,389 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use prometheus_client::encoding::EncodeLabelSet;
-use prometheus_client::metrics::counter::Counter;
-use prometheus_client::metrics::family::Family;
-use prometheus_client::metrics::histogram::Histogram;
-use prometheus_client::registry::Registry;
-use tokio_metrics::TaskMetrics;
-
-const INSTRUMENTED_COUNT: (&str, &str) = (
-    "nucliadb_node_instrumented_count",
-    "The number of tasks instrumented",
-);
-
-const DROPPED_COUNT: (&str, &str) = ("nucliadb_node_dropped_count", "The number of tasks dropped");
-
-const FIRST_POLL_COUNT: (&str, &str) = (
-    "nucliadb_node_first_poll_count",
-    "The number of tasks polled for the first time",
-);
-
-const TOTAL_FIRST_POLL_DELAY: (&str, &str) = (
-    "nucliadb_node_total_first_poll_delay",
-    "The total duration elapsed between the instant tasks are instrumented, and the instant they \
-     are first polled",
-);
-
-const TOTAL_IDLED_COUNT: (&str, &str) = (
-    "nucliadb_node_total_idled_count",
-    "The total number of times that tasks idled, waiting to be awoken. [...]",
-);
-
-const TOTAL_IDLE_DURATION: (&str, &str) = (
-    "nucliadb_node_total_idle_duration",
-    "The total duration that tasks idled. [...]",
-);
-
-const TOTAL_SCHEDULED_COUNT: (&str, &str) = (
-    "nucliadb_node_total_scheduled_count",
-    "The total number of times that tasks were awoken (and then, presumably, scheduled for \
-     execution)",
-);
-
-const TOTAL_SCHEDULED_DURATION: (&str, &str) = (
-    "nucliadb_node_total_scheduled_duration",
-    "The total duration that tasks spent waiting to be polled after awakening",
-);
-
-const TOTAL_POLL_COUNT: (&str, &str) = (
-    "nucliadb_node_total_poll_count",
-    "The total number of times that tasks where polled",
-);
-
-const TOTAL_POLL_DURATION: (&str, &str) = (
-    "nucliadb_node_total_poll_duration",
-    "The total duration elapsed during polls",
-);
-
-const TOTAL_FAST_POLL_COUNT: (&str, &str) = (
-    "nucliadb_node_total_fast_poll_count",
-    "The total number of times that polling tasks completed swiftly. [...]",
-);
-
-const TOTAL_FAST_POLL_DURATION: (&str, &str) = (
-    "nucliadb_node_total_fast_poll_duration",
-    "The total duration of fast polls. [...]",
-);
-
-const TOTAL_SLOW_POLL_COUNT: (&str, &str) = (
-    "nucliadb_node_total_slow_poll_count",
-    "The total number of times that polling tasks completed slowly. [...]",
-);
-
-const TOTAL_SLOW_POLL_DURATION: (&str, &str) = (
-    "nucliadb_node_total_slow_poll_duration",
-    "The total duration of slow polls. [...]",
-);
-
-const TOTAL_SHORT_DELAY_COUNT: (&str, &str) = (
-    "nucliadb_node_total_short_delay_count",
-    "The total count of tasks with short scheduling delays. [...]",
-);
-
-const TOTAL_LONG_DELAY_COUNT: (&str, &str) = (
-    "nucliadb_node_total_long_delay_count",
-    "The total count of tasks with long scheduling delays. [...]",
-);
-
-const TOTAL_SHORT_DELAY_DURATION: (&str, &str) = (
-    "nucliadb_node_total_short_delay_duration",
-    "The total duration of tasks with short scheduling delays. [...]",
-);
-
-const TOTAL_LONG_DELAY_DURATION: (&str, &str) = (
-    "nucliadb_node_total_long_delay_duration",
-    "The total duration of tasks with long scheduling delays. [...]",
-);
-
-#[derive(Clone, Debug, Hash, PartialEq, Eq, EncodeLabelSet)]
-pub struct TaskLabels {
-    pub request: String, // gRPC method (/NewShard, /SetResource...)
-}
-
-// Tasks in tokio should never block longer than 10-100s
-
-// TODO we are trying bucket values for everything. After an evaluation on
-// production, we should reconsider changing them and customize for every
-// Histogram metric
-const BUCKETS: [f64; 15] = [
-    0.000010, 0.000025, 0.000050, 0.000100, 0.000250, 0.000500, 0.001, 0.002, 0.005, 0.010, 0.100,
-    0.250, 0.500, 1.0, 5.0,
-];
-
-pub struct TokioTaskMetrics {
-    instrumented_count: Family<TaskLabels, Counter>,
-    dropped_count: Family<TaskLabels, Counter>,
-    first_poll_count: Family<TaskLabels, Counter>,
-    total_first_poll_delay: Family<TaskLabels, Histogram>,
-    total_idled_count: Family<TaskLabels, Counter>,
-    total_idle_duration: Family<TaskLabels, Histogram>,
-    total_scheduled_count: Family<TaskLabels, Counter>,
-    total_scheduled_duration: Family<TaskLabels, Histogram>,
-    total_poll_count: Family<TaskLabels, Counter>,
-    total_poll_duration: Family<TaskLabels, Histogram>,
-    total_fast_poll_count: Family<TaskLabels, Counter>,
-    total_fast_poll_duration: Family<TaskLabels, Histogram>,
-    total_slow_poll_count: Family<TaskLabels, Counter>,
-    total_slow_poll_duration: Family<TaskLabels, Histogram>,
-    total_short_delay_count: Family<TaskLabels, Counter>,
-    total_long_delay_count: Family<TaskLabels, Counter>,
-    total_short_delay_duration: Family<TaskLabels, Histogram>,
-    total_long_delay_duration: Family<TaskLabels, Histogram>,
-}
-
-impl TokioTaskMetrics {
-    fn new(registry: &mut Registry) -> Self {
-        let histogram_constructor = || Histogram::new(BUCKETS.iter().copied());
-
-        let instrumented_count = Family::default();
-        registry.register(
-            INSTRUMENTED_COUNT.0,
-            INSTRUMENTED_COUNT.1,
-            instrumented_count.clone(),
-        );
-
-        let dropped_count = Family::default();
-        registry.register(DROPPED_COUNT.0, DROPPED_COUNT.1, dropped_count.clone());
-
-        let first_poll_count = Family::default();
-        registry.register(
-            FIRST_POLL_COUNT.0,
-            FIRST_POLL_COUNT.1,
-            first_poll_count.clone(),
-        );
-
-        let total_first_poll_delay =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_FIRST_POLL_DELAY.0,
-            TOTAL_FIRST_POLL_DELAY.1,
-            total_first_poll_delay.clone(),
-        );
-
-        let total_idled_count = Family::default();
-        registry.register(
-            TOTAL_IDLED_COUNT.0,
-            TOTAL_IDLED_COUNT.1,
-            total_idled_count.clone(),
-        );
-
-        let total_idle_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_IDLE_DURATION.0,
-            TOTAL_IDLE_DURATION.1,
-            total_idle_duration.clone(),
-        );
-
-        let total_scheduled_count = Family::default();
-        registry.register(
-            TOTAL_SCHEDULED_COUNT.0,
-            TOTAL_SCHEDULED_COUNT.1,
-            total_scheduled_count.clone(),
-        );
-
-        let total_scheduled_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_SCHEDULED_DURATION.0,
-            TOTAL_SCHEDULED_DURATION.1,
-            total_scheduled_duration.clone(),
-        );
-
-        let total_poll_count = Family::default();
-        registry.register(
-            TOTAL_POLL_COUNT.0,
-            TOTAL_POLL_COUNT.1,
-            total_poll_count.clone(),
-        );
-
-        let total_poll_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_POLL_DURATION.0,
-            TOTAL_POLL_DURATION.1,
-            total_poll_duration.clone(),
-        );
-
-        let total_fast_poll_count = Family::default();
-        registry.register(
-            TOTAL_FAST_POLL_COUNT.0,
-            TOTAL_FAST_POLL_COUNT.1,
-            total_fast_poll_count.clone(),
-        );
-
-        let total_fast_poll_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_FAST_POLL_DURATION.0,
-            TOTAL_FAST_POLL_DURATION.1,
-            total_fast_poll_duration.clone(),
-        );
-
-        let total_slow_poll_count = Family::default();
-        registry.register(
-            TOTAL_SLOW_POLL_COUNT.0,
-            TOTAL_SLOW_POLL_COUNT.1,
-            total_slow_poll_count.clone(),
-        );
-
-        let total_slow_poll_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_SLOW_POLL_DURATION.0,
-            TOTAL_SLOW_POLL_DURATION.1,
-            total_slow_poll_duration.clone(),
-        );
-
-        let total_short_delay_count = Family::default();
-        registry.register(
-            TOTAL_SHORT_DELAY_COUNT.0,
-            TOTAL_SHORT_DELAY_COUNT.1,
-            total_short_delay_count.clone(),
-        );
-
-        let total_long_delay_count = Family::default();
-        registry.register(
-            TOTAL_LONG_DELAY_COUNT.0,
-            TOTAL_LONG_DELAY_COUNT.1,
-            total_long_delay_count.clone(),
-        );
-
-        let total_short_delay_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_SHORT_DELAY_DURATION.0,
-            TOTAL_SHORT_DELAY_DURATION.1,
-            total_short_delay_duration.clone(),
-        );
-
-        let total_long_delay_duration =
-            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
-        registry.register(
-            TOTAL_LONG_DELAY_DURATION.0,
-            TOTAL_LONG_DELAY_DURATION.1,
-            total_long_delay_duration.clone(),
-        );
-
-        Self {
-            instrumented_count,
-            dropped_count,
-            first_poll_count,
-            total_first_poll_delay,
-            total_idled_count,
-            total_idle_duration,
-            total_scheduled_count,
-            total_scheduled_duration,
-            total_poll_count,
-            total_poll_duration,
-            total_fast_poll_count,
-            total_fast_poll_duration,
-            total_slow_poll_count,
-            total_slow_poll_duration,
-            total_short_delay_count,
-            total_long_delay_count,
-            total_short_delay_duration,
-            total_long_delay_duration,
-        }
-    }
-
-    /// Collected metrics must belong to the interval from last `collect` call
-    /// until the current call.
-    pub fn collect(&self, labels: TaskLabels, metrics: TaskMetrics) {
-        // TODO? Should we add a declarative macro to improve this?
-
-        self.instrumented_count
-            .get_or_create(&labels)
-            .inc_by(metrics.instrumented_count);
-
-        self.dropped_count
-            .get_or_create(&labels)
-            .inc_by(metrics.dropped_count);
-
-        self.first_poll_count
-            .get_or_create(&labels)
-            .inc_by(metrics.first_poll_count);
-
-        self.total_first_poll_delay
-            .get_or_create(&labels)
-            .observe(metrics.total_first_poll_delay.as_secs_f64());
-
-        self.total_idled_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_idled_count);
-
-        self.total_idle_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_idle_duration.as_secs_f64());
-
-        self.total_scheduled_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_scheduled_count);
-
-        self.total_scheduled_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_scheduled_duration.as_secs_f64());
-
-        self.total_poll_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_poll_count);
-
-        self.total_poll_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_poll_duration.as_secs_f64());
-
-        self.total_fast_poll_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_fast_poll_count);
-
-        self.total_fast_poll_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_fast_poll_duration.as_secs_f64());
-
-        self.total_slow_poll_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_slow_poll_count);
-
-        self.total_slow_poll_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_slow_poll_duration.as_secs_f64());
-
-        self.total_short_delay_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_short_delay_count);
-
-        self.total_long_delay_count
-            .get_or_create(&labels)
-            .inc_by(metrics.total_long_delay_count);
-
-        self.total_short_delay_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_short_delay_duration.as_secs_f64());
-
-        self.total_long_delay_duration
-            .get_or_create(&labels)
-            .observe(metrics.total_long_delay_duration.as_secs_f64());
-    }
-}
-
-pub fn register_tokio_task_metrics(registry: &mut Registry) -> TokioTaskMetrics {
-    TokioTaskMetrics::new(registry)
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use prometheus_client::encoding::EncodeLabelSet;
+use prometheus_client::metrics::counter::Counter;
+use prometheus_client::metrics::family::Family;
+use prometheus_client::metrics::histogram::Histogram;
+use prometheus_client::registry::Registry;
+use tokio_metrics::TaskMetrics;
+
+const INSTRUMENTED_COUNT: (&str, &str) = (
+    "nucliadb_node_instrumented_count",
+    "The number of tasks instrumented",
+);
+
+const DROPPED_COUNT: (&str, &str) = ("nucliadb_node_dropped_count", "The number of tasks dropped");
+
+const FIRST_POLL_COUNT: (&str, &str) = (
+    "nucliadb_node_first_poll_count",
+    "The number of tasks polled for the first time",
+);
+
+const TOTAL_FIRST_POLL_DELAY: (&str, &str) = (
+    "nucliadb_node_total_first_poll_delay",
+    "The total duration elapsed between the instant tasks are instrumented, and the instant they \
+     are first polled",
+);
+
+const TOTAL_IDLED_COUNT: (&str, &str) = (
+    "nucliadb_node_total_idled_count",
+    "The total number of times that tasks idled, waiting to be awoken. [...]",
+);
+
+const TOTAL_IDLE_DURATION: (&str, &str) = (
+    "nucliadb_node_total_idle_duration",
+    "The total duration that tasks idled. [...]",
+);
+
+const TOTAL_SCHEDULED_COUNT: (&str, &str) = (
+    "nucliadb_node_total_scheduled_count",
+    "The total number of times that tasks were awoken (and then, presumably, scheduled for \
+     execution)",
+);
+
+const TOTAL_SCHEDULED_DURATION: (&str, &str) = (
+    "nucliadb_node_total_scheduled_duration",
+    "The total duration that tasks spent waiting to be polled after awakening",
+);
+
+const TOTAL_POLL_COUNT: (&str, &str) = (
+    "nucliadb_node_total_poll_count",
+    "The total number of times that tasks where polled",
+);
+
+const TOTAL_POLL_DURATION: (&str, &str) = (
+    "nucliadb_node_total_poll_duration",
+    "The total duration elapsed during polls",
+);
+
+const TOTAL_FAST_POLL_COUNT: (&str, &str) = (
+    "nucliadb_node_total_fast_poll_count",
+    "The total number of times that polling tasks completed swiftly. [...]",
+);
+
+const TOTAL_FAST_POLL_DURATION: (&str, &str) = (
+    "nucliadb_node_total_fast_poll_duration",
+    "The total duration of fast polls. [...]",
+);
+
+const TOTAL_SLOW_POLL_COUNT: (&str, &str) = (
+    "nucliadb_node_total_slow_poll_count",
+    "The total number of times that polling tasks completed slowly. [...]",
+);
+
+const TOTAL_SLOW_POLL_DURATION: (&str, &str) = (
+    "nucliadb_node_total_slow_poll_duration",
+    "The total duration of slow polls. [...]",
+);
+
+const TOTAL_SHORT_DELAY_COUNT: (&str, &str) = (
+    "nucliadb_node_total_short_delay_count",
+    "The total count of tasks with short scheduling delays. [...]",
+);
+
+const TOTAL_LONG_DELAY_COUNT: (&str, &str) = (
+    "nucliadb_node_total_long_delay_count",
+    "The total count of tasks with long scheduling delays. [...]",
+);
+
+const TOTAL_SHORT_DELAY_DURATION: (&str, &str) = (
+    "nucliadb_node_total_short_delay_duration",
+    "The total duration of tasks with short scheduling delays. [...]",
+);
+
+const TOTAL_LONG_DELAY_DURATION: (&str, &str) = (
+    "nucliadb_node_total_long_delay_duration",
+    "The total duration of tasks with long scheduling delays. [...]",
+);
+
+#[derive(Clone, Debug, Hash, PartialEq, Eq, EncodeLabelSet)]
+pub struct TaskLabels {
+    pub request: String, // gRPC method (/NewShard, /SetResource...)
+}
+
+// Tasks in tokio should never block longer than 10-100s
+
+// TODO we are trying bucket values for everything. After an evaluation on
+// production, we should reconsider changing them and customize for every
+// Histogram metric
+const BUCKETS: [f64; 15] = [
+    0.000010, 0.000025, 0.000050, 0.000100, 0.000250, 0.000500, 0.001, 0.002, 0.005, 0.010, 0.100,
+    0.250, 0.500, 1.0, 5.0,
+];
+
+pub struct TokioTaskMetrics {
+    instrumented_count: Family<TaskLabels, Counter>,
+    dropped_count: Family<TaskLabels, Counter>,
+    first_poll_count: Family<TaskLabels, Counter>,
+    total_first_poll_delay: Family<TaskLabels, Histogram>,
+    total_idled_count: Family<TaskLabels, Counter>,
+    total_idle_duration: Family<TaskLabels, Histogram>,
+    total_scheduled_count: Family<TaskLabels, Counter>,
+    total_scheduled_duration: Family<TaskLabels, Histogram>,
+    total_poll_count: Family<TaskLabels, Counter>,
+    total_poll_duration: Family<TaskLabels, Histogram>,
+    total_fast_poll_count: Family<TaskLabels, Counter>,
+    total_fast_poll_duration: Family<TaskLabels, Histogram>,
+    total_slow_poll_count: Family<TaskLabels, Counter>,
+    total_slow_poll_duration: Family<TaskLabels, Histogram>,
+    total_short_delay_count: Family<TaskLabels, Counter>,
+    total_long_delay_count: Family<TaskLabels, Counter>,
+    total_short_delay_duration: Family<TaskLabels, Histogram>,
+    total_long_delay_duration: Family<TaskLabels, Histogram>,
+}
+
+impl TokioTaskMetrics {
+    fn new(registry: &mut Registry) -> Self {
+        let histogram_constructor = || Histogram::new(BUCKETS.iter().copied());
+
+        let instrumented_count = Family::default();
+        registry.register(
+            INSTRUMENTED_COUNT.0,
+            INSTRUMENTED_COUNT.1,
+            instrumented_count.clone(),
+        );
+
+        let dropped_count = Family::default();
+        registry.register(DROPPED_COUNT.0, DROPPED_COUNT.1, dropped_count.clone());
+
+        let first_poll_count = Family::default();
+        registry.register(
+            FIRST_POLL_COUNT.0,
+            FIRST_POLL_COUNT.1,
+            first_poll_count.clone(),
+        );
+
+        let total_first_poll_delay =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_FIRST_POLL_DELAY.0,
+            TOTAL_FIRST_POLL_DELAY.1,
+            total_first_poll_delay.clone(),
+        );
+
+        let total_idled_count = Family::default();
+        registry.register(
+            TOTAL_IDLED_COUNT.0,
+            TOTAL_IDLED_COUNT.1,
+            total_idled_count.clone(),
+        );
+
+        let total_idle_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_IDLE_DURATION.0,
+            TOTAL_IDLE_DURATION.1,
+            total_idle_duration.clone(),
+        );
+
+        let total_scheduled_count = Family::default();
+        registry.register(
+            TOTAL_SCHEDULED_COUNT.0,
+            TOTAL_SCHEDULED_COUNT.1,
+            total_scheduled_count.clone(),
+        );
+
+        let total_scheduled_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_SCHEDULED_DURATION.0,
+            TOTAL_SCHEDULED_DURATION.1,
+            total_scheduled_duration.clone(),
+        );
+
+        let total_poll_count = Family::default();
+        registry.register(
+            TOTAL_POLL_COUNT.0,
+            TOTAL_POLL_COUNT.1,
+            total_poll_count.clone(),
+        );
+
+        let total_poll_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_POLL_DURATION.0,
+            TOTAL_POLL_DURATION.1,
+            total_poll_duration.clone(),
+        );
+
+        let total_fast_poll_count = Family::default();
+        registry.register(
+            TOTAL_FAST_POLL_COUNT.0,
+            TOTAL_FAST_POLL_COUNT.1,
+            total_fast_poll_count.clone(),
+        );
+
+        let total_fast_poll_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_FAST_POLL_DURATION.0,
+            TOTAL_FAST_POLL_DURATION.1,
+            total_fast_poll_duration.clone(),
+        );
+
+        let total_slow_poll_count = Family::default();
+        registry.register(
+            TOTAL_SLOW_POLL_COUNT.0,
+            TOTAL_SLOW_POLL_COUNT.1,
+            total_slow_poll_count.clone(),
+        );
+
+        let total_slow_poll_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_SLOW_POLL_DURATION.0,
+            TOTAL_SLOW_POLL_DURATION.1,
+            total_slow_poll_duration.clone(),
+        );
+
+        let total_short_delay_count = Family::default();
+        registry.register(
+            TOTAL_SHORT_DELAY_COUNT.0,
+            TOTAL_SHORT_DELAY_COUNT.1,
+            total_short_delay_count.clone(),
+        );
+
+        let total_long_delay_count = Family::default();
+        registry.register(
+            TOTAL_LONG_DELAY_COUNT.0,
+            TOTAL_LONG_DELAY_COUNT.1,
+            total_long_delay_count.clone(),
+        );
+
+        let total_short_delay_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_SHORT_DELAY_DURATION.0,
+            TOTAL_SHORT_DELAY_DURATION.1,
+            total_short_delay_duration.clone(),
+        );
+
+        let total_long_delay_duration =
+            Family::<TaskLabels, Histogram>::new_with_constructor(histogram_constructor);
+        registry.register(
+            TOTAL_LONG_DELAY_DURATION.0,
+            TOTAL_LONG_DELAY_DURATION.1,
+            total_long_delay_duration.clone(),
+        );
+
+        Self {
+            instrumented_count,
+            dropped_count,
+            first_poll_count,
+            total_first_poll_delay,
+            total_idled_count,
+            total_idle_duration,
+            total_scheduled_count,
+            total_scheduled_duration,
+            total_poll_count,
+            total_poll_duration,
+            total_fast_poll_count,
+            total_fast_poll_duration,
+            total_slow_poll_count,
+            total_slow_poll_duration,
+            total_short_delay_count,
+            total_long_delay_count,
+            total_short_delay_duration,
+            total_long_delay_duration,
+        }
+    }
+
+    /// Collected metrics must belong to the interval from last `collect` call
+    /// until the current call.
+    pub fn collect(&self, labels: TaskLabels, metrics: TaskMetrics) {
+        // TODO? Should we add a declarative macro to improve this?
+
+        self.instrumented_count
+            .get_or_create(&labels)
+            .inc_by(metrics.instrumented_count);
+
+        self.dropped_count
+            .get_or_create(&labels)
+            .inc_by(metrics.dropped_count);
+
+        self.first_poll_count
+            .get_or_create(&labels)
+            .inc_by(metrics.first_poll_count);
+
+        self.total_first_poll_delay
+            .get_or_create(&labels)
+            .observe(metrics.total_first_poll_delay.as_secs_f64());
+
+        self.total_idled_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_idled_count);
+
+        self.total_idle_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_idle_duration.as_secs_f64());
+
+        self.total_scheduled_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_scheduled_count);
+
+        self.total_scheduled_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_scheduled_duration.as_secs_f64());
+
+        self.total_poll_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_poll_count);
+
+        self.total_poll_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_poll_duration.as_secs_f64());
+
+        self.total_fast_poll_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_fast_poll_count);
+
+        self.total_fast_poll_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_fast_poll_duration.as_secs_f64());
+
+        self.total_slow_poll_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_slow_poll_count);
+
+        self.total_slow_poll_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_slow_poll_duration.as_secs_f64());
+
+        self.total_short_delay_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_short_delay_count);
+
+        self.total_long_delay_count
+            .get_or_create(&labels)
+            .inc_by(metrics.total_long_delay_count);
+
+        self.total_short_delay_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_short_delay_duration.as_secs_f64());
+
+        self.total_long_delay_duration
+            .get_or_create(&labels)
+            .observe(metrics.total_long_delay_duration.as_secs_f64());
+    }
+}
+
+pub fn register_tokio_task_metrics(registry: &mut Registry) -> TokioTaskMetrics {
+    TokioTaskMetrics::new(registry)
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/middleware.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/middleware.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::task::{Context, Poll};
-
-use futures::future::BoxFuture;
-use hyper::Body;
-use tonic::body::BoxBody;
-use tower::{Layer, Service};
-
-use crate::metrics;
-
-#[derive(Debug, Clone, Default)]
-pub struct MetricsLayer;
-
-impl<S> Layer<S> for MetricsLayer {
-    type Service = EndpointMetrics<S>;
-
-    fn layer(&self, service: S) -> Self::Service {
-        EndpointMetrics { inner: service }
-    }
-}
-
-/// Dynamically instrument service calls by HTTP path / gRPC method.
-#[derive(Debug, Clone)]
-pub struct EndpointMetrics<S> {
-    inner: S,
-}
-
-impl<S> Service<hyper::Request<Body>> for EndpointMetrics<S>
-where
-    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
-    S::Future: Send + 'static,
-{
-    type Response = S::Response;
-    type Error = S::Error;
-    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
-
-    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
-        self.inner.poll_ready(cx)
-    }
-
-    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
-        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
-        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
-        // for details on why this is necessary
-        let clone = self.inner.clone();
-        // We need to swap the clone and the original to avoid a not ready
-        // service. See
-        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
-        // for more details
-        let mut inner = std::mem::replace(&mut self.inner, clone);
-
-        Box::pin(async move {
-            let task_id = req.uri().path().into();
-            let call = inner.call(req);
-            let response = match metrics::get_metrics().task_monitor(task_id) {
-                Some(monitor) => {
-                    let instrumented = monitor.instrument(call);
-                    instrumented.await
-                }
-                None => call.await,
-            };
-
-            response
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::task::{Context, Poll};
+
+use futures::future::BoxFuture;
+use hyper::Body;
+use tonic::body::BoxBody;
+use tower::{Layer, Service};
+
+use crate::metrics;
+
+#[derive(Debug, Clone, Default)]
+pub struct MetricsLayer;
+
+impl<S> Layer<S> for MetricsLayer {
+    type Service = EndpointMetrics<S>;
+
+    fn layer(&self, service: S) -> Self::Service {
+        EndpointMetrics { inner: service }
+    }
+}
+
+/// Dynamically instrument service calls by HTTP path / gRPC method.
+#[derive(Debug, Clone)]
+pub struct EndpointMetrics<S> {
+    inner: S,
+}
+
+impl<S> Service<hyper::Request<Body>> for EndpointMetrics<S>
+where
+    S: Service<hyper::Request<Body>, Response = hyper::Response<BoxBody>> + Clone + Send + 'static,
+    S::Future: Send + 'static,
+{
+    type Response = S::Response;
+    type Error = S::Error;
+    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    fn poll_ready(&mut self, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
+        self.inner.poll_ready(cx)
+    }
+
+    fn call(&mut self, req: hyper::Request<Body>) -> Self::Future {
+        // This is necessary because tonic internally uses `tower::buffer::Buffer`.
+        // See https://github.com/tower-rs/tower/issues/547#issuecomment-767629149
+        // for details on why this is necessary
+        let clone = self.inner.clone();
+        // We need to swap the clone and the original to avoid a not ready
+        // service. See
+        // https://docs.rs/tower/0.4.13/tower/trait.Service.html#be-careful-when-cloning-inner-services
+        // for more details
+        let mut inner = std::mem::replace(&mut self.inner, clone);
+
+        Box::pin(async move {
+            let task_id = req.uri().path().into();
+            let call = inner.call(req);
+            let response = match metrics::get_metrics().task_monitor(task_id) {
+                Some(monitor) => {
+                    let instrumented = monitor.instrument(call);
+                    instrumented.await
+                }
+                None => call.await,
+            };
+
+            response
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/mod.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/mod.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-pub mod middleware;
-
-mod meters;
-mod metric;
-mod task_monitor;
-
-use std::sync::Arc;
-
-use lazy_static::lazy_static;
-pub use metric::request_time;
-
-use self::meters::Meter;
-
-lazy_static! {
-    static ref METRICS: Arc<dyn Meter> = create_metrics();
-}
-
-#[cfg(prometheus_metrics)]
-fn create_metrics() -> Arc<dyn Meter> {
-    Arc::new(meters::PrometheusMeter::new())
-}
-
-#[cfg(log_metrics)]
-fn create_metrics() -> Arc<dyn Meter> {
-    Arc::new(meters::ConsoleMeter)
-}
-
-#[cfg(not(any(prometheus_metrics, log_metrics)))]
-fn create_metrics() -> Arc<dyn Meter> {
-    Arc::new(meters::NoOpMeter)
-}
-
-pub fn get_metrics() -> Arc<dyn Meter> {
-    Arc::clone(&METRICS)
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+pub mod middleware;
+
+mod meters;
+mod metric;
+mod task_monitor;
+
+use std::sync::Arc;
+
+use lazy_static::lazy_static;
+pub use metric::request_time;
+
+use self::meters::Meter;
+
+lazy_static! {
+    static ref METRICS: Arc<dyn Meter> = create_metrics();
+}
+
+#[cfg(prometheus_metrics)]
+fn create_metrics() -> Arc<dyn Meter> {
+    Arc::new(meters::PrometheusMeter::new())
+}
+
+#[cfg(log_metrics)]
+fn create_metrics() -> Arc<dyn Meter> {
+    Arc::new(meters::ConsoleMeter)
+}
+
+#[cfg(not(any(prometheus_metrics, log_metrics)))]
+fn create_metrics() -> Arc<dyn Meter> {
+    Arc::new(meters::NoOpMeter)
+}
+
+pub fn get_metrics() -> Arc<dyn Meter> {
+    Arc::clone(&METRICS)
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/metrics/task_monitor.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/metrics/task_monitor.rs`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use dashmap::DashMap;
-use tokio_metrics::{Instrumented, TaskMetrics, TaskMonitor};
-
-pub type TaskId = String;
-
-pub struct MultiTaskMonitor {
-    task_monitors: DashMap<TaskId, TaskMonitor>,
-}
-
-impl MultiTaskMonitor {
-    pub fn new() -> Self {
-        Self {
-            task_monitors: DashMap::new(),
-        }
-    }
-
-    pub fn task_monitor(&self, task_id: TaskId) -> Monitor {
-        Monitor {
-            task_id,
-            monitors: &self.task_monitors,
-        }
-    }
-
-    pub fn export_all(&self) -> impl Iterator<Item = (TaskId, TaskMetrics)> + '_ {
-        self.task_monitors.iter().filter_map(|item| {
-            let task_id = item.key().to_owned();
-            let metrics = item.value().intervals().next();
-            metrics.map(|metrics| (task_id, metrics))
-        })
-    }
-}
-
-pub struct Monitor<'a> {
-    task_id: TaskId,
-    monitors: &'a DashMap<TaskId, TaskMonitor>,
-}
-
-impl<'a> Monitor<'a> {
-    // Consuming `self` ensures a short life for the reference inside a DashMap,
-    // avoiding potential deadlocks. See DashMap docs for more info.
-    pub fn instrument<F>(self, task: F) -> Instrumented<F> {
-        if !self.monitors.contains_key(&self.task_id) {
-            let monitor = TaskMonitor::new();
-            self.monitors.insert(self.task_id.clone(), monitor);
-        }
-        let monitor = self
-            .monitors
-            .get(&self.task_id)
-            .expect("Task existed or just inserted");
-
-        monitor.instrument(task)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use dashmap::DashMap;
+use tokio_metrics::{Instrumented, TaskMetrics, TaskMonitor};
+
+pub type TaskId = String;
+
+pub struct MultiTaskMonitor {
+    task_monitors: DashMap<TaskId, TaskMonitor>,
+}
+
+impl MultiTaskMonitor {
+    pub fn new() -> Self {
+        Self {
+            task_monitors: DashMap::new(),
+        }
+    }
+
+    pub fn task_monitor(&self, task_id: TaskId) -> Monitor {
+        Monitor {
+            task_id,
+            monitors: &self.task_monitors,
+        }
+    }
+
+    pub fn export_all(&self) -> impl Iterator<Item = (TaskId, TaskMetrics)> + '_ {
+        self.task_monitors.iter().filter_map(|item| {
+            let task_id = item.key().to_owned();
+            let metrics = item.value().intervals().next();
+            metrics.map(|metrics| (task_id, metrics))
+        })
+    }
+}
+
+pub struct Monitor<'a> {
+    task_id: TaskId,
+    monitors: &'a DashMap<TaskId, TaskMonitor>,
+}
+
+impl<'a> Monitor<'a> {
+    // Consuming `self` ensures a short life for the reference inside a DashMap,
+    // avoiding potential deadlocks. See DashMap docs for more info.
+    pub fn instrument<F>(self, task: F) -> Instrumented<F> {
+        if !self.monitors.contains_key(&self.task_id) {
+            let monitor = TaskMonitor::new();
+            self.monitors.insert(self.task_id.clone(), monitor);
+        }
+        let monitor = self
+            .monitors
+            .get(&self.task_id)
+            .expect("Task existed or just inserted");
+
+        monitor.instrument(task)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/paragraphs.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/paragraphs.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use crate::prelude::*;
-use crate::protos::*;
-
-pub type ParagraphsReaderPointer = Arc<dyn ParagraphReader>;
-pub type ParagraphsWriterPointer = Arc<RwLock<dyn ParagraphWriter>>;
-
-pub struct ParagraphConfig {
-    pub path: PathBuf,
-}
-
-pub struct ParagraphIterator(Box<dyn Iterator<Item = ParagraphItem> + Send>);
-impl ParagraphIterator {
-    pub fn new<I>(inner: I) -> ParagraphIterator
-    where I: Iterator<Item = ParagraphItem> + Send + 'static {
-        ParagraphIterator(Box::new(inner))
-    }
-}
-impl Iterator for ParagraphIterator {
-    type Item = ParagraphItem;
-    fn next(&mut self) -> Option<Self::Item> {
-        self.0.next()
-    }
-}
-
-pub trait ParagraphReader:
-    ReaderChild<Request = ParagraphSearchRequest, Response = ParagraphSearchResponse>
-{
-    fn iterator(&self, request: &StreamRequest) -> NodeResult<ParagraphIterator>;
-    fn suggest(&self, request: &SuggestRequest) -> NodeResult<Self::Response>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait ParagraphWriter: WriterChild {}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use crate::prelude::*;
+use crate::protos::*;
+
+pub type ParagraphsReaderPointer = Arc<dyn ParagraphReader>;
+pub type ParagraphsWriterPointer = Arc<RwLock<dyn ParagraphWriter>>;
+
+pub struct ParagraphConfig {
+    pub path: PathBuf,
+}
+
+pub struct ParagraphIterator(Box<dyn Iterator<Item = ParagraphItem> + Send>);
+impl ParagraphIterator {
+    pub fn new<I>(inner: I) -> ParagraphIterator
+    where I: Iterator<Item = ParagraphItem> + Send + 'static {
+        ParagraphIterator(Box::new(inner))
+    }
+}
+impl Iterator for ParagraphIterator {
+    type Item = ParagraphItem;
+    fn next(&mut self) -> Option<Self::Item> {
+        self.0.next()
+    }
+}
+
+pub trait ParagraphReader:
+    ReaderChild<Request = ParagraphSearchRequest, Response = ParagraphSearchResponse>
+{
+    fn iterator(&self, request: &StreamRequest) -> NodeResult<ParagraphIterator>;
+    fn suggest(&self, request: &SuggestRequest) -> NodeResult<Self::Response>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait ParagraphWriter: WriterChild {}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/relations.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/relations.rs`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use crate::prelude::*;
-use crate::protos::*;
-
-pub type RelationsReaderPointer = Arc<dyn RelationReader>;
-pub type RelationsWriterPointer = Arc<RwLock<dyn RelationWriter>>;
-
-#[derive(Clone)]
-pub struct RelationConfig {
-    pub path: PathBuf,
-}
-
-pub trait RelationReader:
-    ReaderChild<Request = RelationSearchRequest, Response = RelationSearchResponse>
-{
-    fn get_edges(&self) -> NodeResult<EdgeList>;
-    fn get_node_types(&self) -> NodeResult<TypeList>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait RelationWriter: WriterChild {
-    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()>;
-    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()>;
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use crate::prelude::*;
+use crate::protos::*;
+
+pub type RelationsReaderPointer = Arc<dyn RelationReader>;
+pub type RelationsWriterPointer = Arc<RwLock<dyn RelationWriter>>;
+
+#[derive(Clone)]
+pub struct RelationConfig {
+    pub path: PathBuf,
+}
+
+pub trait RelationReader:
+    ReaderChild<Request = RelationSearchRequest, Response = RelationSearchResponse>
+{
+    fn get_edges(&self) -> NodeResult<EdgeList>;
+    fn get_node_types(&self) -> NodeResult<TypeList>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait RelationWriter: WriterChild {
+    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()>;
+    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()>;
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_core/src/texts.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_core/src/vectors.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,50 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use crate::prelude::*;
-use crate::protos::*;
-
-pub type TextsReaderPointer = Arc<dyn FieldReader>;
-pub type TextsWriterPointer = Arc<RwLock<dyn FieldWriter>>;
-pub struct TextConfig {
-    pub path: PathBuf,
-}
-
-pub struct DocumentIterator(Box<dyn Iterator<Item = DocumentItem> + Send>);
-impl DocumentIterator {
-    pub fn new<I>(inner: I) -> DocumentIterator
-    where I: Iterator<Item = DocumentItem> + Send + 'static {
-        DocumentIterator(Box::new(inner))
-    }
-}
-impl Iterator for DocumentIterator {
-    type Item = DocumentItem;
-    fn next(&mut self) -> Option<Self::Item> {
-        self.0.next()
-    }
-}
-
-pub trait FieldReader:
-    ReaderChild<Request = DocumentSearchRequest, Response = DocumentSearchResponse>
-{
-    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait FieldWriter: WriterChild {}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use crate::prelude::*;
+use crate::protos::*;
+
+pub type VectorsReaderPointer = Arc<dyn VectorReader>;
+pub type VectorsWriterPointer = Arc<RwLock<dyn VectorWriter>>;
+
+#[derive(Clone)]
+pub struct VectorConfig {
+    pub similarity: Option<VectorSimilarity>,
+    pub path: PathBuf,
+    pub vectorset: PathBuf,
+}
+
+pub trait VectorReader:
+    ReaderChild<Request = VectorSearchRequest, Response = VectorSearchResponse>
+{
+    fn count(&self, vectorset: &str) -> NodeResult<usize>;
+}
+
+pub trait VectorWriter: WriterChild {
+    fn list_vectorsets(&self) -> NodeResult<Vec<String>>;
+    fn remove_vectorset(&mut self, setid: &VectorSetId) -> NodeResult<()>;
+    fn add_vectorset(
+        &mut self,
+        setid: &VectorSetId,
+        similarity: VectorSimilarity,
+    ) -> NodeResult<()>;
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/build.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/build.rs`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-#![warn(clippy::pedantic)]
-
-use std::path::Path;
-use std::{fs, io};
-
-use itertools::Itertools;
-
-fn main() -> io::Result<()> {
-    println!("cargo:rerun-if-changed=stop_words");
-
-    let stop_words = fs::read_dir("./stop_words")?
-        // first filters out nested directories
-        .filter_ok(|entry| entry.file_type().map_or(false, |entry| entry.is_file()))
-        // then transforms JSON array into a list of strings
-        .map(|entry| {
-            let content_file = fs::read_to_string(entry?.path())?;
-            let stop_words = serde_json::from_str(&content_file)?;
-
-            Ok(stop_words)
-        })
-        .flatten_ok::<Vec<String>, io::Error>()
-        .collect::<Result<Vec<_>, _>>()?;
-
-    let mut contents = String::new();
-    contents.push_str("pub fn is_stop_word(x: &str) -> bool {\n");
-    contents.push_str(&format!(
-        r#"    x == "{}""#,
-        stop_words
-            .first()
-            .expect("Empty stop words file not allowed")
-    ));
-    contents.extend(
-        stop_words
-            .into_iter()
-            .skip(1)
-            .map(|word| format!("\n        || x == \"{word}\"")),
-    );
-    contents.push_str("\n}\n");
-
-    fs::write(Path::new("./src/stop_words.rs"), contents)?;
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+#![warn(clippy::pedantic)]
+
+use std::path::Path;
+use std::{fs, io};
+
+use itertools::Itertools;
+
+fn main() -> io::Result<()> {
+    println!("cargo:rerun-if-changed=stop_words");
+
+    let stop_words = fs::read_dir("./stop_words")?
+        // first filters out nested directories
+        .filter_ok(|entry| entry.file_type().map_or(false, |entry| entry.is_file()))
+        // then transforms JSON array into a list of strings
+        .map(|entry| {
+            let content_file = fs::read_to_string(entry?.path())?;
+            let stop_words = serde_json::from_str(&content_file)?;
+
+            Ok(stop_words)
+        })
+        .flatten_ok::<Vec<String>, io::Error>()
+        .collect::<Result<Vec<_>, _>>()?;
+
+    let mut contents = String::new();
+    contents.push_str("pub fn is_stop_word(x: &str) -> bool {\n");
+    contents.push_str(&format!(
+        r#"    x == "{}""#,
+        stop_words
+            .first()
+            .expect("Empty stop words file not allowed")
+    ));
+    contents.extend(
+        stop_words
+            .into_iter()
+            .skip(1)
+            .map(|word| format!("\n        || x == \"{word}\"")),
+    );
+    contents.push_str("\n}\n");
+
+    fs::write(Path::new("./src/stop_words.rs"), contents)?;
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,248 +1,248 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::collections::HashMap;
-use std::io;
-use std::ops::Range;
-use std::sync::Arc;
-
-use levenshtein_automata::{Distance, LevenshteinAutomatonBuilder, DFA};
-use once_cell::sync::Lazy;
-use tantivy::query::{BitSetDocSet, ConstScorer, Explanation, Query, Scorer, Weight};
-use tantivy::schema::{Field, IndexRecordOption, Term};
-use tantivy::termdict::{TermDictionary, TermStreamer};
-use tantivy::TantivyError::InvalidArgument;
-use tantivy::{DocId, DocSet, Score, Searcher, SegmentReader, TantivyError};
-use tantivy_common::BitSet;
-use tantivy_fst::Automaton;
-
-use crate::search_query::SharedTermC;
-
-/// A weight struct for Fuzzy Term and Regex Queries
-pub struct AutomatonWeight<A> {
-    terms: SharedTermC,
-    field: Field,
-    automaton: Arc<A>,
-}
-
-impl<A> AutomatonWeight<A>
-where
-    A: Automaton + Send + Sync + 'static,
-    A::State: Clone,
-{
-    /// Create a new AutomationWeight
-    pub fn new<IntoArcA: Into<Arc<A>>>(
-        field: Field,
-        automaton: IntoArcA,
-        terms: SharedTermC,
-    ) -> AutomatonWeight<A> {
-        AutomatonWeight {
-            field,
-            terms,
-            automaton: automaton.into(),
-        }
-    }
-
-    fn automaton_stream<'a>(
-        &'a self,
-        term_dict: &'a TermDictionary,
-    ) -> io::Result<TermStreamer<'a, &'a A>> {
-        let automaton: &A = &self.automaton;
-        let term_stream_builder = term_dict.search(automaton);
-        term_stream_builder.into_stream()
-    }
-}
-
-impl<A> Weight for AutomatonWeight<A>
-where
-    A: Automaton + Send + Sync + 'static,
-    A::State: Clone,
-{
-    fn scorer(&self, reader: &SegmentReader, boost: Score) -> tantivy::Result<Box<dyn Scorer>> {
-        let max_doc = reader.max_doc();
-        let mut doc_bitset = BitSet::with_max_value(max_doc);
-        let inverted_index = reader.inverted_index(self.field)?;
-        let term_dict = inverted_index.terms();
-        let mut termc = self.terms.get_termc();
-        let mut term_stream = self.automaton_stream(term_dict)?;
-        while term_stream.advance() {
-            let term_key = term_stream.term_ord();
-            let term_info = term_stream.value();
-            let mut block_segment_postings = inverted_index
-                .read_block_postings_from_terminfo(term_info, IndexRecordOption::Basic)?;
-            loop {
-                let docs = block_segment_postings.docs();
-                if docs.is_empty() {
-                    break;
-                }
-                for &doc in docs {
-                    termc.log_fterm(doc, (inverted_index.clone(), term_key));
-                    doc_bitset.insert(doc);
-                }
-                block_segment_postings.advance();
-            }
-        }
-        self.terms.set_termc(termc);
-        let doc_bitset = BitSetDocSet::from(doc_bitset);
-        let const_scorer = ConstScorer::new(doc_bitset, boost);
-        Ok(Box::new(const_scorer))
-    }
-
-    fn explain(&self, reader: &SegmentReader, doc: DocId) -> tantivy::Result<Explanation> {
-        let mut scorer = self.scorer(reader, 1.0)?;
-        if scorer.seek(doc) == doc {
-            Ok(Explanation::new("AutomatonScorer", 1.0))
-        } else {
-            Err(TantivyError::InvalidArgument(
-                "Document does not exist".to_string(),
-            ))
-        }
-    }
-}
-
-pub(crate) struct DfaWrapper(pub DFA);
-
-impl Automaton for DfaWrapper {
-    type State = u32;
-
-    fn start(&self) -> Self::State {
-        self.0.initial_state()
-    }
-
-    fn is_match(&self, state: &Self::State) -> bool {
-        match self.0.distance(*state) {
-            Distance::Exact(_) => true,
-            Distance::AtLeast(_) => false,
-        }
-    }
-
-    fn can_match(&self, state: &u32) -> bool {
-        *state != levenshtein_automata::SINK_STATE
-    }
-
-    fn accept(&self, state: &Self::State, byte: u8) -> Self::State {
-        self.0.transition(*state, byte)
-    }
-}
-
-/// A range of Levenshtein distances that we will build DFAs for our terms
-/// The computation is exponential, so best keep it to low single digits
-const VALID_LEVENSHTEIN_DISTANCE_RANGE: Range<u8> = 0..3;
-
-static LEV_BUILDER: Lazy<HashMap<(u8, bool), LevenshteinAutomatonBuilder>> = Lazy::new(|| {
-    let mut lev_builder_cache = HashMap::new();
-    // TODO make population lazy on a `(distance, val)` basis
-    for distance in VALID_LEVENSHTEIN_DISTANCE_RANGE {
-        for &transposition in &[false, true] {
-            let lev_automaton_builder = LevenshteinAutomatonBuilder::new(distance, transposition);
-            lev_builder_cache.insert((distance, transposition), lev_automaton_builder);
-        }
-    }
-    lev_builder_cache
-});
-
-#[derive(Clone)]
-pub struct FuzzyTermQuery {
-    termc: SharedTermC,
-    /// What term are we searching
-    term: Term,
-    /// How many changes are we going to allow
-    distance: u8,
-    /// Should a transposition cost 1 or 2?
-    transposition_cost_one: bool,
-    ///
-    prefix: bool,
-}
-
-impl std::fmt::Debug for FuzzyTermQuery {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.write_str("Fuzzy")
-    }
-}
-impl FuzzyTermQuery {
-    /// Creates a new Fuzzy Query
-    pub fn new(
-        term: Term,
-        distance: u8,
-        transposition_cost_one: bool,
-        termc: SharedTermC,
-    ) -> FuzzyTermQuery {
-        FuzzyTermQuery {
-            term,
-            termc,
-            distance,
-            transposition_cost_one,
-            prefix: false,
-        }
-    }
-
-    /// Creates a new Fuzzy Query of the Term prefix
-    pub fn new_prefix(
-        term: Term,
-        distance: u8,
-        transposition_cost_one: bool,
-        termc: SharedTermC,
-    ) -> FuzzyTermQuery {
-        FuzzyTermQuery {
-            term,
-            termc,
-            distance,
-            transposition_cost_one,
-            prefix: true,
-        }
-    }
-
-    fn specialized_weight(&self) -> tantivy::Result<AutomatonWeight<DfaWrapper>> {
-        // LEV_BUILDER is a HashMap, whose `get` method returns an Option
-        match LEV_BUILDER.get(&(self.distance, self.transposition_cost_one)) {
-            // Unwrap the option and build the Ok(AutomatonWeight)
-            Some(automaton_builder) => {
-                let term_text = self.term.as_str().ok_or_else(|| {
-                    tantivy::TantivyError::InvalidArgument(
-                        "The fuzzy term query requires a string term.".to_string(),
-                    )
-                })?;
-                let automaton = if self.prefix {
-                    automaton_builder.build_prefix_dfa(term_text)
-                } else {
-                    automaton_builder.build_dfa(term_text)
-                };
-                Ok(AutomatonWeight::new(
-                    self.term.field(),
-                    DfaWrapper(automaton),
-                    self.termc.clone(),
-                ))
-            }
-            None => Err(InvalidArgument(format!(
-                "Levenshtein distance of {} is not allowed. Choose a value in the {:?} range",
-                self.distance, VALID_LEVENSHTEIN_DISTANCE_RANGE
-            ))),
-        }
-    }
-}
-
-impl Query for FuzzyTermQuery {
-    fn weight(
-        &self,
-        _searcher: &Searcher,
-        _scoring_enabled: bool,
-    ) -> tantivy::Result<Box<dyn Weight>> {
-        Ok(Box::new(self.specialized_weight()?))
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::collections::HashMap;
+use std::io;
+use std::ops::Range;
+use std::sync::Arc;
+
+use levenshtein_automata::{Distance, LevenshteinAutomatonBuilder, DFA};
+use once_cell::sync::Lazy;
+use tantivy::query::{BitSetDocSet, ConstScorer, Explanation, Query, Scorer, Weight};
+use tantivy::schema::{Field, IndexRecordOption, Term};
+use tantivy::termdict::{TermDictionary, TermStreamer};
+use tantivy::TantivyError::InvalidArgument;
+use tantivy::{DocId, DocSet, Score, Searcher, SegmentReader, TantivyError};
+use tantivy_common::BitSet;
+use tantivy_fst::Automaton;
+
+use crate::search_query::SharedTermC;
+
+/// A weight struct for Fuzzy Term and Regex Queries
+pub struct AutomatonWeight<A> {
+    terms: SharedTermC,
+    field: Field,
+    automaton: Arc<A>,
+}
+
+impl<A> AutomatonWeight<A>
+where
+    A: Automaton + Send + Sync + 'static,
+    A::State: Clone,
+{
+    /// Create a new AutomationWeight
+    pub fn new<IntoArcA: Into<Arc<A>>>(
+        field: Field,
+        automaton: IntoArcA,
+        terms: SharedTermC,
+    ) -> AutomatonWeight<A> {
+        AutomatonWeight {
+            field,
+            terms,
+            automaton: automaton.into(),
+        }
+    }
+
+    fn automaton_stream<'a>(
+        &'a self,
+        term_dict: &'a TermDictionary,
+    ) -> io::Result<TermStreamer<'a, &'a A>> {
+        let automaton: &A = &self.automaton;
+        let term_stream_builder = term_dict.search(automaton);
+        term_stream_builder.into_stream()
+    }
+}
+
+impl<A> Weight for AutomatonWeight<A>
+where
+    A: Automaton + Send + Sync + 'static,
+    A::State: Clone,
+{
+    fn scorer(&self, reader: &SegmentReader, boost: Score) -> tantivy::Result<Box<dyn Scorer>> {
+        let max_doc = reader.max_doc();
+        let mut doc_bitset = BitSet::with_max_value(max_doc);
+        let inverted_index = reader.inverted_index(self.field)?;
+        let term_dict = inverted_index.terms();
+        let mut termc = self.terms.get_termc();
+        let mut term_stream = self.automaton_stream(term_dict)?;
+        while term_stream.advance() {
+            let term_key = term_stream.term_ord();
+            let term_info = term_stream.value();
+            let mut block_segment_postings = inverted_index
+                .read_block_postings_from_terminfo(term_info, IndexRecordOption::Basic)?;
+            loop {
+                let docs = block_segment_postings.docs();
+                if docs.is_empty() {
+                    break;
+                }
+                for &doc in docs {
+                    termc.log_fterm(doc, (inverted_index.clone(), term_key));
+                    doc_bitset.insert(doc);
+                }
+                block_segment_postings.advance();
+            }
+        }
+        self.terms.set_termc(termc);
+        let doc_bitset = BitSetDocSet::from(doc_bitset);
+        let const_scorer = ConstScorer::new(doc_bitset, boost);
+        Ok(Box::new(const_scorer))
+    }
+
+    fn explain(&self, reader: &SegmentReader, doc: DocId) -> tantivy::Result<Explanation> {
+        let mut scorer = self.scorer(reader, 1.0)?;
+        if scorer.seek(doc) == doc {
+            Ok(Explanation::new("AutomatonScorer", 1.0))
+        } else {
+            Err(TantivyError::InvalidArgument(
+                "Document does not exist".to_string(),
+            ))
+        }
+    }
+}
+
+pub(crate) struct DfaWrapper(pub DFA);
+
+impl Automaton for DfaWrapper {
+    type State = u32;
+
+    fn start(&self) -> Self::State {
+        self.0.initial_state()
+    }
+
+    fn is_match(&self, state: &Self::State) -> bool {
+        match self.0.distance(*state) {
+            Distance::Exact(_) => true,
+            Distance::AtLeast(_) => false,
+        }
+    }
+
+    fn can_match(&self, state: &u32) -> bool {
+        *state != levenshtein_automata::SINK_STATE
+    }
+
+    fn accept(&self, state: &Self::State, byte: u8) -> Self::State {
+        self.0.transition(*state, byte)
+    }
+}
+
+/// A range of Levenshtein distances that we will build DFAs for our terms
+/// The computation is exponential, so best keep it to low single digits
+const VALID_LEVENSHTEIN_DISTANCE_RANGE: Range<u8> = 0..3;
+
+static LEV_BUILDER: Lazy<HashMap<(u8, bool), LevenshteinAutomatonBuilder>> = Lazy::new(|| {
+    let mut lev_builder_cache = HashMap::new();
+    // TODO make population lazy on a `(distance, val)` basis
+    for distance in VALID_LEVENSHTEIN_DISTANCE_RANGE {
+        for &transposition in &[false, true] {
+            let lev_automaton_builder = LevenshteinAutomatonBuilder::new(distance, transposition);
+            lev_builder_cache.insert((distance, transposition), lev_automaton_builder);
+        }
+    }
+    lev_builder_cache
+});
+
+#[derive(Clone)]
+pub struct FuzzyTermQuery {
+    termc: SharedTermC,
+    /// What term are we searching
+    term: Term,
+    /// How many changes are we going to allow
+    distance: u8,
+    /// Should a transposition cost 1 or 2?
+    transposition_cost_one: bool,
+    ///
+    prefix: bool,
+}
+
+impl std::fmt::Debug for FuzzyTermQuery {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.write_str("Fuzzy")
+    }
+}
+impl FuzzyTermQuery {
+    /// Creates a new Fuzzy Query
+    pub fn new(
+        term: Term,
+        distance: u8,
+        transposition_cost_one: bool,
+        termc: SharedTermC,
+    ) -> FuzzyTermQuery {
+        FuzzyTermQuery {
+            term,
+            termc,
+            distance,
+            transposition_cost_one,
+            prefix: false,
+        }
+    }
+
+    /// Creates a new Fuzzy Query of the Term prefix
+    pub fn new_prefix(
+        term: Term,
+        distance: u8,
+        transposition_cost_one: bool,
+        termc: SharedTermC,
+    ) -> FuzzyTermQuery {
+        FuzzyTermQuery {
+            term,
+            termc,
+            distance,
+            transposition_cost_one,
+            prefix: true,
+        }
+    }
+
+    fn specialized_weight(&self) -> tantivy::Result<AutomatonWeight<DfaWrapper>> {
+        // LEV_BUILDER is a HashMap, whose `get` method returns an Option
+        match LEV_BUILDER.get(&(self.distance, self.transposition_cost_one)) {
+            // Unwrap the option and build the Ok(AutomatonWeight)
+            Some(automaton_builder) => {
+                let term_text = self.term.as_str().ok_or_else(|| {
+                    tantivy::TantivyError::InvalidArgument(
+                        "The fuzzy term query requires a string term.".to_string(),
+                    )
+                })?;
+                let automaton = if self.prefix {
+                    automaton_builder.build_prefix_dfa(term_text)
+                } else {
+                    automaton_builder.build_dfa(term_text)
+                };
+                Ok(AutomatonWeight::new(
+                    self.term.field(),
+                    DfaWrapper(automaton),
+                    self.termc.clone(),
+                ))
+            }
+            None => Err(InvalidArgument(format!(
+                "Levenshtein distance of {} is not allowed. Choose a value in the {:?} range",
+                self.distance, VALID_LEVENSHTEIN_DISTANCE_RANGE
+            ))),
+        }
+    }
+}
+
+impl Query for FuzzyTermQuery {
+    fn weight(
+        &self,
+        _searcher: &Searcher,
+        _scoring_enabled: bool,
+    ) -> tantivy::Result<Box<dyn Weight>> {
+        Ok(Box::new(self.specialized_weight()?))
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/search_query.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/search_query.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,524 +1,524 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::borrow::Cow;
-use std::collections::{HashMap, HashSet};
-use std::sync::{Arc, Mutex};
-
-use itertools::Itertools;
-use nucliadb_core::protos::{ParagraphSearchRequest, StreamRequest, SuggestRequest};
-use tantivy::query::*;
-use tantivy::schema::{Facet, IndexRecordOption};
-use tantivy::{DocId, InvertedIndexReader, Term};
-
-use crate::fuzzy_query::FuzzyTermQuery;
-use crate::schema::ParagraphSchema;
-use crate::stop_words::is_stop_word;
-
-type QueryP = (Occur, Box<dyn Query>);
-
-// Used to identify the terms matched by tantivy
-#[derive(Clone)]
-pub struct TermCollector {
-    pub eterms: HashSet<String>,
-    pub fterms: HashMap<DocId, Vec<(Arc<InvertedIndexReader>, u64)>>,
-}
-impl Default for TermCollector {
-    fn default() -> Self {
-        Self::new()
-    }
-}
-impl TermCollector {
-    pub fn new() -> TermCollector {
-        TermCollector {
-            fterms: HashMap::new(),
-            eterms: HashSet::new(),
-        }
-    }
-    pub fn log_eterm(&mut self, term: String) {
-        self.eterms.insert(term);
-    }
-    pub fn log_fterm(&mut self, doc: DocId, data: (Arc<InvertedIndexReader>, u64)) {
-        self.fterms.entry(doc).or_insert_with(Vec::new).push(data);
-    }
-    pub fn get_fterms(&self, doc: DocId) -> Vec<String> {
-        let mut terms = Vec::new();
-        for (index, term) in self.fterms.get(&doc).iter().flat_map(|v| v.iter()).cloned() {
-            let term_dict = index.terms();
-            let mut term_s = vec![];
-            let found = term_dict.ord_to_term(term, &mut term_s).unwrap_or(false);
-            let elem = if found { term_s } else { vec![] };
-            match String::from_utf8(elem).ok() {
-                Some(v) if v.len() > 2 => terms.push(v),
-                _ => (),
-            }
-        }
-        terms
-    }
-}
-
-#[derive(Default, Clone)]
-pub struct SharedTermC(Arc<Mutex<TermCollector>>);
-impl SharedTermC {
-    pub fn from(termc: TermCollector) -> SharedTermC {
-        SharedTermC(Arc::new(Mutex::new(termc)))
-    }
-    pub fn new() -> SharedTermC {
-        SharedTermC::default()
-    }
-    pub fn get_termc(&self) -> TermCollector {
-        std::mem::take(&mut self.0.lock().unwrap())
-    }
-    pub fn set_termc(&self, termc: TermCollector) {
-        *self.0.lock().unwrap() = termc;
-    }
-}
-
-fn term_to_fuzzy(
-    query: Box<dyn Query>,
-    distance: u8,
-    termc: SharedTermC,
-    as_prefix: bool,
-) -> Box<dyn Query> {
-    let term_query: &TermQuery = query.downcast_ref().unwrap();
-    let term = term_query.term().clone();
-    let term_as_str = term.as_str();
-    let should_be_prefixed = term_as_str
-        .map(|s| as_prefix && s.len() > 3)
-        .unwrap_or_default();
-    if should_be_prefixed {
-        Box::new(FuzzyTermQuery::new_prefix(term, distance, true, termc))
-    } else {
-        Box::new(FuzzyTermQuery::new(term, distance, true, termc))
-    }
-}
-
-fn queryp_map(
-    queries: Vec<QueryP>,
-    distance: u8,
-    as_prefix: Option<usize>,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    queries
-        .into_iter()
-        .enumerate()
-        .map(|(id, (_, query))| {
-            let query = if query.is::<TermQuery>() {
-                term_to_fuzzy(
-                    query,
-                    distance,
-                    termc.clone(),
-                    as_prefix.map_or(false, |v| id == v),
-                )
-            } else {
-                query
-            };
-            (Occur::Must, query)
-        })
-        .collect()
-}
-
-fn flat_bool_query(query: BooleanQuery, collector: (usize, Vec<QueryP>)) -> (usize, Vec<QueryP>) {
-    query
-        .clauses()
-        .iter()
-        .map(|(occur, subq)| (*occur, subq.box_clone()))
-        .fold(collector, |(mut id, mut c), (occur, subq)| {
-            if subq.is::<BooleanQuery>() {
-                let subq: Box<BooleanQuery> = subq.downcast().unwrap();
-                flat_bool_query(*subq, (id, c))
-            } else if subq.is::<TermQuery>() {
-                id = c.len();
-                c.push((occur, subq));
-                (id, c)
-            } else {
-                c.push((occur, subq));
-                (id, c)
-            }
-        })
-}
-
-fn flat_and_adapt(
-    query: Box<dyn Query>,
-    prefixed: bool,
-    distance: u8,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    let (queries, as_prefix) = if query.is::<BooleanQuery>() {
-        let query: Box<BooleanQuery> = query.downcast().unwrap();
-        let (as_prefix, queries) = flat_bool_query(*query, (usize::MAX, vec![]));
-        (queries, as_prefix)
-    } else if query.is::<TermQuery>() {
-        let queries = vec![(Occur::Must, query)];
-        let as_prefix = 0;
-        (queries, as_prefix)
-    } else {
-        let queries = vec![(Occur::Must, query)];
-        let as_prefix = 1;
-        (queries, as_prefix)
-    };
-    queryp_map(
-        queries,
-        distance,
-        if prefixed { Some(as_prefix) } else { None },
-        termc,
-    )
-}
-
-fn fuzzied_queries(
-    query: Box<dyn Query>,
-    prefixed: bool,
-    distance: u8,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    if query.is::<AllQuery>() {
-        vec![]
-    } else {
-        flat_and_adapt(query, prefixed, distance, termc)
-    }
-}
-
-fn parse_query(parser: &QueryParser, text: &str) -> Box<dyn Query> {
-    if text.is_empty() {
-        Box::new(AllQuery) as Box<dyn Query>
-    } else {
-        parser
-            .parse_query(text)
-            .ok()
-            .unwrap_or_else(|| Box::new(AllQuery))
-    }
-}
-
-/// Removes all fuzzy terms identified as stop word.
-///
-/// A stop word is any fuzzy term that match the following criterias:
-/// - Presents in the given list of stop words
-/// - Is **NOT** the last term in the query
-///
-/// The last term of the query is a prefix fuzzy term and must be preserved.
-fn remove_stop_words(query: &str) -> Cow<'_, str> {
-    match query.rsplit_once(' ') {
-        Some((query, last_term)) => query
-            .split(' ')
-            .filter(|term| !is_stop_word(&term.to_lowercase()))
-            .chain([last_term])
-            .join(" ")
-            .into(),
-        None => query.into(),
-    }
-}
-
-#[derive(Debug)]
-struct ProcessedQuery {
-    fuzzy_query: String,
-    regular_query: String,
-}
-fn preprocess_raw_query(query: &str, tc: &mut TermCollector) -> ProcessedQuery {
-    let mut regular_query = String::new();
-    let mut fuzzy_query = String::new();
-    let mut quote_starts = vec![];
-    let mut quote_ends = vec![];
-    let mut start = 0;
-    for (i, d) in query.match_indices('\"').enumerate() {
-        if i % 2 == 0 {
-            quote_starts.push(d.0)
-        } else {
-            quote_ends.push(d.0)
-        }
-    }
-    for (qstart, qend) in quote_starts.into_iter().zip(quote_ends.into_iter()) {
-        let quote = query[(qstart + 1)..qend].trim();
-        let unquote = query[start..qstart].trim();
-        let unquote = remove_stop_words(unquote);
-
-        unquote
-            .split(' ')
-            .filter(|s| !s.is_empty())
-            .for_each(|t| tc.log_eterm(t.to_string()));
-        tc.log_eterm(quote.to_string());
-
-        if !regular_query.is_empty() {
-            regular_query.push(' ');
-        }
-        regular_query.push_str(&unquote);
-        regular_query.push(' ');
-        regular_query.push('"');
-        regular_query.push_str(quote);
-        regular_query.push('"');
-
-        if !fuzzy_query.is_empty() {
-            fuzzy_query.push(' ');
-        }
-        fuzzy_query.push_str(&unquote);
-
-        start = qend + 1;
-    }
-    if start < query.len() {
-        let tail = query[start..].trim();
-        let tail = remove_stop_words(tail);
-
-        tail.split(' ')
-            .filter(|s| !s.is_empty())
-            .for_each(|t| tc.log_eterm(t.to_string()));
-
-        if !regular_query.is_empty() {
-            regular_query.push(' ');
-        }
-        regular_query.push_str(&tail);
-
-        if !fuzzy_query.is_empty() {
-            fuzzy_query.push(' ');
-        }
-        fuzzy_query.push_str(&tail);
-    }
-    ProcessedQuery {
-        regular_query,
-        fuzzy_query,
-    }
-}
-pub fn suggest_query(
-    parser: &QueryParser,
-    text: &str,
-    request: &SuggestRequest,
-    schema: &ParagraphSchema,
-    distance: u8,
-) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
-    let mut term_collector = TermCollector::default();
-    let processed = preprocess_raw_query(text, &mut term_collector);
-    let query = parse_query(parser, &processed.regular_query);
-    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
-    let termc = SharedTermC::from(term_collector);
-    let mut fuzzies = fuzzied_queries(fuzzy_query, true, distance, termc.clone());
-    let mut originals = vec![(Occur::Must, query)];
-    let term = Term::from_field_u64(schema.repeated_in_field, 0);
-    let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-    fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-    originals.push((Occur::Must, Box::new(term_query)));
-
-    // Fields
-    request
-        .fields
-        .iter()
-        .map(|value| format!("/{value}"))
-        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.field, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    // Filters
-    request
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
-        let original = originals.pop().unwrap().1;
-        let fuzzy = Box::new(BooleanQuery::new(vec![]));
-        (original, termc, fuzzy)
-    } else {
-        if processed.fuzzy_query.is_empty() {
-            fuzzies.clear();
-        }
-        let original = Box::new(BooleanQuery::new(originals));
-        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
-        (original, termc, fuzzied)
-    }
-}
-
-pub fn search_query(
-    parser: &QueryParser,
-    text: &str,
-    search: &ParagraphSearchRequest,
-    schema: &ParagraphSchema,
-    distance: u8,
-    with_advance: Option<Box<dyn Query>>,
-) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
-    let mut term_collector = TermCollector::default();
-    let processed = preprocess_raw_query(text, &mut term_collector);
-    let query = parse_query(parser, &processed.regular_query);
-    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
-    let termc = SharedTermC::from(term_collector);
-    let mut fuzzies = fuzzied_queries(fuzzy_query, false, distance, termc.clone());
-    let mut originals = vec![(Occur::Must, query)];
-    if let Some(advance) = with_advance {
-        originals.push((Occur::Must, advance.box_clone()));
-        fuzzies.push((Occur::Must, advance));
-    }
-    if !search.uuid.is_empty() {
-        let term = Term::from_field_text(schema.uuid, &search.uuid);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-        originals.push((Occur::Must, Box::new(term_query)))
-    }
-    if !search.with_duplicates {
-        let term = Term::from_field_u64(schema.repeated_in_field, 0);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-        originals.push((Occur::Must, Box::new(term_query)))
-    }
-    // Fields
-    let mut field_filter: Vec<(Occur, Box<dyn Query>)> = vec![];
-    search
-        .fields
-        .iter()
-        .map(|value| format!("/{value}"))
-        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.field, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            field_filter.push((Occur::Should, Box::new(facet_term_query)));
-        });
-    if !field_filter.is_empty() {
-        let field_filter = Box::new(BooleanQuery::new(field_filter));
-        fuzzies.push((Occur::Must, field_filter.clone()));
-        originals.push((Occur::Must, field_filter));
-    }
-    // Add filter
-    search
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-    // Keys
-    search.key_filters.iter().for_each(|uuid| {
-        let term = Term::from_field_text(schema.uuid, uuid);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-        originals.push((Occur::Must, Box::new(term_query)));
-    });
-
-    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
-        let original = originals.pop().unwrap().1;
-        let fuzzy = Box::new(BooleanQuery::new(vec![]));
-        (original, termc, fuzzy)
-    } else {
-        if processed.fuzzy_query.is_empty() {
-            fuzzies.clear();
-        }
-        let original = Box::new(BooleanQuery::new(originals));
-        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
-        (original, termc, fuzzied)
-    }
-}
-
-pub fn streaming_query(schema: &ParagraphSchema, request: &StreamRequest) -> Box<dyn Query> {
-    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
-    queries.push((Occur::Must, Box::new(AllQuery)));
-    request
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            queries.push((Occur::Must, Box::new(facet_term_query)));
-        });
-    Box::new(BooleanQuery::new(queries))
-}
-
-#[cfg(test)]
-mod tests {
-    use tantivy::schema::Field;
-
-    use super::*;
-    fn dummy_term_query() -> Box<dyn Query> {
-        let field = Field::from_field_id(0);
-        let term = Term::from_field_u64(field, 0);
-        Box::new(TermQuery::new(term, IndexRecordOption::Basic))
-    }
-
-    #[test]
-    fn test_preprocessor() {
-        let text = "own test \"This is great\"";
-        let mut term_collector = TermCollector::default();
-        let _ = preprocess_raw_query(text, &mut term_collector);
-        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
-        let expect = HashSet::from(["This is great", "test"]);
-        assert_eq!(terms, expect);
-
-        let text = "The test \"is correct\" always";
-        let mut term_collector = TermCollector::default();
-        let processed = preprocess_raw_query(text, &mut term_collector);
-        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
-        let expect = HashSet::from(["test", "always", "is correct"]);
-        assert_eq!(terms, expect);
-        assert_eq!(processed.regular_query, "test \"is correct\" always");
-        assert_eq!(processed.fuzzy_query, "test always");
-    }
-
-    #[test]
-    fn test() {
-        let subqueries0: Vec<_> = vec![dummy_term_query; 12]
-            .into_iter()
-            .map(|f| (Occur::Must, f()))
-            .collect();
-        let subqueries1: Vec<_> = vec![dummy_term_query; 12]
-            .into_iter()
-            .map(|f| (Occur::Must, f()))
-            .collect();
-        let boolean0: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries0));
-        let boolean1: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries1));
-        let nested = BooleanQuery::new(vec![(Occur::Should, boolean0), (Occur::Should, boolean1)]);
-        let adapted = flat_and_adapt(Box::new(nested), true, 2, SharedTermC::new());
-        assert_eq!(adapted.len(), 24);
-        assert!(adapted.iter().all(|(occur, _)| *occur == Occur::Must));
-        assert!(adapted
-            .iter()
-            .all(|(_, query)| query.is::<FuzzyTermQuery>()));
-    }
-
-    #[test]
-    fn it_removes_stop_word_fterms() {
-        let tests = [
-            (
-                "nuclia is a database for unstructured data",
-                "nuclia database unstructured data",
-            ),
-            (
-                "nuclia is a",
-                // keeps last term even if is a stop word
-                "nuclia a",
-            ),
-            ("is a for and", "and"),
-        ];
-
-        for (query, expected_fuzzy_query) in tests {
-            let fuzzy_query = remove_stop_words(query);
-
-            assert_eq!(fuzzy_query, expected_fuzzy_query);
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::borrow::Cow;
+use std::collections::{HashMap, HashSet};
+use std::sync::{Arc, Mutex};
+
+use itertools::Itertools;
+use nucliadb_core::protos::{ParagraphSearchRequest, StreamRequest, SuggestRequest};
+use tantivy::query::*;
+use tantivy::schema::{Facet, IndexRecordOption};
+use tantivy::{DocId, InvertedIndexReader, Term};
+
+use crate::fuzzy_query::FuzzyTermQuery;
+use crate::schema::ParagraphSchema;
+use crate::stop_words::is_stop_word;
+
+type QueryP = (Occur, Box<dyn Query>);
+
+// Used to identify the terms matched by tantivy
+#[derive(Clone)]
+pub struct TermCollector {
+    pub eterms: HashSet<String>,
+    pub fterms: HashMap<DocId, Vec<(Arc<InvertedIndexReader>, u64)>>,
+}
+impl Default for TermCollector {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+impl TermCollector {
+    pub fn new() -> TermCollector {
+        TermCollector {
+            fterms: HashMap::new(),
+            eterms: HashSet::new(),
+        }
+    }
+    pub fn log_eterm(&mut self, term: String) {
+        self.eterms.insert(term);
+    }
+    pub fn log_fterm(&mut self, doc: DocId, data: (Arc<InvertedIndexReader>, u64)) {
+        self.fterms.entry(doc).or_insert_with(Vec::new).push(data);
+    }
+    pub fn get_fterms(&self, doc: DocId) -> Vec<String> {
+        let mut terms = Vec::new();
+        for (index, term) in self.fterms.get(&doc).iter().flat_map(|v| v.iter()).cloned() {
+            let term_dict = index.terms();
+            let mut term_s = vec![];
+            let found = term_dict.ord_to_term(term, &mut term_s).unwrap_or(false);
+            let elem = if found { term_s } else { vec![] };
+            match String::from_utf8(elem).ok() {
+                Some(v) if v.len() > 2 => terms.push(v),
+                _ => (),
+            }
+        }
+        terms
+    }
+}
+
+#[derive(Default, Clone)]
+pub struct SharedTermC(Arc<Mutex<TermCollector>>);
+impl SharedTermC {
+    pub fn from(termc: TermCollector) -> SharedTermC {
+        SharedTermC(Arc::new(Mutex::new(termc)))
+    }
+    pub fn new() -> SharedTermC {
+        SharedTermC::default()
+    }
+    pub fn get_termc(&self) -> TermCollector {
+        std::mem::take(&mut self.0.lock().unwrap())
+    }
+    pub fn set_termc(&self, termc: TermCollector) {
+        *self.0.lock().unwrap() = termc;
+    }
+}
+
+fn term_to_fuzzy(
+    query: Box<dyn Query>,
+    distance: u8,
+    termc: SharedTermC,
+    as_prefix: bool,
+) -> Box<dyn Query> {
+    let term_query: &TermQuery = query.downcast_ref().unwrap();
+    let term = term_query.term().clone();
+    let term_as_str = term.as_str();
+    let should_be_prefixed = term_as_str
+        .map(|s| as_prefix && s.len() > 3)
+        .unwrap_or_default();
+    if should_be_prefixed {
+        Box::new(FuzzyTermQuery::new_prefix(term, distance, true, termc))
+    } else {
+        Box::new(FuzzyTermQuery::new(term, distance, true, termc))
+    }
+}
+
+fn queryp_map(
+    queries: Vec<QueryP>,
+    distance: u8,
+    as_prefix: Option<usize>,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    queries
+        .into_iter()
+        .enumerate()
+        .map(|(id, (_, query))| {
+            let query = if query.is::<TermQuery>() {
+                term_to_fuzzy(
+                    query,
+                    distance,
+                    termc.clone(),
+                    as_prefix.map_or(false, |v| id == v),
+                )
+            } else {
+                query
+            };
+            (Occur::Must, query)
+        })
+        .collect()
+}
+
+fn flat_bool_query(query: BooleanQuery, collector: (usize, Vec<QueryP>)) -> (usize, Vec<QueryP>) {
+    query
+        .clauses()
+        .iter()
+        .map(|(occur, subq)| (*occur, subq.box_clone()))
+        .fold(collector, |(mut id, mut c), (occur, subq)| {
+            if subq.is::<BooleanQuery>() {
+                let subq: Box<BooleanQuery> = subq.downcast().unwrap();
+                flat_bool_query(*subq, (id, c))
+            } else if subq.is::<TermQuery>() {
+                id = c.len();
+                c.push((occur, subq));
+                (id, c)
+            } else {
+                c.push((occur, subq));
+                (id, c)
+            }
+        })
+}
+
+fn flat_and_adapt(
+    query: Box<dyn Query>,
+    prefixed: bool,
+    distance: u8,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    let (queries, as_prefix) = if query.is::<BooleanQuery>() {
+        let query: Box<BooleanQuery> = query.downcast().unwrap();
+        let (as_prefix, queries) = flat_bool_query(*query, (usize::MAX, vec![]));
+        (queries, as_prefix)
+    } else if query.is::<TermQuery>() {
+        let queries = vec![(Occur::Must, query)];
+        let as_prefix = 0;
+        (queries, as_prefix)
+    } else {
+        let queries = vec![(Occur::Must, query)];
+        let as_prefix = 1;
+        (queries, as_prefix)
+    };
+    queryp_map(
+        queries,
+        distance,
+        if prefixed { Some(as_prefix) } else { None },
+        termc,
+    )
+}
+
+fn fuzzied_queries(
+    query: Box<dyn Query>,
+    prefixed: bool,
+    distance: u8,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    if query.is::<AllQuery>() {
+        vec![]
+    } else {
+        flat_and_adapt(query, prefixed, distance, termc)
+    }
+}
+
+fn parse_query(parser: &QueryParser, text: &str) -> Box<dyn Query> {
+    if text.is_empty() {
+        Box::new(AllQuery) as Box<dyn Query>
+    } else {
+        parser
+            .parse_query(text)
+            .ok()
+            .unwrap_or_else(|| Box::new(AllQuery))
+    }
+}
+
+/// Removes all fuzzy terms identified as stop word.
+///
+/// A stop word is any fuzzy term that match the following criterias:
+/// - Presents in the given list of stop words
+/// - Is **NOT** the last term in the query
+///
+/// The last term of the query is a prefix fuzzy term and must be preserved.
+fn remove_stop_words(query: &str) -> Cow<'_, str> {
+    match query.rsplit_once(' ') {
+        Some((query, last_term)) => query
+            .split(' ')
+            .filter(|term| !is_stop_word(&term.to_lowercase()))
+            .chain([last_term])
+            .join(" ")
+            .into(),
+        None => query.into(),
+    }
+}
+
+#[derive(Debug)]
+struct ProcessedQuery {
+    fuzzy_query: String,
+    regular_query: String,
+}
+fn preprocess_raw_query(query: &str, tc: &mut TermCollector) -> ProcessedQuery {
+    let mut regular_query = String::new();
+    let mut fuzzy_query = String::new();
+    let mut quote_starts = vec![];
+    let mut quote_ends = vec![];
+    let mut start = 0;
+    for (i, d) in query.match_indices('\"').enumerate() {
+        if i % 2 == 0 {
+            quote_starts.push(d.0)
+        } else {
+            quote_ends.push(d.0)
+        }
+    }
+    for (qstart, qend) in quote_starts.into_iter().zip(quote_ends.into_iter()) {
+        let quote = query[(qstart + 1)..qend].trim();
+        let unquote = query[start..qstart].trim();
+        let unquote = remove_stop_words(unquote);
+
+        unquote
+            .split(' ')
+            .filter(|s| !s.is_empty())
+            .for_each(|t| tc.log_eterm(t.to_string()));
+        tc.log_eterm(quote.to_string());
+
+        if !regular_query.is_empty() {
+            regular_query.push(' ');
+        }
+        regular_query.push_str(&unquote);
+        regular_query.push(' ');
+        regular_query.push('"');
+        regular_query.push_str(quote);
+        regular_query.push('"');
+
+        if !fuzzy_query.is_empty() {
+            fuzzy_query.push(' ');
+        }
+        fuzzy_query.push_str(&unquote);
+
+        start = qend + 1;
+    }
+    if start < query.len() {
+        let tail = query[start..].trim();
+        let tail = remove_stop_words(tail);
+
+        tail.split(' ')
+            .filter(|s| !s.is_empty())
+            .for_each(|t| tc.log_eterm(t.to_string()));
+
+        if !regular_query.is_empty() {
+            regular_query.push(' ');
+        }
+        regular_query.push_str(&tail);
+
+        if !fuzzy_query.is_empty() {
+            fuzzy_query.push(' ');
+        }
+        fuzzy_query.push_str(&tail);
+    }
+    ProcessedQuery {
+        regular_query,
+        fuzzy_query,
+    }
+}
+pub fn suggest_query(
+    parser: &QueryParser,
+    text: &str,
+    request: &SuggestRequest,
+    schema: &ParagraphSchema,
+    distance: u8,
+) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
+    let mut term_collector = TermCollector::default();
+    let processed = preprocess_raw_query(text, &mut term_collector);
+    let query = parse_query(parser, &processed.regular_query);
+    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
+    let termc = SharedTermC::from(term_collector);
+    let mut fuzzies = fuzzied_queries(fuzzy_query, true, distance, termc.clone());
+    let mut originals = vec![(Occur::Must, query)];
+    let term = Term::from_field_u64(schema.repeated_in_field, 0);
+    let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+    fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+    originals.push((Occur::Must, Box::new(term_query)));
+
+    // Fields
+    request
+        .fields
+        .iter()
+        .map(|value| format!("/{value}"))
+        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.field, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    // Filters
+    request
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
+        let original = originals.pop().unwrap().1;
+        let fuzzy = Box::new(BooleanQuery::new(vec![]));
+        (original, termc, fuzzy)
+    } else {
+        if processed.fuzzy_query.is_empty() {
+            fuzzies.clear();
+        }
+        let original = Box::new(BooleanQuery::new(originals));
+        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
+        (original, termc, fuzzied)
+    }
+}
+
+pub fn search_query(
+    parser: &QueryParser,
+    text: &str,
+    search: &ParagraphSearchRequest,
+    schema: &ParagraphSchema,
+    distance: u8,
+    with_advance: Option<Box<dyn Query>>,
+) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
+    let mut term_collector = TermCollector::default();
+    let processed = preprocess_raw_query(text, &mut term_collector);
+    let query = parse_query(parser, &processed.regular_query);
+    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
+    let termc = SharedTermC::from(term_collector);
+    let mut fuzzies = fuzzied_queries(fuzzy_query, false, distance, termc.clone());
+    let mut originals = vec![(Occur::Must, query)];
+    if let Some(advance) = with_advance {
+        originals.push((Occur::Must, advance.box_clone()));
+        fuzzies.push((Occur::Must, advance));
+    }
+    if !search.uuid.is_empty() {
+        let term = Term::from_field_text(schema.uuid, &search.uuid);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+        originals.push((Occur::Must, Box::new(term_query)))
+    }
+    if !search.with_duplicates {
+        let term = Term::from_field_u64(schema.repeated_in_field, 0);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+        originals.push((Occur::Must, Box::new(term_query)))
+    }
+    // Fields
+    let mut field_filter: Vec<(Occur, Box<dyn Query>)> = vec![];
+    search
+        .fields
+        .iter()
+        .map(|value| format!("/{value}"))
+        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.field, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            field_filter.push((Occur::Should, Box::new(facet_term_query)));
+        });
+    if !field_filter.is_empty() {
+        let field_filter = Box::new(BooleanQuery::new(field_filter));
+        fuzzies.push((Occur::Must, field_filter.clone()));
+        originals.push((Occur::Must, field_filter));
+    }
+    // Add filter
+    search
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+    // Keys
+    search.key_filters.iter().for_each(|uuid| {
+        let term = Term::from_field_text(schema.uuid, uuid);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+        originals.push((Occur::Must, Box::new(term_query)));
+    });
+
+    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
+        let original = originals.pop().unwrap().1;
+        let fuzzy = Box::new(BooleanQuery::new(vec![]));
+        (original, termc, fuzzy)
+    } else {
+        if processed.fuzzy_query.is_empty() {
+            fuzzies.clear();
+        }
+        let original = Box::new(BooleanQuery::new(originals));
+        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
+        (original, termc, fuzzied)
+    }
+}
+
+pub fn streaming_query(schema: &ParagraphSchema, request: &StreamRequest) -> Box<dyn Query> {
+    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
+    queries.push((Occur::Must, Box::new(AllQuery)));
+    request
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            queries.push((Occur::Must, Box::new(facet_term_query)));
+        });
+    Box::new(BooleanQuery::new(queries))
+}
+
+#[cfg(test)]
+mod tests {
+    use tantivy::schema::Field;
+
+    use super::*;
+    fn dummy_term_query() -> Box<dyn Query> {
+        let field = Field::from_field_id(0);
+        let term = Term::from_field_u64(field, 0);
+        Box::new(TermQuery::new(term, IndexRecordOption::Basic))
+    }
+
+    #[test]
+    fn test_preprocessor() {
+        let text = "own test \"This is great\"";
+        let mut term_collector = TermCollector::default();
+        let _ = preprocess_raw_query(text, &mut term_collector);
+        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
+        let expect = HashSet::from(["This is great", "test"]);
+        assert_eq!(terms, expect);
+
+        let text = "The test \"is correct\" always";
+        let mut term_collector = TermCollector::default();
+        let processed = preprocess_raw_query(text, &mut term_collector);
+        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
+        let expect = HashSet::from(["test", "always", "is correct"]);
+        assert_eq!(terms, expect);
+        assert_eq!(processed.regular_query, "test \"is correct\" always");
+        assert_eq!(processed.fuzzy_query, "test always");
+    }
+
+    #[test]
+    fn test() {
+        let subqueries0: Vec<_> = vec![dummy_term_query; 12]
+            .into_iter()
+            .map(|f| (Occur::Must, f()))
+            .collect();
+        let subqueries1: Vec<_> = vec![dummy_term_query; 12]
+            .into_iter()
+            .map(|f| (Occur::Must, f()))
+            .collect();
+        let boolean0: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries0));
+        let boolean1: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries1));
+        let nested = BooleanQuery::new(vec![(Occur::Should, boolean0), (Occur::Should, boolean1)]);
+        let adapted = flat_and_adapt(Box::new(nested), true, 2, SharedTermC::new());
+        assert_eq!(adapted.len(), 24);
+        assert!(adapted.iter().all(|(occur, _)| *occur == Occur::Must));
+        assert!(adapted
+            .iter()
+            .all(|(_, query)| query.is::<FuzzyTermQuery>()));
+    }
+
+    #[test]
+    fn it_removes_stop_word_fterms() {
+        let tests = [
+            (
+                "nuclia is a database for unstructured data",
+                "nuclia database unstructured data",
+            ),
+            (
+                "nuclia is a",
+                // keeps last term even if is a stop word
+                "nuclia a",
+            ),
+            ("is a for and", "and"),
+        ];
+
+        for (query, expected_fuzzy_query) in tests {
+            let fuzzy_query = remove_stop_words(query);
+
+            assert_eq!(fuzzy_query, expected_fuzzy_query);
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/src/writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/src/writer.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,497 +1,493 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashMap;
-use std::fmt::Debug;
-use std::fs;
-use std::time::SystemTime;
-
-use nucliadb_core::metrics;
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::prost::Message;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{Resource, ResourceId};
-use nucliadb_core::tracing::{self, *};
-use regex::Regex;
-use tantivy::collector::Count;
-use tantivy::query::AllQuery;
-use tantivy::schema::*;
-use tantivy::{doc, Index, IndexSettings, IndexSortByField, IndexWriter, Order};
-
-use super::schema::ParagraphSchema;
-use crate::schema::timestamp_to_datetime_utc;
-
-lazy_static::lazy_static! {
-    static ref REGEX: Regex = Regex::new(r"\\[a-zA-Z1-9]").unwrap();
-}
-
-pub struct ParagraphWriterService {
-    pub index: Index,
-    pub schema: ParagraphSchema,
-    writer: IndexWriter,
-}
-
-impl Debug for ParagraphWriterService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("TextService")
-            .field("index", &self.index)
-            .field("schema", &self.schema)
-            .finish()
-    }
-}
-
-impl ParagraphWriter for ParagraphWriterService {}
-
-impl WriterChild for ParagraphWriterService {
-    fn stop(&mut self) -> NodeResult<()> {
-        debug!("Stopping Paragraph Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn count(&self) -> NodeResult<usize> {
-        let time = SystemTime::now();
-        let id: Option<String> = None;
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Count starting at {v} ms");
-        }
-        let reader = self.index.reader()?;
-        let searcher = reader.searcher();
-        let count = searcher.search(&AllQuery, &Count)?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Ending at: {v} ms");
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::paragraphs("count".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(count)
-    }
-    #[tracing::instrument(skip_all)]
-    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
-        let time = SystemTime::now();
-        let id = Some(&resource.shard_id);
-
-        if resource.status != ResourceStatus::Delete as i32 {
-            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-                debug!("{id:?} - Indexing paragraphs: starts at {v} ms");
-            }
-            let _ = self.index_paragraph(resource);
-            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-                debug!("{id:?} - Indexing paragraphs: ends at {v} ms");
-            }
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Processing paragraphs to delete: starts at {v} ms");
-        }
-        for paragraph_id in &resource.paragraphs_to_delete {
-            let uuid_term = Term::from_field_text(self.schema.paragraph, paragraph_id);
-            self.writer.delete_term(uuid_term);
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Processing paragraphs to delete: ends at {v} ms");
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Commit: starts at {v} ms");
-        }
-        self.writer.commit()?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Commit: ends at {v} ms");
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::paragraphs("set_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()> {
-        let time = SystemTime::now();
-        let id = Some(&resource_id.shard_id);
-
-        let uuid_field = self.schema.uuid;
-        let uuid_term = Term::from_field_text(uuid_field, &resource_id.uuid);
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Delete term: starts at {v} ms");
-        }
-        self.writer.delete_term(uuid_term);
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Delete term: ends at {v} ms");
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Commit: starts at {v} ms");
-        }
-        self.writer.commit()?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Commit: ends at {v} ms");
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::paragraphs("delete_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn garbage_collection(&mut self) -> NodeResult<()> {
-        Ok(())
-    }
-}
-
-impl ParagraphWriterService {
-    #[tracing::instrument(skip_all)]
-    pub fn start(config: &ParagraphConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            match ParagraphWriterService::new(config) {
-                Err(e) if path.exists() => {
-                    std::fs::remove_dir(path)?;
-                    Err(e)
-                }
-                Err(e) => Err(e),
-                Ok(v) => Ok(v),
-            }
-        } else {
-            Ok(ParagraphWriterService::open(config)?)
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn new(config: &ParagraphConfig) -> NodeResult<ParagraphWriterService> {
-        let paragraph_schema = ParagraphSchema::default();
-
-        fs::create_dir(&config.path)?;
-
-        let mut index_builder = Index::builder().schema(paragraph_schema.schema.clone());
-        let settings = IndexSettings {
-            sort_by_field: Some(IndexSortByField {
-                field: "created".to_string(),
-                order: Order::Desc,
-            }),
-            ..Default::default()
-        };
-
-        index_builder = index_builder.settings(settings);
-        let index = index_builder.create_in_dir(&config.path).unwrap();
-
-        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
-
-        Ok(ParagraphWriterService {
-            index,
-            writer,
-            schema: paragraph_schema,
-        })
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn open(config: &ParagraphConfig) -> NodeResult<ParagraphWriterService> {
-        let paragraph_schema = ParagraphSchema::default();
-
-        let index = Index::open_in_dir(&config.path)?;
-
-        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
-
-        Ok(ParagraphWriterService {
-            index,
-            writer,
-            schema: paragraph_schema,
-        })
-    }
-
-    fn index_paragraph(&mut self, resource: &Resource) -> tantivy::Result<()> {
-        let metadata = resource
-            .metadata
-            .as_ref()
-            .expect("Missing resource metadata");
-        let modified = metadata
-            .modified
-            .as_ref()
-            .expect("Missing resource modified date in metadata");
-        let created = metadata
-            .created
-            .as_ref()
-            .expect("Missing resource created date in metadata");
-
-        let empty_paragraph = HashMap::with_capacity(0);
-        let inspect_paragraph = |field: &str| {
-            resource
-                .paragraphs
-                .get(field)
-                .map_or_else(|| &empty_paragraph, |i| &i.paragraphs)
-        };
-        let resource_facets = resource
-            .labels
-            .iter()
-            .map(Facet::from_text)
-            .collect::<Result<Vec<_>, _>>()
-            .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
-        let mut paragraph_counter = 0;
-        for (field, text_info) in &resource.texts {
-            let text_labels = text_info
-                .labels
-                .iter()
-                .map(Facet::from_text)
-                .collect::<Result<Vec<_>, _>>()
-                .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
-            for (paragraph_id, p) in inspect_paragraph(field) {
-                paragraph_counter += 1;
-                let paragraph_term = Term::from_field_text(self.schema.paragraph, paragraph_id);
-                let chars: Vec<char> = REGEX.replace_all(&text_info.text, " ").chars().collect();
-                let start_pos = p.start as u64;
-                let end_pos = p.end as u64;
-                let index = p.index;
-                let split = &p.split;
-                let lower_bound = std::cmp::min(start_pos as usize, chars.len());
-                let upper_bound = std::cmp::min(end_pos as usize, chars.len());
-                let text: String = chars[lower_bound..upper_bound].iter().collect();
-                let facet_field = format!("/{field}");
-                let paragraph_labels = p
-                    .labels
-                    .iter()
-                    .map(Facet::from_text)
-                    .collect::<Result<Vec<_>, _>>()
-                    .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
-
-                let mut doc = doc!(
-                    self.schema.uuid => resource.resource.as_ref().expect("Missing resource details").uuid.as_str(),
-                    self.schema.modified => timestamp_to_datetime_utc(modified),
-                    self.schema.created => timestamp_to_datetime_utc(created),
-                    self.schema.status => resource.status as u64,
-                    self.schema.repeated_in_field => p.repeated_in_field as u64,
-                );
-
-                if let Some(ref metadata) = p.metadata {
-                    doc.add_bytes(self.schema.metadata, metadata.encode_to_vec());
-                }
-
-                resource_facets
-                    .iter()
-                    .chain(text_labels.iter())
-                    .chain(paragraph_labels.iter())
-                    .cloned()
-                    .for_each(|facet| doc.add_facet(self.schema.facets, facet));
-                doc.add_facet(self.schema.field, Facet::from(&facet_field));
-                doc.add_text(self.schema.paragraph, paragraph_id.clone());
-                doc.add_text(self.schema.text, &text);
-                doc.add_u64(self.schema.start_pos, start_pos);
-                doc.add_u64(self.schema.end_pos, end_pos);
-                doc.add_u64(self.schema.index, index);
-                doc.add_text(self.schema.split, split);
-                self.writer.delete_term(paragraph_term);
-                self.writer.add_document(doc).unwrap();
-                if paragraph_counter % 500 == 0 {
-                    self.writer.commit().unwrap();
-                }
-            }
-        }
-        Ok(())
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashMap;
-    use std::time::SystemTime;
-
-    use nucliadb_core::protos::prost_types::Timestamp;
-    use nucliadb_core::protos::resource::ResourceStatus;
-    use nucliadb_core::protos::{
-        IndexMetadata, IndexParagraph, IndexParagraphs, Resource, ResourceId, TextInformation,
-    };
-    use nucliadb_core::NodeResult;
-    use tantivy::collector::{Count, TopDocs};
-    use tantivy::query::{AllQuery, TermQuery};
-    use tempfile::TempDir;
-
-    use super::*;
-
-    fn create_resource(shard_id: String) -> Resource {
-        const UUID: &str = "f56c58ac-b4f9-4d61-a077-ffccaadd0001";
-        let resource_id = ResourceId {
-            shard_id: shard_id.to_string(),
-            uuid: UUID.to_string(),
-        };
-
-        let now = SystemTime::now()
-            .duration_since(SystemTime::UNIX_EPOCH)
-            .unwrap();
-        let timestamp = Timestamp {
-            seconds: now.as_secs() as i64,
-            nanos: 0,
-        };
-
-        let metadata = IndexMetadata {
-            created: Some(timestamp.clone()),
-            modified: Some(timestamp),
-        };
-
-        const DOC1_TI: &str = "This the first document";
-        const DOC1_P1: &str = "This is the text of the second paragraph.";
-        const DOC1_P2: &str = "This should be enough to test the tantivy.";
-        const DOC1_P3: &str = "But I wanted to make it three anyway.";
-
-        let ti_title = TextInformation {
-            text: DOC1_TI.to_string(),
-            labels: vec!["/l/mylabel".to_string(), "/e/myentity".to_string()],
-        };
-
-        let ti_body = TextInformation {
-            text: DOC1_P1.to_string() + DOC1_P2 + DOC1_P3,
-            labels: vec!["/f/body".to_string(), "/l/mylabel2".to_string()],
-        };
-
-        let mut texts = HashMap::new();
-        texts.insert("title".to_string(), ti_title);
-        texts.insert("body".to_string(), ti_body);
-
-        let p1 = IndexParagraph {
-            start: 0,
-            end: DOC1_P1.len() as i32,
-            sentences: HashMap::new(),
-            field: "body".to_string(),
-            labels: vec!["/nsfw".to_string()],
-            index: 0,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let p1_uuid = format!("{}/{}/{}-{}", UUID, "body", 0, DOC1_P1.len());
-
-        let p2 = IndexParagraph {
-            start: DOC1_P1.len() as i32,
-            end: (DOC1_P1.len() + DOC1_P2.len()) as i32,
-            sentences: HashMap::new(),
-            field: "body".to_string(),
-            labels: vec![
-                "/tantivy".to_string(),
-                "/test".to_string(),
-                "/label1".to_string(),
-            ],
-            index: 1,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let p2_uuid = format!(
-            "{}/{}/{}-{}",
-            UUID,
-            "body",
-            DOC1_P1.len(),
-            DOC1_P1.len() + DOC1_P2.len()
-        );
-
-        let p3 = IndexParagraph {
-            start: (DOC1_P1.len() + DOC1_P2.len()) as i32,
-            end: (DOC1_P1.len() + DOC1_P2.len() + DOC1_P3.len()) as i32,
-            sentences: HashMap::new(),
-            field: "body".to_string(),
-            labels: vec!["/three".to_string(), "/label2".to_string()],
-            index: 2,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let p3_uuid = format!(
-            "{}/{}/{}-{}",
-            UUID,
-            "body",
-            DOC1_P1.len() + DOC1_P2.len(),
-            DOC1_P1.len() + DOC1_P2.len() + DOC1_P3.len()
-        );
-
-        let body_paragraphs = IndexParagraphs {
-            paragraphs: [(p1_uuid, p1), (p2_uuid, p2), (p3_uuid, p3)]
-                .into_iter()
-                .collect(),
-        };
-
-        let p4 = IndexParagraph {
-            start: 0,
-            end: DOC1_TI.len() as i32,
-            sentences: HashMap::new(),
-            field: "title".to_string(),
-            labels: vec!["/cool".to_string()],
-            index: 3,
-            split: "".to_string(),
-            repeated_in_field: false,
-            metadata: None,
-        };
-        let p4_uuid = format!("{}/{}/{}-{}", UUID, "body", 0, DOC1_TI.len());
-
-        let title_paragraphs = IndexParagraphs {
-            paragraphs: [(p4_uuid, p4)].into_iter().collect(),
-        };
-
-        let paragraphs = [
-            ("body".to_string(), body_paragraphs),
-            ("title".to_string(), title_paragraphs),
-        ]
-        .into_iter()
-        .collect();
-
-        Resource {
-            resource: Some(resource_id),
-            metadata: Some(metadata),
-            texts,
-            status: ResourceStatus::Processed as i32,
-            labels: vec!["/l/mylabel_resource".to_string()],
-            paragraphs,
-            paragraphs_to_delete: vec![],
-            sentences_to_delete: vec![],
-            relations_to_delete: vec![],
-            relations: vec![],
-            vectors: HashMap::default(),
-            vectors_to_delete: HashMap::default(),
-            shard_id,
-        }
-    }
-
-    #[test]
-    fn test_new_writer() -> NodeResult<()> {
-        let dir = TempDir::new().unwrap();
-        let psc = ParagraphConfig {
-            path: dir.path().join("paragraphs"),
-        };
-
-        let mut paragraph_writer_service = ParagraphWriterService::start(&psc).unwrap();
-        let resource1 = create_resource("shard1".to_string());
-        let _ = paragraph_writer_service.set_resource(&resource1);
-        let _ = paragraph_writer_service.set_resource(&resource1);
-
-        let reader = paragraph_writer_service.index.reader()?;
-        let searcher = reader.searcher();
-
-        let query = TermQuery::new(
-            Term::from_field_text(paragraph_writer_service.schema.text, "document"),
-            IndexRecordOption::Basic,
-        );
-
-        let (_top_docs, count) = searcher.search(&query, &(TopDocs::with_limit(2), Count))?;
-        assert_eq!(count, 1);
-
-        let (_top_docs, count) = searcher.search(&AllQuery, &(TopDocs::with_limit(10), Count))?;
-        assert_eq!(count, 4);
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::collections::HashMap;
+use std::fmt::Debug;
+use std::fs;
+use std::time::SystemTime;
+
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::prost::Message;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{Resource, ResourceId};
+use nucliadb_core::tracing::{self, *};
+use regex::Regex;
+use tantivy::collector::Count;
+use tantivy::query::AllQuery;
+use tantivy::schema::*;
+use tantivy::{doc, Index, IndexSettings, IndexSortByField, IndexWriter, Order};
+
+use super::schema::ParagraphSchema;
+use crate::schema::timestamp_to_datetime_utc;
+
+lazy_static::lazy_static! {
+    static ref REGEX: Regex = Regex::new(r"\\[a-zA-Z1-9]").unwrap();
+}
+
+pub struct ParagraphWriterService {
+    pub index: Index,
+    pub schema: ParagraphSchema,
+    writer: IndexWriter,
+}
+
+impl Debug for ParagraphWriterService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("TextService")
+            .field("index", &self.index)
+            .field("schema", &self.schema)
+            .finish()
+    }
+}
+
+impl ParagraphWriter for ParagraphWriterService {}
+
+impl WriterChild for ParagraphWriterService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let time = SystemTime::now();
+        let id: Option<String> = None;
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Count starting at {v} ms");
+        }
+        let reader = self.index.reader()?;
+        let searcher = reader.searcher();
+        let count = searcher.search(&AllQuery, &Count)?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at: {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::paragraphs("count".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(count)
+    }
+    #[tracing::instrument(skip_all)]
+    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
+        let time = SystemTime::now();
+        let id = Some(&resource.shard_id);
+
+        if resource.status != ResourceStatus::Delete as i32 {
+            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+                debug!("{id:?} - Indexing paragraphs: starts at {v} ms");
+            }
+            let _ = self.index_paragraph(resource);
+            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+                debug!("{id:?} - Indexing paragraphs: ends at {v} ms");
+            }
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Processing paragraphs to delete: starts at {v} ms");
+        }
+        for paragraph_id in &resource.paragraphs_to_delete {
+            let uuid_term = Term::from_field_text(self.schema.paragraph, paragraph_id);
+            self.writer.delete_term(uuid_term);
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Processing paragraphs to delete: ends at {v} ms");
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: starts at {v} ms");
+        }
+        self.writer.commit()?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: ends at {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::paragraphs("set_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()> {
+        let time = SystemTime::now();
+        let id = Some(&resource_id.shard_id);
+
+        let uuid_field = self.schema.uuid;
+        let uuid_term = Term::from_field_text(uuid_field, &resource_id.uuid);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete term: starts at {v} ms");
+        }
+        self.writer.delete_term(uuid_term);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete term: ends at {v} ms");
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: starts at {v} ms");
+        }
+        self.writer.commit()?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: ends at {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::paragraphs("delete_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn garbage_collection(&mut self) -> NodeResult<()> {
+        Ok(())
+    }
+}
+
+impl ParagraphWriterService {
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &ParagraphConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            match ParagraphWriterService::new(config) {
+                Err(e) if path.exists() => {
+                    std::fs::remove_dir(path)?;
+                    Err(e)
+                }
+                Err(e) => Err(e),
+                Ok(v) => Ok(v),
+            }
+        } else {
+            Ok(ParagraphWriterService::open(config)?)
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn new(config: &ParagraphConfig) -> NodeResult<ParagraphWriterService> {
+        let paragraph_schema = ParagraphSchema::default();
+
+        fs::create_dir(&config.path)?;
+
+        let mut index_builder = Index::builder().schema(paragraph_schema.schema.clone());
+        let settings = IndexSettings {
+            sort_by_field: Some(IndexSortByField {
+                field: "created".to_string(),
+                order: Order::Desc,
+            }),
+            ..Default::default()
+        };
+
+        index_builder = index_builder.settings(settings);
+        let index = index_builder.create_in_dir(&config.path).unwrap();
+
+        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
+
+        Ok(ParagraphWriterService {
+            index,
+            writer,
+            schema: paragraph_schema,
+        })
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn open(config: &ParagraphConfig) -> NodeResult<ParagraphWriterService> {
+        let paragraph_schema = ParagraphSchema::default();
+
+        let index = Index::open_in_dir(&config.path)?;
+
+        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
+
+        Ok(ParagraphWriterService {
+            index,
+            writer,
+            schema: paragraph_schema,
+        })
+    }
+
+    fn index_paragraph(&mut self, resource: &Resource) -> tantivy::Result<()> {
+        let metadata = resource
+            .metadata
+            .as_ref()
+            .expect("Missing resource metadata");
+        let modified = metadata
+            .modified
+            .as_ref()
+            .expect("Missing resource modified date in metadata");
+        let created = metadata
+            .created
+            .as_ref()
+            .expect("Missing resource created date in metadata");
+
+        let empty_paragraph = HashMap::with_capacity(0);
+        let inspect_paragraph = |field: &str| {
+            resource
+                .paragraphs
+                .get(field)
+                .map_or_else(|| &empty_paragraph, |i| &i.paragraphs)
+        };
+        let resource_facets = resource
+            .labels
+            .iter()
+            .map(Facet::from_text)
+            .collect::<Result<Vec<_>, _>>()
+            .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
+        let mut paragraph_counter = 0;
+        for (field, text_info) in &resource.texts {
+            let text_labels = text_info
+                .labels
+                .iter()
+                .map(Facet::from_text)
+                .collect::<Result<Vec<_>, _>>()
+                .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
+            for (paragraph_id, p) in inspect_paragraph(field) {
+                paragraph_counter += 1;
+                let paragraph_term = Term::from_field_text(self.schema.paragraph, paragraph_id);
+                let chars: Vec<char> = REGEX.replace_all(&text_info.text, " ").chars().collect();
+                let start_pos = p.start as u64;
+                let end_pos = p.end as u64;
+                let index = p.index;
+                let split = &p.split;
+                let lower_bound = std::cmp::min(start_pos as usize, chars.len());
+                let upper_bound = std::cmp::min(end_pos as usize, chars.len());
+                let text: String = chars[lower_bound..upper_bound].iter().collect();
+                let facet_field = format!("/{field}");
+                let paragraph_labels = p
+                    .labels
+                    .iter()
+                    .map(Facet::from_text)
+                    .collect::<Result<Vec<_>, _>>()
+                    .map_err(|e| tantivy::TantivyError::InvalidArgument(e.to_string()))?;
+
+                let mut doc = doc!(
+                    self.schema.uuid => resource.resource.as_ref().expect("Missing resource details").uuid.as_str(),
+                    self.schema.modified => timestamp_to_datetime_utc(modified),
+                    self.schema.created => timestamp_to_datetime_utc(created),
+                    self.schema.status => resource.status as u64,
+                    self.schema.repeated_in_field => p.repeated_in_field as u64,
+                );
+
+                if let Some(ref metadata) = p.metadata {
+                    doc.add_bytes(self.schema.metadata, metadata.encode_to_vec());
+                }
+
+                resource_facets
+                    .iter()
+                    .chain(text_labels.iter())
+                    .chain(paragraph_labels.iter())
+                    .cloned()
+                    .for_each(|facet| doc.add_facet(self.schema.facets, facet));
+                doc.add_facet(self.schema.field, Facet::from(&facet_field));
+                doc.add_text(self.schema.paragraph, paragraph_id.clone());
+                doc.add_text(self.schema.text, &text);
+                doc.add_u64(self.schema.start_pos, start_pos);
+                doc.add_u64(self.schema.end_pos, end_pos);
+                doc.add_u64(self.schema.index, index);
+                doc.add_text(self.schema.split, split);
+                self.writer.delete_term(paragraph_term);
+                self.writer.add_document(doc).unwrap();
+                if paragraph_counter % 500 == 0 {
+                    self.writer.commit().unwrap();
+                }
+            }
+        }
+        Ok(())
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashMap;
+    use std::time::SystemTime;
+
+    use nucliadb_core::protos::prost_types::Timestamp;
+    use nucliadb_core::protos::resource::ResourceStatus;
+    use nucliadb_core::protos::{
+        IndexMetadata, IndexParagraph, IndexParagraphs, Resource, ResourceId, TextInformation,
+    };
+    use nucliadb_core::NodeResult;
+    use tantivy::collector::{Count, TopDocs};
+    use tantivy::query::{AllQuery, TermQuery};
+    use tempfile::TempDir;
+
+    use super::*;
+
+    fn create_resource(shard_id: String) -> Resource {
+        const UUID: &str = "f56c58ac-b4f9-4d61-a077-ffccaadd0001";
+        let resource_id = ResourceId {
+            shard_id: shard_id.to_string(),
+            uuid: UUID.to_string(),
+        };
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64,
+            nanos: 0,
+        };
+
+        let metadata = IndexMetadata {
+            created: Some(timestamp.clone()),
+            modified: Some(timestamp),
+        };
+
+        const DOC1_TI: &str = "This the first document";
+        const DOC1_P1: &str = "This is the text of the second paragraph.";
+        const DOC1_P2: &str = "This should be enough to test the tantivy.";
+        const DOC1_P3: &str = "But I wanted to make it three anyway.";
+
+        let ti_title = TextInformation {
+            text: DOC1_TI.to_string(),
+            labels: vec!["/l/mylabel".to_string(), "/e/myentity".to_string()],
+        };
+
+        let ti_body = TextInformation {
+            text: DOC1_P1.to_string() + DOC1_P2 + DOC1_P3,
+            labels: vec!["/f/body".to_string(), "/l/mylabel2".to_string()],
+        };
+
+        let mut texts = HashMap::new();
+        texts.insert("title".to_string(), ti_title);
+        texts.insert("body".to_string(), ti_body);
+
+        let p1 = IndexParagraph {
+            start: 0,
+            end: DOC1_P1.len() as i32,
+            sentences: HashMap::new(),
+            field: "body".to_string(),
+            labels: vec!["/nsfw".to_string()],
+            index: 0,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let p1_uuid = format!("{}/{}/{}-{}", UUID, "body", 0, DOC1_P1.len());
+
+        let p2 = IndexParagraph {
+            start: DOC1_P1.len() as i32,
+            end: (DOC1_P1.len() + DOC1_P2.len()) as i32,
+            sentences: HashMap::new(),
+            field: "body".to_string(),
+            labels: vec![
+                "/tantivy".to_string(),
+                "/test".to_string(),
+                "/label1".to_string(),
+            ],
+            index: 1,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let p2_uuid = format!(
+            "{}/{}/{}-{}",
+            UUID,
+            "body",
+            DOC1_P1.len(),
+            DOC1_P1.len() + DOC1_P2.len()
+        );
+
+        let p3 = IndexParagraph {
+            start: (DOC1_P1.len() + DOC1_P2.len()) as i32,
+            end: (DOC1_P1.len() + DOC1_P2.len() + DOC1_P3.len()) as i32,
+            sentences: HashMap::new(),
+            field: "body".to_string(),
+            labels: vec!["/three".to_string(), "/label2".to_string()],
+            index: 2,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let p3_uuid = format!(
+            "{}/{}/{}-{}",
+            UUID,
+            "body",
+            DOC1_P1.len() + DOC1_P2.len(),
+            DOC1_P1.len() + DOC1_P2.len() + DOC1_P3.len()
+        );
+
+        let body_paragraphs = IndexParagraphs {
+            paragraphs: [(p1_uuid, p1), (p2_uuid, p2), (p3_uuid, p3)]
+                .into_iter()
+                .collect(),
+        };
+
+        let p4 = IndexParagraph {
+            start: 0,
+            end: DOC1_TI.len() as i32,
+            sentences: HashMap::new(),
+            field: "title".to_string(),
+            labels: vec!["/cool".to_string()],
+            index: 3,
+            split: "".to_string(),
+            repeated_in_field: false,
+            metadata: None,
+        };
+        let p4_uuid = format!("{}/{}/{}-{}", UUID, "body", 0, DOC1_TI.len());
+
+        let title_paragraphs = IndexParagraphs {
+            paragraphs: [(p4_uuid, p4)].into_iter().collect(),
+        };
+
+        let paragraphs = [
+            ("body".to_string(), body_paragraphs),
+            ("title".to_string(), title_paragraphs),
+        ]
+        .into_iter()
+        .collect();
+
+        Resource {
+            resource: Some(resource_id),
+            metadata: Some(metadata),
+            texts,
+            status: ResourceStatus::Processed as i32,
+            labels: vec!["/l/mylabel_resource".to_string()],
+            paragraphs,
+            paragraphs_to_delete: vec![],
+            sentences_to_delete: vec![],
+            relations_to_delete: vec![],
+            relations: vec![],
+            vectors: HashMap::default(),
+            vectors_to_delete: HashMap::default(),
+            shard_id,
+        }
+    }
+
+    #[test]
+    fn test_new_writer() -> NodeResult<()> {
+        let dir = TempDir::new().unwrap();
+        let psc = ParagraphConfig {
+            path: dir.path().join("paragraphs"),
+        };
+
+        let mut paragraph_writer_service = ParagraphWriterService::start(&psc).unwrap();
+        let resource1 = create_resource("shard1".to_string());
+        let _ = paragraph_writer_service.set_resource(&resource1);
+        let _ = paragraph_writer_service.set_resource(&resource1);
+
+        let reader = paragraph_writer_service.index.reader()?;
+        let searcher = reader.searcher();
+
+        let query = TermQuery::new(
+            Term::from_field_text(paragraph_writer_service.schema.text, "document"),
+            IndexRecordOption::Basic,
+        );
+
+        let (_top_docs, count) = searcher.search(&query, &(TopDocs::with_limit(2), Count))?;
+        assert_eq!(count, 1);
+
+        let (_top_docs, count) = searcher.search(&AllQuery, &(TopDocs::with_limit(10), Count))?;
+        assert_eq!(count, 4);
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/ca.json` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/ca.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 0% similar despite different names*

```diff
@@ -87,8 +87,8 @@
 00000560: 222c 2275 6e65 7322 2c22 756e 7322 2c22  ","unes","uns","
 00000570: 7573 222c 2276 6122 2c22 7661 6967 222c  us","va","vaig",
 00000580: 2276 616d 222c 2276 616e 222c 2276 6173  "vam","van","vas
 00000590: 222c 2276 6575 222c 2276 6f73 616c 7472  ","veu","vosaltr
 000005a0: 6573 222c 2276 6f73 7472 6122 2c22 766f  es","vostra","vo
 000005b0: 7374 7265 222c 2276 6f73 7472 6573 222c  stre","vostres",
 000005c0: 22c3 a972 656d 222c 22c3 a972 6575 222c  "..rem","..reu",
-000005d0: 22c3 a973 225d 0a                        "..s"].
+000005d0: 22c3 a973 225d 0d0a                      "..s"]..
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/en.json` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/en.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/es.json` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/es.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 0% similar despite different names*

```diff
@@ -276,8 +276,8 @@
 00001130: 7a22 2c22 c3a9 6c22 2c22 c3a9 7361 222c  z","..l","..sa",
 00001140: 22c3 a973 6173 222c 22c3 a973 6522 2c22  "..sas","..se","
 00001150: c3a9 736f 7322 2c22 c3a9 7374 6122 2c22  ..sos","..sta","
 00001160: c3a9 7374 6173 222c 22c3 a973 7465 222c  ..stas","..ste",
 00001170: 22c3 a973 746f 7322 2c22 c3ba 6c74 696d  "..stos","..ltim
 00001180: 6122 2c22 c3ba 6c74 696d 6173 222c 22c3  a","..ltimas",".
 00001190: ba6c 7469 6d6f 222c 22c3 ba6c 7469 6d6f  .ltimo","..ltimo
-000011a0: 7322 5d0a                                s"].
+000011a0: 7322 5d0d 0a                             s"]..
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_paragraphs/stop_words/fr.json` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_paragraphs/stop_words/fr.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/Cargo.toml` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/Cargo.toml`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/bfs_engine.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/bfs_engine.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,263 +1,263 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::{HashMap, HashSet, LinkedList};
-
-use super::errors::*;
-use crate::graph_db::*;
-
-// BfsGuide allows the user to modify how the search will be performed.
-// By default a BfsGuide does not interfere in the search.
-pub trait BfsGuide {
-    fn free_jump(&self, _cnx: GCnx) -> bool {
-        false
-    }
-    fn node_allowed(&self, _node: Entity) -> bool {
-        true
-    }
-    fn edge_allowed(&self, _edge: Entity) -> bool {
-        true
-    }
-}
-
-// Refers to a node that has been reached by the BFS
-// but has not been expanded yet.
-#[derive(Clone, Copy, Debug)]
-struct BfsNode {
-    // GId of the node.
-    point: Entity,
-    // Depth at which the node was found.
-    depth: usize,
-}
-
-#[derive(derive_builder::Builder)]
-#[builder(name = "BfsEngineBuilder", pattern = "owned")]
-pub struct BfsEngine<'a, Guide>
-where Guide: BfsGuide
-{
-    #[builder(setter(skip))]
-    #[builder(default = "LinkedList::new()")]
-    work_stack: LinkedList<BfsNode>,
-
-    #[builder(setter(skip))]
-    #[builder(default = "HashSet::new()")]
-    visited: HashSet<Entity>,
-
-    #[builder(setter(skip))]
-    #[builder(default = "HashSet::new()")]
-    subgraph: HashSet<GCnx>,
-
-    entry_points: Vec<Entity>,
-    max_depth: usize,
-    guide: Guide,
-    txn: &'a RoToken<'a>,
-    graph: &'a GraphDB,
-}
-
-impl<'a, Guide> BfsEngineBuilder<'a, Guide>
-where Guide: BfsGuide
-{
-    pub fn new() -> BfsEngineBuilder<'a, Guide> {
-        BfsEngineBuilder::create_empty()
-    }
-}
-impl<'a, Guide> BfsEngine<'a, Guide>
-where Guide: BfsGuide
-{
-    pub fn search(mut self) -> RResult<impl Iterator<Item = GCnx>> {
-        self.entry_points
-            .iter()
-            .copied()
-            .map(|point| (BfsNode { point, depth: 0 }, self.visited.insert(point)))
-            .filter(|(_, v)| *v)
-            .for_each(|(e, _)| self.work_stack.push_back(e));
-        while let Some(node) = self.work_stack.pop_front() {
-            self.expand(node)?;
-        }
-        Ok(self.subgraph.into_iter())
-    }
-    fn expand(&mut self, node: BfsNode) -> RResult<()> {
-        // same_level nodes are reached by a free_edge
-        // which means that they belong to the level being explored now.
-        let mut same_level = HashMap::new();
-        // next_level nodes are reached by a edge that increases the level.
-        let mut next_level = HashMap::new();
-        self.graph
-            .get_outedges(self.txn, node.point)?
-            .chain(self.graph.get_inedges(self.txn, node.point)?)
-            .flat_map(|a| a.ok().into_iter())
-            .filter(|edge| node.depth < self.max_depth || self.guide.free_jump(*edge))
-            .filter(|edge| self.guide.edge_allowed(edge.edge()))
-            .filter(|edge| self.guide.node_allowed(edge.to()))
-            .for_each(|edge| {
-                let is_free_jump = self.guide.free_jump(edge);
-                let can_use_free_jump = same_level.contains_key(&node.point);
-                if !is_free_jump && !can_use_free_jump {
-                    let node = BfsNode {
-                        point: edge.to(),
-                        // Exploring a further node without free jump increases
-                        // by one the depth of the BFS, i.e., the distance to
-                        // the entry point
-                        depth: node.depth + 1,
-                    };
-                    next_level.insert(node.point, node);
-                } else if is_free_jump {
-                    let node = BfsNode {
-                        point: edge.to(),
-                        depth: node.depth,
-                    };
-                    next_level.remove(&node.point);
-                    same_level.insert(node.point, node);
-                }
-                self.subgraph.insert(edge);
-            });
-        same_level.into_values().for_each(|node| {
-            if !self.visited.contains(&node.point) {
-                self.visited.insert(node.point);
-                // In order to maintain all the advantages of BFS
-                // even when free edges are present we need to maintain the following invariant:
-                // For every i,j if i < j then the nodes from level i are visited before the nodes
-                // from level j.
-                // The invariant only holds if the nodes reached by a
-                // free edge are pushed to the front of the stack. We are avoiding
-                // the aditional complexity of Dijkstra's algorithm.
-                self.work_stack.push_front(node);
-            }
-        });
-        next_level.into_values().for_each(|node| {
-            if !self.visited.contains(&node.point) {
-                self.visited.insert(node.point);
-                self.work_stack.push_back(node);
-            }
-        });
-        Ok(())
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use std::path::Path;
-
-    use super::*;
-    use crate::graph_test_utils::*;
-    fn graph(dir: &Path) -> (Vec<Entity>, GraphDB) {
-        let graphdb = GraphDB::new(dir, SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let ids = UNodes
-            .take(4)
-            .map(|node| graphdb.add_node(&mut txn, &node).unwrap())
-            .collect::<Vec<_>>();
-        UEdges
-            .take(ids.len() - 1)
-            .enumerate()
-            .for_each(|(i, edge)| {
-                graphdb
-                    .connect(&mut txn, ids[i], &edge, ids[i + 1], None)
-                    .unwrap();
-            });
-        let backedge = UEdges.next().unwrap();
-        graphdb
-            .connect(&mut txn, ids[3], &backedge, ids[0], None)
-            .unwrap();
-        txn.commit().unwrap();
-        (ids, graphdb)
-    }
-
-    #[test]
-    fn full_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(usize::MAX)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn full_reverse_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[3]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(usize::MAX)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn limit_depth_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(1)
-            .build()
-            .unwrap();
-        let expected = vec![nodes[0], nodes[1], nodes[3]];
-        let result = bfs.search().unwrap();
-        let mut result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        result.push(nodes[0]);
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn always_jump() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(FreeJumps)
-            .max_depth(0)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::{HashMap, HashSet, LinkedList};
+
+use super::errors::*;
+use crate::graph_db::*;
+
+// BfsGuide allows the user to modify how the search will be performed.
+// By default a BfsGuide does not interfere in the search.
+pub trait BfsGuide {
+    fn free_jump(&self, _cnx: GCnx) -> bool {
+        false
+    }
+    fn node_allowed(&self, _node: Entity) -> bool {
+        true
+    }
+    fn edge_allowed(&self, _edge: Entity) -> bool {
+        true
+    }
+}
+
+// Refers to a node that has been reached by the BFS
+// but has not been expanded yet.
+#[derive(Clone, Copy, Debug)]
+struct BfsNode {
+    // GId of the node.
+    point: Entity,
+    // Depth at which the node was found.
+    depth: usize,
+}
+
+#[derive(derive_builder::Builder)]
+#[builder(name = "BfsEngineBuilder", pattern = "owned")]
+pub struct BfsEngine<'a, Guide>
+where Guide: BfsGuide
+{
+    #[builder(setter(skip))]
+    #[builder(default = "LinkedList::new()")]
+    work_stack: LinkedList<BfsNode>,
+
+    #[builder(setter(skip))]
+    #[builder(default = "HashSet::new()")]
+    visited: HashSet<Entity>,
+
+    #[builder(setter(skip))]
+    #[builder(default = "HashSet::new()")]
+    subgraph: HashSet<GCnx>,
+
+    entry_points: Vec<Entity>,
+    max_depth: usize,
+    guide: Guide,
+    txn: &'a RoToken<'a>,
+    graph: &'a GraphDB,
+}
+
+impl<'a, Guide> BfsEngineBuilder<'a, Guide>
+where Guide: BfsGuide
+{
+    pub fn new() -> BfsEngineBuilder<'a, Guide> {
+        BfsEngineBuilder::create_empty()
+    }
+}
+impl<'a, Guide> BfsEngine<'a, Guide>
+where Guide: BfsGuide
+{
+    pub fn search(mut self) -> RResult<impl Iterator<Item = GCnx>> {
+        self.entry_points
+            .iter()
+            .copied()
+            .map(|point| (BfsNode { point, depth: 0 }, self.visited.insert(point)))
+            .filter(|(_, v)| *v)
+            .for_each(|(e, _)| self.work_stack.push_back(e));
+        while let Some(node) = self.work_stack.pop_front() {
+            self.expand(node)?;
+        }
+        Ok(self.subgraph.into_iter())
+    }
+    fn expand(&mut self, node: BfsNode) -> RResult<()> {
+        // same_level nodes are reached by a free_edge
+        // which means that they belong to the level being explored now.
+        let mut same_level = HashMap::new();
+        // next_level nodes are reached by a edge that increases the level.
+        let mut next_level = HashMap::new();
+        self.graph
+            .get_outedges(self.txn, node.point)?
+            .chain(self.graph.get_inedges(self.txn, node.point)?)
+            .flat_map(|a| a.ok().into_iter())
+            .filter(|edge| node.depth < self.max_depth || self.guide.free_jump(*edge))
+            .filter(|edge| self.guide.edge_allowed(edge.edge()))
+            .filter(|edge| self.guide.node_allowed(edge.to()))
+            .for_each(|edge| {
+                let is_free_jump = self.guide.free_jump(edge);
+                let can_use_free_jump = same_level.contains_key(&node.point);
+                if !is_free_jump && !can_use_free_jump {
+                    let node = BfsNode {
+                        point: edge.to(),
+                        // Exploring a further node without free jump increases
+                        // by one the depth of the BFS, i.e., the distance to
+                        // the entry point
+                        depth: node.depth + 1,
+                    };
+                    next_level.insert(node.point, node);
+                } else if is_free_jump {
+                    let node = BfsNode {
+                        point: edge.to(),
+                        depth: node.depth,
+                    };
+                    next_level.remove(&node.point);
+                    same_level.insert(node.point, node);
+                }
+                self.subgraph.insert(edge);
+            });
+        same_level.into_values().for_each(|node| {
+            if !self.visited.contains(&node.point) {
+                self.visited.insert(node.point);
+                // In order to maintain all the advantages of BFS
+                // even when free edges are present we need to maintain the following invariant:
+                // For every i,j if i < j then the nodes from level i are visited before the nodes
+                // from level j.
+                // The invariant only holds if the nodes reached by a
+                // free edge are pushed to the front of the stack. We are avoiding
+                // the aditional complexity of Dijkstra's algorithm.
+                self.work_stack.push_front(node);
+            }
+        });
+        next_level.into_values().for_each(|node| {
+            if !self.visited.contains(&node.point) {
+                self.visited.insert(node.point);
+                self.work_stack.push_back(node);
+            }
+        });
+        Ok(())
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use std::path::Path;
+
+    use super::*;
+    use crate::graph_test_utils::*;
+    fn graph(dir: &Path) -> (Vec<Entity>, GraphDB) {
+        let graphdb = GraphDB::new(dir, SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let ids = UNodes
+            .take(4)
+            .map(|node| graphdb.add_node(&mut txn, &node).unwrap())
+            .collect::<Vec<_>>();
+        UEdges
+            .take(ids.len() - 1)
+            .enumerate()
+            .for_each(|(i, edge)| {
+                graphdb
+                    .connect(&mut txn, ids[i], &edge, ids[i + 1], None)
+                    .unwrap();
+            });
+        let backedge = UEdges.next().unwrap();
+        graphdb
+            .connect(&mut txn, ids[3], &backedge, ids[0], None)
+            .unwrap();
+        txn.commit().unwrap();
+        (ids, graphdb)
+    }
+
+    #[test]
+    fn full_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(usize::MAX)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn full_reverse_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[3]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(usize::MAX)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn limit_depth_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(1)
+            .build()
+            .unwrap();
+        let expected = vec![nodes[0], nodes[1], nodes[3]];
+        let result = bfs.search().unwrap();
+        let mut result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        result.push(nodes[0]);
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn always_jump() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(FreeJumps)
+            .max_depth(0)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/errors.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/errors.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use nucliadb_core::fs_state::FsError;
-use tantivy::TantivyError;
-
-#[derive(Debug, thiserror::Error)]
-pub enum RelationsErr {
-    #[error("Graph error: {0}")]
-    GraphDBError(String),
-    #[error("Bincode error: {0}")]
-    BincodeError(#[from] bincode::Error),
-    #[error("IO error: {0}")]
-    IOError(#[from] std::io::Error),
-    #[error("Disk error: {0}")]
-    DiskError(#[from] FsError),
-    #[error("Tantivy error: {0}")]
-    TantivyError(#[from] TantivyError),
-    #[error("Database is full")]
-    NeedsResize,
-    #[error("UBehaviour")]
-    UBehaviour,
-}
-
-impl From<heed::Error> for RelationsErr {
-    fn from(err: heed::Error) -> Self {
-        use heed::{Error, MdbError};
-        match err {
-            Error::Mdb(MdbError::MapFull) => RelationsErr::NeedsResize,
-            err => RelationsErr::GraphDBError(format!("{err:?}")),
-        }
-    }
-}
-pub type RResult<O> = Result<O, RelationsErr>;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use nucliadb_core::fs_state::FsError;
+use tantivy::TantivyError;
+
+#[derive(Debug, thiserror::Error)]
+pub enum RelationsErr {
+    #[error("Graph error: {0}")]
+    GraphDBError(String),
+    #[error("Bincode error: {0}")]
+    BincodeError(#[from] bincode::Error),
+    #[error("IO error: {0}")]
+    IOError(#[from] std::io::Error),
+    #[error("Disk error: {0}")]
+    DiskError(#[from] FsError),
+    #[error("Tantivy error: {0}")]
+    TantivyError(#[from] TantivyError),
+    #[error("Database is full")]
+    NeedsResize,
+    #[error("UBehaviour")]
+    UBehaviour,
+}
+
+impl From<heed::Error> for RelationsErr {
+    fn from(err: heed::Error) -> Self {
+        use heed::{Error, MdbError};
+        match err {
+            Error::Mdb(MdbError::MapFull) => RelationsErr::NeedsResize,
+            err => RelationsErr::GraphDBError(format!("{err:?}")),
+        }
+    }
+}
+pub type RResult<O> = Result<O, RelationsErr>;
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/graph_db.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/graph_db.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,567 +1,567 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashSet;
-use std::path::Path;
-
-use heed::flags::Flags;
-use heed::types::{SerdeBincode, Str, Unit};
-use heed::{Database, Env, EnvOpenOptions, RoTxn, RwTxn};
-use serde::de::DeserializeOwned;
-use serde::{Deserialize, Serialize};
-use uuid::Uuid;
-
-use super::errors::{RResult, RelationsErr};
-use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
-
-pub type RwToken<'a> = RwTxn<'a, 'a>;
-pub type RoToken<'a> = RoTxn<'a>;
-pub trait GIdProperty: Serialize + DeserializeOwned + 'static {}
-impl<T: Serialize + DeserializeOwned + 'static> GIdProperty for T {}
-
-#[derive(Serialize, Deserialize, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Clone, Copy)]
-pub struct Entity(Uuid);
-
-mod gid_impl {
-    use uuid::Uuid;
-
-    use super::Entity;
-    impl Default for Entity {
-        fn default() -> Self {
-            Entity(Uuid::new_v4())
-        }
-    }
-    impl Entity {
-        pub fn new() -> Self {
-            Self::default()
-        }
-    }
-    impl std::fmt::Display for Entity {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            self.0.fmt(f)
-        }
-    }
-}
-
-fn encode_connexion(
-    from: Option<Entity>,
-    to: Option<Entity>,
-    with: Option<Entity>,
-) -> RResult<String> {
-    match (from, to, with) {
-        (Some(from), Some(to), Some(with)) => Ok(format!("({from},{to},{with})")),
-        (Some(from), Some(to), None) => Ok(format!("({from},{to},")),
-        (Some(from), None, None) => Ok(format!("({from},")),
-        _ => Err(RelationsErr::UBehaviour),
-    }
-}
-fn decode_connexion(elements: &str) -> (Entity, Entity, Entity) {
-    let uuids: Vec<_> = elements
-        .strip_prefix('(')
-        .unwrap()
-        .strip_suffix(')')
-        .unwrap()
-        .split(',')
-        .map(|v| Uuid::parse_str(v).unwrap())
-        .collect();
-    (Entity(uuids[0]), Entity(uuids[1]), Entity(uuids[2]))
-}
-
-#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
-pub struct GCnx(Entity, Entity, Entity);
-impl GCnx {
-    fn decode(elems: &str) -> GCnx {
-        let (from, to, with) = decode_connexion(elems);
-        GCnx(from, to, with)
-    }
-    fn decode_inversed(elems: &str) -> GCnx {
-        let (to, from, with) = decode_connexion(elems);
-        GCnx(from, to, with)
-    }
-    fn encode(&self) -> RResult<String> {
-        encode_connexion(Some(self.0), Some(self.1), Some(self.2))
-    }
-    pub fn new(from: Entity, to: Entity, with: Entity) -> GCnx {
-        GCnx(from, to, with)
-    }
-    pub fn from(&self) -> Entity {
-        self.0
-    }
-    pub fn to(&self) -> Entity {
-        self.1
-    }
-    pub fn edge(&self) -> Entity {
-        self.2
-    }
-}
-
-pub struct GraphDB {
-    env: Env,
-    // Given a node enconding returns its Entity
-    nodes: Database<Str, SerdeBincode<Entity>>,
-    // Encodings for the out edges (source, edge, target)
-    outedges: Database<Str, Unit>,
-    // Encodings for the in edges (target, edge, source)
-    inedges: Database<Str, Unit>,
-    // Given an entity, it returns its edge component
-    edge_component: Database<SerdeBincode<Entity>, SerdeBincode<IoEdge>>,
-    // Given an entity, it returns its edge metadata. It is not stored
-    // inside the IoEdge type to avoid the deserialization cost during BFS.
-    edge_metadata: Database<SerdeBincode<Entity>, SerdeBincode<IoEdgeMetadata>>,
-    // Given an entity, it returns its node component
-    node_component: Database<SerdeBincode<Entity>, SerdeBincode<IoNode>>,
-}
-
-macro_rules! database_name {
-    (const $l:ident) => {
-        const $l: &str = stringify!($l);
-    };
-}
-impl GraphDB {
-    const MAX_DBS: u32 = 6;
-    database_name!(const NODES);
-    database_name!(const OUTEDGES);
-    database_name!(const INEDGES);
-    database_name!(const NODE_COMPONENT);
-    database_name!(const EDGE_COMPONENT);
-    database_name!(const EDGE_METADATA);
-    pub fn new(path: &Path, db_size: usize) -> RResult<Self> {
-        let mut env_builder = EnvOpenOptions::new();
-        env_builder.max_dbs(Self::MAX_DBS);
-        env_builder.map_size(db_size);
-        unsafe {
-            env_builder.flag(Flags::MdbNoLock);
-        }
-        let env = env_builder.open(path)?;
-        let nodes = env.create_database(Some(Self::NODES))?;
-        let outedges = env.create_database(Some(Self::OUTEDGES))?;
-        let inedges = env.create_database(Some(Self::INEDGES))?;
-        let node_component = env.create_database(Some(Self::NODE_COMPONENT))?;
-        let edge_component = env.create_database(Some(Self::EDGE_COMPONENT))?;
-        let edge_metadata = env.create_database(Some(Self::EDGE_METADATA))?;
-        Ok(GraphDB {
-            env,
-            nodes,
-            outedges,
-            inedges,
-            node_component,
-            edge_component,
-            edge_metadata,
-        })
-    }
-    pub fn rw_txn(&self) -> RResult<RwToken<'_>> {
-        Ok(self.env.write_txn()?)
-    }
-    pub fn ro_txn(&self) -> RResult<RoTxn<'_>> {
-        Ok(self.env.read_txn()?)
-    }
-    pub fn add_node(&self, txn: &mut RwTxn, node: &IoNode) -> RResult<Entity> {
-        match self.nodes.get(txn, node.hash())? {
-            Some(v) => Ok(v),
-            None => {
-                let id = Entity::new();
-                self.nodes.put(txn, node.hash(), &id)?;
-                self.node_component.put(txn, &id, node)?;
-                Ok(id)
-            }
-        }
-    }
-    pub fn delete_node(&self, txn: &mut RwTxn, node_id: Entity) -> RResult<HashSet<Entity>> {
-        let node = self.get_node(txn, node_id)?;
-        self.nodes.delete(txn, node.hash())?;
-        self.node_component.delete(txn, &node_id)?;
-        let prefix = encode_connexion(Some(node_id), None, None)?;
-        // Deleting out connexions
-        let mut affected = HashSet::new();
-        let mut in_edges = vec![];
-        let mut out_iter = self.outedges.prefix_iter_mut(txn, &prefix)?;
-        while let Some((out_edge, _)) = out_iter.next().transpose()? {
-            let in_edge = GCnx::decode_inversed(out_edge);
-            affected.insert(in_edge.from());
-            in_edges.push(in_edge);
-            out_iter.del_current()?;
-        }
-        std::mem::drop(out_iter);
-        for edge in in_edges.into_iter().map(|e| e.encode()) {
-            let edge = edge?;
-            self.inedges.delete(txn, &edge)?;
-        }
-        // Deleting in connexions
-        let mut out_edges = vec![];
-        let mut in_iter = self.inedges.prefix_iter_mut(txn, &prefix)?;
-        while let Some((out_edge, _)) = in_iter.next().transpose()? {
-            let out_edge = GCnx::decode_inversed(out_edge);
-            affected.insert(out_edge.from());
-            out_edges.push(out_edge);
-            in_iter.del_current()?;
-        }
-        std::mem::drop(in_iter);
-        for edge in out_edges.into_iter().map(|e| e.encode()) {
-            let edge = edge?;
-            self.outedges.delete(txn, &edge)?;
-        }
-        Ok(affected)
-    }
-    pub fn connect(
-        &self,
-        txn: &mut RwTxn,
-        from: Entity,
-        edge: &IoEdge,
-        to: Entity,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        let mut exits = false;
-        let mut iter = self.connected_by(txn, from, to)?;
-        loop {
-            match iter.next() {
-                _ if exits => break,
-                Some(cnx) => {
-                    let cnx = cnx?;
-                    let cnx = self.get_edge(txn, cnx.edge())?;
-                    exits = *edge == cnx;
-                }
-                None => {
-                    std::mem::drop(iter);
-                    let with = Entity::new();
-                    let out_edge = GCnx::new(from, to, with).encode()?;
-                    let in_edge = GCnx::new(to, from, with).encode()?;
-                    self.edge_component.put(txn, &with, edge)?;
-                    self.outedges.put(txn, &out_edge, &())?;
-                    self.inedges.put(txn, &in_edge, &())?;
-                    if let Some(metadata) = edge_metadata {
-                        self.edge_metadata.put(txn, &with, metadata)?;
-                    }
-                    break;
-                }
-            }
-        }
-        Ok(!exits)
-    }
-    pub fn iter_node_ids<'a>(
-        &self,
-        txn: &'a RoTxn,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        Ok(self
-            .node_component
-            .iter(txn)
-            .map_err(RelationsErr::from)?
-            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
-    }
-    pub fn iter_edge_ids<'a>(
-        &self,
-        txn: &'a RoTxn,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        Ok(self
-            .edge_component
-            .iter(txn)
-            .map_err(RelationsErr::from)?
-            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
-    }
-    pub fn get_node(&self, txn: &RoTxn, x: Entity) -> RResult<IoNode> {
-        let node = self.node_component.get(txn, &x)?;
-        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
-    }
-    pub fn get_nodeid(&self, txn: &RoTxn, x: &str) -> RResult<Option<Entity>> {
-        Ok(self.nodes.get(txn, x)?)
-    }
-    pub fn get_edge(&self, txn: &RoTxn, x: Entity) -> RResult<IoEdge> {
-        let node = self.edge_component.get(txn, &x)?;
-        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
-    }
-    pub fn get_edge_metadata(&self, txn: &RoTxn, x: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        Ok(self.edge_metadata.get(txn, &x)?)
-    }
-    pub fn connected_by<'a>(
-        &self,
-        txn: &'a RoTxn,
-        x: Entity,
-        y: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let right_arrow = encode_connexion(Some(x), Some(y), None)?;
-        let out = self.outedges.prefix_iter(txn, &right_arrow)?;
-        let iter = out.map(|r| r.map_err(RelationsErr::from).map(|(v, _)| GCnx::decode(v)));
-        Ok(iter)
-    }
-    pub fn get_outedges<'a>(
-        &self,
-        txn: &'a RoTxn,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let prefix = encode_connexion(Some(from), None, None)?;
-        let out = self.outedges.prefix_iter(txn, &prefix)?;
-        let iter = out
-            .map(|r| r.map_err(RelationsErr::from))
-            .map(|r| r.map(|(v, _)| GCnx::decode(v)));
-        Ok(iter)
-    }
-    pub fn get_inedges<'a>(
-        &self,
-        txn: &'a RoTxn,
-        to: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let prefix = encode_connexion(Some(to), None, None)?;
-        let xin = self.inedges.prefix_iter(txn, &prefix)?;
-        let inv_iter = xin
-            .map(|r| r.map_err(RelationsErr::from))
-            .map(|r| r.map(|(v, _)| GCnx::decode_inversed(v)));
-        Ok(inv_iter)
-    }
-
-    pub fn no_nodes(&self, txn: &RoTxn) -> RResult<u64> {
-        Ok(self.nodes.len(txn)?)
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashSet;
-
-    use super::*;
-    use crate::graph_test_utils::*;
-
-    #[test]
-    fn creation_test() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let node1 = fresh_node();
-        let node2 = fresh_node();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let id1 = graphdb.add_node(&mut txn, &node1).unwrap();
-        let id2 = graphdb.add_node(&mut txn, &node2).unwrap();
-        let idr = graphdb.add_node(&mut txn, &node1).unwrap();
-        txn.commit().unwrap();
-        assert_eq!(id1, idr);
-        assert_ne!(id1, id2);
-
-        let txn = graphdb.ro_txn().unwrap();
-        let gnode1 = graphdb.get_node(&txn, id1).unwrap();
-        let gnode2 = graphdb.get_node(&txn, id2).unwrap();
-        assert_eq!(gnode1, node1);
-        assert_eq!(gnode2, node2);
-    }
-
-    #[test]
-    fn deletion_test() {
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        let e4 = fresh_edge();
-        let e5 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
-        // Deleting N4
-        // N1 should have 0 in-edges
-        // N3 should have 0 out-edges
-        // N2 should have 0 out-edges
-        let io_n4 = graphdb.get_node(&txn, n4).unwrap();
-        let expected_affected = HashSet::from([n1, n2, n3]);
-        let got_affected = graphdb.delete_node(&mut txn, n4).unwrap();
-        assert_eq!(expected_affected, got_affected);
-        let n1_out = graphdb.get_outedges(&txn, n1).unwrap().count();
-        let n1_in = graphdb.get_inedges(&txn, n1).unwrap().count();
-        let n2_in = graphdb.get_inedges(&txn, n2).unwrap().count();
-        let n2_out = graphdb.get_outedges(&txn, n2).unwrap().count();
-        let n3_in = graphdb.get_inedges(&txn, n3).unwrap().count();
-        let n3_out = graphdb.get_outedges(&txn, n3).unwrap().count();
-        let n4_out = graphdb.get_outedges(&txn, n4).unwrap().count();
-        let n4_in = graphdb.get_inedges(&txn, n4).unwrap().count();
-        assert_eq!(n4_out, 0);
-        assert_eq!(n4_in, 0);
-        assert_eq!(n1_out, 2);
-        assert_eq!(n1_in, 0);
-        assert_eq!(n2_in, 1);
-        assert_eq!(n2_out, 0);
-        assert_eq!(n3_in, 1);
-        assert_eq!(n3_out, 0);
-        let n4_id = graphdb.get_nodeid(&txn, io_n4.hash()).unwrap();
-        assert_eq!(n4_id, None);
-    }
-    #[test]
-    fn connexions_test() {
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        let e4 = fresh_edge();
-        let e5 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
-
-        // out edges test
-        let expected_outn1 = HashSet::from([(n2, e1.clone()), (n3, e2.clone())]);
-        let expected_outn2 = HashSet::from([(n4, e3.clone())]);
-        let expected_outn3 = HashSet::from([(n4, e4.clone())]);
-        let expected_outn4 = HashSet::from([(n1, e5.clone())]);
-        let got_outn1 = graphdb
-            .get_outedges(&txn, n1)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn2 = graphdb
-            .get_outedges(&txn, n2)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn3 = graphdb
-            .get_outedges(&txn, n3)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn4 = graphdb
-            .get_outedges(&txn, n4)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        assert_eq!(got_outn1.len(), expected_outn1.len());
-        assert!(got_outn1.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n1 && expected_outn1.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn2.len(), expected_outn2.len());
-        assert!(got_outn2.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n2 && expected_outn2.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn3.len(), expected_outn3.len());
-        assert!(got_outn3.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n3 && expected_outn3.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn4.len(), expected_outn4.len());
-        assert!(got_outn4.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n4 && expected_outn4.contains(&(cnx.to(), edge))
-        }));
-        // in edges test
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let expected_inn1 = HashSet::from([(n4, e5.clone())]);
-        let expected_inn2 = HashSet::from([(n1, e1.clone())]);
-        let expected_inn3 = HashSet::from([(n1, e2.clone())]);
-        let expected_inn4 = HashSet::from([(n2, e3.clone()), (n3, e4.clone())]);
-        let got_inn1 = graphdb
-            .get_inedges(&txn, n1)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn2 = graphdb
-            .get_inedges(&txn, n2)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn3 = graphdb
-            .get_inedges(&txn, n3)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn4 = graphdb
-            .get_inedges(&txn, n4)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        assert_eq!(got_inn1.len(), expected_inn1.len());
-        assert!(got_inn1.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n1 && expected_inn1.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn2.len(), expected_inn2.len());
-        assert!(got_inn2.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n2 && expected_inn2.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn3.len(), expected_inn3.len());
-        assert!(got_inn3.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n3 && expected_inn3.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn4.len(), expected_inn4.len());
-        assert!(got_inn4.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n4 && expected_inn4.contains(&(cnx.from(), edge))
-        }));
-    }
-
-    #[test]
-    fn connexion_similarity_test() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
-        let expect = HashSet::from([e1, e3]);
-        let n1_n2 = graphdb
-            .connected_by(&txn, n1, n2)
-            .unwrap()
-            .map(|c| c.unwrap())
-            .map(|c| c.edge())
-            .map(|c| graphdb.get_edge(&txn, c).unwrap())
-            .collect::<Vec<_>>();
-        assert_eq!(n1_n2.len(), expect.len());
-        assert!(n1_n2.iter().all(|v| expect.contains(v)));
-        let mut n2_n1 = graphdb
-            .connected_by(&txn, n2, n1)
-            .unwrap()
-            .map(|c| c.unwrap())
-            .map(|c| c.edge())
-            .map(|c| graphdb.get_edge(&txn, c).unwrap());
-        assert_eq!(Some(e2), n2_n1.next());
-        assert_eq!(None, n2_n1.next());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::collections::HashSet;
+use std::path::Path;
+
+use heed::flags::Flags;
+use heed::types::{SerdeBincode, Str, Unit};
+use heed::{Database, Env, EnvOpenOptions, RoTxn, RwTxn};
+use serde::de::DeserializeOwned;
+use serde::{Deserialize, Serialize};
+use uuid::Uuid;
+
+use super::errors::{RResult, RelationsErr};
+use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
+
+pub type RwToken<'a> = RwTxn<'a, 'a>;
+pub type RoToken<'a> = RoTxn<'a>;
+pub trait GIdProperty: Serialize + DeserializeOwned + 'static {}
+impl<T: Serialize + DeserializeOwned + 'static> GIdProperty for T {}
+
+#[derive(Serialize, Deserialize, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Clone, Copy)]
+pub struct Entity(Uuid);
+
+mod gid_impl {
+    use uuid::Uuid;
+
+    use super::Entity;
+    impl Default for Entity {
+        fn default() -> Self {
+            Entity(Uuid::new_v4())
+        }
+    }
+    impl Entity {
+        pub fn new() -> Self {
+            Self::default()
+        }
+    }
+    impl std::fmt::Display for Entity {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            self.0.fmt(f)
+        }
+    }
+}
+
+fn encode_connexion(
+    from: Option<Entity>,
+    to: Option<Entity>,
+    with: Option<Entity>,
+) -> RResult<String> {
+    match (from, to, with) {
+        (Some(from), Some(to), Some(with)) => Ok(format!("({from},{to},{with})")),
+        (Some(from), Some(to), None) => Ok(format!("({from},{to},")),
+        (Some(from), None, None) => Ok(format!("({from},")),
+        _ => Err(RelationsErr::UBehaviour),
+    }
+}
+fn decode_connexion(elements: &str) -> (Entity, Entity, Entity) {
+    let uuids: Vec<_> = elements
+        .strip_prefix('(')
+        .unwrap()
+        .strip_suffix(')')
+        .unwrap()
+        .split(',')
+        .map(|v| Uuid::parse_str(v).unwrap())
+        .collect();
+    (Entity(uuids[0]), Entity(uuids[1]), Entity(uuids[2]))
+}
+
+#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
+pub struct GCnx(Entity, Entity, Entity);
+impl GCnx {
+    fn decode(elems: &str) -> GCnx {
+        let (from, to, with) = decode_connexion(elems);
+        GCnx(from, to, with)
+    }
+    fn decode_inversed(elems: &str) -> GCnx {
+        let (to, from, with) = decode_connexion(elems);
+        GCnx(from, to, with)
+    }
+    fn encode(&self) -> RResult<String> {
+        encode_connexion(Some(self.0), Some(self.1), Some(self.2))
+    }
+    pub fn new(from: Entity, to: Entity, with: Entity) -> GCnx {
+        GCnx(from, to, with)
+    }
+    pub fn from(&self) -> Entity {
+        self.0
+    }
+    pub fn to(&self) -> Entity {
+        self.1
+    }
+    pub fn edge(&self) -> Entity {
+        self.2
+    }
+}
+
+pub struct GraphDB {
+    env: Env,
+    // Given a node enconding returns its Entity
+    nodes: Database<Str, SerdeBincode<Entity>>,
+    // Encodings for the out edges (source, edge, target)
+    outedges: Database<Str, Unit>,
+    // Encodings for the in edges (target, edge, source)
+    inedges: Database<Str, Unit>,
+    // Given an entity, it returns its edge component
+    edge_component: Database<SerdeBincode<Entity>, SerdeBincode<IoEdge>>,
+    // Given an entity, it returns its edge metadata. It is not stored
+    // inside the IoEdge type to avoid the deserialization cost during BFS.
+    edge_metadata: Database<SerdeBincode<Entity>, SerdeBincode<IoEdgeMetadata>>,
+    // Given an entity, it returns its node component
+    node_component: Database<SerdeBincode<Entity>, SerdeBincode<IoNode>>,
+}
+
+macro_rules! database_name {
+    (const $l:ident) => {
+        const $l: &str = stringify!($l);
+    };
+}
+impl GraphDB {
+    const MAX_DBS: u32 = 6;
+    database_name!(const NODES);
+    database_name!(const OUTEDGES);
+    database_name!(const INEDGES);
+    database_name!(const NODE_COMPONENT);
+    database_name!(const EDGE_COMPONENT);
+    database_name!(const EDGE_METADATA);
+    pub fn new(path: &Path, db_size: usize) -> RResult<Self> {
+        let mut env_builder = EnvOpenOptions::new();
+        env_builder.max_dbs(Self::MAX_DBS);
+        env_builder.map_size(db_size);
+        unsafe {
+            env_builder.flag(Flags::MdbNoLock);
+        }
+        let env = env_builder.open(path)?;
+        let nodes = env.create_database(Some(Self::NODES))?;
+        let outedges = env.create_database(Some(Self::OUTEDGES))?;
+        let inedges = env.create_database(Some(Self::INEDGES))?;
+        let node_component = env.create_database(Some(Self::NODE_COMPONENT))?;
+        let edge_component = env.create_database(Some(Self::EDGE_COMPONENT))?;
+        let edge_metadata = env.create_database(Some(Self::EDGE_METADATA))?;
+        Ok(GraphDB {
+            env,
+            nodes,
+            outedges,
+            inedges,
+            node_component,
+            edge_component,
+            edge_metadata,
+        })
+    }
+    pub fn rw_txn(&self) -> RResult<RwToken<'_>> {
+        Ok(self.env.write_txn()?)
+    }
+    pub fn ro_txn(&self) -> RResult<RoTxn<'_>> {
+        Ok(self.env.read_txn()?)
+    }
+    pub fn add_node(&self, txn: &mut RwTxn, node: &IoNode) -> RResult<Entity> {
+        match self.nodes.get(txn, node.hash())? {
+            Some(v) => Ok(v),
+            None => {
+                let id = Entity::new();
+                self.nodes.put(txn, node.hash(), &id)?;
+                self.node_component.put(txn, &id, node)?;
+                Ok(id)
+            }
+        }
+    }
+    pub fn delete_node(&self, txn: &mut RwTxn, node_id: Entity) -> RResult<HashSet<Entity>> {
+        let node = self.get_node(txn, node_id)?;
+        self.nodes.delete(txn, node.hash())?;
+        self.node_component.delete(txn, &node_id)?;
+        let prefix = encode_connexion(Some(node_id), None, None)?;
+        // Deleting out connexions
+        let mut affected = HashSet::new();
+        let mut in_edges = vec![];
+        let mut out_iter = self.outedges.prefix_iter_mut(txn, &prefix)?;
+        while let Some((out_edge, _)) = out_iter.next().transpose()? {
+            let in_edge = GCnx::decode_inversed(out_edge);
+            affected.insert(in_edge.from());
+            in_edges.push(in_edge);
+            out_iter.del_current()?;
+        }
+        std::mem::drop(out_iter);
+        for edge in in_edges.into_iter().map(|e| e.encode()) {
+            let edge = edge?;
+            self.inedges.delete(txn, &edge)?;
+        }
+        // Deleting in connexions
+        let mut out_edges = vec![];
+        let mut in_iter = self.inedges.prefix_iter_mut(txn, &prefix)?;
+        while let Some((out_edge, _)) = in_iter.next().transpose()? {
+            let out_edge = GCnx::decode_inversed(out_edge);
+            affected.insert(out_edge.from());
+            out_edges.push(out_edge);
+            in_iter.del_current()?;
+        }
+        std::mem::drop(in_iter);
+        for edge in out_edges.into_iter().map(|e| e.encode()) {
+            let edge = edge?;
+            self.outedges.delete(txn, &edge)?;
+        }
+        Ok(affected)
+    }
+    pub fn connect(
+        &self,
+        txn: &mut RwTxn,
+        from: Entity,
+        edge: &IoEdge,
+        to: Entity,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        let mut exits = false;
+        let mut iter = self.connected_by(txn, from, to)?;
+        loop {
+            match iter.next() {
+                _ if exits => break,
+                Some(cnx) => {
+                    let cnx = cnx?;
+                    let cnx = self.get_edge(txn, cnx.edge())?;
+                    exits = *edge == cnx;
+                }
+                None => {
+                    std::mem::drop(iter);
+                    let with = Entity::new();
+                    let out_edge = GCnx::new(from, to, with).encode()?;
+                    let in_edge = GCnx::new(to, from, with).encode()?;
+                    self.edge_component.put(txn, &with, edge)?;
+                    self.outedges.put(txn, &out_edge, &())?;
+                    self.inedges.put(txn, &in_edge, &())?;
+                    if let Some(metadata) = edge_metadata {
+                        self.edge_metadata.put(txn, &with, metadata)?;
+                    }
+                    break;
+                }
+            }
+        }
+        Ok(!exits)
+    }
+    pub fn iter_node_ids<'a>(
+        &self,
+        txn: &'a RoTxn,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        Ok(self
+            .node_component
+            .iter(txn)
+            .map_err(RelationsErr::from)?
+            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
+    }
+    pub fn iter_edge_ids<'a>(
+        &self,
+        txn: &'a RoTxn,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        Ok(self
+            .edge_component
+            .iter(txn)
+            .map_err(RelationsErr::from)?
+            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
+    }
+    pub fn get_node(&self, txn: &RoTxn, x: Entity) -> RResult<IoNode> {
+        let node = self.node_component.get(txn, &x)?;
+        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
+    }
+    pub fn get_nodeid(&self, txn: &RoTxn, x: &str) -> RResult<Option<Entity>> {
+        Ok(self.nodes.get(txn, x)?)
+    }
+    pub fn get_edge(&self, txn: &RoTxn, x: Entity) -> RResult<IoEdge> {
+        let node = self.edge_component.get(txn, &x)?;
+        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
+    }
+    pub fn get_edge_metadata(&self, txn: &RoTxn, x: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        Ok(self.edge_metadata.get(txn, &x)?)
+    }
+    pub fn connected_by<'a>(
+        &self,
+        txn: &'a RoTxn,
+        x: Entity,
+        y: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let right_arrow = encode_connexion(Some(x), Some(y), None)?;
+        let out = self.outedges.prefix_iter(txn, &right_arrow)?;
+        let iter = out.map(|r| r.map_err(RelationsErr::from).map(|(v, _)| GCnx::decode(v)));
+        Ok(iter)
+    }
+    pub fn get_outedges<'a>(
+        &self,
+        txn: &'a RoTxn,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let prefix = encode_connexion(Some(from), None, None)?;
+        let out = self.outedges.prefix_iter(txn, &prefix)?;
+        let iter = out
+            .map(|r| r.map_err(RelationsErr::from))
+            .map(|r| r.map(|(v, _)| GCnx::decode(v)));
+        Ok(iter)
+    }
+    pub fn get_inedges<'a>(
+        &self,
+        txn: &'a RoTxn,
+        to: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let prefix = encode_connexion(Some(to), None, None)?;
+        let xin = self.inedges.prefix_iter(txn, &prefix)?;
+        let inv_iter = xin
+            .map(|r| r.map_err(RelationsErr::from))
+            .map(|r| r.map(|(v, _)| GCnx::decode_inversed(v)));
+        Ok(inv_iter)
+    }
+
+    pub fn no_nodes(&self, txn: &RoTxn) -> RResult<u64> {
+        Ok(self.nodes.len(txn)?)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashSet;
+
+    use super::*;
+    use crate::graph_test_utils::*;
+
+    #[test]
+    fn creation_test() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let node1 = fresh_node();
+        let node2 = fresh_node();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let id1 = graphdb.add_node(&mut txn, &node1).unwrap();
+        let id2 = graphdb.add_node(&mut txn, &node2).unwrap();
+        let idr = graphdb.add_node(&mut txn, &node1).unwrap();
+        txn.commit().unwrap();
+        assert_eq!(id1, idr);
+        assert_ne!(id1, id2);
+
+        let txn = graphdb.ro_txn().unwrap();
+        let gnode1 = graphdb.get_node(&txn, id1).unwrap();
+        let gnode2 = graphdb.get_node(&txn, id2).unwrap();
+        assert_eq!(gnode1, node1);
+        assert_eq!(gnode2, node2);
+    }
+
+    #[test]
+    fn deletion_test() {
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        let e4 = fresh_edge();
+        let e5 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
+        // Deleting N4
+        // N1 should have 0 in-edges
+        // N3 should have 0 out-edges
+        // N2 should have 0 out-edges
+        let io_n4 = graphdb.get_node(&txn, n4).unwrap();
+        let expected_affected = HashSet::from([n1, n2, n3]);
+        let got_affected = graphdb.delete_node(&mut txn, n4).unwrap();
+        assert_eq!(expected_affected, got_affected);
+        let n1_out = graphdb.get_outedges(&txn, n1).unwrap().count();
+        let n1_in = graphdb.get_inedges(&txn, n1).unwrap().count();
+        let n2_in = graphdb.get_inedges(&txn, n2).unwrap().count();
+        let n2_out = graphdb.get_outedges(&txn, n2).unwrap().count();
+        let n3_in = graphdb.get_inedges(&txn, n3).unwrap().count();
+        let n3_out = graphdb.get_outedges(&txn, n3).unwrap().count();
+        let n4_out = graphdb.get_outedges(&txn, n4).unwrap().count();
+        let n4_in = graphdb.get_inedges(&txn, n4).unwrap().count();
+        assert_eq!(n4_out, 0);
+        assert_eq!(n4_in, 0);
+        assert_eq!(n1_out, 2);
+        assert_eq!(n1_in, 0);
+        assert_eq!(n2_in, 1);
+        assert_eq!(n2_out, 0);
+        assert_eq!(n3_in, 1);
+        assert_eq!(n3_out, 0);
+        let n4_id = graphdb.get_nodeid(&txn, io_n4.hash()).unwrap();
+        assert_eq!(n4_id, None);
+    }
+    #[test]
+    fn connexions_test() {
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        let e4 = fresh_edge();
+        let e5 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
+
+        // out edges test
+        let expected_outn1 = HashSet::from([(n2, e1.clone()), (n3, e2.clone())]);
+        let expected_outn2 = HashSet::from([(n4, e3.clone())]);
+        let expected_outn3 = HashSet::from([(n4, e4.clone())]);
+        let expected_outn4 = HashSet::from([(n1, e5.clone())]);
+        let got_outn1 = graphdb
+            .get_outedges(&txn, n1)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn2 = graphdb
+            .get_outedges(&txn, n2)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn3 = graphdb
+            .get_outedges(&txn, n3)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn4 = graphdb
+            .get_outedges(&txn, n4)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        assert_eq!(got_outn1.len(), expected_outn1.len());
+        assert!(got_outn1.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n1 && expected_outn1.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn2.len(), expected_outn2.len());
+        assert!(got_outn2.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n2 && expected_outn2.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn3.len(), expected_outn3.len());
+        assert!(got_outn3.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n3 && expected_outn3.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn4.len(), expected_outn4.len());
+        assert!(got_outn4.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n4 && expected_outn4.contains(&(cnx.to(), edge))
+        }));
+        // in edges test
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let expected_inn1 = HashSet::from([(n4, e5.clone())]);
+        let expected_inn2 = HashSet::from([(n1, e1.clone())]);
+        let expected_inn3 = HashSet::from([(n1, e2.clone())]);
+        let expected_inn4 = HashSet::from([(n2, e3.clone()), (n3, e4.clone())]);
+        let got_inn1 = graphdb
+            .get_inedges(&txn, n1)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn2 = graphdb
+            .get_inedges(&txn, n2)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn3 = graphdb
+            .get_inedges(&txn, n3)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn4 = graphdb
+            .get_inedges(&txn, n4)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        assert_eq!(got_inn1.len(), expected_inn1.len());
+        assert!(got_inn1.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n1 && expected_inn1.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn2.len(), expected_inn2.len());
+        assert!(got_inn2.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n2 && expected_inn2.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn3.len(), expected_inn3.len());
+        assert!(got_inn3.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n3 && expected_inn3.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn4.len(), expected_inn4.len());
+        assert!(got_inn4.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n4 && expected_inn4.contains(&(cnx.from(), edge))
+        }));
+    }
+
+    #[test]
+    fn connexion_similarity_test() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
+        let expect = HashSet::from([e1, e3]);
+        let n1_n2 = graphdb
+            .connected_by(&txn, n1, n2)
+            .unwrap()
+            .map(|c| c.unwrap())
+            .map(|c| c.edge())
+            .map(|c| graphdb.get_edge(&txn, c).unwrap())
+            .collect::<Vec<_>>();
+        assert_eq!(n1_n2.len(), expect.len());
+        assert!(n1_n2.iter().all(|v| expect.contains(v)));
+        let mut n2_n1 = graphdb
+            .connected_by(&txn, n2, n1)
+            .unwrap()
+            .map(|c| c.unwrap())
+            .map(|c| c.edge())
+            .map(|c| graphdb.get_edge(&txn, c).unwrap());
+        assert_eq!(Some(e2), n2_n1.next());
+        assert_eq!(None, n2_n1.next());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/index.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/index.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,307 +1,307 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashSet;
-use std::path::Path;
-
-use super::bfs_engine::{BfsEngineBuilder, BfsGuide};
-use super::errors::*;
-pub use super::graph_db::{Entity, GCnx};
-use super::graph_db::{GraphDB, RoToken, RwToken};
-use super::node_dictionary::{DReader, DWriter, NodeDictionary};
-pub use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
-
-pub struct RMode(DReader);
-pub struct WMode(DWriter);
-
-pub struct Index {
-    graphdb: GraphDB,
-    dictionary: NodeDictionary,
-}
-impl Index {
-    const DICTIONARY_PATH: &str = "NodeDictionary";
-    const GRAPH_PATH: &str = "GraphDB";
-    const GRAPH_SIZE: usize = 1048576 * 100000;
-    fn connect(
-        &self,
-        graph_txn: &mut RwToken,
-        dict_writer: &DWriter,
-        from: &IoNode,
-        to: &IoNode,
-        edge: &IoEdge,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        self.dictionary.add_node(dict_writer, from)?;
-        self.dictionary.add_node(dict_writer, to)?;
-        let from = self.graphdb.add_node(graph_txn, from)?;
-        let to = self.graphdb.add_node(graph_txn, to)?;
-        self.graphdb
-            .connect(graph_txn, from, edge, to, edge_metadata)
-    }
-    fn graph_search<G: BfsGuide>(
-        &self,
-        txn: &RoToken,
-        guide: G,
-        max_depth: usize,
-        entry_points: Vec<Entity>,
-    ) -> RResult<impl Iterator<Item = GCnx>> {
-        BfsEngineBuilder::new()
-            .graph(&self.graphdb)
-            .txn(txn)
-            .max_depth(max_depth)
-            .guide(guide)
-            .entry_points(entry_points)
-            .build()
-            .unwrap()
-            .search()
-    }
-    fn delete_node(
-        &self,
-        graph_txn: &mut RwToken,
-        dict_writer: &DWriter,
-        node_id: Entity,
-    ) -> RResult<HashSet<Entity>> {
-        let value = self.graphdb.get_node(graph_txn, node_id)?;
-        self.dictionary.delete_node(dict_writer, &value);
-        self.graphdb.delete_node(graph_txn, node_id)
-    }
-    fn no_nodes(&self, txn: &RoToken) -> RResult<u64> {
-        self.graphdb.no_nodes(txn)
-    }
-    fn iter_node_ids<'a>(
-        &self,
-        txn: &'a RoToken,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.graphdb.iter_node_ids(txn)
-    }
-    fn iter_edge_ids<'a>(
-        &self,
-        txn: &'a RoToken,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.graphdb.iter_edge_ids(txn)
-    }
-    pub fn get_outedges<'a>(
-        &self,
-        txn: &'a RoToken,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.graphdb.get_outedges(txn, from)
-    }
-    fn get_edge(&self, txn: &RoToken, id: Entity) -> RResult<IoEdge> {
-        self.graphdb.get_edge(txn, id)
-    }
-    fn get_edge_metadata(&self, txn: &RoToken, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.graphdb.get_edge_metadata(txn, id)
-    }
-    fn get_node(&self, txn: &RoToken, id: Entity) -> RResult<IoNode> {
-        self.graphdb.get_node(txn, id)
-    }
-    fn get_nodeid(&self, txn: &RoToken, x: &str) -> RResult<Option<Entity>> {
-        self.graphdb.get_nodeid(txn, x)
-    }
-    fn prefix_search(&self, dict_reader: &DReader, prefix: &str) -> RResult<Vec<String>> {
-        self.dictionary.search(dict_reader, prefix)
-    }
-    fn get_inedges<'a>(
-        &self,
-        txn: &'a RoToken,
-        to: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.graphdb.get_inedges(txn, to)
-    }
-    pub fn new_writer(location: &Path) -> RResult<(Index, WMode)> {
-        let dictionary_address = location.join(Self::DICTIONARY_PATH);
-        let graph_address = location.join(Self::GRAPH_PATH);
-        if !graph_address.exists() {
-            std::fs::create_dir(&graph_address)?;
-        }
-        if !dictionary_address.exists() {
-            std::fs::create_dir(&dictionary_address)?;
-        }
-        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
-        let (dictionary, mode) = NodeDictionary::new_writer(&dictionary_address)?;
-        let index = Index {
-            graphdb,
-            dictionary,
-        };
-        Ok((index, WMode(mode)))
-    }
-    pub fn new_reader(location: &Path) -> RResult<(Index, RMode)> {
-        let dictionary_address = location.join(Self::DICTIONARY_PATH);
-        let graph_address = location.join(Self::GRAPH_PATH);
-        if !graph_address.exists() {
-            std::fs::create_dir(&graph_address)?;
-        }
-        if !dictionary_address.exists() {
-            std::fs::create_dir(&dictionary_address)?;
-        }
-        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
-        let (dictionary, mode) = NodeDictionary::new_reader(&dictionary_address)?;
-        let index = Index {
-            graphdb,
-            dictionary,
-        };
-        Ok((index, RMode(mode)))
-    }
-    pub fn start_reading(&self) -> RResult<GraphReader> {
-        Ok(GraphReader {
-            graph_txn: self.graphdb.ro_txn()?,
-            index: self,
-        })
-    }
-    pub fn start_writing(&self) -> RResult<GraphWriter> {
-        Ok(GraphWriter {
-            graph_txn: self.graphdb.rw_txn()?,
-            index: self,
-        })
-    }
-}
-
-pub struct GraphReader<'a> {
-    graph_txn: RoToken<'a>,
-    index: &'a Index,
-}
-impl<'a> GraphReader<'a> {
-    pub fn reload(&self, RMode(reader): &RMode) -> RResult<()> {
-        Ok(reader.reload()?)
-    }
-    pub fn get_outedges(
-        &'a self,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_outedges(&self.graph_txn, from)
-    }
-    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_inedges(&self.graph_txn, to)
-    }
-    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_node_ids(&self.graph_txn)
-    }
-    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_edge_ids(&self.graph_txn)
-    }
-    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
-        self.index.get_edge(&self.graph_txn, id)
-    }
-    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.index.get_edge_metadata(&self.graph_txn, id)
-    }
-    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
-        self.index.get_node(&self.graph_txn, id)
-    }
-    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
-        self.index.get_nodeid(&self.graph_txn, x)
-    }
-    pub fn prefix_search(&self, RMode(reader): &RMode, prefix: &str) -> RResult<Vec<String>> {
-        self.index.prefix_search(reader, prefix)
-    }
-    pub fn search<G: BfsGuide>(
-        &self,
-        guide: G,
-        max_depth: usize,
-        entry_points: Vec<Entity>,
-    ) -> RResult<impl Iterator<Item = GCnx>> {
-        self.index
-            .graph_search(&self.graph_txn, guide, max_depth, entry_points)
-    }
-    pub fn no_nodes(&self) -> RResult<u64> {
-        self.index.no_nodes(&self.graph_txn)
-    }
-}
-
-pub struct GraphWriter<'a> {
-    graph_txn: RwToken<'a>,
-    index: &'a Index,
-}
-impl<'a> GraphWriter<'a> {
-    pub fn commit(self, WMode(writer): &mut WMode) -> RResult<()> {
-        writer.commit()?;
-        self.graph_txn.commit()?;
-        Ok(())
-    }
-    pub fn get_outedges(
-        &'a self,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_outedges(&self.graph_txn, from)
-    }
-    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_inedges(&self.graph_txn, to)
-    }
-    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_node_ids(&self.graph_txn)
-    }
-    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_edge_ids(&self.graph_txn)
-    }
-    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
-        self.index.get_edge(&self.graph_txn, id)
-    }
-    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.index.get_edge_metadata(&self.graph_txn, id)
-    }
-    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
-        self.index.get_node(&self.graph_txn, id)
-    }
-    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
-        self.index.get_nodeid(&self.graph_txn, x)
-    }
-    pub fn no_nodes(&self) -> RResult<u64> {
-        self.index.no_nodes(&self.graph_txn)
-    }
-    pub fn connect(
-        &mut self,
-        WMode(writer): &WMode,
-        from: &IoNode,
-        to: &IoNode,
-        edge: &IoEdge,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        self.index
-            .connect(&mut self.graph_txn, writer, from, to, edge, edge_metadata)
-    }
-    pub fn delete_node(
-        &mut self,
-        WMode(writer): &WMode,
-        node_id: Entity,
-    ) -> RResult<HashSet<Entity>> {
-        self.index.delete_node(&mut self.graph_txn, writer, node_id)
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    #[test]
-    pub fn create_and_insert() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (index, mut wmode) = Index::new_writer(dir.path()).unwrap();
-        let n1 = IoNode::new("N1".to_string(), "T1".to_string(), None);
-        let n2 = IoNode::new("N2".to_string(), "T2".to_string(), None);
-        let edge = IoEdge::new("T2".to_string(), None);
-        let mut writer = index.start_writing().unwrap();
-        writer.connect(&wmode, &n1, &n2, &edge, None).unwrap();
-        writer.connect(&wmode, &n2, &n1, &edge, None).unwrap();
-        writer.commit(&mut wmode).unwrap();
-        let (index, _rmode) = Index::new_reader(dir.path()).unwrap();
-        let reader = index.start_reading().unwrap();
-        std::mem::drop(reader);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashSet;
+use std::path::Path;
+
+use super::bfs_engine::{BfsEngineBuilder, BfsGuide};
+use super::errors::*;
+pub use super::graph_db::{Entity, GCnx};
+use super::graph_db::{GraphDB, RoToken, RwToken};
+use super::node_dictionary::{DReader, DWriter, NodeDictionary};
+pub use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
+
+pub struct RMode(DReader);
+pub struct WMode(DWriter);
+
+pub struct Index {
+    graphdb: GraphDB,
+    dictionary: NodeDictionary,
+}
+impl Index {
+    const DICTIONARY_PATH: &str = "NodeDictionary";
+    const GRAPH_PATH: &str = "GraphDB";
+    const GRAPH_SIZE: usize = 1048576 * 100000;
+    fn connect(
+        &self,
+        graph_txn: &mut RwToken,
+        dict_writer: &DWriter,
+        from: &IoNode,
+        to: &IoNode,
+        edge: &IoEdge,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        self.dictionary.add_node(dict_writer, from)?;
+        self.dictionary.add_node(dict_writer, to)?;
+        let from = self.graphdb.add_node(graph_txn, from)?;
+        let to = self.graphdb.add_node(graph_txn, to)?;
+        self.graphdb
+            .connect(graph_txn, from, edge, to, edge_metadata)
+    }
+    fn graph_search<G: BfsGuide>(
+        &self,
+        txn: &RoToken,
+        guide: G,
+        max_depth: usize,
+        entry_points: Vec<Entity>,
+    ) -> RResult<impl Iterator<Item = GCnx>> {
+        BfsEngineBuilder::new()
+            .graph(&self.graphdb)
+            .txn(txn)
+            .max_depth(max_depth)
+            .guide(guide)
+            .entry_points(entry_points)
+            .build()
+            .unwrap()
+            .search()
+    }
+    fn delete_node(
+        &self,
+        graph_txn: &mut RwToken,
+        dict_writer: &DWriter,
+        node_id: Entity,
+    ) -> RResult<HashSet<Entity>> {
+        let value = self.graphdb.get_node(graph_txn, node_id)?;
+        self.dictionary.delete_node(dict_writer, &value);
+        self.graphdb.delete_node(graph_txn, node_id)
+    }
+    fn no_nodes(&self, txn: &RoToken) -> RResult<u64> {
+        self.graphdb.no_nodes(txn)
+    }
+    fn iter_node_ids<'a>(
+        &self,
+        txn: &'a RoToken,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.graphdb.iter_node_ids(txn)
+    }
+    fn iter_edge_ids<'a>(
+        &self,
+        txn: &'a RoToken,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.graphdb.iter_edge_ids(txn)
+    }
+    pub fn get_outedges<'a>(
+        &self,
+        txn: &'a RoToken,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.graphdb.get_outedges(txn, from)
+    }
+    fn get_edge(&self, txn: &RoToken, id: Entity) -> RResult<IoEdge> {
+        self.graphdb.get_edge(txn, id)
+    }
+    fn get_edge_metadata(&self, txn: &RoToken, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.graphdb.get_edge_metadata(txn, id)
+    }
+    fn get_node(&self, txn: &RoToken, id: Entity) -> RResult<IoNode> {
+        self.graphdb.get_node(txn, id)
+    }
+    fn get_nodeid(&self, txn: &RoToken, x: &str) -> RResult<Option<Entity>> {
+        self.graphdb.get_nodeid(txn, x)
+    }
+    fn prefix_search(&self, dict_reader: &DReader, prefix: &str) -> RResult<Vec<String>> {
+        self.dictionary.search(dict_reader, prefix)
+    }
+    fn get_inedges<'a>(
+        &self,
+        txn: &'a RoToken,
+        to: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.graphdb.get_inedges(txn, to)
+    }
+    pub fn new_writer(location: &Path) -> RResult<(Index, WMode)> {
+        let dictionary_address = location.join(Self::DICTIONARY_PATH);
+        let graph_address = location.join(Self::GRAPH_PATH);
+        if !graph_address.exists() {
+            std::fs::create_dir(&graph_address)?;
+        }
+        if !dictionary_address.exists() {
+            std::fs::create_dir(&dictionary_address)?;
+        }
+        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
+        let (dictionary, mode) = NodeDictionary::new_writer(&dictionary_address)?;
+        let index = Index {
+            graphdb,
+            dictionary,
+        };
+        Ok((index, WMode(mode)))
+    }
+    pub fn new_reader(location: &Path) -> RResult<(Index, RMode)> {
+        let dictionary_address = location.join(Self::DICTIONARY_PATH);
+        let graph_address = location.join(Self::GRAPH_PATH);
+        if !graph_address.exists() {
+            std::fs::create_dir(&graph_address)?;
+        }
+        if !dictionary_address.exists() {
+            std::fs::create_dir(&dictionary_address)?;
+        }
+        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
+        let (dictionary, mode) = NodeDictionary::new_reader(&dictionary_address)?;
+        let index = Index {
+            graphdb,
+            dictionary,
+        };
+        Ok((index, RMode(mode)))
+    }
+    pub fn start_reading(&self) -> RResult<GraphReader> {
+        Ok(GraphReader {
+            graph_txn: self.graphdb.ro_txn()?,
+            index: self,
+        })
+    }
+    pub fn start_writing(&self) -> RResult<GraphWriter> {
+        Ok(GraphWriter {
+            graph_txn: self.graphdb.rw_txn()?,
+            index: self,
+        })
+    }
+}
+
+pub struct GraphReader<'a> {
+    graph_txn: RoToken<'a>,
+    index: &'a Index,
+}
+impl<'a> GraphReader<'a> {
+    pub fn reload(&self, RMode(reader): &RMode) -> RResult<()> {
+        Ok(reader.reload()?)
+    }
+    pub fn get_outedges(
+        &'a self,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_outedges(&self.graph_txn, from)
+    }
+    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_inedges(&self.graph_txn, to)
+    }
+    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_node_ids(&self.graph_txn)
+    }
+    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_edge_ids(&self.graph_txn)
+    }
+    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
+        self.index.get_edge(&self.graph_txn, id)
+    }
+    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.index.get_edge_metadata(&self.graph_txn, id)
+    }
+    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
+        self.index.get_node(&self.graph_txn, id)
+    }
+    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
+        self.index.get_nodeid(&self.graph_txn, x)
+    }
+    pub fn prefix_search(&self, RMode(reader): &RMode, prefix: &str) -> RResult<Vec<String>> {
+        self.index.prefix_search(reader, prefix)
+    }
+    pub fn search<G: BfsGuide>(
+        &self,
+        guide: G,
+        max_depth: usize,
+        entry_points: Vec<Entity>,
+    ) -> RResult<impl Iterator<Item = GCnx>> {
+        self.index
+            .graph_search(&self.graph_txn, guide, max_depth, entry_points)
+    }
+    pub fn no_nodes(&self) -> RResult<u64> {
+        self.index.no_nodes(&self.graph_txn)
+    }
+}
+
+pub struct GraphWriter<'a> {
+    graph_txn: RwToken<'a>,
+    index: &'a Index,
+}
+impl<'a> GraphWriter<'a> {
+    pub fn commit(self, WMode(writer): &mut WMode) -> RResult<()> {
+        writer.commit()?;
+        self.graph_txn.commit()?;
+        Ok(())
+    }
+    pub fn get_outedges(
+        &'a self,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_outedges(&self.graph_txn, from)
+    }
+    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_inedges(&self.graph_txn, to)
+    }
+    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_node_ids(&self.graph_txn)
+    }
+    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_edge_ids(&self.graph_txn)
+    }
+    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
+        self.index.get_edge(&self.graph_txn, id)
+    }
+    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.index.get_edge_metadata(&self.graph_txn, id)
+    }
+    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
+        self.index.get_node(&self.graph_txn, id)
+    }
+    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
+        self.index.get_nodeid(&self.graph_txn, x)
+    }
+    pub fn no_nodes(&self) -> RResult<u64> {
+        self.index.no_nodes(&self.graph_txn)
+    }
+    pub fn connect(
+        &mut self,
+        WMode(writer): &WMode,
+        from: &IoNode,
+        to: &IoNode,
+        edge: &IoEdge,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        self.index
+            .connect(&mut self.graph_txn, writer, from, to, edge, edge_metadata)
+    }
+    pub fn delete_node(
+        &mut self,
+        WMode(writer): &WMode,
+        node_id: Entity,
+    ) -> RResult<HashSet<Entity>> {
+        self.index.delete_node(&mut self.graph_txn, writer, node_id)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    #[test]
+    pub fn create_and_insert() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (index, mut wmode) = Index::new_writer(dir.path()).unwrap();
+        let n1 = IoNode::new("N1".to_string(), "T1".to_string(), None);
+        let n2 = IoNode::new("N2".to_string(), "T2".to_string(), None);
+        let edge = IoEdge::new("T2".to_string(), None);
+        let mut writer = index.start_writing().unwrap();
+        writer.connect(&wmode, &n1, &n2, &edge, None).unwrap();
+        writer.connect(&wmode, &n2, &n1, &edge, None).unwrap();
+        writer.commit(&mut wmode).unwrap();
+        let (index, _rmode) = Index::new_reader(dir.path()).unwrap();
+        let reader = index.start_reading().unwrap();
+        std::mem::drop(reader);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/lib.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/lib.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,36 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod bfs_engine;
-mod errors;
-mod graph_db;
-#[cfg(test)]
-mod graph_test_utils;
-pub mod index;
-mod node_dictionary;
-mod relations_io;
-pub mod service;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+#![allow(clippy::derive_partial_eq_without_eq)]
+
+pub mod fdbwriter;
+pub mod knowledgebox;
+pub mod nodereader;
+pub mod noderesources;
+pub mod nodewriter;
+pub mod resources;
+pub mod utils;
+
+pub mod prelude {
+    pub use super::nodereader::*;
+    pub use super::noderesources::*;
+    pub use super::nodewriter::*;
+    pub use super::utils::*;
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/node_dictionary.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/node_dictionary.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,156 +1,156 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::path::Path;
-
-use itertools::Itertools;
-use tantivy::collector::DocSetCollector;
-use tantivy::query::RegexQuery;
-use tantivy::schema::{Field, Schema, Term, TextFieldIndexing, TextOptions, STORED, STRING};
-use tantivy::{doc, Index, IndexReader, IndexWriter, ReloadPolicy};
-
-use super::errors::*;
-use super::relations_io::IoNode;
-
-pub type DReader = IndexReader;
-pub type DWriter = IndexWriter;
-
-pub struct NodeDictionary {
-    node_value: Field,
-    node_hash: Field,
-    #[allow(unused)]
-    index: Index,
-}
-impl NodeDictionary {
-    const NODE_VALUE: &str = "value";
-    const NODE_HASH: &str = "hash";
-    const NUM_THREADS: usize = 1;
-    const MEM_LIMIT: usize = 6_000_000;
-
-    fn adapt_text(&self, text: &str) -> String {
-        deunicode::deunicode(text).to_lowercase()
-    }
-    fn build_query(&self, text: &str) -> String {
-        let query = text
-            .split(' ')
-            .filter(|s| !s.is_empty())
-            .map(|s| regex::escape(s.trim()))
-            .join(r"\s+");
-        format!("(?im){query}.*")
-    }
-    fn new(path: &Path) -> RResult<NodeDictionary> {
-        let text_options = TextOptions::default()
-            .set_indexing_options(TextFieldIndexing::default().set_tokenizer("raw"))
-            .set_stored();
-        let mut schema_builder = Schema::builder();
-        let node_hash = schema_builder.add_text_field(Self::NODE_HASH, STRING | STORED);
-        let node_value = schema_builder.add_text_field(Self::NODE_VALUE, text_options);
-        let schema = schema_builder.build();
-        let index = Index::create_in_dir(path, schema).or_else(|_| Index::open_in_dir(path))?;
-        Ok(NodeDictionary {
-            index,
-            node_hash,
-            node_value,
-        })
-    }
-    pub fn new_writer(path: &Path) -> RResult<(NodeDictionary, IndexWriter)> {
-        let dictionary = Self::new(path)?;
-        let writer = dictionary
-            .index
-            .writer_with_num_threads(Self::NUM_THREADS, Self::MEM_LIMIT)?;
-        Ok((dictionary, writer))
-    }
-    pub fn new_reader(path: &Path) -> RResult<(NodeDictionary, IndexReader)> {
-        let dictionary = Self::new(path)?;
-        let reader = dictionary
-            .index
-            .reader_builder()
-            .reload_policy(ReloadPolicy::OnCommit)
-            .try_into()?;
-        Ok((dictionary, reader))
-    }
-    pub fn search(&self, reader: &IndexReader, query: &str) -> RResult<Vec<String>> {
-        let query = self.adapt_text(query);
-        let query = self.build_query(&query);
-        let termq = Box::new(RegexQuery::from_pattern(&query, self.node_value)?);
-        let collector = DocSetCollector;
-        let searcher = reader.searcher();
-        let results = searcher
-            .search(termq.as_ref(), &collector)?
-            .into_iter()
-            .flat_map(|d| searcher.doc(d).ok())
-            .flat_map(|d| {
-                d.get_first(self.node_hash)
-                    .and_then(|v| v.as_text())
-                    .map(|v| v.to_string())
-            })
-            .collect();
-        Ok(results)
-    }
-    pub fn add_node(&self, writer: &IndexWriter, node: &IoNode) -> RResult<()> {
-        let document = doc!(
-            self.node_hash => node.hash(),
-            self.node_value => self.adapt_text(node.name())
-        );
-        self.delete_node(writer, node);
-        writer.add_document(document)?;
-        Ok(())
-    }
-    pub fn delete_node(&self, writer: &IndexWriter, node: &IoNode) {
-        writer.delete_term(Term::from_field_text(self.node_hash, node.hash()));
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use super::*;
-    #[test]
-    fn search_test() {
-        let dir = tempfile::tempdir().unwrap();
-        let (index, mut writer) = NodeDictionary::new_writer(dir.path()).unwrap();
-        let node = IoNode::new("New york".to_string(), "Untyped".to_string(), None);
-        index.add_node(&writer, &node).unwrap();
-        let node = IoNode::new("Barcelona".to_string(), "Untyped".to_string(), None);
-        index.add_node(&writer, &node).unwrap();
-        writer.commit().unwrap();
-        let (index, reader) = NodeDictionary::new_reader(dir.path()).unwrap();
-        let r1 = index.search(&reader, "new york").unwrap();
-        let r2 = index.search(&reader, "new York").unwrap();
-        let r3 = index.search(&reader, "new").unwrap();
-        let r4 = index.search(&reader, "york").unwrap();
-        let r5 = index.search(&reader, "br").unwrap();
-
-        assert_eq!(r1.len(), 1);
-        assert_eq!(r2.len(), 1);
-        assert_eq!(r3.len(), 1);
-        assert_eq!(r4.len(), 0);
-        assert_eq!(r5.len(), 1);
-    }
-    #[test]
-    fn open_reader() {
-        let dir = tempfile::tempdir().unwrap();
-        NodeDictionary::new_reader(dir.path()).unwrap();
-    }
-    #[test]
-    fn open_writer() {
-        let dir = tempfile::tempdir().unwrap();
-        NodeDictionary::new_writer(dir.path()).unwrap();
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::path::Path;
+
+use itertools::Itertools;
+use tantivy::collector::DocSetCollector;
+use tantivy::query::RegexQuery;
+use tantivy::schema::{Field, Schema, Term, TextFieldIndexing, TextOptions, STORED, STRING};
+use tantivy::{doc, Index, IndexReader, IndexWriter, ReloadPolicy};
+
+use super::errors::*;
+use super::relations_io::IoNode;
+
+pub type DReader = IndexReader;
+pub type DWriter = IndexWriter;
+
+pub struct NodeDictionary {
+    node_value: Field,
+    node_hash: Field,
+    #[allow(unused)]
+    index: Index,
+}
+impl NodeDictionary {
+    const NODE_VALUE: &str = "value";
+    const NODE_HASH: &str = "hash";
+    const NUM_THREADS: usize = 1;
+    const MEM_LIMIT: usize = 6_000_000;
+
+    fn adapt_text(&self, text: &str) -> String {
+        deunicode::deunicode(text).to_lowercase()
+    }
+    fn build_query(&self, text: &str) -> String {
+        let query = text
+            .split(' ')
+            .filter(|s| !s.is_empty())
+            .map(|s| regex::escape(s.trim()))
+            .join(r"\s+");
+        format!("(?im){query}.*")
+    }
+    fn new(path: &Path) -> RResult<NodeDictionary> {
+        let text_options = TextOptions::default()
+            .set_indexing_options(TextFieldIndexing::default().set_tokenizer("raw"))
+            .set_stored();
+        let mut schema_builder = Schema::builder();
+        let node_hash = schema_builder.add_text_field(Self::NODE_HASH, STRING | STORED);
+        let node_value = schema_builder.add_text_field(Self::NODE_VALUE, text_options);
+        let schema = schema_builder.build();
+        let index = Index::create_in_dir(path, schema).or_else(|_| Index::open_in_dir(path))?;
+        Ok(NodeDictionary {
+            index,
+            node_hash,
+            node_value,
+        })
+    }
+    pub fn new_writer(path: &Path) -> RResult<(NodeDictionary, IndexWriter)> {
+        let dictionary = Self::new(path)?;
+        let writer = dictionary
+            .index
+            .writer_with_num_threads(Self::NUM_THREADS, Self::MEM_LIMIT)?;
+        Ok((dictionary, writer))
+    }
+    pub fn new_reader(path: &Path) -> RResult<(NodeDictionary, IndexReader)> {
+        let dictionary = Self::new(path)?;
+        let reader = dictionary
+            .index
+            .reader_builder()
+            .reload_policy(ReloadPolicy::OnCommit)
+            .try_into()?;
+        Ok((dictionary, reader))
+    }
+    pub fn search(&self, reader: &IndexReader, query: &str) -> RResult<Vec<String>> {
+        let query = self.adapt_text(query);
+        let query = self.build_query(&query);
+        let termq = Box::new(RegexQuery::from_pattern(&query, self.node_value)?);
+        let collector = DocSetCollector;
+        let searcher = reader.searcher();
+        let results = searcher
+            .search(termq.as_ref(), &collector)?
+            .into_iter()
+            .flat_map(|d| searcher.doc(d).ok())
+            .flat_map(|d| {
+                d.get_first(self.node_hash)
+                    .and_then(|v| v.as_text())
+                    .map(|v| v.to_string())
+            })
+            .collect();
+        Ok(results)
+    }
+    pub fn add_node(&self, writer: &IndexWriter, node: &IoNode) -> RResult<()> {
+        let document = doc!(
+            self.node_hash => node.hash(),
+            self.node_value => self.adapt_text(node.name())
+        );
+        self.delete_node(writer, node);
+        writer.add_document(document)?;
+        Ok(())
+    }
+    pub fn delete_node(&self, writer: &IndexWriter, node: &IoNode) {
+        writer.delete_term(Term::from_field_text(self.node_hash, node.hash()));
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+    #[test]
+    fn search_test() {
+        let dir = tempfile::tempdir().unwrap();
+        let (index, mut writer) = NodeDictionary::new_writer(dir.path()).unwrap();
+        let node = IoNode::new("New york".to_string(), "Untyped".to_string(), None);
+        index.add_node(&writer, &node).unwrap();
+        let node = IoNode::new("Barcelona".to_string(), "Untyped".to_string(), None);
+        index.add_node(&writer, &node).unwrap();
+        writer.commit().unwrap();
+        let (index, reader) = NodeDictionary::new_reader(dir.path()).unwrap();
+        let r1 = index.search(&reader, "new york").unwrap();
+        let r2 = index.search(&reader, "new York").unwrap();
+        let r3 = index.search(&reader, "new").unwrap();
+        let r4 = index.search(&reader, "york").unwrap();
+        let r5 = index.search(&reader, "br").unwrap();
+
+        assert_eq!(r1.len(), 1);
+        assert_eq!(r2.len(), 1);
+        assert_eq!(r3.len(), 1);
+        assert_eq!(r4.len(), 0);
+        assert_eq!(r5.len(), 1);
+    }
+    #[test]
+    fn open_reader() {
+        let dir = tempfile::tempdir().unwrap();
+        NodeDictionary::new_reader(dir.path()).unwrap();
+    }
+    #[test]
+    fn open_writer() {
+        let dir = tempfile::tempdir().unwrap();
+        NodeDictionary::new_writer(dir.path()).unwrap();
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/relations_io.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/relations_io.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::hash::Hash;
-
-use data_encoding::HEXUPPER;
-use nucliadb_core::protos::RelationMetadata;
-use ring::digest::{Context, SHA256};
-use serde::{Deserialize, Serialize};
-
-pub fn compute_hash<D: AsRef<[u8]>>(data: &[D]) -> String {
-    let mut context = Context::new(&SHA256);
-    data.iter().for_each(|d| context.update(d.as_ref()));
-    let digest = context.finish();
-    HEXUPPER.encode(digest.as_ref())
-}
-
-#[derive(Default, Debug, Serialize, Deserialize, Clone, Copy, Eq, Ord, PartialEq, PartialOrd)]
-pub enum Source {
-    #[default]
-    Null,
-    User,
-    System,
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd)]
-pub struct IoNode {
-    source: Source,
-    name: String,
-    xtype: String,
-    subtype: Option<String>,
-    hash: String,
-}
-impl IoNode {
-    fn inner_new(source: Source, name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        let hash = compute_hash(&[
-            name.as_bytes(),
-            xtype.as_bytes(),
-            subtype.as_ref().map(|s| s.as_bytes()).unwrap_or(&[]),
-        ]);
-        IoNode {
-            name,
-            xtype,
-            subtype,
-            hash,
-            source,
-        }
-    }
-    pub fn user_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::User, name, xtype, subtype)
-    }
-    pub fn system_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::System, name, xtype, subtype)
-    }
-    pub fn new(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::default(), name, xtype, subtype)
-    }
-    pub fn name(&self) -> &str {
-        &self.name
-    }
-    pub fn xtype(&self) -> &str {
-        &self.xtype
-    }
-    pub fn subtype(&self) -> Option<&str> {
-        self.subtype.as_deref()
-    }
-    pub fn defined_by_user(&self) -> bool {
-        self.source == Source::User
-    }
-    pub fn hash(&self) -> &str {
-        &self.hash
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
-pub struct IoEdgeMetadata {
-    pub paragraph_id: Option<String>,
-    pub source_start: Option<i32>,
-    pub source_end: Option<i32>,
-    pub to_start: Option<i32>,
-    pub to_end: Option<i32>,
-}
-impl From<RelationMetadata> for IoEdgeMetadata {
-    fn from(value: RelationMetadata) -> Self {
-        IoEdgeMetadata {
-            paragraph_id: value.paragraph_id,
-            source_start: value.source_start,
-            source_end: value.source_end,
-            to_start: value.to_start,
-            to_end: value.to_end,
-        }
-    }
-}
-
-impl From<IoEdgeMetadata> for RelationMetadata {
-    fn from(value: IoEdgeMetadata) -> Self {
-        RelationMetadata {
-            paragraph_id: value.paragraph_id,
-            source_start: value.source_start,
-            source_end: value.source_end,
-            to_start: value.to_start,
-            to_end: value.to_end,
-        }
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
-pub struct IoEdge {
-    xtype: String,
-    subtype: Option<String>,
-}
-impl IoEdge {
-    pub fn new(xtype: String, subtype: Option<String>) -> IoEdge {
-        IoEdge { xtype, subtype }
-    }
-    pub fn xtype(&self) -> &str {
-        &self.xtype
-    }
-    pub fn subtype(&self) -> Option<&str> {
-        self.subtype.as_deref()
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::graph_db::*;
-    #[test]
-    fn graph_insertion() {
-        let dir = tempfile::tempdir().unwrap();
-        let graph = GraphDB::new(dir.path(), 1048576 * 100000).unwrap();
-        let node1 = IoNode::new("N1".to_string(), "T1".to_string(), Some("ST1".to_string()));
-        let node1p = IoNode::new("N1".to_string(), "T1".to_string(), None);
-        let node2 = IoNode::new("N2".to_string(), "T2".to_string(), Some("ST2".to_string()));
-        let mut txn = graph.rw_txn().unwrap();
-        let node1_uid = graph.add_node(&mut txn, &node1).unwrap();
-        let node1_uidf = graph.add_node(&mut txn, &node1).unwrap();
-        assert_eq!(node1_uid, node1_uidf);
-        let node1p_uid = graph.add_node(&mut txn, &node1p).unwrap();
-        assert_ne!(node1_uid, node1p_uid);
-        let node2_uid = graph.add_node(&mut txn, &node2).unwrap();
-        assert_ne!(node1_uid, node2_uid);
-        assert_ne!(node1p_uid, node2_uid);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::hash::Hash;
+
+use data_encoding::HEXUPPER;
+use nucliadb_core::protos::RelationMetadata;
+use ring::digest::{Context, SHA256};
+use serde::{Deserialize, Serialize};
+
+pub fn compute_hash<D: AsRef<[u8]>>(data: &[D]) -> String {
+    let mut context = Context::new(&SHA256);
+    data.iter().for_each(|d| context.update(d.as_ref()));
+    let digest = context.finish();
+    HEXUPPER.encode(digest.as_ref())
+}
+
+#[derive(Default, Debug, Serialize, Deserialize, Clone, Copy, Eq, Ord, PartialEq, PartialOrd)]
+pub enum Source {
+    #[default]
+    Null,
+    User,
+    System,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd)]
+pub struct IoNode {
+    source: Source,
+    name: String,
+    xtype: String,
+    subtype: Option<String>,
+    hash: String,
+}
+impl IoNode {
+    fn inner_new(source: Source, name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        let hash = compute_hash(&[
+            name.as_bytes(),
+            xtype.as_bytes(),
+            subtype.as_ref().map(|s| s.as_bytes()).unwrap_or(&[]),
+        ]);
+        IoNode {
+            name,
+            xtype,
+            subtype,
+            hash,
+            source,
+        }
+    }
+    pub fn user_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::User, name, xtype, subtype)
+    }
+    pub fn system_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::System, name, xtype, subtype)
+    }
+    pub fn new(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::default(), name, xtype, subtype)
+    }
+    pub fn name(&self) -> &str {
+        &self.name
+    }
+    pub fn xtype(&self) -> &str {
+        &self.xtype
+    }
+    pub fn subtype(&self) -> Option<&str> {
+        self.subtype.as_deref()
+    }
+    pub fn defined_by_user(&self) -> bool {
+        self.source == Source::User
+    }
+    pub fn hash(&self) -> &str {
+        &self.hash
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
+pub struct IoEdgeMetadata {
+    pub paragraph_id: Option<String>,
+    pub source_start: Option<i32>,
+    pub source_end: Option<i32>,
+    pub to_start: Option<i32>,
+    pub to_end: Option<i32>,
+}
+impl From<RelationMetadata> for IoEdgeMetadata {
+    fn from(value: RelationMetadata) -> Self {
+        IoEdgeMetadata {
+            paragraph_id: value.paragraph_id,
+            source_start: value.source_start,
+            source_end: value.source_end,
+            to_start: value.to_start,
+            to_end: value.to_end,
+        }
+    }
+}
+
+impl From<IoEdgeMetadata> for RelationMetadata {
+    fn from(value: IoEdgeMetadata) -> Self {
+        RelationMetadata {
+            paragraph_id: value.paragraph_id,
+            source_start: value.source_start,
+            source_end: value.source_end,
+            to_start: value.to_start,
+            to_end: value.to_end,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
+pub struct IoEdge {
+    xtype: String,
+    subtype: Option<String>,
+}
+impl IoEdge {
+    pub fn new(xtype: String, subtype: Option<String>) -> IoEdge {
+        IoEdge { xtype, subtype }
+    }
+    pub fn xtype(&self) -> &str {
+        &self.xtype
+    }
+    pub fn subtype(&self) -> Option<&str> {
+        self.subtype.as_deref()
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::graph_db::*;
+    #[test]
+    fn graph_insertion() {
+        let dir = tempfile::tempdir().unwrap();
+        let graph = GraphDB::new(dir.path(), 1048576 * 100000).unwrap();
+        let node1 = IoNode::new("N1".to_string(), "T1".to_string(), Some("ST1".to_string()));
+        let node1p = IoNode::new("N1".to_string(), "T1".to_string(), None);
+        let node2 = IoNode::new("N2".to_string(), "T2".to_string(), Some("ST2".to_string()));
+        let mut txn = graph.rw_txn().unwrap();
+        let node1_uid = graph.add_node(&mut txn, &node1).unwrap();
+        let node1_uidf = graph.add_node(&mut txn, &node1).unwrap();
+        assert_eq!(node1_uid, node1_uidf);
+        let node1p_uid = graph.add_node(&mut txn, &node1p).unwrap();
+        assert_ne!(node1_uid, node1p_uid);
+        let node2_uid = graph.add_node(&mut txn, &node2).unwrap();
+        assert_ne!(node1_uid, node2_uid);
+        assert_ne!(node1p_uid, node2_uid);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/bfs.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/bfs.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,78 +1,78 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashSet;
-use std::fmt::Debug;
-
-use nucliadb_core::tracing::*;
-
-use crate::bfs_engine::*;
-use crate::errors::*;
-use crate::index::*;
-
-pub struct GrpcGuide<'a> {
-    pub reader: &'a GraphReader<'a>,
-    pub node_filters: HashSet<(&'a str, Option<&'a str>)>,
-    pub edge_filters: HashSet<(&'a str, Option<&'a str>)>,
-    pub jump_always: &'a str,
-}
-impl<'a> GrpcGuide<'a> {
-    fn treat_bfs_error<A, B, F>(&self, default: B, input: A, f: F) -> B
-    where
-        F: Fn(A) -> RResult<B>,
-        A: Debug + Copy,
-        B: Default,
-    {
-        match f(input) {
-            Err(e) => {
-                info!("{e:?} during BFS looking at {input:?}");
-                default
-            }
-            Ok(result) => result,
-        }
-    }
-}
-impl<'a> BfsGuide for GrpcGuide<'a> {
-    fn edge_allowed(&self, edge: Entity) -> bool {
-        self.treat_bfs_error(false, edge, |edge: Entity| {
-            self.reader.get_edge(edge).map(|edge| {
-                self.edge_filters.is_empty()
-                    || self.edge_filters.contains(&(edge.xtype(), edge.subtype()))
-                    || self.edge_filters.contains(&(edge.xtype(), None))
-            })
-        })
-    }
-    fn node_allowed(&self, node: Entity) -> bool {
-        self.treat_bfs_error(false, node, |node: Entity| {
-            self.reader.get_node(node).map(|node| {
-                self.node_filters.is_empty()
-                    || self.node_filters.contains(&(node.xtype(), node.subtype()))
-                    || self.node_filters.contains(&(node.xtype(), None))
-            })
-        })
-    }
-    fn free_jump(&self, cnx: GCnx) -> bool {
-        self.treat_bfs_error(false, cnx.edge(), |edge: Entity| {
-            self.reader
-                .get_edge(edge)
-                .map(|edge| edge.xtype() == self.jump_always)
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashSet;
+use std::fmt::Debug;
+
+use nucliadb_core::tracing::*;
+
+use crate::bfs_engine::*;
+use crate::errors::*;
+use crate::index::*;
+
+pub struct GrpcGuide<'a> {
+    pub reader: &'a GraphReader<'a>,
+    pub node_filters: HashSet<(&'a str, Option<&'a str>)>,
+    pub edge_filters: HashSet<(&'a str, Option<&'a str>)>,
+    pub jump_always: &'a str,
+}
+impl<'a> GrpcGuide<'a> {
+    fn treat_bfs_error<A, B, F>(&self, default: B, input: A, f: F) -> B
+    where
+        F: Fn(A) -> RResult<B>,
+        A: Debug + Copy,
+        B: Default,
+    {
+        match f(input) {
+            Err(e) => {
+                info!("{e:?} during BFS looking at {input:?}");
+                default
+            }
+            Ok(result) => result,
+        }
+    }
+}
+impl<'a> BfsGuide for GrpcGuide<'a> {
+    fn edge_allowed(&self, edge: Entity) -> bool {
+        self.treat_bfs_error(false, edge, |edge: Entity| {
+            self.reader.get_edge(edge).map(|edge| {
+                self.edge_filters.is_empty()
+                    || self.edge_filters.contains(&(edge.xtype(), edge.subtype()))
+                    || self.edge_filters.contains(&(edge.xtype(), None))
+            })
+        })
+    }
+    fn node_allowed(&self, node: Entity) -> bool {
+        self.treat_bfs_error(false, node, |node: Entity| {
+            self.reader.get_node(node).map(|node| {
+                self.node_filters.is_empty()
+                    || self.node_filters.contains(&(node.xtype(), node.subtype()))
+                    || self.node_filters.contains(&(node.xtype(), None))
+            })
+        })
+    }
+    fn free_jump(&self, cnx: GCnx) -> bool {
+        self.treat_bfs_error(false, cnx.edge(), |edge: Entity| {
+            self.reader
+                .get_edge(edge)
+                .map(|edge| edge.xtype() == self.jump_always)
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/tests.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/writer.rs`

 * *Files 27% similar despite different names*

```diff
@@ -1,435 +1,352 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashMap;
-use std::path::Path;
-
-use lazy_static::lazy_static;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::*;
-use prost_types::Timestamp;
-use relation::*;
-use relation_node::NodeType;
-
-use super::*;
-
-lazy_static! {
-    static ref SHARD_ID: String = "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string();
-    static ref E0: RelationNode = RelationNode {
-        value: "E0".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "".to_string(),
-    };
-    static ref E1: RelationNode = RelationNode {
-        value: "E1".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "Official".to_string(),
-    };
-    static ref E2: RelationNode = RelationNode {
-        value: "E2".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "Propaganda".to_string(),
-    };
-    static ref NODE_TYPES: TypeList = TypeList {
-        list: vec![
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "Official".to_string(),
-            },
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "".to_string(),
-            },
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "Propaganda".to_string(),
-            },
-        ]
-    };
-    static ref REQUEST_BONES: RelationSearchRequest = RelationSearchRequest {
-        shard_id: SHARD_ID.clone(),
-        reload: false,
-        prefix: None,
-        subgraph: None,
-    };
-    static ref REQUEST0: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
-        entry_points: vec![E0.clone()],
-        node_filters: vec![
-            RelationNodeFilter {
-                node_type: NodeType::Entity as i32,
-                node_subtype: None
-            },
-            RelationNodeFilter {
-                node_type: NodeType::Entity as i32,
-                node_subtype: Some("Nonexisting".to_string())
-            }
-        ],
-        depth: Some(1),
-        edge_filters: vec![],
-    };
-    static ref RESPONSE0: Vec<RelationNode> = vec![E0.clone(), E1.clone(), E2.clone()];
-    static ref REQUEST1: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
-        entry_points: vec![E0.clone()],
-        node_filters: vec![RelationNodeFilter {
-            node_type: NodeType::Entity as i32,
-            node_subtype: Some("Official".to_string())
-        },],
-        depth: Some(1),
-        edge_filters: vec![],
-    };
-    static ref RESPONSE1: Vec<RelationNode> = vec![E0.clone(), E1.clone()];
-    static ref EDGE_LIST: EdgeList = EdgeList {
-        list: vec![
-            RelationEdge {
-                edge_type: RelationType::Entity as i32,
-                property: "".to_string()
-            },
-            RelationEdge {
-                edge_type: RelationType::Child as i32,
-                property: "".to_string()
-            },
-        ]
-    };
-}
-
-fn create_empty_resource(shard_id: String) -> Resource {
-    let resource_id = ResourceId {
-        shard_id: SHARD_ID.clone(),
-        uuid: SHARD_ID.clone(),
-    };
-    let timestamp = Timestamp {
-        seconds: 0,
-        nanos: 0,
-    };
-
-    let metadata = IndexMetadata {
-        created: Some(timestamp.clone()),
-        modified: Some(timestamp),
-    };
-
-    Resource {
-        resource: Some(resource_id),
-        metadata: Some(metadata),
-        texts: HashMap::with_capacity(0),
-        status: ResourceStatus::Processed as i32,
-        labels: vec![],
-        paragraphs: HashMap::with_capacity(0),
-        paragraphs_to_delete: vec![],
-        sentences_to_delete: vec![],
-        relations_to_delete: vec![],
-        relations: vec![],
-        vectors: HashMap::default(),
-        vectors_to_delete: HashMap::default(),
-        shard_id,
-    }
-}
-
-fn empty_graph() -> Vec<Relation> {
-    vec![]
-}
-
-fn entities(mut edges: Vec<Relation>) -> Vec<Relation> {
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r0".to_string()),
-        ..Default::default()
-    };
-    let r0 = Relation {
-        relation: RelationType::Child as i32,
-        source: Some(E1.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r1".to_string()),
-        ..Default::default()
-    };
-    let r1 = Relation {
-        relation: RelationType::Entity as i32,
-        source: Some(E0.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r2".to_string()),
-        ..Default::default()
-    };
-    let r2 = Relation {
-        relation: RelationType::Entity as i32,
-        source: Some(E0.clone()),
-        to: Some(E1.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    edges.append(&mut vec![r0, r1, r2]);
-    edges
-}
-
-fn similatity_edges(mut edges: Vec<Relation>) -> Vec<Relation> {
-    let r0 = Relation {
-        relation: RelationType::Synonym as i32,
-        source: Some(E0.clone()),
-        to: Some(E1.clone()),
-        relation_label: "".to_string(),
-        metadata: None,
-    };
-    let r1 = Relation {
-        relation: RelationType::Synonym as i32,
-        source: Some(E1.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: None,
-    };
-    edges.append(&mut vec![r0, r1]);
-    edges
-}
-
-fn simple_graph(at: &Path) -> (RelationsWriterService, RelationsReaderService) {
-    let rsc = RelationConfig {
-        path: at.join("relations"),
-    };
-    println!("Writer starts");
-    let writer = RelationsWriterService::start(&rsc).unwrap();
-    let reader = RelationsReaderService::start(&rsc).unwrap();
-    (writer, reader)
-}
-
-#[test]
-fn simple_request() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-
-    reader.reload();
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE0.len());
-    Ok(())
-}
-
-#[test]
-fn join_graph_test() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = empty_graph();
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    let got = reader.count().unwrap();
-    assert_eq!(got, 0);
-
-    let graph = JoinGraph {
-        nodes: HashMap::from([(0i32, E0.clone()), (1i32, E1.clone()), (2i32, E2.clone())]),
-        edges: vec![
-            JoinGraphEdge {
-                source: 2,
-                target: 1,
-                rtype: RelationType::Child as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-            JoinGraphEdge {
-                source: 0,
-                target: 2,
-                rtype: RelationType::Entity as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-            JoinGraphEdge {
-                source: 0,
-                target: 1,
-                rtype: RelationType::Entity as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-        ],
-    };
-    writer.join_graph(&graph).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE0.len());
-    Ok(())
-}
-
-#[test]
-fn simple_request_with_similarity() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = similatity_edges(entities(empty_graph()));
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len, RESPONSE0.len() + 2);
-
-    Ok(())
-}
-
-#[test]
-fn typed_request() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST1.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE1.contains(&v.0))
-        .filter(|v| RESPONSE1.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE1.len());
-
-    Ok(())
-}
-
-#[test]
-fn just_prefix_querying() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-
-    reader.reload();
-    let mut request = REQUEST_BONES.clone();
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "E".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    let is_permutation = prefix_response
-        .nodes
-        .iter()
-        .all(|member| RESPONSE0.contains(member));
-    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
-
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "e".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    let is_permutation = prefix_response
-        .nodes
-        .iter()
-        .all(|member| RESPONSE0.contains(member));
-    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
-
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "not".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    assert!(prefix_response.nodes.is_empty());
-
-    Ok(())
-}
-
-#[test]
-fn getting_node_types() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-    let node_types = reader.get_node_types().unwrap();
-    assert_eq!(node_types.list.len(), NODE_TYPES.list.len());
-    assert!(node_types
-        .list
-        .iter()
-        .all(|member| NODE_TYPES.list.contains(member)));
-
-    let edges = reader.get_edges().unwrap();
-    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
-    assert!(edges
-        .list
-        .iter()
-        .all(|member| EDGE_LIST.list.contains(member)));
-    Ok(())
-}
-
-#[test]
-fn getting_edges() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-    let edges = reader.get_edges().unwrap();
-    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
-    assert!(edges
-        .list
-        .iter()
-        .all(|member| EDGE_LIST.list.contains(member)));
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fmt::Debug;
+use std::fs;
+use std::time::SystemTime;
+
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{Resource, ResourceId};
+use nucliadb_core::tracing::{self, *};
+use tantivy::collector::Count;
+use tantivy::query::AllQuery;
+use tantivy::schema::*;
+use tantivy::{doc, Index, IndexSettings, IndexSortByField, IndexWriter, Order};
+
+use super::schema::{timestamp_to_datetime_utc, TextSchema};
+
+pub struct TextWriterService {
+    index: Index,
+    pub schema: TextSchema,
+    writer: IndexWriter,
+}
+
+impl Debug for TextWriterService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("FieldWriterService")
+            .field("index", &self.index)
+            .field("schema", &self.schema)
+            .finish()
+    }
+}
+
+impl FieldWriter for TextWriterService {}
+
+impl WriterChild for TextWriterService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let time = SystemTime::now();
+
+        let id: Option<String> = None;
+        let reader = self.index.reader()?;
+        let searcher = reader.searcher();
+        let count = searcher.search(&AllQuery, &Count)?;
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::texts("count".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took}");
+
+        Ok(count)
+    }
+    #[tracing::instrument(skip_all)]
+    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = Some(&resource.shard_id);
+        let resource_id = resource.resource.as_ref().expect("Missing resource ID");
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete existing uuid: starts at {v} ms");
+        }
+        let uuid_field = self.schema.uuid;
+        let uuid_term = Term::from_field_text(uuid_field, &resource_id.uuid);
+        self.writer.delete_term(uuid_term);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete existing uuid: ends at {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Indexing document: starts at {v} ms");
+        }
+        if resource.status != ResourceStatus::Delete as i32 {
+            self.index_document(resource);
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Indexing document: starts at {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: starts at {v} ms");
+        }
+        self.writer.commit()?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: ends at {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::texts("set_resource".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took}");
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn delete_resource(&mut self, resource_id: &ResourceId) -> NodeResult<()> {
+        let time = SystemTime::now();
+        let id = Some(&resource_id.shard_id);
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete existing uuid: starts at {v} ms");
+        }
+        let uuid_field = self.schema.uuid;
+        let uuid_term = Term::from_field_text(uuid_field, &resource_id.uuid);
+        self.writer.delete_term(uuid_term);
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Delete existing uuid: ends at {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: starts at {v} ms");
+        }
+        self.writer.commit()?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Commit: ends at {v} ms");
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::texts("delete_resource".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took}");
+
+        Ok(())
+    }
+    fn garbage_collection(&mut self) -> NodeResult<()> {
+        Ok(())
+    }
+}
+
+impl TextWriterService {
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &TextConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            match TextWriterService::new(config) {
+                Err(e) if path.exists() => {
+                    std::fs::remove_dir(path)?;
+                    Err(e)
+                }
+                Err(e) => Err(e),
+                Ok(v) => Ok(v),
+            }
+        } else {
+            Ok(TextWriterService::open(config)?)
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn new(config: &TextConfig) -> NodeResult<Self> {
+        let field_schema = TextSchema::new();
+        fs::create_dir(&config.path)?;
+        let mut index_builder = Index::builder().schema(field_schema.schema.clone());
+        let settings = IndexSettings {
+            sort_by_field: Some(IndexSortByField {
+                field: "created".to_string(),
+                order: Order::Desc,
+            }),
+            ..Default::default()
+        };
+
+        index_builder = index_builder.settings(settings);
+        let index = index_builder.create_in_dir(&config.path).unwrap();
+
+        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
+
+        Ok(TextWriterService {
+            index,
+            writer,
+            schema: field_schema,
+        })
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn open(config: &TextConfig) -> NodeResult<Self> {
+        let field_schema = TextSchema::new();
+
+        let index = Index::open_in_dir(&config.path)?;
+
+        let writer = index.writer_with_num_threads(1, 6_000_000).unwrap();
+
+        Ok(TextWriterService {
+            index,
+            writer,
+            schema: field_schema,
+        })
+    }
+
+    fn index_document(&mut self, resource: &Resource) {
+        let resource_id = resource
+            .resource
+            .as_ref()
+            .expect("Missing resource ID")
+            .uuid
+            .as_str();
+        let metadata = resource
+            .metadata
+            .as_ref()
+            .expect("Missing resource metadata");
+        let modified = metadata
+            .modified
+            .as_ref()
+            .expect("Missing resource modified date in metadata");
+        let created = metadata
+            .created
+            .as_ref()
+            .expect("Missing resource created date in metadata");
+
+        let mut base_doc = doc!(
+            self.schema.uuid => resource_id,
+            self.schema.modified => timestamp_to_datetime_utc(modified),
+            self.schema.created => timestamp_to_datetime_utc(created),
+            self.schema.status => resource.status as u64,
+        );
+
+        for label in resource.labels.iter() {
+            let facet = Facet::from(label.as_str());
+            base_doc.add_facet(self.schema.facets, facet);
+        }
+
+        for (field, text_info) in &resource.texts {
+            let mut field_doc = base_doc.clone();
+            let mut facet_key: String = "/".to_owned();
+            facet_key.push_str(field.as_str());
+            let facet_field = Facet::from(facet_key.as_str());
+            field_doc.add_facet(self.schema.field, facet_field);
+            field_doc.add_text(self.schema.text, &text_info.text);
+
+            for label in text_info.labels.iter() {
+                let facet = Facet::from(label.as_str());
+                field_doc.add_facet(self.schema.facets, facet);
+            }
+            self.writer.add_document(field_doc).unwrap();
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashMap;
+    use std::time::SystemTime;
+
+    use nucliadb_core::protos::prost_types::Timestamp;
+    use nucliadb_core::{protos, NodeResult};
+    use tantivy::collector::{Count, TopDocs};
+    use tantivy::query::{AllQuery, TermQuery};
+    use tempfile::TempDir;
+
+    use super::*;
+
+    fn create_resource(shard_id: String) -> Resource {
+        let resource_id = ResourceId {
+            shard_id: shard_id.to_string(),
+            uuid: "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string(),
+        };
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64,
+            nanos: 0,
+        };
+
+        let metadata = protos::IndexMetadata {
+            created: Some(timestamp.clone()),
+            modified: Some(timestamp),
+        };
+
+        const DOC1_TI: &str = "This is the first document";
+        const DOC1_P1: &str = "This is the text of the second paragraph.";
+        const DOC1_P2: &str = "This should be enough to test the tantivy.";
+        const DOC1_P3: &str = "But I wanted to make it three anyway.";
+
+        let ti_title = protos::TextInformation {
+            text: DOC1_TI.to_string(),
+            labels: vec!["/l/mylabel".to_string()],
+        };
+
+        let ti_body = protos::TextInformation {
+            text: DOC1_P1.to_string() + DOC1_P2 + DOC1_P3,
+            labels: vec!["/f/body".to_string()],
+        };
+
+        let mut texts = HashMap::new();
+        texts.insert("title".to_string(), ti_title);
+        texts.insert("body".to_string(), ti_body);
+
+        Resource {
+            resource: Some(resource_id),
+            metadata: Some(metadata),
+            texts,
+            status: protos::resource::ResourceStatus::Processed as i32,
+            labels: vec![],
+            paragraphs: HashMap::new(),
+            paragraphs_to_delete: vec![],
+            sentences_to_delete: vec![],
+            relations_to_delete: vec![],
+            relations: vec![],
+            vectors: HashMap::default(),
+            vectors_to_delete: HashMap::default(),
+            shard_id,
+        }
+    }
+
+    #[test]
+    fn test_new_writer() -> NodeResult<()> {
+        let dir = TempDir::new().unwrap();
+        let fsc = TextConfig {
+            path: dir.path().join("texts"),
+        };
+
+        let mut field_writer_service = TextWriterService::start(&fsc).unwrap();
+        let resource1 = create_resource("shard1".to_string());
+        let _ = field_writer_service.set_resource(&resource1);
+        let _ = field_writer_service.set_resource(&resource1);
+
+        let reader = field_writer_service.index.reader()?;
+        let searcher = reader.searcher();
+
+        let query = TermQuery::new(
+            Term::from_field_text(field_writer_service.schema.text, "document"),
+            IndexRecordOption::Basic,
+        );
+
+        let (_top_docs, count) = searcher.search(&query, &(TopDocs::with_limit(2), Count))?;
+        assert_eq!(count, 1);
+
+        let (_top_docs, count) = searcher.search(&AllQuery, &(TopDocs::with_limit(10), Count))?;
+        assert_eq!(count, 2);
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/utils.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/utils.rs`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,87 +1,87 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use nucliadb_core::protos::relation::RelationType;
-use nucliadb_core::protos::relation_node::NodeType;
-
-pub mod dictionary {
-    pub const ENTITY: &str = "Entity";
-    pub const ABOUT: &str = "About";
-    pub const CHILD: &str = "Child";
-    pub const COLAB: &str = "Colab";
-    pub const SYNONYM: &str = "Synonym";
-    pub const OTHER: &str = "Other";
-    pub const RESOURCE: &str = "Resource";
-    pub const USER: &str = "User";
-    pub const LABEL: &str = "Label";
-}
-
-pub fn relation_type_parsing(rtype: RelationType, subtype: &str) -> (&str, Option<&str>) {
-    let subtype = if subtype.is_empty() {
-        None
-    } else {
-        Some(subtype)
-    };
-    let xtype = match rtype {
-        RelationType::Entity => dictionary::ENTITY,
-        RelationType::About => dictionary::ABOUT,
-        RelationType::Child => dictionary::CHILD,
-        RelationType::Colab => dictionary::COLAB,
-        RelationType::Synonym => dictionary::SYNONYM,
-        RelationType::Other => dictionary::OTHER,
-    };
-    (xtype, subtype)
-}
-
-pub fn string_to_rtype(rtype: &str) -> RelationType {
-    match rtype {
-        dictionary::ENTITY => RelationType::Entity,
-        dictionary::ABOUT => RelationType::About,
-        dictionary::CHILD => RelationType::Child,
-        dictionary::COLAB => RelationType::Colab,
-        dictionary::SYNONYM => RelationType::Synonym,
-        dictionary::OTHER => RelationType::Other,
-        v => unreachable!("unknown type {v}"),
-    }
-}
-
-pub fn node_type_parsing(rtype: NodeType, subtype: &str) -> (&'static str, Option<&str>) {
-    let subtype = if subtype.is_empty() {
-        None
-    } else {
-        Some(subtype)
-    };
-    let xtype = match rtype {
-        NodeType::Entity => dictionary::ENTITY,
-        NodeType::Label => dictionary::LABEL,
-        NodeType::Resource => dictionary::RESOURCE,
-        NodeType::User => dictionary::USER,
-    };
-    (xtype, subtype)
-}
-
-pub fn string_to_node_type(rtype: &str) -> NodeType {
-    match rtype {
-        dictionary::ENTITY => NodeType::Entity,
-        dictionary::LABEL => NodeType::Label,
-        dictionary::RESOURCE => NodeType::Resource,
-        dictionary::USER => NodeType::User,
-        v => panic!("Invalid node type {}", v),
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use nucliadb_core::protos::relation::RelationType;
+use nucliadb_core::protos::relation_node::NodeType;
+
+pub mod dictionary {
+    pub const ENTITY: &str = "Entity";
+    pub const ABOUT: &str = "About";
+    pub const CHILD: &str = "Child";
+    pub const COLAB: &str = "Colab";
+    pub const SYNONYM: &str = "Synonym";
+    pub const OTHER: &str = "Other";
+    pub const RESOURCE: &str = "Resource";
+    pub const USER: &str = "User";
+    pub const LABEL: &str = "Label";
+}
+
+pub fn relation_type_parsing(rtype: RelationType, subtype: &str) -> (&str, Option<&str>) {
+    let subtype = if subtype.is_empty() {
+        None
+    } else {
+        Some(subtype)
+    };
+    let xtype = match rtype {
+        RelationType::Entity => dictionary::ENTITY,
+        RelationType::About => dictionary::ABOUT,
+        RelationType::Child => dictionary::CHILD,
+        RelationType::Colab => dictionary::COLAB,
+        RelationType::Synonym => dictionary::SYNONYM,
+        RelationType::Other => dictionary::OTHER,
+    };
+    (xtype, subtype)
+}
+
+pub fn string_to_rtype(rtype: &str) -> RelationType {
+    match rtype {
+        dictionary::ENTITY => RelationType::Entity,
+        dictionary::ABOUT => RelationType::About,
+        dictionary::CHILD => RelationType::Child,
+        dictionary::COLAB => RelationType::Colab,
+        dictionary::SYNONYM => RelationType::Synonym,
+        dictionary::OTHER => RelationType::Other,
+        v => unreachable!("unknown type {v}"),
+    }
+}
+
+pub fn node_type_parsing(rtype: NodeType, subtype: &str) -> (&'static str, Option<&str>) {
+    let subtype = if subtype.is_empty() {
+        None
+    } else {
+        Some(subtype)
+    };
+    let xtype = match rtype {
+        NodeType::Entity => dictionary::ENTITY,
+        NodeType::Label => dictionary::LABEL,
+        NodeType::Resource => dictionary::RESOURCE,
+        NodeType::User => dictionary::USER,
+    };
+    (xtype, subtype)
+}
+
+pub fn string_to_node_type(rtype: &str) -> NodeType {
+    match rtype {
+        dictionary::ENTITY => NodeType::Entity,
+        dictionary::LABEL => NodeType::Label,
+        dictionary::RESOURCE => NodeType::Resource,
+        dictionary::USER => NodeType::User,
+        v => panic!("Invalid node type {}", v),
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_relations/src/service/writer.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_relations/src/service/writer.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,288 +1,283 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashMap;
-use std::time::SystemTime;
-
-use nucliadb_core::metrics;
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{DeleteGraphNodes, JoinGraph, Resource, ResourceId};
-use nucliadb_core::tracing::{self, *};
-
-use super::utils::*;
-use crate::errors::RelationsErr as InnerErr;
-use crate::index::*;
-
-pub struct RelationsWriterService {
-    wmode: WMode,
-    index: Index,
-}
-impl RelationsWriterService {
-    #[tracing::instrument(skip_all)]
-    fn delete_node(&self, writer: &mut GraphWriter, id: Entity) -> NodeResult<()> {
-        let time = SystemTime::now();
-        let affects = writer.delete_node(&self.wmode, id)?;
-        for affected in affects {
-            let affected_value = writer.get_node(affected)?;
-            let no_in = writer.get_inedges(affected)?.count();
-            let no_out = writer.get_outedges(affected)?.count();
-            if no_in == 0 && no_out == 0 && !affected_value.defined_by_user() {
-                writer.delete_node(&self.wmode, affected)?;
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Ending at {v} ms")
-        }
-        Ok(())
-    }
-}
-impl RelationWriter for RelationsWriterService {
-    #[tracing::instrument(skip_all)]
-    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = graph.shard_id.as_ref().map(|s| &s.id);
-        let mut writer = self.index.start_writing()?;
-        for node in graph.nodes.iter() {
-            let name = node.value.clone();
-            let (xtype, subtype) = node_type_parsing(node.ntype(), &node.subtype);
-            let node = IoNode::new(name, xtype.to_string(), subtype.map(|s| s.to_string()));
-            if let Some(id) = writer.get_node_id(node.hash())? {
-                self.delete_node(&mut writer, id)?;
-            }
-        }
-        let result = Ok(writer.commit(&mut self.wmode)?);
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::relations("delete_nodes".to_string());
-        metrics.record_request_time(metric, took);
-        debug!("{id:?} - Ending at {took} ms");
-
-        result
-    }
-    #[tracing::instrument(skip_all)]
-    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Creating nodes: starts {v} ms");
-        }
-
-        let mut writer = self.index.start_writing()?;
-        let nodes: HashMap<_, _> = graph
-            .nodes
-            .iter()
-            .map(|(k, v)| (k, v.value.clone(), node_type_parsing(v.ntype(), &v.subtype)))
-            .map(|(key, value, (xtype, subtype))| (key, value, xtype, subtype))
-            .map(|(key, value, xtype, subtype)| {
-                (
-                    key,
-                    value,
-                    xtype.to_string(),
-                    subtype.map(|s| s.to_string()),
-                )
-            })
-            .map(|(&key, value, xtype, subtype)| (key, IoNode::user_node(value, xtype, subtype)))
-            .collect();
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Creating nodes: ends {v} ms");
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Populating the graph: starts {v} ms");
-        }
-
-        let ubehaviour = || Err(InnerErr::UBehaviour);
-        for protos_edge in graph.edges.iter() {
-            let from = nodes.get(&protos_edge.source).map_or_else(ubehaviour, Ok)?;
-            let to = nodes.get(&protos_edge.target).map_or_else(ubehaviour, Ok)?;
-            let edge = relation_type_parsing(protos_edge.rtype(), &protos_edge.rsubtype);
-            let edge = IoEdge::new(edge.0.to_string(), edge.1.map(|s| s.to_string()));
-            let metadata = protos_edge.metadata.clone().map(IoEdgeMetadata::from);
-            writer.connect(&self.wmode, from, to, &edge, metadata.as_ref())?;
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Populating the graph: ends {v} ms");
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Ending at {v} ms")
-        }
-
-        let result = Ok(writer.commit(&mut self.wmode)?);
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::relations("join_graph".to_string());
-        metrics.record_request_time(metric, took);
-
-        result
-    }
-}
-impl std::fmt::Debug for RelationsWriterService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("RelationWriterService").finish()
-    }
-}
-
-impl WriterChild for RelationsWriterService {
-    #[tracing::instrument(skip_all)]
-    fn stop(&mut self) -> NodeResult<()> {
-        debug!("Stopping relation writer Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn count(&self) -> NodeResult<usize> {
-        let time = SystemTime::now();
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Count starting at {v} ms");
-        }
-        let count = self
-            .index
-            .start_reading()
-            .and_then(|reader| reader.no_nodes())?;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("Ending at {v} ms")
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::relations("count".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(count as usize)
-    }
-    #[tracing::instrument(skip_all)]
-    fn delete_resource(&mut self, x: &ResourceId) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = Some(&x.shard_id);
-        let node = IoNode::new(x.uuid.clone(), dictionary::ENTITY.to_string(), None);
-        let mut writer = self.index.start_writing()?;
-        if let Some(id) = writer.get_node_id(node.hash())? {
-            self.delete_node(&mut writer, id)?;
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Ending at {v} ms")
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::relations("delete_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
-        let time = SystemTime::now();
-
-        let id = Some(&resource.shard_id);
-        if resource.status != ResourceStatus::Delete as i32 {
-            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-                debug!("{id:?} - Populating the graph: starts {v} ms");
-            }
-            let iter = resource
-                .relations
-                .iter()
-                .filter(|rel| rel.to.is_some() || rel.source.is_some());
-            let mut writer = self.index.start_writing()?;
-            for rel in iter {
-                let edge = relation_type_parsing(rel.relation(), &rel.relation_label);
-                let from = rel.source.as_ref().unwrap();
-                let from_type = node_type_parsing(from.ntype(), &from.subtype);
-                let to = rel.to.as_ref().unwrap();
-                let to_type = node_type_parsing(to.ntype(), &to.subtype);
-                let from = IoNode::system_node(
-                    from.value.clone(),
-                    from_type.0.to_string(),
-                    from_type.1.map(|s| s.to_string()),
-                );
-                let to = IoNode::system_node(
-                    to.value.clone(),
-                    to_type.0.to_string(),
-                    to_type.1.map(|s| s.to_string()),
-                );
-                let edge = IoEdge::new(edge.0.to_string(), edge.1.map(|s| s.to_string()));
-                let metadata = rel.metadata.clone().map(IoEdgeMetadata::from);
-                writer.connect(&self.wmode, &from, &to, &edge, metadata.as_ref())?;
-            }
-            writer.commit(&mut self.wmode)?;
-            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-                debug!("{id:?} - Populating the graph: ends {v} ms");
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Ending at {v} ms")
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::relations("set_resource".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(())
-    }
-    fn garbage_collection(&mut self) -> NodeResult<()> {
-        Ok(())
-    }
-}
-
-impl RelationsWriterService {
-    pub fn start(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            match RelationsWriterService::new(config) {
-                Err(e) if path.exists() => {
-                    std::fs::remove_dir(path)?;
-                    Err(e)
-                }
-                Err(e) => Err(e),
-                Ok(v) => Ok(v),
-            }
-        } else {
-            Ok(RelationsWriterService::open(config)?)
-        }
-    }
-    pub fn new(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if path.exists() {
-            Err(node_error!("Shard does exist".to_string()))
-        } else {
-            std::fs::create_dir(path)?;
-            let (index, wmode) = Index::new_writer(path)?;
-            Ok(RelationsWriterService { index, wmode })
-        }
-    }
-
-    pub fn open(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            Err(node_error!("Shard does not exist".to_string()))
-        } else {
-            let (index, wmode) = Index::new_writer(path)?;
-            Ok(RelationsWriterService { index, wmode })
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashMap;
+use std::time::SystemTime;
+
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{DeleteGraphNodes, JoinGraph, Resource, ResourceId};
+use nucliadb_core::tracing::{self, *};
+
+use super::utils::*;
+use crate::errors::RelationsErr as InnerErr;
+use crate::index::*;
+
+pub struct RelationsWriterService {
+    wmode: WMode,
+    index: Index,
+}
+impl RelationsWriterService {
+    #[tracing::instrument(skip_all)]
+    fn delete_node(&self, writer: &mut GraphWriter, id: Entity) -> NodeResult<()> {
+        let time = SystemTime::now();
+        let affects = writer.delete_node(&self.wmode, id)?;
+        for affected in affects {
+            let affected_value = writer.get_node(affected)?;
+            let no_in = writer.get_inedges(affected)?.count();
+            let no_out = writer.get_outedges(affected)?.count();
+            if no_in == 0 && no_out == 0 && !affected_value.defined_by_user() {
+                writer.delete_node(&self.wmode, affected)?;
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms")
+        }
+        Ok(())
+    }
+}
+impl RelationWriter for RelationsWriterService {
+    #[tracing::instrument(skip_all)]
+    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = graph.shard_id.as_ref().map(|s| &s.id);
+        let mut writer = self.index.start_writing()?;
+        for node in graph.nodes.iter() {
+            let name = node.value.clone();
+            let (xtype, subtype) = node_type_parsing(node.ntype(), &node.subtype);
+            let node = IoNode::new(name, xtype.to_string(), subtype.map(|s| s.to_string()));
+            if let Some(id) = writer.get_node_id(node.hash())? {
+                self.delete_node(&mut writer, id)?;
+            }
+        }
+        let result = Ok(writer.commit(&mut self.wmode)?);
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("delete_nodes".to_string());
+        metrics.record_request_time(metric, took);
+        debug!("{id:?} - Ending at {took} ms");
+
+        result
+    }
+    #[tracing::instrument(skip_all)]
+    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Creating nodes: starts {v} ms");
+        }
+
+        let mut writer = self.index.start_writing()?;
+        let nodes: HashMap<_, _> = graph
+            .nodes
+            .iter()
+            .map(|(k, v)| (k, v.value.clone(), node_type_parsing(v.ntype(), &v.subtype)))
+            .map(|(key, value, (xtype, subtype))| (key, value, xtype, subtype))
+            .map(|(key, value, xtype, subtype)| {
+                (
+                    key,
+                    value,
+                    xtype.to_string(),
+                    subtype.map(|s| s.to_string()),
+                )
+            })
+            .map(|(&key, value, xtype, subtype)| (key, IoNode::user_node(value, xtype, subtype)))
+            .collect();
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Creating nodes: ends {v} ms");
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Populating the graph: starts {v} ms");
+        }
+
+        let ubehaviour = || Err(InnerErr::UBehaviour);
+        for protos_edge in graph.edges.iter() {
+            let from = nodes.get(&protos_edge.source).map_or_else(ubehaviour, Ok)?;
+            let to = nodes.get(&protos_edge.target).map_or_else(ubehaviour, Ok)?;
+            let edge = relation_type_parsing(protos_edge.rtype(), &protos_edge.rsubtype);
+            let edge = IoEdge::new(edge.0.to_string(), edge.1.map(|s| s.to_string()));
+            let metadata = protos_edge.metadata.clone().map(IoEdgeMetadata::from);
+            writer.connect(&self.wmode, from, to, &edge, metadata.as_ref())?;
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Populating the graph: ends {v} ms");
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Ending at {v} ms")
+        }
+
+        let result = Ok(writer.commit(&mut self.wmode)?);
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("join_graph".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+}
+impl std::fmt::Debug for RelationsWriterService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("RelationWriterService").finish()
+    }
+}
+
+impl WriterChild for RelationsWriterService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let time = SystemTime::now();
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Count starting at {v} ms");
+        }
+        let count = self
+            .index
+            .start_reading()
+            .and_then(|reader| reader.no_nodes())?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("Ending at {v} ms")
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("count".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(count as usize)
+    }
+    #[tracing::instrument(skip_all)]
+    fn delete_resource(&mut self, x: &ResourceId) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = Some(&x.shard_id);
+        let node = IoNode::new(x.uuid.clone(), dictionary::ENTITY.to_string(), None);
+        let mut writer = self.index.start_writing()?;
+        if let Some(id) = writer.get_node_id(node.hash())? {
+            self.delete_node(&mut writer, id)?;
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms")
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("delete_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
+        let time = SystemTime::now();
+
+        let id = Some(&resource.shard_id);
+        if resource.status != ResourceStatus::Delete as i32 {
+            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+                debug!("{id:?} - Populating the graph: starts {v} ms");
+            }
+            let iter = resource
+                .relations
+                .iter()
+                .filter(|rel| rel.to.is_some() || rel.source.is_some());
+            let mut writer = self.index.start_writing()?;
+            for rel in iter {
+                let edge = relation_type_parsing(rel.relation(), &rel.relation_label);
+                let from = rel.source.as_ref().unwrap();
+                let from_type = node_type_parsing(from.ntype(), &from.subtype);
+                let to = rel.to.as_ref().unwrap();
+                let to_type = node_type_parsing(to.ntype(), &to.subtype);
+                let from = IoNode::system_node(
+                    from.value.clone(),
+                    from_type.0.to_string(),
+                    from_type.1.map(|s| s.to_string()),
+                );
+                let to = IoNode::system_node(
+                    to.value.clone(),
+                    to_type.0.to_string(),
+                    to_type.1.map(|s| s.to_string()),
+                );
+                let edge = IoEdge::new(edge.0.to_string(), edge.1.map(|s| s.to_string()));
+                let metadata = rel.metadata.clone().map(IoEdgeMetadata::from);
+                writer.connect(&self.wmode, &from, &to, &edge, metadata.as_ref())?;
+            }
+            writer.commit(&mut self.wmode)?;
+            if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+                debug!("{id:?} - Populating the graph: ends {v} ms");
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms")
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("set_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(())
+    }
+    fn garbage_collection(&mut self) -> NodeResult<()> {
+        Ok(())
+    }
+}
+
+impl RelationsWriterService {
+    pub fn start(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            match RelationsWriterService::new(config) {
+                Err(e) if path.exists() => {
+                    std::fs::remove_dir(path)?;
+                    Err(e)
+                }
+                Err(e) => Err(e),
+                Ok(v) => Ok(v),
+            }
+        } else {
+            Ok(RelationsWriterService::open(config)?)
+        }
+    }
+    pub fn new(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if path.exists() {
+            Err(node_error!("Shard does exist".to_string()))
+        } else {
+            std::fs::create_dir(path)?;
+            let (index, wmode) = Index::new_writer(path)?;
+            Ok(RelationsWriterService { index, wmode })
+        }
+    }
+
+    pub fn open(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            Err(node_error!("Shard does not exist".to_string()))
+        } else {
+            let (index, wmode) = Index::new_writer(path)?;
+            Ok(RelationsWriterService { index, wmode })
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/reader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/reader.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,788 +1,737 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashMap;
-use std::fmt::Debug;
-use std::time::*;
-
-use itertools::Itertools;
-use nucliadb_core::metrics;
-use nucliadb_core::metrics::request_time;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::order_by::{OrderField, OrderType};
-use nucliadb_core::protos::{
-    DocumentItem, DocumentResult, DocumentSearchRequest, DocumentSearchResponse, FacetResult,
-    FacetResults, OrderBy, ResourceId, ResultScore, StreamRequest,
-};
-use nucliadb_core::tracing::{self, *};
-use tantivy::collector::{Collector, Count, DocSetCollector, FacetCollector, FacetCounts, TopDocs};
-use tantivy::query::{AllQuery, Query, QueryParser, TermQuery};
-use tantivy::schema::*;
-use tantivy::{DocAddress, Index, IndexReader, LeasedItem, ReloadPolicy, Searcher};
-
-use super::schema::TextSchema;
-use super::search_query;
-
-fn facet_count(facet: &str, facets_count: &FacetCounts) -> Vec<FacetResult> {
-    facets_count
-        .top_k(facet, 50)
-        .into_iter()
-        .map(|(facet, count)| FacetResult {
-            tag: facet.to_string(),
-            total: count as i32,
-        })
-        .collect()
-}
-
-fn produce_facets(facets: Vec<String>, facets_count: FacetCounts) -> HashMap<String, FacetResults> {
-    facets
-        .into_iter()
-        .map(|facet| (&facets_count, facet))
-        .map(|(facets_count, facet)| (facet_count(&facet, facets_count), facet))
-        .filter(|(r, _)| !r.is_empty())
-        .map(|(facetresults, facet)| (facet, FacetResults { facetresults }))
-        .collect()
-}
-
-pub struct SearchResponse<'a, S> {
-    pub query: &'a str,
-    pub facets_count: FacetCounts,
-    pub facets: Vec<String>,
-    pub top_docs: Vec<(S, DocAddress)>,
-    pub order_by: Option<OrderBy>,
-    pub page_number: i32,
-    pub results_per_page: i32,
-    pub total: usize,
-}
-
-pub struct TextReaderService {
-    index: Index,
-    pub schema: TextSchema,
-    pub reader: IndexReader,
-}
-
-impl Debug for TextReaderService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("FieldReaderService")
-            .field("index", &self.index)
-            .field("schema", &self.schema)
-            .finish()
-    }
-}
-
-impl FieldReader for TextReaderService {
-    #[tracing::instrument(skip_all)]
-    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator> {
-        let producer = BatchProducer {
-            offset: 0,
-            total: self.count()?,
-            field_field: self.schema.field,
-            uuid_field: self.schema.uuid,
-            facet_field: self.schema.facets,
-            searcher: self.reader.searcher(),
-            query: search_query::create_streaming_query(&self.schema, request),
-        };
-        Ok(DocumentIterator::new(producer.flatten()))
-    }
-
-    #[tracing::instrument(skip_all)]
-    fn count(&self) -> NodeResult<usize> {
-        let id: Option<String> = None;
-        let time = SystemTime::now();
-        let searcher = self.reader.searcher();
-        let count = searcher.search(&AllQuery, &Count).unwrap_or_default();
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Ending at: {v} ms");
-        }
-        Ok(count)
-    }
-}
-
-impl ReaderChild for TextReaderService {
-    type Request = DocumentSearchRequest;
-    type Response = DocumentSearchResponse;
-    #[tracing::instrument(skip_all)]
-    fn stop(&self) -> NodeResult<()> {
-        debug!("Stopping Reader Text Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
-        let time = SystemTime::now();
-
-        let result = self.do_search(request);
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::texts("search".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(result?)
-    }
-    #[tracing::instrument(skip_all)]
-    fn reload(&self) {
-        self.reader.reload().unwrap();
-    }
-    #[tracing::instrument(skip_all)]
-    fn stored_ids(&self) -> NodeResult<Vec<String>> {
-        let time = SystemTime::now();
-
-        let mut keys = vec![];
-        let searcher = self.reader.searcher();
-        for addr in searcher.search(&AllQuery, &DocSetCollector)? {
-            let Some(key) = searcher
-                .doc(addr)?
-                .get_first(self.schema.uuid)
-                .and_then(|i| i.as_text().map(String::from)) else { continue };
-            keys.push(key);
-        }
-
-        let metrics = metrics::get_metrics();
-        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
-        let metric = request_time::RequestTimeKey::texts("stored_ids".to_string());
-        metrics.record_request_time(metric, took);
-
-        Ok(keys)
-    }
-}
-
-impl TextReaderService {
-    fn custom_order_collector(
-        &self,
-        order: OrderBy,
-        limit: usize,
-        offset: usize,
-    ) -> impl Collector<Fruit = Vec<(u64, DocAddress)>> {
-        use tantivy::fastfield::{FastFieldReader, FastValue};
-        use tantivy::{DocId, SegmentReader};
-        let created = self.schema.created;
-        let modified = self.schema.modified;
-        let sorter = match order.r#type() {
-            OrderType::Desc => |t: u64| t,
-            OrderType::Asc => |t: u64| u64::MAX - t,
-        };
-        TopDocs::with_limit(limit).and_offset(offset).custom_score(
-            move |segment_reader: &SegmentReader| {
-                let reader = match order.sort_by() {
-                    OrderField::Created => segment_reader.fast_fields().date(created).unwrap(),
-                    OrderField::Modified => segment_reader.fast_fields().date(modified).unwrap(),
-                };
-                move |doc: DocId| sorter(reader.get(doc).to_u64())
-            },
-        )
-    }
-
-    pub fn find_one(&self, resource_id: &ResourceId) -> tantivy::Result<Option<Document>> {
-        let uuid_term = Term::from_field_text(self.schema.uuid, &resource_id.uuid);
-        let uuid_query = TermQuery::new(uuid_term, IndexRecordOption::Basic);
-        let searcher = self.reader.searcher();
-        searcher
-            .search(&uuid_query, &TopDocs::with_limit(1))?
-            .first()
-            .map(|(_, doc_address)| searcher.doc(*doc_address))
-            .transpose()
-    }
-
-    pub fn find_resource(&self, resource_id: &ResourceId) -> tantivy::Result<Vec<Document>> {
-        let uuid_field = self.schema.uuid;
-        let uuid_term = Term::from_field_text(uuid_field, &resource_id.uuid);
-        let uuid_query = TermQuery::new(uuid_term, IndexRecordOption::Basic);
-
-        let searcher = self.reader.searcher();
-
-        let top_docs = searcher.search(&uuid_query, &TopDocs::with_limit(1000))?;
-        let mut docs = Vec::with_capacity(1000);
-
-        for (_score, doc_address) in top_docs {
-            let doc = searcher.doc(doc_address)?;
-            docs.push(doc);
-        }
-
-        Ok(docs)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn start(config: &TextConfig) -> NodeResult<Self> {
-        if !config.path.exists() {
-            return Err(node_error!("Invalid path {:?}", config.path));
-        }
-        let field_schema = TextSchema::new();
-        let index = Index::open_in_dir(&config.path)?;
-
-        let reader = index
-            .reader_builder()
-            .reload_policy(ReloadPolicy::OnCommit)
-            .try_into()?;
-
-        Ok(TextReaderService {
-            index,
-            reader,
-            schema: field_schema,
-        })
-    }
-
-    fn convert_int_order(
-        &self,
-        response: SearchResponse<u64>,
-        searcher: &Searcher,
-    ) -> DocumentSearchResponse {
-        let total = response.total as i32;
-        let retrieved_results = (response.page_number + 1) * response.results_per_page;
-        let next_page = total > retrieved_results;
-        let mut results = Vec::with_capacity(response.top_docs.len());
-        for (id, (_, doc_address)) in response.top_docs.into_iter().enumerate() {
-            match searcher.doc(doc_address) {
-                Ok(doc) => {
-                    let score = Some(ResultScore {
-                        bm25: 0.0,
-                        booster: id as f32,
-                    });
-                    let uuid = doc
-                        .get_first(self.schema.uuid)
-                        .expect("document doesn't appear to have uuid.")
-                        .as_text()
-                        .unwrap()
-                        .to_string();
-
-                    let field = doc
-                        .get_first(self.schema.field)
-                        .expect("document doesn't appear to have field.")
-                        .as_facet()
-                        .unwrap()
-                        .to_path_string();
-
-                    let labels = doc
-                        .get_all(self.schema.facets)
-                        .map(|x| x.as_facet().unwrap().to_path_string())
-                        .filter(|x| x.starts_with("/l/"))
-                        .collect_vec();
-
-                    let result = DocumentResult {
-                        uuid,
-                        field,
-                        score,
-                        labels,
-                    };
-                    results.push(result);
-                }
-                Err(e) => error!("Error retrieving document from index: {}", e),
-            }
-        }
-
-        let facets = produce_facets(response.facets, response.facets_count);
-        DocumentSearchResponse {
-            total,
-            results,
-            facets,
-            page_number: response.page_number,
-            result_per_page: response.results_per_page,
-            query: response.query.to_string(),
-            next_page,
-            bm25: false,
-        }
-    }
-
-    fn convert_bm25_order(
-        &self,
-        response: SearchResponse<f32>,
-        searcher: &Searcher,
-    ) -> DocumentSearchResponse {
-        let total = response.total as i32;
-        let retrieved_results = (response.page_number + 1) * response.results_per_page;
-        let next_page = total > retrieved_results;
-        let results_per_page = response.results_per_page as usize;
-        let result_stream = response
-            .top_docs
-            .into_iter()
-            .take(results_per_page)
-            .enumerate();
-
-        let mut results = Vec::with_capacity(results_per_page);
-        for (id, (score, doc_address)) in result_stream {
-            match searcher.doc(doc_address) {
-                Ok(doc) => {
-                    let score = Some(ResultScore {
-                        bm25: score,
-                        booster: id as f32,
-                    });
-                    let uuid = doc
-                        .get_first(self.schema.uuid)
-                        .expect("document doesn't appear to have uuid.")
-                        .as_text()
-                        .unwrap()
-                        .to_string();
-
-                    let field = doc
-                        .get_first(self.schema.field)
-                        .expect("document doesn't appear to have field.")
-                        .as_facet()
-                        .unwrap()
-                        .to_path_string();
-
-                    let labels = doc
-                        .get_all(self.schema.facets)
-                        .flat_map(|x| x.as_facet())
-                        .map(|x| x.to_path_string())
-                        .filter(|x| x.starts_with("/l/"))
-                        .collect_vec();
-
-                    let result = DocumentResult {
-                        uuid,
-                        field,
-                        score,
-                        labels,
-                    };
-                    results.push(result);
-                }
-                Err(e) => error!("Error retrieving document from index: {}", e),
-            }
-        }
-
-        let facets = produce_facets(response.facets, response.facets_count);
-        DocumentSearchResponse {
-            results,
-            facets,
-            total: response.total as i32,
-            page_number: response.page_number,
-            result_per_page: response.results_per_page,
-            query: response.query.to_string(),
-            next_page,
-            bm25: true,
-        }
-    }
-
-    fn adapt_text(parser: &QueryParser, text: &str) -> String {
-        match text {
-            "" => text.to_string(),
-            text => parser
-                .parse_query(text)
-                .map(|_| text.to_string())
-                .unwrap_or_else(|_| format!("\"{}\"", text.replace('"', ""))),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    fn do_search(
-        &self,
-        request: &DocumentSearchRequest,
-    ) -> tantivy::Result<DocumentSearchResponse> {
-        use crate::search_query::create_query;
-        let id = Some(&request.id);
-        let time = SystemTime::now();
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating query: starts at {v} ms");
-        }
-        let query_parser = {
-            let mut query_parser = QueryParser::for_index(&self.index, vec![self.schema.text]);
-            query_parser.set_conjunction_by_default();
-            query_parser
-        };
-        let text = TextReaderService::adapt_text(&query_parser, &request.body);
-        let advanced_query = request
-            .advanced_query
-            .as_ref()
-            .map(|query| query_parser.parse_query(query))
-            .transpose()?;
-        let query = create_query(&query_parser, request, &self.schema, &text, advanced_query);
-
-        // Offset to search from
-        let results = request.result_per_page as usize;
-        let offset = results * request.page_number as usize;
-        let extra_result = results + 1;
-        let maybe_order = request.order.clone();
-        let valid_facet_iter = request.faceted.iter().flat_map(|v| {
-            v.tags
-                .iter()
-                .filter(|s| TextReaderService::is_valid_facet(s))
-        });
-
-        let mut facets = vec![];
-        let mut facet_collector = FacetCollector::for_field(self.schema.facets);
-        for facet in valid_facet_iter {
-            facets.push(facet.clone());
-            facet_collector.add_facet(Facet::from(facet));
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Creating query: ends at {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Searching: starts at {v} ms");
-        }
-        let searcher = self.reader.searcher();
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("{id:?} - Searching: ends at {v} ms");
-        }
-        match maybe_order {
-            _ if request.only_faceted => {
-                // Just a facet search
-                let facets_count = searcher.search(&query, &facet_collector).unwrap();
-                Ok(DocumentSearchResponse {
-                    facets: produce_facets(facets, facets_count),
-                    ..Default::default()
-                })
-            }
-            Some(order_by) => {
-                let topdocs_collector = self.custom_order_collector(order_by, extra_result, offset);
-                let multicollector = &(facet_collector, topdocs_collector, Count);
-                let (facets_count, top_docs, total) = searcher.search(&query, multicollector)?;
-                let result = self.convert_int_order(
-                    SearchResponse {
-                        facets_count,
-                        facets,
-                        top_docs,
-                        total,
-                        query: &text,
-                        order_by: request.order.clone(),
-                        page_number: request.page_number,
-                        results_per_page: results as i32,
-                    },
-                    &searcher,
-                );
-                Ok(result)
-            }
-            None => {
-                let topdocs_collector = TopDocs::with_limit(extra_result).and_offset(offset);
-                let multicollector = &(facet_collector, topdocs_collector, Count);
-                let (facets_count, top_docs, total) = searcher.search(&query, multicollector)?;
-                let result = self.convert_bm25_order(
-                    SearchResponse {
-                        facets_count,
-                        facets,
-                        top_docs,
-                        total,
-                        query: &text,
-                        order_by: request.order.clone(),
-                        page_number: request.page_number,
-                        results_per_page: results as i32,
-                    },
-                    &searcher,
-                );
-                Ok(result)
-            }
-        }
-    }
-    fn is_valid_facet(maybe_facet: &str) -> bool {
-        Facet::from_text(maybe_facet).is_ok()
-    }
-}
-
-pub struct BatchProducer {
-    total: usize,
-    offset: usize,
-    query: Box<dyn Query>,
-    field_field: Field,
-    uuid_field: Field,
-    facet_field: Field,
-    searcher: LeasedItem<tantivy::Searcher>,
-}
-impl BatchProducer {
-    const BATCH: usize = 1000;
-}
-impl Iterator for BatchProducer {
-    type Item = Vec<DocumentItem>;
-    fn next(&mut self) -> Option<Self::Item> {
-        let time = SystemTime::now();
-        if self.offset >= self.total {
-            debug!("No more batches available");
-            return None;
-        }
-        debug!("Producing a new batch with offset: {}", self.offset);
-        let top_docs = TopDocs::with_limit(Self::BATCH).and_offset(self.offset);
-        let top_docs = self.searcher.search(&self.query, &top_docs).unwrap();
-        let mut items = vec![];
-        for doc in top_docs.into_iter().flat_map(|i| self.searcher.doc(i.1)) {
-            let uuid = doc
-                .get_first(self.uuid_field)
-                .expect("document doesn't appear to have uuid.")
-                .as_text()
-                .unwrap()
-                .to_string();
-
-            let field = doc
-                .get_first(self.field_field)
-                .expect("document doesn't appear to have field.")
-                .as_facet()
-                .unwrap()
-                .to_path_string();
-
-            let labels = doc
-                .get_all(self.facet_field)
-                .flat_map(|x| x.as_facet())
-                .map(|x| x.to_path_string())
-                .filter(|x| x.starts_with("/l/"))
-                .collect_vec();
-            items.push(DocumentItem {
-                field,
-                uuid,
-                labels,
-            });
-        }
-        self.offset += Self::BATCH;
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            debug!("New batch created, took {v} ms");
-        }
-        Some(items)
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashMap;
-    use std::time::SystemTime;
-
-    use nucliadb_core::protos::prost_types::Timestamp;
-    use nucliadb_core::protos::resource::ResourceStatus;
-    use nucliadb_core::protos::{
-        Faceted, Filter, IndexMetadata, OrderBy, Resource, ResourceId, TextInformation, Timestamps,
-    };
-    use nucliadb_core::NodeResult;
-    use tempfile::TempDir;
-
-    use super::*;
-    use crate::writer::TextWriterService;
-
-    fn create_resource(shard_id: String) -> Resource {
-        let resource_id = ResourceId {
-            shard_id: shard_id.to_string(),
-            uuid: "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string(),
-        };
-
-        let now = SystemTime::now()
-            .duration_since(SystemTime::UNIX_EPOCH)
-            .unwrap();
-        let timestamp = Timestamp {
-            seconds: now.as_secs() as i64,
-            nanos: 0,
-        };
-
-        let metadata = IndexMetadata {
-            created: Some(timestamp.clone()),
-            modified: Some(timestamp),
-        };
-
-        const DOC1_TI: &str = "This is the first document";
-        const DOC1_P1: &str = "This is the text of the second paragraph.";
-        const DOC1_P2: &str = "This should be enough to test the tantivy.";
-        const DOC1_P3: &str = "But I wanted to make it three anyway.";
-
-        let ti_title = TextInformation {
-            text: DOC1_TI.to_string(),
-            labels: vec!["/l/mylabel".to_string(), "/e/myentity".to_string()],
-        };
-
-        let ti_body = TextInformation {
-            text: DOC1_P1.to_string() + DOC1_P2 + DOC1_P3,
-            labels: vec!["/f/body".to_string(), "/l/mylabel2".to_string()],
-        };
-
-        let mut texts = HashMap::new();
-        texts.insert("title".to_string(), ti_title);
-        texts.insert("body".to_string(), ti_body);
-
-        Resource {
-            resource: Some(resource_id),
-            metadata: Some(metadata),
-            texts,
-            status: ResourceStatus::Processed as i32,
-            labels: vec![],
-            paragraphs: HashMap::new(),
-            paragraphs_to_delete: vec![],
-            sentences_to_delete: vec![],
-            relations_to_delete: vec![],
-            relations: vec![],
-            vectors: HashMap::default(),
-            vectors_to_delete: HashMap::default(),
-            shard_id,
-        }
-    }
-
-    #[test]
-    fn test_new_reader() -> NodeResult<()> {
-        let dir = TempDir::new().unwrap();
-        let fsc = TextConfig {
-            path: dir.path().join("texts"),
-        };
-
-        let mut field_writer_service = TextWriterService::start(&fsc).unwrap();
-        let resource1 = create_resource("shard1".to_string());
-        let _ = field_writer_service.set_resource(&resource1);
-
-        let field_reader_service = TextReaderService::start(&fsc).unwrap();
-
-        let rid = ResourceId {
-            shard_id: "shard1".to_string(),
-            uuid: "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string(),
-        };
-        let result = field_reader_service.find_one(&rid).unwrap();
-        assert!(result.is_some());
-
-        let filter = Filter {
-            tags: vec!["/l/mylabel2".to_string()],
-        };
-
-        let faceted = Faceted {
-            tags: vec!["/".to_string(), "/l".to_string(), "/t".to_string()],
-        };
-
-        let now = SystemTime::now()
-            .duration_since(SystemTime::UNIX_EPOCH)
-            .unwrap();
-
-        let timestamp = Timestamp {
-            seconds: now.as_secs() as i64,
-            nanos: 0,
-        };
-
-        let old_timestamp = Timestamp {
-            seconds: 0_i64,
-            nanos: 0,
-        };
-
-        let timestamps = Timestamps {
-            from_modified: Some(old_timestamp.clone()),
-            to_modified: Some(timestamp.clone()),
-            from_created: Some(old_timestamp),
-            to_created: Some(timestamp),
-        };
-
-        let order = OrderBy {
-            sort_by: OrderField::Created as i32,
-            r#type: 0,
-            ..Default::default()
-        };
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "\"enough test\"".to_string(),
-            fields: vec!["body".to_string()],
-            filter: Some(filter.clone()),
-            faceted: Some(faceted.clone()),
-            order: Some(order.clone()),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps.clone()),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.total, 0);
-
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "enough test".to_string(),
-            fields: vec!["body".to_string()],
-            filter: Some(filter.clone()),
-            faceted: Some(faceted.clone()),
-            order: Some(order.clone()),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps.clone()),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.total, 1);
-
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "enough - test".to_string(),
-            fields: vec!["body".to_string()],
-            filter: Some(filter.clone()),
-            faceted: Some(faceted.clone()),
-            order: Some(order.clone()),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps.clone()),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.query, "\"enough - test\"");
-        assert_eq!(result.total, 0);
-
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "enough - test\"".to_string(),
-            fields: vec!["body".to_string()],
-            filter: Some(filter.clone()),
-            faceted: Some(faceted.clone()),
-            order: Some(order.clone()),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps.clone()),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.query, "\"enough - test\"");
-        assert_eq!(result.total, 0);
-
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "".to_string(),
-            fields: vec!["body".to_string()],
-            filter: None,
-            faceted: Some(faceted.clone()),
-            order: Some(order.clone()),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps.clone()),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.total, 1);
-
-        let search = DocumentSearchRequest {
-            id: "shard1".to_string(),
-            body: "".to_string(),
-            fields: vec!["body".to_string()],
-            filter: Some(filter),
-            faceted: Some(faceted),
-            order: Some(order),
-            page_number: 0,
-            result_per_page: 20,
-            timestamps: Some(timestamps),
-            reload: false,
-            only_faceted: false,
-            ..Default::default()
-        };
-
-        let result = field_reader_service.search(&search).unwrap();
-        assert_eq!(result.total, 1);
-
-        let request = StreamRequest {
-            shard_id: None,
-            filter: None,
-            reload: false,
-            ..Default::default()
-        };
-        let iter = field_reader_service.iterator(&request).unwrap();
-        let count = iter.count();
-        assert_eq!(count, 2);
-
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::collections::HashMap;
+use std::fmt::Debug;
+use std::time::*;
+
+use itertools::Itertools;
+use nucliadb_core::metrics;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::order_by::{OrderField, OrderType};
+use nucliadb_core::protos::{
+    DocumentItem, DocumentResult, DocumentSearchRequest, DocumentSearchResponse, FacetResult,
+    FacetResults, OrderBy, ResultScore, StreamRequest,
+};
+use nucliadb_core::tracing::{self, *};
+use tantivy::collector::{Collector, Count, DocSetCollector, FacetCollector, FacetCounts, TopDocs};
+use tantivy::query::{AllQuery, Query, QueryParser};
+use tantivy::schema::*;
+use tantivy::{DocAddress, Index, IndexReader, LeasedItem, ReloadPolicy, Searcher};
+
+use super::schema::TextSchema;
+use super::search_query;
+
+fn facet_count(facet: &str, facets_count: &FacetCounts) -> Vec<FacetResult> {
+    facets_count
+        .top_k(facet, 50)
+        .into_iter()
+        .map(|(facet, count)| FacetResult {
+            tag: facet.to_string(),
+            total: count as i32,
+        })
+        .collect()
+}
+
+fn produce_facets(facets: Vec<String>, facets_count: FacetCounts) -> HashMap<String, FacetResults> {
+    facets
+        .into_iter()
+        .map(|facet| (&facets_count, facet))
+        .map(|(facets_count, facet)| (facet_count(&facet, facets_count), facet))
+        .filter(|(r, _)| !r.is_empty())
+        .map(|(facetresults, facet)| (facet, FacetResults { facetresults }))
+        .collect()
+}
+
+pub struct SearchResponse<'a, S> {
+    pub query: &'a str,
+    pub facets_count: FacetCounts,
+    pub facets: Vec<String>,
+    pub top_docs: Vec<(S, DocAddress)>,
+    pub order_by: Option<OrderBy>,
+    pub page_number: i32,
+    pub results_per_page: i32,
+    pub total: usize,
+}
+
+pub struct TextReaderService {
+    index: Index,
+    pub schema: TextSchema,
+    pub reader: IndexReader,
+}
+
+impl Debug for TextReaderService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("FieldReaderService")
+            .field("index", &self.index)
+            .field("schema", &self.schema)
+            .finish()
+    }
+}
+
+impl FieldReader for TextReaderService {
+    #[tracing::instrument(skip_all)]
+    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator> {
+        let producer = BatchProducer {
+            offset: 0,
+            total: self.count()?,
+            field_field: self.schema.field,
+            uuid_field: self.schema.uuid,
+            facet_field: self.schema.facets,
+            searcher: self.reader.searcher(),
+            query: search_query::create_streaming_query(&self.schema, request),
+        };
+        Ok(DocumentIterator::new(producer.flatten()))
+    }
+
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let id: Option<String> = None;
+        let time = SystemTime::now();
+        let searcher = self.reader.searcher();
+        let count = searcher.search(&AllQuery, &Count)?;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at: {v} ms");
+        }
+        Ok(count)
+    }
+}
+
+impl ReaderChild for TextReaderService {
+    type Request = DocumentSearchRequest;
+    type Response = DocumentSearchResponse;
+    #[tracing::instrument(skip_all)]
+    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
+        let time = SystemTime::now();
+
+        let result = self.do_search(request)?;
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::texts("search".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(result)
+    }
+    #[tracing::instrument(skip_all)]
+    fn stored_ids(&self) -> NodeResult<Vec<String>> {
+        let time = SystemTime::now();
+
+        let mut keys = vec![];
+        let searcher = self.reader.searcher();
+        for addr in searcher.search(&AllQuery, &DocSetCollector)? {
+            let Some(key) = searcher
+                .doc(addr)?
+                .get_first(self.schema.uuid)
+                .and_then(|i| i.as_text().map(String::from))
+            else {
+                continue;
+            };
+            keys.push(key);
+        }
+
+        let metrics = metrics::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::texts("stored_ids".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(keys)
+    }
+}
+
+impl TextReaderService {
+    fn custom_order_collector(
+        &self,
+        order: OrderBy,
+        limit: usize,
+        offset: usize,
+    ) -> impl Collector<Fruit = Vec<(u64, DocAddress)>> {
+        use tantivy::fastfield::{FastFieldReader, FastValue};
+        use tantivy::{DocId, SegmentReader};
+        let created = self.schema.created;
+        let modified = self.schema.modified;
+        let sorter = match order.r#type() {
+            OrderType::Desc => |t: u64| t,
+            OrderType::Asc => |t: u64| u64::MAX - t,
+        };
+        TopDocs::with_limit(limit).and_offset(offset).custom_score(
+            move |segment_reader: &SegmentReader| {
+                let reader = match order.sort_by() {
+                    OrderField::Created => segment_reader.fast_fields().date(created).unwrap(),
+                    OrderField::Modified => segment_reader.fast_fields().date(modified).unwrap(),
+                };
+                move |doc: DocId| sorter(reader.get(doc).to_u64())
+            },
+        )
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &TextConfig) -> NodeResult<Self> {
+        if !config.path.exists() {
+            return Err(node_error!("Invalid path {:?}", config.path));
+        }
+        let field_schema = TextSchema::new();
+        let index = Index::open_in_dir(&config.path)?;
+
+        let reader = index
+            .reader_builder()
+            .reload_policy(ReloadPolicy::OnCommit)
+            .try_into()?;
+
+        Ok(TextReaderService {
+            index,
+            reader,
+            schema: field_schema,
+        })
+    }
+
+    fn convert_int_order(
+        &self,
+        response: SearchResponse<u64>,
+        searcher: &Searcher,
+    ) -> DocumentSearchResponse {
+        let total = response.total as i32;
+        let retrieved_results = (response.page_number + 1) * response.results_per_page;
+        let next_page = total > retrieved_results;
+        let mut results = Vec::with_capacity(response.top_docs.len());
+        for (id, (_, doc_address)) in response.top_docs.into_iter().enumerate() {
+            match searcher.doc(doc_address) {
+                Ok(doc) => {
+                    let score = Some(ResultScore {
+                        bm25: 0.0,
+                        booster: id as f32,
+                    });
+                    let uuid = doc
+                        .get_first(self.schema.uuid)
+                        .expect("document doesn't appear to have uuid.")
+                        .as_text()
+                        .unwrap()
+                        .to_string();
+
+                    let field = doc
+                        .get_first(self.schema.field)
+                        .expect("document doesn't appear to have field.")
+                        .as_facet()
+                        .unwrap()
+                        .to_path_string();
+
+                    let labels = doc
+                        .get_all(self.schema.facets)
+                        .map(|x| x.as_facet().unwrap().to_path_string())
+                        .filter(|x| x.starts_with("/l/"))
+                        .collect_vec();
+
+                    let result = DocumentResult {
+                        uuid,
+                        field,
+                        score,
+                        labels,
+                    };
+                    results.push(result);
+                }
+                Err(e) => error!("Error retrieving document from index: {}", e),
+            }
+        }
+
+        let facets = produce_facets(response.facets, response.facets_count);
+        DocumentSearchResponse {
+            total,
+            results,
+            facets,
+            page_number: response.page_number,
+            result_per_page: response.results_per_page,
+            query: response.query.to_string(),
+            next_page,
+            bm25: false,
+        }
+    }
+
+    fn convert_bm25_order(
+        &self,
+        response: SearchResponse<f32>,
+        searcher: &Searcher,
+    ) -> DocumentSearchResponse {
+        let total = response.total as i32;
+        let retrieved_results = (response.page_number + 1) * response.results_per_page;
+        let next_page = total > retrieved_results;
+        let results_per_page = response.results_per_page as usize;
+        let result_stream = response
+            .top_docs
+            .into_iter()
+            .take(results_per_page)
+            .enumerate();
+
+        let mut results = Vec::with_capacity(results_per_page);
+        for (id, (score, doc_address)) in result_stream {
+            match searcher.doc(doc_address) {
+                Ok(doc) => {
+                    let score = Some(ResultScore {
+                        bm25: score,
+                        booster: id as f32,
+                    });
+                    let uuid = doc
+                        .get_first(self.schema.uuid)
+                        .expect("document doesn't appear to have uuid.")
+                        .as_text()
+                        .unwrap()
+                        .to_string();
+
+                    let field = doc
+                        .get_first(self.schema.field)
+                        .expect("document doesn't appear to have field.")
+                        .as_facet()
+                        .unwrap()
+                        .to_path_string();
+
+                    let labels = doc
+                        .get_all(self.schema.facets)
+                        .flat_map(|x| x.as_facet())
+                        .map(|x| x.to_path_string())
+                        .filter(|x| x.starts_with("/l/"))
+                        .collect_vec();
+
+                    let result = DocumentResult {
+                        uuid,
+                        field,
+                        score,
+                        labels,
+                    };
+                    results.push(result);
+                }
+                Err(e) => error!("Error retrieving document from index: {}", e),
+            }
+        }
+
+        let facets = produce_facets(response.facets, response.facets_count);
+        DocumentSearchResponse {
+            results,
+            facets,
+            total: response.total as i32,
+            page_number: response.page_number,
+            result_per_page: response.results_per_page,
+            query: response.query.to_string(),
+            next_page,
+            bm25: true,
+        }
+    }
+
+    fn adapt_text(parser: &QueryParser, text: &str) -> String {
+        match text {
+            "" => text.to_string(),
+            text => parser
+                .parse_query(text)
+                .map(|_| text.to_string())
+                .unwrap_or_else(|_| format!("\"{}\"", text.replace('"', ""))),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    fn do_search(&self, request: &DocumentSearchRequest) -> NodeResult<DocumentSearchResponse> {
+        use crate::search_query::create_query;
+        let id = Some(&request.id);
+        let time = SystemTime::now();
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating query: starts at {v} ms");
+        }
+        let query_parser = {
+            let mut query_parser = QueryParser::for_index(&self.index, vec![self.schema.text]);
+            query_parser.set_conjunction_by_default();
+            query_parser
+        };
+        let text = TextReaderService::adapt_text(&query_parser, &request.body);
+        let advanced_query = request
+            .advanced_query
+            .as_ref()
+            .map(|query| query_parser.parse_query(query))
+            .transpose()?;
+        let query = create_query(&query_parser, request, &self.schema, &text, advanced_query);
+
+        // Offset to search from
+        let results = request.result_per_page as usize;
+        let offset = results * request.page_number as usize;
+        let extra_result = results + 1;
+        let maybe_order = request.order.clone();
+        let valid_facet_iter = request.faceted.iter().flat_map(|v| {
+            v.tags
+                .iter()
+                .filter(|s| TextReaderService::is_valid_facet(s))
+        });
+
+        let mut facets = vec![];
+        let mut facet_collector = FacetCollector::for_field(self.schema.facets);
+        for facet in valid_facet_iter {
+            facets.push(facet.clone());
+            facet_collector.add_facet(Facet::from(facet));
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Creating query: ends at {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Searching: starts at {v} ms");
+        }
+        let searcher = self.reader.searcher();
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Searching: ends at {v} ms");
+        }
+        match maybe_order {
+            _ if request.only_faceted => {
+                // Just a facet search
+                let facets_count = searcher.search(&query, &facet_collector)?;
+                Ok(DocumentSearchResponse {
+                    facets: produce_facets(facets, facets_count),
+                    ..Default::default()
+                })
+            }
+            Some(order_by) => {
+                let topdocs_collector = self.custom_order_collector(order_by, extra_result, offset);
+                let multicollector = &(facet_collector, topdocs_collector, Count);
+                let (facets_count, top_docs, total) = searcher.search(&query, multicollector)?;
+                let result = self.convert_int_order(
+                    SearchResponse {
+                        facets_count,
+                        facets,
+                        top_docs,
+                        total,
+                        query: &text,
+                        order_by: request.order.clone(),
+                        page_number: request.page_number,
+                        results_per_page: results as i32,
+                    },
+                    &searcher,
+                );
+                Ok(result)
+            }
+            None => {
+                let topdocs_collector = TopDocs::with_limit(extra_result).and_offset(offset);
+                let multicollector = &(facet_collector, topdocs_collector, Count);
+                let (facets_count, top_docs, total) = searcher.search(&query, multicollector)?;
+                let result = self.convert_bm25_order(
+                    SearchResponse {
+                        facets_count,
+                        facets,
+                        top_docs,
+                        total,
+                        query: &text,
+                        order_by: request.order.clone(),
+                        page_number: request.page_number,
+                        results_per_page: results as i32,
+                    },
+                    &searcher,
+                );
+                Ok(result)
+            }
+        }
+    }
+    fn is_valid_facet(maybe_facet: &str) -> bool {
+        Facet::from_text(maybe_facet).is_ok()
+    }
+}
+
+pub struct BatchProducer {
+    total: usize,
+    offset: usize,
+    query: Box<dyn Query>,
+    field_field: Field,
+    uuid_field: Field,
+    facet_field: Field,
+    searcher: LeasedItem<tantivy::Searcher>,
+}
+impl BatchProducer {
+    const BATCH: usize = 1000;
+}
+impl Iterator for BatchProducer {
+    type Item = Vec<DocumentItem>;
+    fn next(&mut self) -> Option<Self::Item> {
+        let time = SystemTime::now();
+        if self.offset >= self.total {
+            debug!("No more batches available");
+            return None;
+        }
+        debug!("Producing a new batch with offset: {}", self.offset);
+        let top_docs = TopDocs::with_limit(Self::BATCH).and_offset(self.offset);
+        let top_docs = self.searcher.search(&self.query, &top_docs).unwrap();
+        let mut items = vec![];
+        for doc in top_docs.into_iter().flat_map(|i| self.searcher.doc(i.1)) {
+            let uuid = doc
+                .get_first(self.uuid_field)
+                .expect("document doesn't appear to have uuid.")
+                .as_text()
+                .unwrap()
+                .to_string();
+
+            let field = doc
+                .get_first(self.field_field)
+                .expect("document doesn't appear to have field.")
+                .as_facet()
+                .unwrap()
+                .to_path_string();
+
+            let labels = doc
+                .get_all(self.facet_field)
+                .flat_map(|x| x.as_facet())
+                .map(|x| x.to_path_string())
+                .filter(|x| x.starts_with("/l/"))
+                .collect_vec();
+            items.push(DocumentItem {
+                field,
+                uuid,
+                labels,
+            });
+        }
+        self.offset += Self::BATCH;
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("New batch created, took {v} ms");
+        }
+        Some(items)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashMap;
+    use std::time::SystemTime;
+
+    use nucliadb_core::protos::prost_types::Timestamp;
+    use nucliadb_core::protos::resource::ResourceStatus;
+    use nucliadb_core::protos::{
+        Faceted, Filter, IndexMetadata, OrderBy, Resource, ResourceId, TextInformation, Timestamps,
+    };
+    use nucliadb_core::NodeResult;
+    use tempfile::TempDir;
+
+    use super::*;
+    use crate::writer::TextWriterService;
+
+    fn create_resource(shard_id: String) -> Resource {
+        let resource_id = ResourceId {
+            shard_id: shard_id.to_string(),
+            uuid: "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string(),
+        };
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64,
+            nanos: 0,
+        };
+
+        let metadata = IndexMetadata {
+            created: Some(timestamp.clone()),
+            modified: Some(timestamp),
+        };
+
+        const DOC1_TI: &str = "This is the first document";
+        const DOC1_P1: &str = "This is the text of the second paragraph.";
+        const DOC1_P2: &str = "This should be enough to test the tantivy.";
+        const DOC1_P3: &str = "But I wanted to make it three anyway.";
+
+        let ti_title = TextInformation {
+            text: DOC1_TI.to_string(),
+            labels: vec!["/l/mylabel".to_string(), "/e/myentity".to_string()],
+        };
+
+        let ti_body = TextInformation {
+            text: DOC1_P1.to_string() + DOC1_P2 + DOC1_P3,
+            labels: vec!["/f/body".to_string(), "/l/mylabel2".to_string()],
+        };
+
+        let mut texts = HashMap::new();
+        texts.insert("title".to_string(), ti_title);
+        texts.insert("body".to_string(), ti_body);
+
+        Resource {
+            resource: Some(resource_id),
+            metadata: Some(metadata),
+            texts,
+            status: ResourceStatus::Processed as i32,
+            labels: vec![],
+            paragraphs: HashMap::new(),
+            paragraphs_to_delete: vec![],
+            sentences_to_delete: vec![],
+            relations_to_delete: vec![],
+            relations: vec![],
+            vectors: HashMap::default(),
+            vectors_to_delete: HashMap::default(),
+            shard_id,
+        }
+    }
+
+    #[test]
+    fn test_new_reader() -> NodeResult<()> {
+        let dir = TempDir::new().unwrap();
+        let fsc = TextConfig {
+            path: dir.path().join("texts"),
+        };
+
+        let mut field_writer_service = TextWriterService::start(&fsc).unwrap();
+        let resource1 = create_resource("shard1".to_string());
+        let _ = field_writer_service.set_resource(&resource1);
+
+        let field_reader_service = TextReaderService::start(&fsc).unwrap();
+
+        let filter = Filter {
+            tags: vec!["/l/mylabel2".to_string()],
+        };
+
+        let faceted = Faceted {
+            tags: vec!["/".to_string(), "/l".to_string(), "/t".to_string()],
+        };
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64,
+            nanos: 0,
+        };
+
+        let old_timestamp = Timestamp {
+            seconds: 0_i64,
+            nanos: 0,
+        };
+
+        let timestamps = Timestamps {
+            from_modified: Some(old_timestamp.clone()),
+            to_modified: Some(timestamp.clone()),
+            from_created: Some(old_timestamp),
+            to_created: Some(timestamp),
+        };
+
+        let order = OrderBy {
+            sort_by: OrderField::Created as i32,
+            r#type: 0,
+            ..Default::default()
+        };
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "\"enough test\"".to_string(),
+            fields: vec!["body".to_string()],
+            filter: Some(filter.clone()),
+            faceted: Some(faceted.clone()),
+            order: Some(order.clone()),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps.clone()),
+            only_faceted: false,
+            ..Default::default()
+        };
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.total, 0);
+
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "enough test".to_string(),
+            fields: vec!["body".to_string()],
+            filter: Some(filter.clone()),
+            faceted: Some(faceted.clone()),
+            order: Some(order.clone()),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps.clone()),
+            only_faceted: false,
+            ..Default::default()
+        };
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.total, 1);
+
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "enough - test".to_string(),
+            fields: vec!["body".to_string()],
+            filter: Some(filter.clone()),
+            faceted: Some(faceted.clone()),
+            order: Some(order.clone()),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps.clone()),
+            only_faceted: false,
+            ..Default::default()
+        };
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.query, "\"enough - test\"");
+        assert_eq!(result.total, 0);
+
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "enough - test\"".to_string(),
+            fields: vec!["body".to_string()],
+            filter: Some(filter.clone()),
+            faceted: Some(faceted.clone()),
+            order: Some(order.clone()),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps.clone()),
+            only_faceted: false,
+            ..Default::default()
+        };
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.query, "\"enough - test\"");
+        assert_eq!(result.total, 0);
+
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "".to_string(),
+            fields: vec!["body".to_string()],
+            filter: None,
+            faceted: Some(faceted.clone()),
+            order: Some(order.clone()),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps.clone()),
+            only_faceted: false,
+            ..Default::default()
+        };
+
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.total, 1);
+
+        let search = DocumentSearchRequest {
+            id: "shard1".to_string(),
+            body: "".to_string(),
+            fields: vec!["body".to_string()],
+            filter: Some(filter),
+            faceted: Some(faceted),
+            order: Some(order),
+            page_number: 0,
+            result_per_page: 20,
+            timestamps: Some(timestamps),
+            only_faceted: false,
+            ..Default::default()
+        };
+
+        let result = field_reader_service.search(&search).unwrap();
+        assert_eq!(result.total, 1);
+
+        let request = StreamRequest {
+            shard_id: None,
+            filter: None,
+            ..Default::default()
+        };
+        let iter = field_reader_service.iterator(&request).unwrap();
+        let count = iter.count();
+        assert_eq!(count, 2);
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_texts/src/search_query.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_texts/src/search_query.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,220 +1,220 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-// use std::convert::TryFrom;
-// use std::time::SystemTime;
-
-use nucliadb_core::protos::stream_filter::Conjunction;
-use nucliadb_core::protos::{DocumentSearchRequest, StreamFilter, StreamRequest};
-use tantivy::query::*;
-use tantivy::schema::{Facet, IndexRecordOption};
-use tantivy::Term;
-
-use crate::schema::TextSchema;
-
-pub fn create_streaming_query(schema: &TextSchema, request: &StreamRequest) -> Box<dyn Query> {
-    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
-    queries.push((Occur::Must, Box::new(AllQuery)));
-
-    if let Some(ref filter) = request.filter {
-        queries.extend(create_stream_filter_queries(schema, filter))
-    }
-
-    Box::new(BooleanQuery::new(queries))
-}
-
-pub fn create_query(
-    parser: &QueryParser,
-    search: &DocumentSearchRequest,
-    schema: &TextSchema,
-    text: &str,
-    with_advance: Option<Box<dyn Query>>,
-) -> Box<dyn Query> {
-    let mut queries = vec![];
-    let main_q = if text.is_empty() {
-        Box::new(AllQuery)
-    } else {
-        parser
-            .parse_query(text)
-            .unwrap_or_else(|_| Box::new(AllQuery))
-    };
-    queries.push((Occur::Must, main_q));
-
-    // Fields
-    search
-        .fields
-        .iter()
-        .map(|value| format!("/{}", value))
-        .flat_map(|facet_key| Facet::from_text(facet_key.as_str()).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.field, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            queries.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    // Add filter
-    search
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            queries.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    // Status filters
-    if let Some(status) = search.with_status.map(|status| status as u64) {
-        let term = Term::from_field_u64(schema.status, status);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        queries.push((Occur::Must, Box::new(term_query)));
-    };
-
-    // Advance query
-    if let Some(query) = with_advance {
-        queries.push((Occur::Must, query));
-    }
-
-    if queries.len() == 1 && queries[0].1.is::<AllQuery>() {
-        queries.pop().unwrap().1
-    } else {
-        Box::new(BooleanQuery::new(queries))
-    }
-}
-
-fn create_stream_filter_queries(
-    schema: &TextSchema,
-    filter: &StreamFilter,
-) -> Vec<(Occur, Box<dyn Query>)> {
-    let mut queries = vec![];
-
-    let conjunction = Conjunction::from_i32(filter.conjunction)
-        .unwrap_or(Conjunction::And)
-        .into_occur();
-
-    filter
-        .tags
-        .iter()
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query: Box<dyn Query> =
-                Box::new(TermQuery::new(facet_term, IndexRecordOption::Basic));
-            queries.push((conjunction, facet_term_query))
-        });
-
-    queries
-}
-
-trait IntoOccur {
-    fn into_occur(self) -> Occur;
-}
-
-impl IntoOccur for Conjunction {
-    fn into_occur(self) -> Occur {
-        match self {
-            Conjunction::And => Occur::Must,
-            Conjunction::Or => Occur::Should,
-            Conjunction::Not => Occur::MustNot,
-        }
-    }
-}
-
-#[cfg(test)]
-#[allow(non_snake_case)]
-mod tests {
-    use super::*;
-
-    #[test]
-    fn test_stream_filter_query_per_tag() {
-        let schema = TextSchema::new();
-
-        let filter = StreamFilter::default();
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert!(queries.is_empty());
-
-        let filter = StreamFilter {
-            tags: vec!["/A".to_string(); 10],
-            ..Default::default()
-        };
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert_eq!(queries.len(), 10);
-    }
-
-    #[test]
-    fn test_default_stream_filter_queries_creation() {
-        let schema = TextSchema::new();
-        let filter = StreamFilter {
-            tags: vec!["/A".to_string(), "/B".to_string()],
-            ..Default::default()
-        };
-
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert_eq!(queries.len(), 2);
-        for (occur, _query) in queries.iter() {
-            assert_eq!(*occur, Occur::Must);
-        }
-    }
-
-    #[test]
-    fn test_AND_stream_filter_queries_creation() {
-        let schema = TextSchema::new();
-        let filter = StreamFilter {
-            tags: vec!["/A".to_string(), "/B".to_string()],
-            conjunction: Conjunction::And.into(),
-        };
-
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert_eq!(queries.len(), 2);
-        for (occur, _query) in queries.iter() {
-            assert_eq!(*occur, Occur::Must);
-        }
-    }
-
-    #[test]
-    fn test_OR_stream_filter_queries_creation() {
-        let schema = TextSchema::new();
-        let filter = StreamFilter {
-            tags: vec!["/A".to_string(), "/B".to_string()],
-            conjunction: Conjunction::Or.into(),
-        };
-
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert_eq!(queries.len(), 2);
-        for (occur, _query) in queries.iter() {
-            assert_eq!(*occur, Occur::Should);
-        }
-    }
-
-    #[test]
-    fn test_NOT_stream_filter_queries_creation() {
-        let schema = TextSchema::new();
-        let filter = StreamFilter {
-            tags: vec!["/A".to_string(), "/B".to_string()],
-            conjunction: Conjunction::Not.into(),
-        };
-
-        let queries = create_stream_filter_queries(&schema, &filter);
-        assert_eq!(queries.len(), 2);
-        for (occur, _query) in queries.iter() {
-            assert_eq!(*occur, Occur::MustNot);
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+// use std::convert::TryFrom;
+// use std::time::SystemTime;
+
+use nucliadb_core::protos::stream_filter::Conjunction;
+use nucliadb_core::protos::{DocumentSearchRequest, StreamFilter, StreamRequest};
+use tantivy::query::*;
+use tantivy::schema::{Facet, IndexRecordOption};
+use tantivy::Term;
+
+use crate::schema::TextSchema;
+
+pub fn create_streaming_query(schema: &TextSchema, request: &StreamRequest) -> Box<dyn Query> {
+    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
+    queries.push((Occur::Must, Box::new(AllQuery)));
+
+    if let Some(ref filter) = request.filter {
+        queries.extend(create_stream_filter_queries(schema, filter))
+    }
+
+    Box::new(BooleanQuery::new(queries))
+}
+
+pub fn create_query(
+    parser: &QueryParser,
+    search: &DocumentSearchRequest,
+    schema: &TextSchema,
+    text: &str,
+    with_advance: Option<Box<dyn Query>>,
+) -> Box<dyn Query> {
+    let mut queries = vec![];
+    let main_q = if text.is_empty() {
+        Box::new(AllQuery)
+    } else {
+        parser
+            .parse_query(text)
+            .unwrap_or_else(|_| Box::new(AllQuery))
+    };
+    queries.push((Occur::Must, main_q));
+
+    // Fields
+    search
+        .fields
+        .iter()
+        .map(|value| format!("/{}", value))
+        .flat_map(|facet_key| Facet::from_text(facet_key.as_str()).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.field, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            queries.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    // Add filter
+    search
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            queries.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    // Status filters
+    if let Some(status) = search.with_status.map(|status| status as u64) {
+        let term = Term::from_field_u64(schema.status, status);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        queries.push((Occur::Must, Box::new(term_query)));
+    };
+
+    // Advance query
+    if let Some(query) = with_advance {
+        queries.push((Occur::Must, query));
+    }
+
+    if queries.len() == 1 && queries[0].1.is::<AllQuery>() {
+        queries.pop().unwrap().1
+    } else {
+        Box::new(BooleanQuery::new(queries))
+    }
+}
+
+fn create_stream_filter_queries(
+    schema: &TextSchema,
+    filter: &StreamFilter,
+) -> Vec<(Occur, Box<dyn Query>)> {
+    let mut queries = vec![];
+
+    let conjunction = Conjunction::from_i32(filter.conjunction)
+        .unwrap_or(Conjunction::And)
+        .into_occur();
+
+    filter
+        .tags
+        .iter()
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query: Box<dyn Query> =
+                Box::new(TermQuery::new(facet_term, IndexRecordOption::Basic));
+            queries.push((conjunction, facet_term_query))
+        });
+
+    queries
+}
+
+trait IntoOccur {
+    fn into_occur(self) -> Occur;
+}
+
+impl IntoOccur for Conjunction {
+    fn into_occur(self) -> Occur {
+        match self {
+            Conjunction::And => Occur::Must,
+            Conjunction::Or => Occur::Should,
+            Conjunction::Not => Occur::MustNot,
+        }
+    }
+}
+
+#[cfg(test)]
+#[allow(non_snake_case)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_stream_filter_query_per_tag() {
+        let schema = TextSchema::new();
+
+        let filter = StreamFilter::default();
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert!(queries.is_empty());
+
+        let filter = StreamFilter {
+            tags: vec!["/A".to_string(); 10],
+            ..Default::default()
+        };
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert_eq!(queries.len(), 10);
+    }
+
+    #[test]
+    fn test_default_stream_filter_queries_creation() {
+        let schema = TextSchema::new();
+        let filter = StreamFilter {
+            tags: vec!["/A".to_string(), "/B".to_string()],
+            ..Default::default()
+        };
+
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert_eq!(queries.len(), 2);
+        for (occur, _query) in queries.iter() {
+            assert_eq!(*occur, Occur::Must);
+        }
+    }
+
+    #[test]
+    fn test_AND_stream_filter_queries_creation() {
+        let schema = TextSchema::new();
+        let filter = StreamFilter {
+            tags: vec!["/A".to_string(), "/B".to_string()],
+            conjunction: Conjunction::And.into(),
+        };
+
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert_eq!(queries.len(), 2);
+        for (occur, _query) in queries.iter() {
+            assert_eq!(*occur, Occur::Must);
+        }
+    }
+
+    #[test]
+    fn test_OR_stream_filter_queries_creation() {
+        let schema = TextSchema::new();
+        let filter = StreamFilter {
+            tags: vec!["/A".to_string(), "/B".to_string()],
+            conjunction: Conjunction::Or.into(),
+        };
+
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert_eq!(queries.len(), 2);
+        for (occur, _query) in queries.iter() {
+            assert_eq!(*occur, Occur::Should);
+        }
+    }
+
+    #[test]
+    fn test_NOT_stream_filter_queries_creation() {
+        let schema = TextSchema::new();
+        let filter = StreamFilter {
+            tags: vec!["/A".to_string(), "/B".to_string()],
+            conjunction: Conjunction::Not.into(),
+        };
+
+        let queries = create_stream_filter_queries(&schema, &filter);
+        assert_eq!(queries.len(), 2);
+        for (occur, _query) in queries.iter() {
+            assert_eq!(*occur, Occur::MustNot);
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/build.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/build.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io::Result;
-
-fn main() -> Result<()> {
-    println!("cargo:rerun-if-changed=../knowledgebox.proto");
-    println!("cargo:rerun-if-changed=../resources.proto");
-    println!("cargo:rerun-if-changed=../noderesources.proto");
-    println!("cargo:rerun-if-changed=../utils.proto");
-    println!("cargo:rerun-if-changed=../writer.proto");
-    println!("cargo:rerun-if-changed=../nodesidecar.proto");
-    println!("cargo:rerun-if-changed=../nodewriter.proto");
-    println!("cargo:rerun-if-changed=../nodereader.proto");
-
-    let mut prost_config = prost_build::Config::default();
-
-    prost_config.out_dir("src").compile_protos(
-        &[
-            "nucliadb_protos/utils.proto",
-            "nucliadb_protos/knowledgebox.proto",
-            "nucliadb_protos/resources.proto",
-            "nucliadb_protos/noderesources.proto",
-            "nucliadb_protos/writer.proto",
-            "nucliadb_protos/nodewriter.proto",
-            "nucliadb_protos/nodereader.proto",
-        ],
-        &["../../"],
-    )?;
-
-    tonic_build::configure()
-        .build_server(true)
-        .out_dir("src")
-        .compile(
-            &[
-                "nucliadb_protos/nodewriter.proto",
-                "nucliadb_protos/nodereader.proto",
-            ],
-            &["../../"],
-        )?;
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io::Result;
+
+fn main() -> Result<()> {
+    println!("cargo:rerun-if-changed=../knowledgebox.proto");
+    println!("cargo:rerun-if-changed=../resources.proto");
+    println!("cargo:rerun-if-changed=../noderesources.proto");
+    println!("cargo:rerun-if-changed=../utils.proto");
+    println!("cargo:rerun-if-changed=../writer.proto");
+    println!("cargo:rerun-if-changed=../nodesidecar.proto");
+    println!("cargo:rerun-if-changed=../nodewriter.proto");
+    println!("cargo:rerun-if-changed=../nodereader.proto");
+
+    let mut prost_config = prost_build::Config::default();
+
+    prost_config.out_dir("src").compile_protos(
+        &[
+            "nucliadb_protos/utils.proto",
+            "nucliadb_protos/knowledgebox.proto",
+            "nucliadb_protos/resources.proto",
+            "nucliadb_protos/noderesources.proto",
+            "nucliadb_protos/writer.proto",
+            "nucliadb_protos/nodewriter.proto",
+            "nucliadb_protos/nodereader.proto",
+        ],
+        &["../../"],
+    )?;
+
+    tonic_build::configure()
+        .build_server(true)
+        .out_dir("src")
+        .compile(
+            &[
+                "nucliadb_protos/nodewriter.proto",
+                "nucliadb_protos/nodereader.proto",
+            ],
+            &["../../"],
+        )?;
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/fdbwriter.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/fdbwriter.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,688 +1,691 @@
-// We receive this information throw an stream system
-
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Audit {
-    #[prost(string, tag="1")]
-    pub user: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="2")]
-    pub when: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(string, tag="3")]
-    pub origin: ::prost::alloc::string::String,
-    #[prost(enumeration="audit::Source", tag="4")]
-    pub source: i32,
-}
-/// Nested message and enum types in `Audit`.
-pub mod audit {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Source {
-        Http = 0,
-        Dashboard = 1,
-        Desktop = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Error {
-    #[prost(string, tag="1")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(enumeration="super::resources::FieldType", tag="2")]
-    pub field_type: i32,
-    #[prost(string, tag="3")]
-    pub error: ::prost::alloc::string::String,
-    #[prost(enumeration="error::ErrorCode", tag="4")]
-    pub code: i32,
-}
-/// Nested message and enum types in `Error`.
-pub mod error {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum ErrorCode {
-        Generic = 0,
-        Extract = 1,
-        Process = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BrokerMessage {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="4")]
-    pub slug: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub audit: ::core::option::Option<Audit>,
-    #[prost(enumeration="broker_message::MessageType", tag="6")]
-    pub r#type: i32,
-    #[prost(string, tag="7")]
-    pub multiid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="8")]
-    pub basic: ::core::option::Option<super::resources::Basic>,
-    #[prost(message, optional, tag="9")]
-    pub origin: ::core::option::Option<super::resources::Origin>,
-    #[prost(message, repeated, tag="10")]
-    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
-    /// Field Conversations
-    #[prost(map="string, message", tag="11")]
-    pub conversations: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::Conversation>,
-    /// Field Layout
-    #[prost(map="string, message", tag="12")]
-    pub layouts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLayout>,
-    /// Field Text
-    #[prost(map="string, message", tag="13")]
-    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldText>,
-    /// Field keyword
-    #[prost(map="string, message", tag="14")]
-    pub keywordsets: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldKeywordset>,
-    /// Field Datetime
-    #[prost(map="string, message", tag="15")]
-    pub datetimes: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldDatetime>,
-    /// Field Links
-    #[prost(map="string, message", tag="16")]
-    pub links: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLink>,
-    /// Field File
-    #[prost(map="string, message", tag="17")]
-    pub files: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldFile>,
-    /// Link extracted extra info
-    #[prost(message, repeated, tag="18")]
-    pub link_extracted_data: ::prost::alloc::vec::Vec<super::resources::LinkExtractedData>,
-    /// File extracted extra info
-    #[prost(message, repeated, tag="19")]
-    pub file_extracted_data: ::prost::alloc::vec::Vec<super::resources::FileExtractedData>,
-    /// Field Extracted/Computed information
-    #[prost(message, repeated, tag="20")]
-    pub extracted_text: ::prost::alloc::vec::Vec<super::resources::ExtractedTextWrapper>,
-    #[prost(message, repeated, tag="21")]
-    pub field_metadata: ::prost::alloc::vec::Vec<super::resources::FieldComputedMetadataWrapper>,
-    #[prost(message, repeated, tag="22")]
-    pub field_vectors: ::prost::alloc::vec::Vec<super::resources::ExtractedVectorsWrapper>,
-    /// Resource Large Computed Metadata
-    #[prost(message, repeated, tag="23")]
-    pub field_large_metadata: ::prost::alloc::vec::Vec<super::resources::LargeComputedMetadataWrapper>,
-    #[prost(message, repeated, tag="24")]
-    pub delete_fields: ::prost::alloc::vec::Vec<super::resources::FieldId>,
-    #[prost(int32, tag="25")]
-    pub origin_seq: i32,
-    #[prost(float, tag="26")]
-    pub slow_processing_time: f32,
-    #[prost(float, tag="28")]
-    pub pre_processing_time: f32,
-    #[prost(message, optional, tag="29")]
-    pub done_time: ::core::option::Option<::prost_types::Timestamp>,
-    /// Not needed anymore
-    #[deprecated]
-    #[prost(int64, tag="30")]
-    pub txseqid: i64,
-    #[prost(message, repeated, tag="31")]
-    pub errors: ::prost::alloc::vec::Vec<Error>,
-    #[prost(string, tag="32")]
-    pub processing_id: ::prost::alloc::string::String,
-    #[prost(enumeration="broker_message::MessageSource", tag="33")]
-    pub source: i32,
-    #[prost(int64, tag="34")]
-    pub account_seq: i64,
-    #[prost(message, repeated, tag="35")]
-    pub user_vectors: ::prost::alloc::vec::Vec<super::resources::UserVectorsWrapper>,
-    /// If true, force reindex all paragraphs in a resource
-    #[prost(bool, tag="36")]
-    pub reindex: bool,
-    #[prost(message, optional, tag="37")]
-    pub extra: ::core::option::Option<super::resources::Extra>,
-}
-/// Nested message and enum types in `BrokerMessage`.
-pub mod broker_message {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum MessageType {
-        Autocommit = 0,
-        Multi = 1,
-        Commit = 2,
-        Rollback = 3,
-        Delete = 4,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum MessageSource {
-        Writer = 0,
-        Processor = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct WriterStatusResponse {
-    #[prost(string, repeated, tag="1")]
-    pub knowledgeboxes: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// map of last message processed
-    #[prost(map="string, int64", tag="2")]
-    pub msgid: ::std::collections::HashMap<::prost::alloc::string::String, i64>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct WriterStatusRequest {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelsResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub labels: ::core::option::Option<super::knowledgebox::Labels>,
-    #[prost(enumeration="get_labels_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetLabelsResponse`.
-pub mod get_labels_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewEntitiesGroupRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewEntitiesGroupResponse {
-    #[prost(enumeration="new_entities_group_response::Status", tag="1")]
-    pub status: i32,
-}
-/// Nested message and enum types in `NewEntitiesGroupResponse`.
-pub mod new_entities_group_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Error = 1,
-        KbNotFound = 2,
-        AlreadyExists = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UpdateEntitiesGroupRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-    /// entity_id: Entity
-    #[prost(map="string, message", tag="3")]
-    pub add: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
-    /// entity_id: Entity
-    #[prost(map="string, message", tag="4")]
-    pub update: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
-    /// entity_id
-    #[prost(string, repeated, tag="5")]
-    pub delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(string, tag="6")]
-    pub title: ::prost::alloc::string::String,
-    #[prost(string, tag="7")]
-    pub color: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UpdateEntitiesGroupResponse {
-    #[prost(enumeration="update_entities_group_response::Status", tag="1")]
-    pub status: i32,
-}
-/// Nested message and enum types in `UpdateEntitiesGroupResponse`.
-pub mod update_entities_group_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Error = 1,
-        KbNotFound = 2,
-        EntitiesGroupNotFound = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListEntitiesGroupsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListEntitiesGroupsResponse {
-    #[prost(map="string, message", tag="1")]
-    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroupSummary>,
-    #[prost(enumeration="list_entities_groups_response::Status", tag="2")]
-    pub status: i32,
-}
-/// Nested message and enum types in `ListEntitiesGroupsResponse`.
-pub mod list_entities_groups_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(map="string, message", tag="2")]
-    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroup>,
-    #[prost(enumeration="get_entities_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetEntitiesResponse`.
-pub mod get_entities_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct MergeEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub from: ::core::option::Option<merge_entities_request::EntityId>,
-    #[prost(message, optional, tag="3")]
-    pub to: ::core::option::Option<merge_entities_request::EntityId>,
-}
-/// Nested message and enum types in `MergeEntitiesRequest`.
-pub mod merge_entities_request {
-    #[derive(Clone, PartialEq, ::prost::Message)]
-    pub struct EntityId {
-        #[prost(string, tag="1")]
-        pub group: ::prost::alloc::string::String,
-        #[prost(string, tag="2")]
-        pub entity: ::prost::alloc::string::String,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub labelset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelSetResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
-    #[prost(enumeration="get_label_set_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetLabelSetResponse`.
-pub mod get_label_set_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesGroupRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesGroupResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub group: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
-    #[prost(enumeration="get_entities_group_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetEntitiesGroupResponse`.
-pub mod get_entities_group_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        KbNotFound = 1,
-        EntitiesGroupNotFound = 2,
-        Error = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetVectorSetsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetVectorSetsResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub vectorsets: ::core::option::Option<super::knowledgebox::VectorSets>,
-    #[prost(enumeration="get_vector_sets_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetVectorSetsResponse`.
-pub mod get_vector_sets_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub vectorset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub vectorset: ::core::option::Option<super::knowledgebox::VectorSet>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OpStatusWriter {
-    #[prost(enumeration="op_status_writer::Status", tag="1")]
-    pub status: i32,
-}
-/// Nested message and enum types in `OpStatusWriter`.
-pub mod op_status_writer {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Error = 1,
-        Notfound = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Notification {
-    #[prost(int32, tag="1")]
-    pub partition: i32,
-    #[prost(string, tag="2")]
-    pub multi: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="4")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(int64, tag="5")]
-    pub seqid: i64,
-    #[prost(enumeration="notification::Action", tag="6")]
-    pub action: i32,
-    #[prost(enumeration="notification::WriteType", tag="7")]
-    pub write_type: i32,
-    #[prost(message, optional, tag="8")]
-    pub message: ::core::option::Option<BrokerMessage>,
-}
-/// Nested message and enum types in `Notification`.
-pub mod notification {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Action {
-        Commit = 0,
-        Abort = 1,
-        Indexed = 2,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum WriteType {
-        Unset = 0,
-        Created = 1,
-        Modified = 2,
-        Deleted = 3,
-    }
-}
-//// The member information.
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Member {
-    //// Member ID.A string of the UUID.
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    //// Cluster listen address. string of IP and port number.
-    //// E.g. 127.0.0.1:5000
-    #[prost(string, tag="2")]
-    pub listen_address: ::prost::alloc::string::String,
-    //// If true, it means self.
-    #[prost(bool, tag="3")]
-    pub is_self: bool,
-    //// Io, Ingest, Search, Train.
-    #[prost(enumeration="member::Type", tag="4")]
-    pub r#type: i32,
-    //// Dummy Member
-    #[prost(bool, tag="5")]
-    pub dummy: bool,
-    //// The load score of the member.
-    #[deprecated]
-    #[prost(float, tag="6")]
-    pub load_score: f32,
-    //// The number of shards in the node.
-    #[prost(uint32, tag="7")]
-    pub shard_count: u32,
-}
-/// Nested message and enum types in `Member`.
-pub mod member {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Type {
-        Io = 0,
-        Search = 1,
-        Ingest = 2,
-        Train = 3,
-        Unknown = 4,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListMembersRequest {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListMembersResponse {
-    #[prost(message, repeated, tag="1")]
-    pub members: ::prost::alloc::vec::Vec<Member>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardReplica {
-    #[prost(message, optional, tag="1")]
-    pub shard: ::core::option::Option<super::noderesources::ShardCreated>,
-    #[prost(string, tag="2")]
-    pub node: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardObject {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(message, repeated, tag="3")]
-    pub replicas: ::prost::alloc::vec::Vec<ShardReplica>,
-    #[prost(message, optional, tag="4")]
-    pub timestamp: ::core::option::Option<::prost_types::Timestamp>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Shards {
-    #[prost(message, repeated, tag="1")]
-    pub shards: ::prost::alloc::vec::Vec<ShardObject>,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    /// current shard that resources index to
-    #[prost(int32, tag="3")]
-    pub actual: i32,
-    #[deprecated]
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="4")]
-    pub similarity: i32,
-    #[prost(message, optional, tag="5")]
-    pub model: ::core::option::Option<super::knowledgebox::SemanticModelMetadata>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceFieldId {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(enumeration="super::resources::FieldType", tag="3")]
-    pub field_type: i32,
-    #[prost(string, tag="4")]
-    pub field: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexResource {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(bool, tag="3")]
-    pub reindex_vectors: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexStatus {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceFieldExistsResponse {
-    #[prost(bool, tag="1")]
-    pub found: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceIdRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub slug: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceIdResponse {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ExportRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorsRequest {
-    #[prost(message, optional, tag="1")]
-    pub vectors: ::core::option::Option<super::utils::VectorObject>,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="4")]
-    pub field: ::core::option::Option<super::resources::FieldId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorsResponse {
-    #[prost(bool, tag="1")]
-    pub found: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FileRequest {
-    #[prost(string, tag="1")]
-    pub bucket: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub key: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BinaryData {
-    #[prost(bytes="vec", tag="1")]
-    pub data: ::prost::alloc::vec::Vec<u8>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BinaryMetadata {
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub key: ::prost::alloc::string::String,
-    #[prost(int32, tag="4")]
-    pub size: i32,
-    #[prost(string, tag="5")]
-    pub filename: ::prost::alloc::string::String,
-    #[prost(string, tag="6")]
-    pub content_type: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UploadBinaryData {
-    #[prost(int32, tag="1")]
-    pub count: i32,
-    #[prost(oneof="upload_binary_data::Data", tags="2, 3")]
-    pub data: ::core::option::Option<upload_binary_data::Data>,
-}
-/// Nested message and enum types in `UploadBinaryData`.
-pub mod upload_binary_data {
-    #[derive(Clone, PartialEq, ::prost::Oneof)]
-    pub enum Data {
-        #[prost(message, tag="2")]
-        Metadata(super::BinaryMetadata),
-        #[prost(bytes, tag="3")]
-        Payload(::prost::alloc::vec::Vec<u8>),
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FileUploaded {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SynonymsRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetSynonymsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kbid: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetSynonymsResponse {
-    #[prost(message, optional, tag="1")]
-    pub status: ::core::option::Option<OpStatusWriter>,
-    #[prost(message, optional, tag="2")]
-    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
-}
+// We receive this information throw an stream system
+
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Audit {
+    #[prost(string, tag="1")]
+    pub user: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="2")]
+    pub when: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(string, tag="3")]
+    pub origin: ::prost::alloc::string::String,
+    #[prost(enumeration="audit::Source", tag="4")]
+    pub source: i32,
+}
+/// Nested message and enum types in `Audit`.
+pub mod audit {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Source {
+        Http = 0,
+        Dashboard = 1,
+        Desktop = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Error {
+    #[prost(string, tag="1")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(enumeration="super::resources::FieldType", tag="2")]
+    pub field_type: i32,
+    #[prost(string, tag="3")]
+    pub error: ::prost::alloc::string::String,
+    #[prost(enumeration="error::ErrorCode", tag="4")]
+    pub code: i32,
+}
+/// Nested message and enum types in `Error`.
+pub mod error {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum ErrorCode {
+        Generic = 0,
+        Extract = 1,
+        Process = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BrokerMessage {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="4")]
+    pub slug: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub audit: ::core::option::Option<Audit>,
+    #[prost(enumeration="broker_message::MessageType", tag="6")]
+    pub r#type: i32,
+    #[prost(string, tag="7")]
+    pub multiid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="8")]
+    pub basic: ::core::option::Option<super::resources::Basic>,
+    #[prost(message, optional, tag="9")]
+    pub origin: ::core::option::Option<super::resources::Origin>,
+    #[prost(message, repeated, tag="10")]
+    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
+    /// Field Conversations
+    #[prost(map="string, message", tag="11")]
+    pub conversations: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::Conversation>,
+    /// Field Layout
+    #[prost(map="string, message", tag="12")]
+    pub layouts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLayout>,
+    /// Field Text
+    #[prost(map="string, message", tag="13")]
+    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldText>,
+    /// Field keyword
+    #[prost(map="string, message", tag="14")]
+    pub keywordsets: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldKeywordset>,
+    /// Field Datetime
+    #[prost(map="string, message", tag="15")]
+    pub datetimes: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldDatetime>,
+    /// Field Links
+    #[prost(map="string, message", tag="16")]
+    pub links: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLink>,
+    /// Field File
+    #[prost(map="string, message", tag="17")]
+    pub files: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldFile>,
+    /// Link extracted extra info
+    #[prost(message, repeated, tag="18")]
+    pub link_extracted_data: ::prost::alloc::vec::Vec<super::resources::LinkExtractedData>,
+    /// File extracted extra info
+    #[prost(message, repeated, tag="19")]
+    pub file_extracted_data: ::prost::alloc::vec::Vec<super::resources::FileExtractedData>,
+    /// Field Extracted/Computed information
+    #[prost(message, repeated, tag="20")]
+    pub extracted_text: ::prost::alloc::vec::Vec<super::resources::ExtractedTextWrapper>,
+    #[prost(message, repeated, tag="21")]
+    pub field_metadata: ::prost::alloc::vec::Vec<super::resources::FieldComputedMetadataWrapper>,
+    #[prost(message, repeated, tag="22")]
+    pub field_vectors: ::prost::alloc::vec::Vec<super::resources::ExtractedVectorsWrapper>,
+    /// Resource Large Computed Metadata
+    #[prost(message, repeated, tag="23")]
+    pub field_large_metadata: ::prost::alloc::vec::Vec<super::resources::LargeComputedMetadataWrapper>,
+    #[prost(message, repeated, tag="24")]
+    pub delete_fields: ::prost::alloc::vec::Vec<super::resources::FieldId>,
+    #[prost(int32, tag="25")]
+    pub origin_seq: i32,
+    #[prost(float, tag="26")]
+    pub slow_processing_time: f32,
+    #[prost(float, tag="28")]
+    pub pre_processing_time: f32,
+    #[prost(message, optional, tag="29")]
+    pub done_time: ::core::option::Option<::prost_types::Timestamp>,
+    /// Not needed anymore
+    #[deprecated]
+    #[prost(int64, tag="30")]
+    pub txseqid: i64,
+    #[prost(message, repeated, tag="31")]
+    pub errors: ::prost::alloc::vec::Vec<Error>,
+    #[prost(string, tag="32")]
+    pub processing_id: ::prost::alloc::string::String,
+    #[prost(enumeration="broker_message::MessageSource", tag="33")]
+    pub source: i32,
+    #[prost(int64, tag="34")]
+    pub account_seq: i64,
+    #[prost(message, repeated, tag="35")]
+    pub user_vectors: ::prost::alloc::vec::Vec<super::resources::UserVectorsWrapper>,
+    /// If true, force reindex all paragraphs in a resource
+    #[prost(bool, tag="36")]
+    pub reindex: bool,
+    #[prost(message, optional, tag="37")]
+    pub extra: ::core::option::Option<super::resources::Extra>,
+}
+/// Nested message and enum types in `BrokerMessage`.
+pub mod broker_message {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum MessageType {
+        Autocommit = 0,
+        Multi = 1,
+        Commit = 2,
+        Rollback = 3,
+        Delete = 4,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum MessageSource {
+        Writer = 0,
+        Processor = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct WriterStatusResponse {
+    #[prost(string, repeated, tag="1")]
+    pub knowledgeboxes: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// map of last message processed
+    #[prost(map="string, int64", tag="2")]
+    pub msgid: ::std::collections::HashMap<::prost::alloc::string::String, i64>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct WriterStatusRequest {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelsResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub labels: ::core::option::Option<super::knowledgebox::Labels>,
+    #[prost(enumeration="get_labels_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetLabelsResponse`.
+pub mod get_labels_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewEntitiesGroupResponse {
+    #[prost(enumeration="new_entities_group_response::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `NewEntitiesGroupResponse`.
+pub mod new_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        KbNotFound = 2,
+        AlreadyExists = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UpdateEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    /// entity_id: Entity
+    #[prost(map="string, message", tag="3")]
+    pub add: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
+    /// entity_id: Entity
+    #[prost(map="string, message", tag="4")]
+    pub update: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
+    /// entity_id
+    #[prost(string, repeated, tag="5")]
+    pub delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(string, tag="6")]
+    pub title: ::prost::alloc::string::String,
+    #[prost(string, tag="7")]
+    pub color: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UpdateEntitiesGroupResponse {
+    #[prost(enumeration="update_entities_group_response::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `UpdateEntitiesGroupResponse`.
+pub mod update_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        KbNotFound = 2,
+        EntitiesGroupNotFound = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListEntitiesGroupsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListEntitiesGroupsResponse {
+    #[prost(map="string, message", tag="1")]
+    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroupSummary>,
+    #[prost(enumeration="list_entities_groups_response::Status", tag="2")]
+    pub status: i32,
+}
+/// Nested message and enum types in `ListEntitiesGroupsResponse`.
+pub mod list_entities_groups_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(map="string, message", tag="2")]
+    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroup>,
+    #[prost(enumeration="get_entities_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetEntitiesResponse`.
+pub mod get_entities_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct MergeEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub from: ::core::option::Option<merge_entities_request::EntityId>,
+    #[prost(message, optional, tag="3")]
+    pub to: ::core::option::Option<merge_entities_request::EntityId>,
+}
+/// Nested message and enum types in `MergeEntitiesRequest`.
+pub mod merge_entities_request {
+    #[derive(Clone, PartialEq, ::prost::Message)]
+    pub struct EntityId {
+        #[prost(string, tag="1")]
+        pub group: ::prost::alloc::string::String,
+        #[prost(string, tag="2")]
+        pub entity: ::prost::alloc::string::String,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub labelset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelSetResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
+    #[prost(enumeration="get_label_set_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetLabelSetResponse`.
+pub mod get_label_set_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesGroupResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub group: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+    #[prost(enumeration="get_entities_group_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetEntitiesGroupResponse`.
+pub mod get_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        KbNotFound = 1,
+        EntitiesGroupNotFound = 2,
+        Error = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetVectorSetsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetVectorSetsResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub vectorsets: ::core::option::Option<super::knowledgebox::VectorSets>,
+    #[prost(enumeration="get_vector_sets_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetVectorSetsResponse`.
+pub mod get_vector_sets_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub vectorset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub vectorset: ::core::option::Option<super::knowledgebox::VectorSet>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OpStatusWriter {
+    #[prost(enumeration="op_status_writer::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `OpStatusWriter`.
+pub mod op_status_writer {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        Notfound = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Notification {
+    #[prost(int32, tag="1")]
+    pub partition: i32,
+    #[prost(string, tag="2")]
+    pub multi: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="4")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(int64, tag="5")]
+    pub seqid: i64,
+    #[prost(enumeration="notification::Action", tag="6")]
+    pub action: i32,
+    #[prost(enumeration="notification::WriteType", tag="7")]
+    pub write_type: i32,
+    #[prost(message, optional, tag="8")]
+    pub message: ::core::option::Option<BrokerMessage>,
+}
+/// Nested message and enum types in `Notification`.
+pub mod notification {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Action {
+        Commit = 0,
+        Abort = 1,
+        Indexed = 2,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum WriteType {
+        Unset = 0,
+        Created = 1,
+        Modified = 2,
+        Deleted = 3,
+    }
+}
+//// The member information.
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Member {
+    //// Member ID.A string of the UUID.
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    //// Cluster listen address. string of IP and port number.
+    //// E.g. 127.0.0.1:5000
+    #[prost(string, tag="2")]
+    pub listen_address: ::prost::alloc::string::String,
+    //// If true, it means self.
+    #[deprecated]
+    #[prost(bool, tag="3")]
+    pub is_self: bool,
+    //// Io, Ingest, Search, Train.
+    #[deprecated]
+    #[prost(enumeration="member::Type", tag="4")]
+    pub r#type: i32,
+    //// Dummy Member
+    #[deprecated]
+    #[prost(bool, tag="5")]
+    pub dummy: bool,
+    //// The load score of the member.
+    #[deprecated]
+    #[prost(float, tag="6")]
+    pub load_score: f32,
+    //// The number of shards in the node.
+    #[prost(uint32, tag="7")]
+    pub shard_count: u32,
+}
+/// Nested message and enum types in `Member`.
+pub mod member {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Type {
+        Io = 0,
+        Search = 1,
+        Ingest = 2,
+        Train = 3,
+        Unknown = 4,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListMembersRequest {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListMembersResponse {
+    #[prost(message, repeated, tag="1")]
+    pub members: ::prost::alloc::vec::Vec<Member>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardReplica {
+    #[prost(message, optional, tag="1")]
+    pub shard: ::core::option::Option<super::noderesources::ShardCreated>,
+    #[prost(string, tag="2")]
+    pub node: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardObject {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(message, repeated, tag="3")]
+    pub replicas: ::prost::alloc::vec::Vec<ShardReplica>,
+    #[prost(message, optional, tag="4")]
+    pub timestamp: ::core::option::Option<::prost_types::Timestamp>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Shards {
+    #[prost(message, repeated, tag="1")]
+    pub shards: ::prost::alloc::vec::Vec<ShardObject>,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    /// current shard that resources index to
+    #[prost(int32, tag="3")]
+    pub actual: i32,
+    #[deprecated]
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="4")]
+    pub similarity: i32,
+    #[prost(message, optional, tag="5")]
+    pub model: ::core::option::Option<super::knowledgebox::SemanticModelMetadata>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceFieldId {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(enumeration="super::resources::FieldType", tag="3")]
+    pub field_type: i32,
+    #[prost(string, tag="4")]
+    pub field: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexResource {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(bool, tag="3")]
+    pub reindex_vectors: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexStatus {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceFieldExistsResponse {
+    #[prost(bool, tag="1")]
+    pub found: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceIdRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub slug: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceIdResponse {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ExportRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorsRequest {
+    #[prost(message, optional, tag="1")]
+    pub vectors: ::core::option::Option<super::utils::VectorObject>,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="4")]
+    pub field: ::core::option::Option<super::resources::FieldId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorsResponse {
+    #[prost(bool, tag="1")]
+    pub found: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FileRequest {
+    #[prost(string, tag="1")]
+    pub bucket: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub key: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BinaryData {
+    #[prost(bytes="vec", tag="1")]
+    pub data: ::prost::alloc::vec::Vec<u8>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BinaryMetadata {
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub key: ::prost::alloc::string::String,
+    #[prost(int32, tag="4")]
+    pub size: i32,
+    #[prost(string, tag="5")]
+    pub filename: ::prost::alloc::string::String,
+    #[prost(string, tag="6")]
+    pub content_type: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UploadBinaryData {
+    #[prost(int32, tag="1")]
+    pub count: i32,
+    #[prost(oneof="upload_binary_data::Data", tags="2, 3")]
+    pub data: ::core::option::Option<upload_binary_data::Data>,
+}
+/// Nested message and enum types in `UploadBinaryData`.
+pub mod upload_binary_data {
+    #[derive(Clone, PartialEq, ::prost::Oneof)]
+    pub enum Data {
+        #[prost(message, tag="2")]
+        Metadata(super::BinaryMetadata),
+        #[prost(bytes, tag="3")]
+        Payload(::prost::alloc::vec::Vec<u8>),
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FileUploaded {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SynonymsRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetSynonymsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kbid: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetSynonymsResponse {
+    #[prost(message, optional, tag="1")]
+    pub status: ::core::option::Option<OpStatusWriter>,
+    #[prost(message, optional, tag="2")]
+    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/nodereader.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/nodereader.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,1657 +1,1663 @@
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Filter {
-    #[prost(string, repeated, tag="1")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct StreamFilter {
-    #[prost(enumeration="stream_filter::Conjunction", tag="1")]
-    pub conjunction: i32,
-    #[prost(string, repeated, tag="2")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-/// Nested message and enum types in `StreamFilter`.
-pub mod stream_filter {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Conjunction {
-        And = 0,
-        Or = 1,
-        Not = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Faceted {
-    #[prost(string, repeated, tag="1")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OrderBy {
-    #[deprecated]
-    #[prost(string, tag="1")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(enumeration="order_by::OrderType", tag="2")]
-    pub r#type: i32,
-    #[prost(enumeration="order_by::OrderField", tag="3")]
-    pub sort_by: i32,
-}
-/// Nested message and enum types in `OrderBy`.
-pub mod order_by {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum OrderType {
-        Desc = 0,
-        Asc = 1,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum OrderField {
-        Created = 0,
-        Modified = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Timestamps {
-    #[prost(message, optional, tag="1")]
-    pub from_modified: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="2")]
-    pub to_modified: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="3")]
-    pub from_created: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="4")]
-    pub to_created: ::core::option::Option<::prost_types::Timestamp>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FacetResult {
-    #[prost(string, tag="1")]
-    pub tag: ::prost::alloc::string::String,
-    #[prost(int32, tag="2")]
-    pub total: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FacetResults {
-    #[prost(message, repeated, tag="1")]
-    pub facetresults: ::prost::alloc::vec::Vec<FacetResult>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentSearchRequest {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(message, optional, tag="4")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="5")]
-    pub order: ::core::option::Option<OrderBy>,
-    #[prost(message, optional, tag="6")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="7")]
-    pub page_number: i32,
-    #[prost(int32, tag="8")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="9")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(bool, tag="10")]
-    pub reload: bool,
-    #[prost(bool, tag="15")]
-    pub only_faceted: bool,
-    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="16")]
-    pub with_status: ::core::option::Option<i32>,
-    #[prost(string, optional, tag="17")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphSearchRequest {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// query this text in all the paragraphs
-    #[prost(string, tag="4")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="7")]
-    pub order: ::core::option::Option<OrderBy>,
-    /// Faceted{ tags: Vec<String>}
-    #[prost(message, optional, tag="8")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="10")]
-    pub page_number: i32,
-    #[prost(int32, tag="11")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="12")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(bool, tag="13")]
-    pub reload: bool,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    #[prost(bool, tag="15")]
-    pub only_faceted: bool,
-    #[prost(string, optional, tag="16")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(string, repeated, tag="17")]
-    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResultScore {
-    #[prost(float, tag="1")]
-    pub bm25: f32,
-    /// In the case of two equal bm25 scores, booster 
-    /// decides
-    #[prost(float, tag="2")]
-    pub booster: f32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentResult {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub score: ::core::option::Option<ResultScore>,
-    #[prost(string, tag="4")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="5")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentSearchResponse {
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<DocumentResult>,
-    #[prost(map="string, message", tag="3")]
-    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    /// The text that lead to this results
-    #[prost(string, tag="6")]
-    pub query: ::prost::alloc::string::String,
-    /// Is there a next page
-    #[prost(bool, tag="7")]
-    pub next_page: bool,
-    #[prost(bool, tag="8")]
-    pub bm25: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphResult {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(uint64, tag="4")]
-    pub start: u64,
-    #[prost(uint64, tag="5")]
-    pub end: u64,
-    #[prost(string, tag="6")]
-    pub paragraph: ::prost::alloc::string::String,
-    #[prost(string, tag="7")]
-    pub split: ::prost::alloc::string::String,
-    #[prost(uint64, tag="8")]
-    pub index: u64,
-    #[prost(message, optional, tag="9")]
-    pub score: ::core::option::Option<ResultScore>,
-    #[prost(string, repeated, tag="10")]
-    pub matches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Metadata that can't be searched with but is returned on search results
-    #[prost(message, optional, tag="11")]
-    pub metadata: ::core::option::Option<super::noderesources::ParagraphMetadata>,
-    #[prost(string, repeated, tag="12")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphSearchResponse {
-    #[prost(int32, tag="10")]
-    pub fuzzy_distance: i32,
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    /// 
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
-    /// For each field what facets are.
-    #[prost(map="string, message", tag="3")]
-    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    /// The text that lead to this results
-    #[prost(string, tag="6")]
-    pub query: ::prost::alloc::string::String,
-    /// Is there a next page
-    #[prost(bool, tag="7")]
-    pub next_page: bool,
-    #[prost(bool, tag="8")]
-    pub bm25: bool,
-    #[prost(string, repeated, tag="9")]
-    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSearchRequest {
-    ///Shard ID
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    /// Embedded vector search.
-    #[prost(float, repeated, tag="2")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    /// tags to filter
-    #[prost(string, repeated, tag="3")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    #[prost(bool, tag="13")]
-    pub reload: bool,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    /// ID for the vector set.
-    /// Empty for searching on the original index
-    #[prost(string, tag="15")]
-    pub vector_set: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="16")]
-    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(float, tag="17")]
-    pub min_score: f32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentVectorIdentifier {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentScored {
-    #[prost(message, optional, tag="1")]
-    pub doc_id: ::core::option::Option<DocumentVectorIdentifier>,
-    #[prost(float, tag="2")]
-    pub score: f32,
-    #[prost(message, optional, tag="3")]
-    pub metadata: ::core::option::Option<super::noderesources::SentenceMetadata>,
-    #[prost(string, repeated, tag="4")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSearchResponse {
-    /// List of docs closer to the asked one.
-    #[prost(message, repeated, tag="1")]
-    pub documents: ::prost::alloc::vec::Vec<DocumentScored>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationNodeFilter {
-    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
-    pub node_type: i32,
-    #[prost(string, optional, tag="2")]
-    pub node_subtype: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationEdgeFilter {
-    /// Will filter the search to edges of type ntype.
-    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
-    pub relation_type: i32,
-    #[prost(string, optional, tag="2")]
-    pub relation_subtype: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationPrefixSearchRequest {
-    #[prost(string, tag="1")]
-    pub prefix: ::prost::alloc::string::String,
-    #[prost(message, repeated, tag="2")]
-    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationPrefixSearchResponse {
-    #[prost(message, repeated, tag="1")]
-    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EntitiesSubgraphRequest {
-    /// List of vertices where search will trigger
-    #[prost(message, repeated, tag="1")]
-    pub entry_points: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-    /// Filters to apply while searching. It's an OR filtering: any
-    /// node (vertex) satisfying one condition will be returned
-    #[prost(message, repeated, tag="2")]
-    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
-    /// Filters to apply while searching. It's an OR filtering: any
-    /// edge satisfying one condition will be returned
-    #[prost(message, repeated, tag="4")]
-    pub edge_filters: ::prost::alloc::vec::Vec<RelationEdgeFilter>,
-    #[prost(int32, optional, tag="3")]
-    pub depth: ::core::option::Option<i32>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EntitiesSubgraphResponse {
-    #[prost(message, repeated, tag="1")]
-    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
-}
-// TODO: uncomment and implement (next iteration)
-// message RelationPathsSearchRequest {
-//     message PathEndpoints {
-//         utils.RelationNode origin = 1;
-//         utils.RelationNode destination = 2;
-//     }
-//     repeated PathEndpoints paths = 1;
-// }
-
-/// Query relation index to obtain different information about the
-/// knowledge graph. It can be queried using the following strategies:
-///
-/// - prefix search over vertex (node) names
-/// - graph search:
-///   - given some entry vertices, get the filtered subgraph around them
-///   - (TODO) given some vertices, get paths between them
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationSearchRequest {
-    #[prost(string, tag="1")]
-    pub shard_id: ::prost::alloc::string::String,
-    #[prost(bool, tag="5")]
-    pub reload: bool,
-    #[prost(message, optional, tag="11")]
-    pub prefix: ::core::option::Option<RelationPrefixSearchRequest>,
-    /// TODO: uncomment and implement (next iteration)
-    /// RelationPathsSearchRequest paths = 13;
-    #[prost(message, optional, tag="12")]
-    pub subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationSearchResponse {
-    #[prost(message, optional, tag="11")]
-    pub prefix: ::core::option::Option<RelationPrefixSearchResponse>,
-    /// TODO: uncomment and implement (next iteration)
-    /// repeated utils.RelationPath paths = 13;
-    #[prost(message, optional, tag="12")]
-    pub subgraph: ::core::option::Option<EntitiesSubgraphResponse>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SearchRequest {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="2")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// query this text in all the paragraphs
-    #[prost(string, tag="3")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="4")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="5")]
-    pub order: ::core::option::Option<OrderBy>,
-    /// Faceted{ tags: Vec<String>}
-    #[prost(message, optional, tag="6")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="7")]
-    pub page_number: i32,
-    #[prost(int32, tag="8")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="9")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    /// Embedded vector search.
-    #[prost(float, repeated, tag="10")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    #[prost(string, tag="15")]
-    pub vectorset: ::prost::alloc::string::String,
-    #[prost(bool, tag="11")]
-    pub reload: bool,
-    #[prost(bool, tag="12")]
-    pub paragraph: bool,
-    #[prost(bool, tag="13")]
-    pub document: bool,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    #[prost(bool, tag="16")]
-    pub only_faceted: bool,
-    #[prost(string, optional, tag="18")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="17")]
-    pub with_status: ::core::option::Option<i32>,
-    /// if provided, search metadata for this nodes (nodes at distance
-    /// one) and get the shortest path between nodes
-    #[deprecated]
-    #[prost(message, optional, tag="19")]
-    pub relations: ::core::option::Option<RelationSearchRequest>,
-    #[prost(message, optional, tag="20")]
-    pub relation_prefix: ::core::option::Option<RelationPrefixSearchRequest>,
-    #[prost(message, optional, tag="21")]
-    pub relation_subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
-    #[prost(string, repeated, tag="22")]
-    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(float, tag="23")]
-    pub min_score: f32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SuggestRequest {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="4")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(string, repeated, tag="5")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelatedEntities {
-    #[prost(string, repeated, tag="1")]
-    pub entities: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(uint32, tag="2")]
-    pub total: u32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SuggestResponse {
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
-    /// The text that lead to this results
-    #[prost(string, tag="3")]
-    pub query: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="4")]
-    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Entities related with the query
-    #[prost(message, optional, tag="5")]
-    pub entities: ::core::option::Option<RelatedEntities>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SearchResponse {
-    #[prost(message, optional, tag="1")]
-    pub document: ::core::option::Option<DocumentSearchResponse>,
-    #[prost(message, optional, tag="2")]
-    pub paragraph: ::core::option::Option<ParagraphSearchResponse>,
-    #[prost(message, optional, tag="3")]
-    pub vector: ::core::option::Option<VectorSearchResponse>,
-    #[prost(message, optional, tag="4")]
-    pub relation: ::core::option::Option<RelationSearchResponse>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IdCollection {
-    #[prost(string, repeated, tag="1")]
-    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationEdge {
-    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
-    pub edge_type: i32,
-    #[prost(string, tag="2")]
-    pub property: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EdgeList {
-    #[prost(message, repeated, tag="1")]
-    pub list: ::prost::alloc::vec::Vec<RelationEdge>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationTypeListMember {
-    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
-    pub with_type: i32,
-    #[prost(string, tag="2")]
-    pub with_subtype: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct TypeList {
-    #[prost(message, repeated, tag="1")]
-    pub list: ::prost::alloc::vec::Vec<RelationTypeListMember>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetShardRequest {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(string, tag="2")]
-    pub vectorset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphItem {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="2")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentItem {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct StreamRequest {
-    #[deprecated]
-    #[prost(message, optional, tag="1")]
-    pub filter_deprecated: ::core::option::Option<Filter>,
-    #[prost(bool, tag="2")]
-    pub reload: bool,
-    #[prost(message, optional, tag="3")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(message, optional, tag="4")]
-    pub filter: ::core::option::Option<StreamFilter>,
-}
-/// Generated client implementations.
-pub mod node_reader_client {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    #[derive(Debug, Clone)]
-    pub struct NodeReaderClient<T> {
-        inner: tonic::client::Grpc<T>,
-    }
-    impl NodeReaderClient<tonic::transport::Channel> {
-        /// Attempt to create a new client by connecting to a given endpoint.
-        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
-        where
-            D: std::convert::TryInto<tonic::transport::Endpoint>,
-            D::Error: Into<StdError>,
-        {
-            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
-            Ok(Self::new(conn))
-        }
-    }
-    impl<T> NodeReaderClient<T>
-    where
-        T: tonic::client::GrpcService<tonic::body::BoxBody>,
-        T::Error: Into<StdError>,
-        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
-        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
-    {
-        pub fn new(inner: T) -> Self {
-            let inner = tonic::client::Grpc::new(inner);
-            Self { inner }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> NodeReaderClient<InterceptedService<T, F>>
-        where
-            F: tonic::service::Interceptor,
-            T::ResponseBody: Default,
-            T: tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-                Response = http::Response<
-                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
-                >,
-            >,
-            <T as tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-            >>::Error: Into<StdError> + Send + Sync,
-        {
-            NodeReaderClient::new(InterceptedService::new(inner, interceptor))
-        }
-        /// Compress requests with `gzip`.
-        ///
-        /// This requires the server to support it otherwise it might respond with an
-        /// error.
-        #[must_use]
-        pub fn send_gzip(mut self) -> Self {
-            self.inner = self.inner.send_gzip();
-            self
-        }
-        /// Enable decompressing responses with `gzip`.
-        #[must_use]
-        pub fn accept_gzip(mut self) -> Self {
-            self.inner = self.inner.accept_gzip();
-            self
-        }
-        pub async fn get_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::GetShardRequest>,
-        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/GetShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn document_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::DocumentSearchRequest>,
-        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/DocumentSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn paragraph_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::ParagraphSearchRequest>,
-        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/ParagraphSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn vector_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::VectorSearchRequest>,
-        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/VectorSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::RelationSearchRequest>,
-        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn document_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/DocumentIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn paragraph_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/ParagraphIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn vector_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/VectorIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_edges(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationEdges",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_types(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::TypeList>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationTypes",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SearchRequest>,
-        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Search",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn suggest(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SuggestRequest>,
-        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Suggest",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        /// Streams
-        pub async fn paragraphs(
-            &mut self,
-            request: impl tonic::IntoRequest<super::StreamRequest>,
-        ) -> Result<
-            tonic::Response<tonic::codec::Streaming<super::ParagraphItem>>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Paragraphs",
-            );
-            self.inner.server_streaming(request.into_request(), path, codec).await
-        }
-        pub async fn documents(
-            &mut self,
-            request: impl tonic::IntoRequest<super::StreamRequest>,
-        ) -> Result<
-            tonic::Response<tonic::codec::Streaming<super::DocumentItem>>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Documents",
-            );
-            self.inner.server_streaming(request.into_request(), path, codec).await
-        }
-    }
-}
-/// Generated server implementations.
-pub mod node_reader_server {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    ///Generated trait containing gRPC methods that should be implemented for use with NodeReaderServer.
-    #[async_trait]
-    pub trait NodeReader: Send + Sync + 'static {
-        async fn get_shard(
-            &self,
-            request: tonic::Request<super::GetShardRequest>,
-        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status>;
-        async fn document_search(
-            &self,
-            request: tonic::Request<super::DocumentSearchRequest>,
-        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status>;
-        async fn paragraph_search(
-            &self,
-            request: tonic::Request<super::ParagraphSearchRequest>,
-        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status>;
-        async fn vector_search(
-            &self,
-            request: tonic::Request<super::VectorSearchRequest>,
-        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status>;
-        async fn relation_search(
-            &self,
-            request: tonic::Request<super::RelationSearchRequest>,
-        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status>;
-        async fn document_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn paragraph_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn vector_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn relation_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn relation_edges(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status>;
-        async fn relation_types(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::TypeList>, tonic::Status>;
-        async fn search(
-            &self,
-            request: tonic::Request<super::SearchRequest>,
-        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status>;
-        async fn suggest(
-            &self,
-            request: tonic::Request<super::SuggestRequest>,
-        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status>;
-        ///Server streaming response type for the Paragraphs method.
-        type ParagraphsStream: futures_core::Stream<
-                Item = Result<super::ParagraphItem, tonic::Status>,
-            >
-            + Send
-            + 'static;
-        /// Streams
-        async fn paragraphs(
-            &self,
-            request: tonic::Request<super::StreamRequest>,
-        ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status>;
-        ///Server streaming response type for the Documents method.
-        type DocumentsStream: futures_core::Stream<
-                Item = Result<super::DocumentItem, tonic::Status>,
-            >
-            + Send
-            + 'static;
-        async fn documents(
-            &self,
-            request: tonic::Request<super::StreamRequest>,
-        ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status>;
-    }
-    #[derive(Debug)]
-    pub struct NodeReaderServer<T: NodeReader> {
-        inner: _Inner<T>,
-        accept_compression_encodings: (),
-        send_compression_encodings: (),
-    }
-    struct _Inner<T>(Arc<T>);
-    impl<T: NodeReader> NodeReaderServer<T> {
-        pub fn new(inner: T) -> Self {
-            Self::from_arc(Arc::new(inner))
-        }
-        pub fn from_arc(inner: Arc<T>) -> Self {
-            let inner = _Inner(inner);
-            Self {
-                inner,
-                accept_compression_encodings: Default::default(),
-                send_compression_encodings: Default::default(),
-            }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> InterceptedService<Self, F>
-        where
-            F: tonic::service::Interceptor,
-        {
-            InterceptedService::new(Self::new(inner), interceptor)
-        }
-    }
-    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeReaderServer<T>
-    where
-        T: NodeReader,
-        B: Body + Send + 'static,
-        B::Error: Into<StdError> + Send + 'static,
-    {
-        type Response = http::Response<tonic::body::BoxBody>;
-        type Error = std::convert::Infallible;
-        type Future = BoxFuture<Self::Response, Self::Error>;
-        fn poll_ready(
-            &mut self,
-            _cx: &mut Context<'_>,
-        ) -> Poll<Result<(), Self::Error>> {
-            Poll::Ready(Ok(()))
-        }
-        fn call(&mut self, req: http::Request<B>) -> Self::Future {
-            let inner = self.inner.clone();
-            match req.uri().path() {
-                "/nodereader.NodeReader/GetShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetShardSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::GetShardRequest>
-                    for GetShardSvc<T> {
-                        type Response = super::super::noderesources::Shard;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::GetShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).get_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/DocumentSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::DocumentSearchRequest>
-                    for DocumentSearchSvc<T> {
-                        type Response = super::DocumentSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::DocumentSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).document_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/ParagraphSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::ParagraphSearchRequest>
-                    for ParagraphSearchSvc<T> {
-                        type Response = super::ParagraphSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::ParagraphSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).paragraph_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/VectorSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct VectorSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::VectorSearchRequest>
-                    for VectorSearchSvc<T> {
-                        type Response = super::VectorSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::VectorSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).vector_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = VectorSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::RelationSearchRequest>
-                    for RelationSearchSvc<T> {
-                        type Response = super::RelationSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::RelationSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/DocumentIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for DocumentIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).document_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/ParagraphIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for ParagraphIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).paragraph_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/VectorIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct VectorIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for VectorIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).vector_ids(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = VectorIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationEdges" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationEdgesSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationEdgesSvc<T> {
-                        type Response = super::EdgeList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_edges(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationEdgesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationTypes" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationTypesSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationTypesSvc<T> {
-                        type Response = super::TypeList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_types(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationTypesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Search" => {
-                    #[allow(non_camel_case_types)]
-                    struct SearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<T: NodeReader> tonic::server::UnaryService<super::SearchRequest>
-                    for SearchSvc<T> {
-                        type Response = super::SearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).search(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Suggest" => {
-                    #[allow(non_camel_case_types)]
-                    struct SuggestSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::SuggestRequest>
-                    for SuggestSvc<T> {
-                        type Response = super::SuggestResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SuggestRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).suggest(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SuggestSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Paragraphs" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::ServerStreamingService<super::StreamRequest>
-                    for ParagraphsSvc<T> {
-                        type Response = super::ParagraphItem;
-                        type ResponseStream = T::ParagraphsStream;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::ResponseStream>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::StreamRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).paragraphs(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.server_streaming(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Documents" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::ServerStreamingService<super::StreamRequest>
-                    for DocumentsSvc<T> {
-                        type Response = super::DocumentItem;
-                        type ResponseStream = T::DocumentsStream;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::ResponseStream>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::StreamRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).documents(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.server_streaming(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                _ => {
-                    Box::pin(async move {
-                        Ok(
-                            http::Response::builder()
-                                .status(200)
-                                .header("grpc-status", "12")
-                                .header("content-type", "application/grpc")
-                                .body(empty_body())
-                                .unwrap(),
-                        )
-                    })
-                }
-            }
-        }
-    }
-    impl<T: NodeReader> Clone for NodeReaderServer<T> {
-        fn clone(&self) -> Self {
-            let inner = self.inner.clone();
-            Self {
-                inner,
-                accept_compression_encodings: self.accept_compression_encodings,
-                send_compression_encodings: self.send_compression_encodings,
-            }
-        }
-    }
-    impl<T: NodeReader> Clone for _Inner<T> {
-        fn clone(&self) -> Self {
-            Self(self.0.clone())
-        }
-    }
-    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            write!(f, "{:?}", self.0)
-        }
-    }
-    impl<T: NodeReader> tonic::transport::NamedService for NodeReaderServer<T> {
-        const NAME: &'static str = "nodereader.NodeReader";
-    }
-}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Filter {
+    #[prost(string, repeated, tag="1")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct StreamFilter {
+    #[prost(enumeration="stream_filter::Conjunction", tag="1")]
+    pub conjunction: i32,
+    #[prost(string, repeated, tag="2")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+/// Nested message and enum types in `StreamFilter`.
+pub mod stream_filter {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Conjunction {
+        And = 0,
+        Or = 1,
+        Not = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Faceted {
+    #[prost(string, repeated, tag="1")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OrderBy {
+    #[deprecated]
+    #[prost(string, tag="1")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(enumeration="order_by::OrderType", tag="2")]
+    pub r#type: i32,
+    #[prost(enumeration="order_by::OrderField", tag="3")]
+    pub sort_by: i32,
+}
+/// Nested message and enum types in `OrderBy`.
+pub mod order_by {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum OrderType {
+        Desc = 0,
+        Asc = 1,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum OrderField {
+        Created = 0,
+        Modified = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Timestamps {
+    #[prost(message, optional, tag="1")]
+    pub from_modified: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="2")]
+    pub to_modified: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="3")]
+    pub from_created: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="4")]
+    pub to_created: ::core::option::Option<::prost_types::Timestamp>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FacetResult {
+    #[prost(string, tag="1")]
+    pub tag: ::prost::alloc::string::String,
+    #[prost(int32, tag="2")]
+    pub total: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FacetResults {
+    #[prost(message, repeated, tag="1")]
+    pub facetresults: ::prost::alloc::vec::Vec<FacetResult>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentSearchRequest {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="5")]
+    pub order: ::core::option::Option<OrderBy>,
+    #[prost(message, optional, tag="6")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="7")]
+    pub page_number: i32,
+    #[prost(int32, tag="8")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="9")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[deprecated]
+    #[prost(bool, tag="10")]
+    pub reload: bool,
+    #[prost(bool, tag="15")]
+    pub only_faceted: bool,
+    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="16")]
+    pub with_status: ::core::option::Option<i32>,
+    #[prost(string, optional, tag="17")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphSearchRequest {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// query this text in all the paragraphs
+    #[prost(string, tag="4")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="7")]
+    pub order: ::core::option::Option<OrderBy>,
+    /// Faceted{ tags: Vec<String>}
+    #[prost(message, optional, tag="8")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="10")]
+    pub page_number: i32,
+    #[prost(int32, tag="11")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="12")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[deprecated]
+    #[prost(bool, tag="13")]
+    pub reload: bool,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    #[prost(bool, tag="15")]
+    pub only_faceted: bool,
+    #[prost(string, optional, tag="16")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(string, repeated, tag="17")]
+    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResultScore {
+    #[prost(float, tag="1")]
+    pub bm25: f32,
+    /// In the case of two equal bm25 scores, booster 
+    /// decides
+    #[prost(float, tag="2")]
+    pub booster: f32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentResult {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub score: ::core::option::Option<ResultScore>,
+    #[prost(string, tag="4")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="5")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentSearchResponse {
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<DocumentResult>,
+    #[prost(map="string, message", tag="3")]
+    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    /// The text that lead to this results
+    #[prost(string, tag="6")]
+    pub query: ::prost::alloc::string::String,
+    /// Is there a next page
+    #[prost(bool, tag="7")]
+    pub next_page: bool,
+    #[prost(bool, tag="8")]
+    pub bm25: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphResult {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(uint64, tag="4")]
+    pub start: u64,
+    #[prost(uint64, tag="5")]
+    pub end: u64,
+    #[prost(string, tag="6")]
+    pub paragraph: ::prost::alloc::string::String,
+    #[prost(string, tag="7")]
+    pub split: ::prost::alloc::string::String,
+    #[prost(uint64, tag="8")]
+    pub index: u64,
+    #[prost(message, optional, tag="9")]
+    pub score: ::core::option::Option<ResultScore>,
+    #[prost(string, repeated, tag="10")]
+    pub matches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Metadata that can't be searched with but is returned on search results
+    #[prost(message, optional, tag="11")]
+    pub metadata: ::core::option::Option<super::noderesources::ParagraphMetadata>,
+    #[prost(string, repeated, tag="12")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphSearchResponse {
+    #[prost(int32, tag="10")]
+    pub fuzzy_distance: i32,
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    /// 
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
+    /// For each field what facets are.
+    #[prost(map="string, message", tag="3")]
+    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    /// The text that lead to this results
+    #[prost(string, tag="6")]
+    pub query: ::prost::alloc::string::String,
+    /// Is there a next page
+    #[prost(bool, tag="7")]
+    pub next_page: bool,
+    #[prost(bool, tag="8")]
+    pub bm25: bool,
+    #[prost(string, repeated, tag="9")]
+    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSearchRequest {
+    ///Shard ID
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    /// Embedded vector search.
+    #[prost(float, repeated, tag="2")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    /// tags to filter
+    #[prost(string, repeated, tag="3")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    #[deprecated]
+    #[prost(bool, tag="13")]
+    pub reload: bool,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    /// ID for the vector set.
+    /// Empty for searching on the original index
+    #[prost(string, tag="15")]
+    pub vector_set: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="16")]
+    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(float, tag="17")]
+    pub min_score: f32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentVectorIdentifier {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentScored {
+    #[prost(message, optional, tag="1")]
+    pub doc_id: ::core::option::Option<DocumentVectorIdentifier>,
+    #[prost(float, tag="2")]
+    pub score: f32,
+    #[prost(message, optional, tag="3")]
+    pub metadata: ::core::option::Option<super::noderesources::SentenceMetadata>,
+    #[prost(string, repeated, tag="4")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSearchResponse {
+    /// List of docs closer to the asked one.
+    #[prost(message, repeated, tag="1")]
+    pub documents: ::prost::alloc::vec::Vec<DocumentScored>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationNodeFilter {
+    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
+    pub node_type: i32,
+    #[prost(string, optional, tag="2")]
+    pub node_subtype: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationEdgeFilter {
+    /// Will filter the search to edges of type ntype.
+    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
+    pub relation_type: i32,
+    #[prost(string, optional, tag="2")]
+    pub relation_subtype: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationPrefixSearchRequest {
+    #[prost(string, tag="1")]
+    pub prefix: ::prost::alloc::string::String,
+    #[prost(message, repeated, tag="2")]
+    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationPrefixSearchResponse {
+    #[prost(message, repeated, tag="1")]
+    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EntitiesSubgraphRequest {
+    /// List of vertices where search will trigger
+    #[prost(message, repeated, tag="1")]
+    pub entry_points: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+    /// Filters to apply while searching. It's an OR filtering: any
+    /// node (vertex) satisfying one condition will be returned
+    #[prost(message, repeated, tag="2")]
+    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
+    /// Filters to apply while searching. It's an OR filtering: any
+    /// edge satisfying one condition will be returned
+    #[prost(message, repeated, tag="4")]
+    pub edge_filters: ::prost::alloc::vec::Vec<RelationEdgeFilter>,
+    #[prost(int32, optional, tag="3")]
+    pub depth: ::core::option::Option<i32>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EntitiesSubgraphResponse {
+    #[prost(message, repeated, tag="1")]
+    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
+}
+// TODO: uncomment and implement (next iteration)
+// message RelationPathsSearchRequest {
+//     message PathEndpoints {
+//         utils.RelationNode origin = 1;
+//         utils.RelationNode destination = 2;
+//     }
+//     repeated PathEndpoints paths = 1;
+// }
+
+/// Query relation index to obtain different information about the
+/// knowledge graph. It can be queried using the following strategies:
+///
+/// - prefix search over vertex (node) names
+/// - graph search:
+///   - given some entry vertices, get the filtered subgraph around them
+///   - (TODO) given some vertices, get paths between them
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationSearchRequest {
+    #[prost(string, tag="1")]
+    pub shard_id: ::prost::alloc::string::String,
+    #[deprecated]
+    #[prost(bool, tag="5")]
+    pub reload: bool,
+    #[prost(message, optional, tag="11")]
+    pub prefix: ::core::option::Option<RelationPrefixSearchRequest>,
+    /// TODO: uncomment and implement (next iteration)
+    /// RelationPathsSearchRequest paths = 13;
+    #[prost(message, optional, tag="12")]
+    pub subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationSearchResponse {
+    #[prost(message, optional, tag="11")]
+    pub prefix: ::core::option::Option<RelationPrefixSearchResponse>,
+    /// TODO: uncomment and implement (next iteration)
+    /// repeated utils.RelationPath paths = 13;
+    #[prost(message, optional, tag="12")]
+    pub subgraph: ::core::option::Option<EntitiesSubgraphResponse>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SearchRequest {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="2")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// query this text in all the paragraphs
+    #[prost(string, tag="3")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="5")]
+    pub order: ::core::option::Option<OrderBy>,
+    /// Faceted{ tags: Vec<String>}
+    #[prost(message, optional, tag="6")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="7")]
+    pub page_number: i32,
+    #[prost(int32, tag="8")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="9")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    /// Embedded vector search.
+    #[prost(float, repeated, tag="10")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    #[prost(string, tag="15")]
+    pub vectorset: ::prost::alloc::string::String,
+    #[deprecated]
+    #[prost(bool, tag="11")]
+    pub reload: bool,
+    #[prost(bool, tag="12")]
+    pub paragraph: bool,
+    #[prost(bool, tag="13")]
+    pub document: bool,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    #[prost(bool, tag="16")]
+    pub only_faceted: bool,
+    #[prost(string, optional, tag="18")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="17")]
+    pub with_status: ::core::option::Option<i32>,
+    /// if provided, search metadata for this nodes (nodes at distance
+    /// one) and get the shortest path between nodes
+    #[deprecated]
+    #[prost(message, optional, tag="19")]
+    pub relations: ::core::option::Option<RelationSearchRequest>,
+    #[prost(message, optional, tag="20")]
+    pub relation_prefix: ::core::option::Option<RelationPrefixSearchRequest>,
+    #[prost(message, optional, tag="21")]
+    pub relation_subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
+    #[prost(string, repeated, tag="22")]
+    pub key_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(float, tag="23")]
+    pub min_score: f32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SuggestRequest {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="4")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[prost(string, repeated, tag="5")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelatedEntities {
+    #[prost(string, repeated, tag="1")]
+    pub entities: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(uint32, tag="2")]
+    pub total: u32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SuggestResponse {
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
+    /// The text that lead to this results
+    #[prost(string, tag="3")]
+    pub query: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="4")]
+    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Entities related with the query
+    #[prost(message, optional, tag="5")]
+    pub entities: ::core::option::Option<RelatedEntities>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SearchResponse {
+    #[prost(message, optional, tag="1")]
+    pub document: ::core::option::Option<DocumentSearchResponse>,
+    #[prost(message, optional, tag="2")]
+    pub paragraph: ::core::option::Option<ParagraphSearchResponse>,
+    #[prost(message, optional, tag="3")]
+    pub vector: ::core::option::Option<VectorSearchResponse>,
+    #[prost(message, optional, tag="4")]
+    pub relation: ::core::option::Option<RelationSearchResponse>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IdCollection {
+    #[prost(string, repeated, tag="1")]
+    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationEdge {
+    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
+    pub edge_type: i32,
+    #[prost(string, tag="2")]
+    pub property: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EdgeList {
+    #[prost(message, repeated, tag="1")]
+    pub list: ::prost::alloc::vec::Vec<RelationEdge>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationTypeListMember {
+    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
+    pub with_type: i32,
+    #[prost(string, tag="2")]
+    pub with_subtype: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct TypeList {
+    #[prost(message, repeated, tag="1")]
+    pub list: ::prost::alloc::vec::Vec<RelationTypeListMember>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetShardRequest {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(string, tag="2")]
+    pub vectorset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphItem {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="2")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentItem {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct StreamRequest {
+    #[deprecated]
+    #[prost(message, optional, tag="1")]
+    pub filter_deprecated: ::core::option::Option<Filter>,
+    #[deprecated]
+    #[prost(bool, tag="2")]
+    pub reload: bool,
+    #[prost(message, optional, tag="3")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<StreamFilter>,
+}
+/// Generated client implementations.
+pub mod node_reader_client {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    #[derive(Debug, Clone)]
+    pub struct NodeReaderClient<T> {
+        inner: tonic::client::Grpc<T>,
+    }
+    impl NodeReaderClient<tonic::transport::Channel> {
+        /// Attempt to create a new client by connecting to a given endpoint.
+        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
+        where
+            D: std::convert::TryInto<tonic::transport::Endpoint>,
+            D::Error: Into<StdError>,
+        {
+            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
+            Ok(Self::new(conn))
+        }
+    }
+    impl<T> NodeReaderClient<T>
+    where
+        T: tonic::client::GrpcService<tonic::body::BoxBody>,
+        T::Error: Into<StdError>,
+        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
+        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
+    {
+        pub fn new(inner: T) -> Self {
+            let inner = tonic::client::Grpc::new(inner);
+            Self { inner }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> NodeReaderClient<InterceptedService<T, F>>
+        where
+            F: tonic::service::Interceptor,
+            T::ResponseBody: Default,
+            T: tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+                Response = http::Response<
+                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
+                >,
+            >,
+            <T as tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+            >>::Error: Into<StdError> + Send + Sync,
+        {
+            NodeReaderClient::new(InterceptedService::new(inner, interceptor))
+        }
+        /// Compress requests with `gzip`.
+        ///
+        /// This requires the server to support it otherwise it might respond with an
+        /// error.
+        #[must_use]
+        pub fn send_gzip(mut self) -> Self {
+            self.inner = self.inner.send_gzip();
+            self
+        }
+        /// Enable decompressing responses with `gzip`.
+        #[must_use]
+        pub fn accept_gzip(mut self) -> Self {
+            self.inner = self.inner.accept_gzip();
+            self
+        }
+        pub async fn get_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::GetShardRequest>,
+        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/GetShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn document_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::DocumentSearchRequest>,
+        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/DocumentSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn paragraph_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::ParagraphSearchRequest>,
+        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/ParagraphSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn vector_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::VectorSearchRequest>,
+        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/VectorSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::RelationSearchRequest>,
+        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn document_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/DocumentIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn paragraph_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/ParagraphIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn vector_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/VectorIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_edges(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationEdges",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_types(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::TypeList>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationTypes",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SearchRequest>,
+        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Search",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn suggest(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SuggestRequest>,
+        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Suggest",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        /// Streams
+        pub async fn paragraphs(
+            &mut self,
+            request: impl tonic::IntoRequest<super::StreamRequest>,
+        ) -> Result<
+            tonic::Response<tonic::codec::Streaming<super::ParagraphItem>>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Paragraphs",
+            );
+            self.inner.server_streaming(request.into_request(), path, codec).await
+        }
+        pub async fn documents(
+            &mut self,
+            request: impl tonic::IntoRequest<super::StreamRequest>,
+        ) -> Result<
+            tonic::Response<tonic::codec::Streaming<super::DocumentItem>>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Documents",
+            );
+            self.inner.server_streaming(request.into_request(), path, codec).await
+        }
+    }
+}
+/// Generated server implementations.
+pub mod node_reader_server {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    ///Generated trait containing gRPC methods that should be implemented for use with NodeReaderServer.
+    #[async_trait]
+    pub trait NodeReader: Send + Sync + 'static {
+        async fn get_shard(
+            &self,
+            request: tonic::Request<super::GetShardRequest>,
+        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status>;
+        async fn document_search(
+            &self,
+            request: tonic::Request<super::DocumentSearchRequest>,
+        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status>;
+        async fn paragraph_search(
+            &self,
+            request: tonic::Request<super::ParagraphSearchRequest>,
+        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status>;
+        async fn vector_search(
+            &self,
+            request: tonic::Request<super::VectorSearchRequest>,
+        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status>;
+        async fn relation_search(
+            &self,
+            request: tonic::Request<super::RelationSearchRequest>,
+        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status>;
+        async fn document_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn paragraph_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn vector_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn relation_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn relation_edges(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status>;
+        async fn relation_types(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::TypeList>, tonic::Status>;
+        async fn search(
+            &self,
+            request: tonic::Request<super::SearchRequest>,
+        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status>;
+        async fn suggest(
+            &self,
+            request: tonic::Request<super::SuggestRequest>,
+        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status>;
+        ///Server streaming response type for the Paragraphs method.
+        type ParagraphsStream: futures_core::Stream<
+                Item = Result<super::ParagraphItem, tonic::Status>,
+            >
+            + Send
+            + 'static;
+        /// Streams
+        async fn paragraphs(
+            &self,
+            request: tonic::Request<super::StreamRequest>,
+        ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status>;
+        ///Server streaming response type for the Documents method.
+        type DocumentsStream: futures_core::Stream<
+                Item = Result<super::DocumentItem, tonic::Status>,
+            >
+            + Send
+            + 'static;
+        async fn documents(
+            &self,
+            request: tonic::Request<super::StreamRequest>,
+        ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status>;
+    }
+    #[derive(Debug)]
+    pub struct NodeReaderServer<T: NodeReader> {
+        inner: _Inner<T>,
+        accept_compression_encodings: (),
+        send_compression_encodings: (),
+    }
+    struct _Inner<T>(Arc<T>);
+    impl<T: NodeReader> NodeReaderServer<T> {
+        pub fn new(inner: T) -> Self {
+            Self::from_arc(Arc::new(inner))
+        }
+        pub fn from_arc(inner: Arc<T>) -> Self {
+            let inner = _Inner(inner);
+            Self {
+                inner,
+                accept_compression_encodings: Default::default(),
+                send_compression_encodings: Default::default(),
+            }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> InterceptedService<Self, F>
+        where
+            F: tonic::service::Interceptor,
+        {
+            InterceptedService::new(Self::new(inner), interceptor)
+        }
+    }
+    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeReaderServer<T>
+    where
+        T: NodeReader,
+        B: Body + Send + 'static,
+        B::Error: Into<StdError> + Send + 'static,
+    {
+        type Response = http::Response<tonic::body::BoxBody>;
+        type Error = std::convert::Infallible;
+        type Future = BoxFuture<Self::Response, Self::Error>;
+        fn poll_ready(
+            &mut self,
+            _cx: &mut Context<'_>,
+        ) -> Poll<Result<(), Self::Error>> {
+            Poll::Ready(Ok(()))
+        }
+        fn call(&mut self, req: http::Request<B>) -> Self::Future {
+            let inner = self.inner.clone();
+            match req.uri().path() {
+                "/nodereader.NodeReader/GetShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetShardSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::GetShardRequest>
+                    for GetShardSvc<T> {
+                        type Response = super::super::noderesources::Shard;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::GetShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).get_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/DocumentSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::DocumentSearchRequest>
+                    for DocumentSearchSvc<T> {
+                        type Response = super::DocumentSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::DocumentSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).document_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/ParagraphSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::ParagraphSearchRequest>
+                    for ParagraphSearchSvc<T> {
+                        type Response = super::ParagraphSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::ParagraphSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).paragraph_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/VectorSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct VectorSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::VectorSearchRequest>
+                    for VectorSearchSvc<T> {
+                        type Response = super::VectorSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::VectorSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).vector_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = VectorSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::RelationSearchRequest>
+                    for RelationSearchSvc<T> {
+                        type Response = super::RelationSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::RelationSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/DocumentIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for DocumentIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).document_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/ParagraphIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for ParagraphIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).paragraph_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/VectorIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct VectorIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for VectorIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).vector_ids(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = VectorIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationEdges" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationEdgesSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationEdgesSvc<T> {
+                        type Response = super::EdgeList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_edges(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationEdgesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationTypes" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationTypesSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationTypesSvc<T> {
+                        type Response = super::TypeList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_types(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationTypesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Search" => {
+                    #[allow(non_camel_case_types)]
+                    struct SearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<T: NodeReader> tonic::server::UnaryService<super::SearchRequest>
+                    for SearchSvc<T> {
+                        type Response = super::SearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).search(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Suggest" => {
+                    #[allow(non_camel_case_types)]
+                    struct SuggestSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::SuggestRequest>
+                    for SuggestSvc<T> {
+                        type Response = super::SuggestResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SuggestRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).suggest(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SuggestSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Paragraphs" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::ServerStreamingService<super::StreamRequest>
+                    for ParagraphsSvc<T> {
+                        type Response = super::ParagraphItem;
+                        type ResponseStream = T::ParagraphsStream;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::ResponseStream>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::StreamRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).paragraphs(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.server_streaming(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Documents" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::ServerStreamingService<super::StreamRequest>
+                    for DocumentsSvc<T> {
+                        type Response = super::DocumentItem;
+                        type ResponseStream = T::DocumentsStream;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::ResponseStream>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::StreamRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).documents(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.server_streaming(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                _ => {
+                    Box::pin(async move {
+                        Ok(
+                            http::Response::builder()
+                                .status(200)
+                                .header("grpc-status", "12")
+                                .header("content-type", "application/grpc")
+                                .body(empty_body())
+                                .unwrap(),
+                        )
+                    })
+                }
+            }
+        }
+    }
+    impl<T: NodeReader> Clone for NodeReaderServer<T> {
+        fn clone(&self) -> Self {
+            let inner = self.inner.clone();
+            Self {
+                inner,
+                accept_compression_encodings: self.accept_compression_encodings,
+                send_compression_encodings: self.send_compression_encodings,
+            }
+        }
+    }
+    impl<T: NodeReader> Clone for _Inner<T> {
+        fn clone(&self) -> Self {
+            Self(self.0.clone())
+        }
+    }
+    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            write!(f, "{:?}", self.0)
+        }
+    }
+    impl<T: NodeReader> tonic::transport::NamedService for NodeReaderServer<T> {
+        const NAME: &'static str = "nodereader.NodeReader";
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/noderesources.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/noderesources.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,265 +1,267 @@
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct TextInformation {
-    #[prost(string, tag="1")]
-    pub text: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="2")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexMetadata {
-    /// Tantivy doc & para
-    #[prost(message, optional, tag="1")]
-    pub modified: ::core::option::Option<::prost_types::Timestamp>,
-    /// Tantivy doc & para
-    #[prost(message, optional, tag="2")]
-    pub created: ::core::option::Option<::prost_types::Timestamp>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardId {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardIds {
-    #[prost(message, repeated, tag="1")]
-    pub ids: ::prost::alloc::vec::Vec<ShardId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardCreated {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(enumeration="shard_created::DocumentService", tag="2")]
-    pub document_service: i32,
-    #[prost(enumeration="shard_created::ParagraphService", tag="3")]
-    pub paragraph_service: i32,
-    #[prost(enumeration="shard_created::VectorService", tag="4")]
-    pub vector_service: i32,
-    #[prost(enumeration="shard_created::RelationService", tag="5")]
-    pub relation_service: i32,
-}
-/// Nested message and enum types in `ShardCreated`.
-pub mod shard_created {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum DocumentService {
-        DocumentV0 = 0,
-        DocumentV1 = 1,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum ParagraphService {
-        ParagraphV0 = 0,
-        ParagraphV1 = 1,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum VectorService {
-        VectorV0 = 0,
-        VectorV1 = 1,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum RelationService {
-        RelationV0 = 0,
-        RelationV1 = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardCleaned {
-    #[prost(enumeration="shard_created::DocumentService", tag="2")]
-    pub document_service: i32,
-    #[prost(enumeration="shard_created::ParagraphService", tag="3")]
-    pub paragraph_service: i32,
-    #[prost(enumeration="shard_created::VectorService", tag="4")]
-    pub vector_service: i32,
-    #[prost(enumeration="shard_created::RelationService", tag="5")]
-    pub relation_service: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceId {
-    #[prost(string, tag="1")]
-    pub shard_id: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub uuid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Shard {
-    #[prost(message, optional, tag="5")]
-    pub metadata: ::core::option::Option<ShardMetadata>,
-    #[prost(string, tag="1")]
-    pub shard_id: ::prost::alloc::string::String,
-    #[prost(uint64, tag="2")]
-    pub resources: u64,
-    #[prost(uint64, tag="3")]
-    pub paragraphs: u64,
-    #[prost(uint64, tag="4")]
-    pub sentences: u64,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EmptyResponse {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EmptyQuery {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Position {
-    #[prost(uint64, tag="1")]
-    pub index: u64,
-    #[prost(uint64, tag="2")]
-    pub start: u64,
-    #[prost(uint64, tag="3")]
-    pub end: u64,
-    /// For pdfs/documents only
-    #[prost(uint64, tag="4")]
-    pub page_number: u64,
-    /// For multimedia only
-    #[prost(uint32, repeated, tag="5")]
-    pub start_seconds: ::prost::alloc::vec::Vec<u32>,
-    #[prost(uint32, repeated, tag="6")]
-    pub end_seconds: ::prost::alloc::vec::Vec<u32>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SentenceMetadata {
-    #[prost(message, optional, tag="1")]
-    pub position: ::core::option::Option<Position>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSentence {
-    #[prost(float, repeated, tag="1")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    #[prost(message, optional, tag="9")]
-    pub metadata: ::core::option::Option<SentenceMetadata>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphMetadata {
-    #[prost(message, optional, tag="1")]
-    pub position: ::core::option::Option<Position>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexParagraph {
-    /// Start end position in field text
-    #[prost(int32, tag="1")]
-    pub start: i32,
-    /// Start end position in field text
-    #[prost(int32, tag="2")]
-    pub end: i32,
-    /// Paragraph specific labels
-    #[prost(string, repeated, tag="3")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// key is full id for vectors
-    #[prost(map="string, message", tag="4")]
-    pub sentences: ::std::collections::HashMap<::prost::alloc::string::String, VectorSentence>,
-    #[prost(string, tag="5")]
-    pub field: ::prost::alloc::string::String,
-    /// split were it belongs
-    #[prost(string, tag="6")]
-    pub split: ::prost::alloc::string::String,
-    #[prost(uint64, tag="7")]
-    pub index: u64,
-    #[prost(bool, tag="8")]
-    pub repeated_in_field: bool,
-    #[prost(message, optional, tag="9")]
-    pub metadata: ::core::option::Option<ParagraphMetadata>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSetId {
-    #[prost(message, optional, tag="1")]
-    pub shard: ::core::option::Option<ShardId>,
-    #[prost(string, tag="2")]
-    pub vectorset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSetList {
-    #[prost(message, optional, tag="1")]
-    pub shard: ::core::option::Option<ShardId>,
-    #[prost(string, repeated, tag="2")]
-    pub vectorset: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexParagraphs {
-    /// id of the paragraph f"{self.rid}/{field_key}/{paragraph.start}-{paragraph.end}"
-    #[prost(map="string, message", tag="1")]
-    pub paragraphs: ::std::collections::HashMap<::prost::alloc::string::String, IndexParagraph>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Resource {
-    #[prost(message, optional, tag="1")]
-    pub resource: ::core::option::Option<ResourceId>,
-    #[prost(message, optional, tag="2")]
-    pub metadata: ::core::option::Option<IndexMetadata>,
-    /// Doc index
-    ///
-    /// Tantivy doc filled by field allways full
-    #[prost(map="string, message", tag="3")]
-    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, TextInformation>,
-    // Key is RID/FIELDID
-
-    /// Document labels always serialized full
-    #[prost(string, repeated, tag="4")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Tantivy doc
-    #[prost(enumeration="resource::ResourceStatus", tag="5")]
-    pub status: i32,
-    /// Paragraph
-    ///
-    /// Paragraphs by field
-    #[prost(map="string, message", tag="6")]
-    pub paragraphs: ::std::collections::HashMap<::prost::alloc::string::String, IndexParagraphs>,
-    #[prost(string, repeated, tag="7")]
-    pub paragraphs_to_delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(string, repeated, tag="8")]
-    pub sentences_to_delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Relations
-    #[prost(message, repeated, tag="9")]
-    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
-    #[prost(message, repeated, tag="10")]
-    pub relations_to_delete: ::prost::alloc::vec::Vec<super::utils::Relation>,
-    #[prost(string, tag="11")]
-    pub shard_id: ::prost::alloc::string::String,
-    /// vectorset is the key 
-    #[prost(map="string, message", tag="12")]
-    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, super::utils::UserVectors>,
-    /// Vectorset prefix vector id
-    #[prost(map="string, message", tag="13")]
-    pub vectors_to_delete: ::std::collections::HashMap<::prost::alloc::string::String, super::utils::UserVectorsList>,
-}
-/// Nested message and enum types in `Resource`.
-pub mod resource {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum ResourceStatus {
-        Processed = 0,
-        Empty = 1,
-        Error = 2,
-        Delete = 3,
-        Pending = 4,
-        Blocked = 5,
-        Expired = 6,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardMetadata {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NodeMetadata {
-    #[deprecated]
-    #[prost(float, tag="1")]
-    pub load_score: f32,
-    #[prost(uint64, tag="2")]
-    pub shard_count: u64,
-    #[prost(map="string, message", tag="3")]
-    pub shards: ::std::collections::HashMap<::prost::alloc::string::String, node_metadata::ShardMetadata>,
-}
-/// Nested message and enum types in `NodeMetadata`.
-pub mod node_metadata {
-    #[derive(Clone, PartialEq, ::prost::Message)]
-    pub struct ShardMetadata {
-        #[prost(string, tag="1")]
-        pub kbid: ::prost::alloc::string::String,
-        #[deprecated]
-        #[prost(float, tag="2")]
-        pub load_score: f32,
-    }
-}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct TextInformation {
+    #[prost(string, tag="1")]
+    pub text: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="2")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexMetadata {
+    /// Tantivy doc & para
+    #[prost(message, optional, tag="1")]
+    pub modified: ::core::option::Option<::prost_types::Timestamp>,
+    /// Tantivy doc & para
+    #[prost(message, optional, tag="2")]
+    pub created: ::core::option::Option<::prost_types::Timestamp>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardId {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardIds {
+    #[prost(message, repeated, tag="1")]
+    pub ids: ::prost::alloc::vec::Vec<ShardId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardCreated {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(enumeration="shard_created::DocumentService", tag="2")]
+    pub document_service: i32,
+    #[prost(enumeration="shard_created::ParagraphService", tag="3")]
+    pub paragraph_service: i32,
+    #[prost(enumeration="shard_created::VectorService", tag="4")]
+    pub vector_service: i32,
+    #[prost(enumeration="shard_created::RelationService", tag="5")]
+    pub relation_service: i32,
+}
+/// Nested message and enum types in `ShardCreated`.
+pub mod shard_created {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum DocumentService {
+        DocumentV0 = 0,
+        DocumentV1 = 1,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum ParagraphService {
+        ParagraphV0 = 0,
+        ParagraphV1 = 1,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum VectorService {
+        VectorV0 = 0,
+        VectorV1 = 1,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum RelationService {
+        RelationV0 = 0,
+        RelationV1 = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardCleaned {
+    #[prost(enumeration="shard_created::DocumentService", tag="2")]
+    pub document_service: i32,
+    #[prost(enumeration="shard_created::ParagraphService", tag="3")]
+    pub paragraph_service: i32,
+    #[prost(enumeration="shard_created::VectorService", tag="4")]
+    pub vector_service: i32,
+    #[prost(enumeration="shard_created::RelationService", tag="5")]
+    pub relation_service: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceId {
+    #[prost(string, tag="1")]
+    pub shard_id: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub uuid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Shard {
+    #[prost(message, optional, tag="5")]
+    pub metadata: ::core::option::Option<ShardMetadata>,
+    #[prost(string, tag="1")]
+    pub shard_id: ::prost::alloc::string::String,
+    #[prost(uint64, tag="2")]
+    pub fields: u64,
+    #[prost(uint64, tag="3")]
+    pub paragraphs: u64,
+    #[prost(uint64, tag="4")]
+    pub sentences: u64,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EmptyResponse {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EmptyQuery {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Position {
+    #[prost(uint64, tag="1")]
+    pub index: u64,
+    #[prost(uint64, tag="2")]
+    pub start: u64,
+    #[prost(uint64, tag="3")]
+    pub end: u64,
+    /// For pdfs/documents only
+    #[prost(uint64, tag="4")]
+    pub page_number: u64,
+    /// For multimedia only
+    #[prost(uint32, repeated, tag="5")]
+    pub start_seconds: ::prost::alloc::vec::Vec<u32>,
+    #[prost(uint32, repeated, tag="6")]
+    pub end_seconds: ::prost::alloc::vec::Vec<u32>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SentenceMetadata {
+    #[prost(message, optional, tag="1")]
+    pub position: ::core::option::Option<Position>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSentence {
+    #[prost(float, repeated, tag="1")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    #[prost(message, optional, tag="9")]
+    pub metadata: ::core::option::Option<SentenceMetadata>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphMetadata {
+    #[prost(message, optional, tag="1")]
+    pub position: ::core::option::Option<Position>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexParagraph {
+    /// Start end position in field text
+    #[prost(int32, tag="1")]
+    pub start: i32,
+    /// Start end position in field text
+    #[prost(int32, tag="2")]
+    pub end: i32,
+    /// Paragraph specific labels
+    #[prost(string, repeated, tag="3")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// key is full id for vectors
+    #[prost(map="string, message", tag="4")]
+    pub sentences: ::std::collections::HashMap<::prost::alloc::string::String, VectorSentence>,
+    #[prost(string, tag="5")]
+    pub field: ::prost::alloc::string::String,
+    /// split were it belongs
+    #[prost(string, tag="6")]
+    pub split: ::prost::alloc::string::String,
+    #[prost(uint64, tag="7")]
+    pub index: u64,
+    #[prost(bool, tag="8")]
+    pub repeated_in_field: bool,
+    #[prost(message, optional, tag="9")]
+    pub metadata: ::core::option::Option<ParagraphMetadata>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSetId {
+    #[prost(message, optional, tag="1")]
+    pub shard: ::core::option::Option<ShardId>,
+    #[prost(string, tag="2")]
+    pub vectorset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSetList {
+    #[prost(message, optional, tag="1")]
+    pub shard: ::core::option::Option<ShardId>,
+    #[prost(string, repeated, tag="2")]
+    pub vectorset: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexParagraphs {
+    /// id of the paragraph f"{self.rid}/{field_key}/{paragraph.start}-{paragraph.end}"
+    #[prost(map="string, message", tag="1")]
+    pub paragraphs: ::std::collections::HashMap<::prost::alloc::string::String, IndexParagraph>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Resource {
+    #[prost(message, optional, tag="1")]
+    pub resource: ::core::option::Option<ResourceId>,
+    #[prost(message, optional, tag="2")]
+    pub metadata: ::core::option::Option<IndexMetadata>,
+    /// Doc index
+    ///
+    /// Tantivy doc filled by field allways full
+    #[prost(map="string, message", tag="3")]
+    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, TextInformation>,
+    // Key is RID/FIELDID
+
+    /// Document labels always serialized full
+    #[prost(string, repeated, tag="4")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Tantivy doc
+    #[prost(enumeration="resource::ResourceStatus", tag="5")]
+    pub status: i32,
+    /// Paragraph
+    ///
+    /// Paragraphs by field
+    #[prost(map="string, message", tag="6")]
+    pub paragraphs: ::std::collections::HashMap<::prost::alloc::string::String, IndexParagraphs>,
+    #[prost(string, repeated, tag="7")]
+    pub paragraphs_to_delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(string, repeated, tag="8")]
+    pub sentences_to_delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Relations
+    #[prost(message, repeated, tag="9")]
+    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
+    #[prost(message, repeated, tag="10")]
+    pub relations_to_delete: ::prost::alloc::vec::Vec<super::utils::Relation>,
+    #[prost(string, tag="11")]
+    pub shard_id: ::prost::alloc::string::String,
+    /// vectorset is the key 
+    #[prost(map="string, message", tag="12")]
+    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, super::utils::UserVectors>,
+    /// Vectorset prefix vector id
+    #[prost(map="string, message", tag="13")]
+    pub vectors_to_delete: ::std::collections::HashMap<::prost::alloc::string::String, super::utils::UserVectorsList>,
+}
+/// Nested message and enum types in `Resource`.
+pub mod resource {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum ResourceStatus {
+        Processed = 0,
+        Empty = 1,
+        Error = 2,
+        Delete = 3,
+        Pending = 4,
+        Blocked = 5,
+        Expired = 6,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardMetadata {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NodeMetadata {
+    #[deprecated]
+    #[prost(float, tag="1")]
+    pub load_score: f32,
+    #[prost(uint64, tag="2")]
+    pub shard_count: u64,
+    #[prost(map="string, message", tag="3")]
+    pub shards: ::std::collections::HashMap<::prost::alloc::string::String, node_metadata::ShardMetadata>,
+    #[prost(string, tag="4")]
+    pub node_id: ::prost::alloc::string::String,
+}
+/// Nested message and enum types in `NodeMetadata`.
+pub mod node_metadata {
+    #[derive(Clone, PartialEq, ::prost::Message)]
+    pub struct ShardMetadata {
+        #[prost(string, tag="1")]
+        pub kbid: ::prost::alloc::string::String,
+        #[deprecated]
+        #[prost(float, tag="2")]
+        pub load_score: f32,
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/nodewriter.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/nodewriter.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,1106 +1,1106 @@
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OpStatus {
-    #[prost(enumeration="op_status::Status", tag="1")]
-    pub status: i32,
-    #[prost(string, tag="2")]
-    pub detail: ::prost::alloc::string::String,
-    #[prost(uint64, tag="3")]
-    pub count: u64,
-    #[prost(uint64, tag="5")]
-    pub count_paragraphs: u64,
-    #[prost(uint64, tag="6")]
-    pub count_sentences: u64,
-    #[prost(string, tag="4")]
-    pub shard_id: ::prost::alloc::string::String,
-}
-/// Nested message and enum types in `OpStatus`.
-pub mod op_status {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Warning = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexMessage {
-    #[prost(string, tag="1")]
-    pub node: ::prost::alloc::string::String,
-    /// physical shard message is for
-    #[prost(string, tag="2")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(uint64, tag="3")]
-    pub txid: u64,
-    #[prost(string, tag="4")]
-    pub resource: ::prost::alloc::string::String,
-    #[prost(enumeration="TypeMessage", tag="5")]
-    pub typemessage: i32,
-    #[prost(string, tag="6")]
-    pub reindex_id: ::prost::alloc::string::String,
-    #[prost(string, optional, tag="7")]
-    pub partition: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(string, tag="8")]
-    pub storage_key: ::prost::alloc::string::String,
-    #[prost(string, tag="9")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetGraph {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(message, optional, tag="2")]
-    pub graph: ::core::option::Option<super::utils::JoinGraph>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DeleteGraphNodes {
-    #[prost(message, optional, tag="2")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(message, repeated, tag="1")]
-    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewShardRequest {
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="1")]
-    pub similarity: i32,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub id: ::core::option::Option<super::noderesources::VectorSetId>,
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="2")]
-    pub similarity: i32,
-}
-// Implemented at nucliadb_object_storage
-
-#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-#[repr(i32)]
-pub enum TypeMessage {
-    Creation = 0,
-    Deletion = 1,
-}
-/// Generated client implementations.
-pub mod node_writer_client {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    #[derive(Debug, Clone)]
-    pub struct NodeWriterClient<T> {
-        inner: tonic::client::Grpc<T>,
-    }
-    impl NodeWriterClient<tonic::transport::Channel> {
-        /// Attempt to create a new client by connecting to a given endpoint.
-        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
-        where
-            D: std::convert::TryInto<tonic::transport::Endpoint>,
-            D::Error: Into<StdError>,
-        {
-            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
-            Ok(Self::new(conn))
-        }
-    }
-    impl<T> NodeWriterClient<T>
-    where
-        T: tonic::client::GrpcService<tonic::body::BoxBody>,
-        T::Error: Into<StdError>,
-        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
-        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
-    {
-        pub fn new(inner: T) -> Self {
-            let inner = tonic::client::Grpc::new(inner);
-            Self { inner }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> NodeWriterClient<InterceptedService<T, F>>
-        where
-            F: tonic::service::Interceptor,
-            T::ResponseBody: Default,
-            T: tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-                Response = http::Response<
-                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
-                >,
-            >,
-            <T as tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-            >>::Error: Into<StdError> + Send + Sync,
-        {
-            NodeWriterClient::new(InterceptedService::new(inner, interceptor))
-        }
-        /// Compress requests with `gzip`.
-        ///
-        /// This requires the server to support it otherwise it might respond with an
-        /// error.
-        #[must_use]
-        pub fn send_gzip(mut self) -> Self {
-            self.inner = self.inner.send_gzip();
-            self
-        }
-        /// Enable decompressing responses with `gzip`.
-        #[must_use]
-        pub fn accept_gzip(mut self) -> Self {
-            self.inner = self.inner.accept_gzip();
-            self
-        }
-        pub async fn new_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::NewShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCreated>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/NewShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn clean_and_upgrade_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCleaned>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/CleanAndUpgradeShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn delete_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/DeleteShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn list_shards(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardIds>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/ListShards",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn gc(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static("/nodewriter.NodeWriter/GC");
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn set_resource(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::Resource>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/SetResource",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn delete_relation_nodes(
-            &mut self,
-            request: impl tonic::IntoRequest<super::DeleteGraphNodes>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/DeleteRelationNodes",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn join_graph(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SetGraph>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/JoinGraph",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn remove_resource(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ResourceId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/RemoveResource",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn add_vector_set(
-            &mut self,
-            request: impl tonic::IntoRequest<super::NewVectorSetRequest>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/AddVectorSet",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn remove_vector_set(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::VectorSetId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/RemoveVectorSet",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn list_vector_sets(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::VectorSetList>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/ListVectorSets",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn get_metadata(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::NodeMetadata>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/GetMetadata",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-    }
-}
-/// Generated server implementations.
-pub mod node_writer_server {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    ///Generated trait containing gRPC methods that should be implemented for use with NodeWriterServer.
-    #[async_trait]
-    pub trait NodeWriter: Send + Sync + 'static {
-        async fn new_shard(
-            &self,
-            request: tonic::Request<super::NewShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCreated>,
-            tonic::Status,
-        >;
-        async fn clean_and_upgrade_shard(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCleaned>,
-            tonic::Status,
-        >;
-        async fn delete_shard(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        >;
-        async fn list_shards(
-            &self,
-            request: tonic::Request<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardIds>,
-            tonic::Status,
-        >;
-        async fn gc(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        >;
-        async fn set_resource(
-            &self,
-            request: tonic::Request<super::super::noderesources::Resource>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn delete_relation_nodes(
-            &self,
-            request: tonic::Request<super::DeleteGraphNodes>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn join_graph(
-            &self,
-            request: tonic::Request<super::SetGraph>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn remove_resource(
-            &self,
-            request: tonic::Request<super::super::noderesources::ResourceId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn add_vector_set(
-            &self,
-            request: tonic::Request<super::NewVectorSetRequest>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn remove_vector_set(
-            &self,
-            request: tonic::Request<super::super::noderesources::VectorSetId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn list_vector_sets(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::VectorSetList>,
-            tonic::Status,
-        >;
-        async fn get_metadata(
-            &self,
-            request: tonic::Request<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::NodeMetadata>,
-            tonic::Status,
-        >;
-    }
-    #[derive(Debug)]
-    pub struct NodeWriterServer<T: NodeWriter> {
-        inner: _Inner<T>,
-        accept_compression_encodings: (),
-        send_compression_encodings: (),
-    }
-    struct _Inner<T>(Arc<T>);
-    impl<T: NodeWriter> NodeWriterServer<T> {
-        pub fn new(inner: T) -> Self {
-            Self::from_arc(Arc::new(inner))
-        }
-        pub fn from_arc(inner: Arc<T>) -> Self {
-            let inner = _Inner(inner);
-            Self {
-                inner,
-                accept_compression_encodings: Default::default(),
-                send_compression_encodings: Default::default(),
-            }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> InterceptedService<Self, F>
-        where
-            F: tonic::service::Interceptor,
-        {
-            InterceptedService::new(Self::new(inner), interceptor)
-        }
-    }
-    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeWriterServer<T>
-    where
-        T: NodeWriter,
-        B: Body + Send + 'static,
-        B::Error: Into<StdError> + Send + 'static,
-    {
-        type Response = http::Response<tonic::body::BoxBody>;
-        type Error = std::convert::Infallible;
-        type Future = BoxFuture<Self::Response, Self::Error>;
-        fn poll_ready(
-            &mut self,
-            _cx: &mut Context<'_>,
-        ) -> Poll<Result<(), Self::Error>> {
-            Poll::Ready(Ok(()))
-        }
-        fn call(&mut self, req: http::Request<B>) -> Self::Future {
-            let inner = self.inner.clone();
-            match req.uri().path() {
-                "/nodewriter.NodeWriter/NewShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct NewShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::NewShardRequest>
-                    for NewShardSvc<T> {
-                        type Response = super::super::noderesources::ShardCreated;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::NewShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).new_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = NewShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/CleanAndUpgradeShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct CleanAndUpgradeShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for CleanAndUpgradeShardSvc<T> {
-                        type Response = super::super::noderesources::ShardCleaned;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).clean_and_upgrade_shard(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = CleanAndUpgradeShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/DeleteShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct DeleteShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for DeleteShardSvc<T> {
-                        type Response = super::super::noderesources::ShardId;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).delete_shard(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DeleteShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/ListShards" => {
-                    #[allow(non_camel_case_types)]
-                    struct ListShardsSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::EmptyQuery,
-                    > for ListShardsSvc<T> {
-                        type Response = super::super::noderesources::ShardIds;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::EmptyQuery,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).list_shards(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ListShardsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/GC" => {
-                    #[allow(non_camel_case_types)]
-                    struct GCSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for GCSvc<T> {
-                        type Response = super::super::noderesources::EmptyResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).gc(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GCSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/SetResource" => {
-                    #[allow(non_camel_case_types)]
-                    struct SetResourceSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::Resource>
-                    for SetResourceSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::Resource,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).set_resource(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SetResourceSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/DeleteRelationNodes" => {
-                    #[allow(non_camel_case_types)]
-                    struct DeleteRelationNodesSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::DeleteGraphNodes>
-                    for DeleteRelationNodesSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::DeleteGraphNodes>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).delete_relation_nodes(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DeleteRelationNodesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/JoinGraph" => {
-                    #[allow(non_camel_case_types)]
-                    struct JoinGraphSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<T: NodeWriter> tonic::server::UnaryService<super::SetGraph>
-                    for JoinGraphSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SetGraph>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).join_graph(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = JoinGraphSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/RemoveResource" => {
-                    #[allow(non_camel_case_types)]
-                    struct RemoveResourceSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::ResourceId,
-                    > for RemoveResourceSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::ResourceId,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).remove_resource(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RemoveResourceSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/AddVectorSet" => {
-                    #[allow(non_camel_case_types)]
-                    struct AddVectorSetSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::NewVectorSetRequest>
-                    for AddVectorSetSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::NewVectorSetRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).add_vector_set(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = AddVectorSetSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/RemoveVectorSet" => {
-                    #[allow(non_camel_case_types)]
-                    struct RemoveVectorSetSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::VectorSetId,
-                    > for RemoveVectorSetSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::VectorSetId,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).remove_vector_set(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RemoveVectorSetSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/ListVectorSets" => {
-                    #[allow(non_camel_case_types)]
-                    struct ListVectorSetsSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for ListVectorSetsSvc<T> {
-                        type Response = super::super::noderesources::VectorSetList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).list_vector_sets(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ListVectorSetsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/GetMetadata" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetMetadataSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::EmptyQuery,
-                    > for GetMetadataSvc<T> {
-                        type Response = super::super::noderesources::NodeMetadata;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::EmptyQuery,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).get_metadata(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetMetadataSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                _ => {
-                    Box::pin(async move {
-                        Ok(
-                            http::Response::builder()
-                                .status(200)
-                                .header("grpc-status", "12")
-                                .header("content-type", "application/grpc")
-                                .body(empty_body())
-                                .unwrap(),
-                        )
-                    })
-                }
-            }
-        }
-    }
-    impl<T: NodeWriter> Clone for NodeWriterServer<T> {
-        fn clone(&self) -> Self {
-            let inner = self.inner.clone();
-            Self {
-                inner,
-                accept_compression_encodings: self.accept_compression_encodings,
-                send_compression_encodings: self.send_compression_encodings,
-            }
-        }
-    }
-    impl<T: NodeWriter> Clone for _Inner<T> {
-        fn clone(&self) -> Self {
-            Self(self.0.clone())
-        }
-    }
-    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            write!(f, "{:?}", self.0)
-        }
-    }
-    impl<T: NodeWriter> tonic::transport::NamedService for NodeWriterServer<T> {
-        const NAME: &'static str = "nodewriter.NodeWriter";
-    }
-}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OpStatus {
+    #[prost(enumeration="op_status::Status", tag="1")]
+    pub status: i32,
+    #[prost(string, tag="2")]
+    pub detail: ::prost::alloc::string::String,
+    #[prost(uint64, tag="3")]
+    pub field_count: u64,
+    #[prost(uint64, tag="5")]
+    pub paragraph_count: u64,
+    #[prost(uint64, tag="6")]
+    pub sentence_count: u64,
+    #[prost(string, tag="4")]
+    pub shard_id: ::prost::alloc::string::String,
+}
+/// Nested message and enum types in `OpStatus`.
+pub mod op_status {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Warning = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexMessage {
+    #[prost(string, tag="1")]
+    pub node: ::prost::alloc::string::String,
+    /// physical shard message is for
+    #[prost(string, tag="2")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(uint64, tag="3")]
+    pub txid: u64,
+    #[prost(string, tag="4")]
+    pub resource: ::prost::alloc::string::String,
+    #[prost(enumeration="TypeMessage", tag="5")]
+    pub typemessage: i32,
+    #[prost(string, tag="6")]
+    pub reindex_id: ::prost::alloc::string::String,
+    #[prost(string, optional, tag="7")]
+    pub partition: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(string, tag="8")]
+    pub storage_key: ::prost::alloc::string::String,
+    #[prost(string, tag="9")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetGraph {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, optional, tag="2")]
+    pub graph: ::core::option::Option<super::utils::JoinGraph>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DeleteGraphNodes {
+    #[prost(message, optional, tag="2")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, repeated, tag="1")]
+    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewShardRequest {
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="1")]
+    pub similarity: i32,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub id: ::core::option::Option<super::noderesources::VectorSetId>,
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="2")]
+    pub similarity: i32,
+}
+// Implemented at nucliadb_object_storage
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+#[repr(i32)]
+pub enum TypeMessage {
+    Creation = 0,
+    Deletion = 1,
+}
+/// Generated client implementations.
+pub mod node_writer_client {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    #[derive(Debug, Clone)]
+    pub struct NodeWriterClient<T> {
+        inner: tonic::client::Grpc<T>,
+    }
+    impl NodeWriterClient<tonic::transport::Channel> {
+        /// Attempt to create a new client by connecting to a given endpoint.
+        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
+        where
+            D: std::convert::TryInto<tonic::transport::Endpoint>,
+            D::Error: Into<StdError>,
+        {
+            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
+            Ok(Self::new(conn))
+        }
+    }
+    impl<T> NodeWriterClient<T>
+    where
+        T: tonic::client::GrpcService<tonic::body::BoxBody>,
+        T::Error: Into<StdError>,
+        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
+        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
+    {
+        pub fn new(inner: T) -> Self {
+            let inner = tonic::client::Grpc::new(inner);
+            Self { inner }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> NodeWriterClient<InterceptedService<T, F>>
+        where
+            F: tonic::service::Interceptor,
+            T::ResponseBody: Default,
+            T: tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+                Response = http::Response<
+                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
+                >,
+            >,
+            <T as tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+            >>::Error: Into<StdError> + Send + Sync,
+        {
+            NodeWriterClient::new(InterceptedService::new(inner, interceptor))
+        }
+        /// Compress requests with `gzip`.
+        ///
+        /// This requires the server to support it otherwise it might respond with an
+        /// error.
+        #[must_use]
+        pub fn send_gzip(mut self) -> Self {
+            self.inner = self.inner.send_gzip();
+            self
+        }
+        /// Enable decompressing responses with `gzip`.
+        #[must_use]
+        pub fn accept_gzip(mut self) -> Self {
+            self.inner = self.inner.accept_gzip();
+            self
+        }
+        pub async fn new_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::NewShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCreated>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/NewShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn clean_and_upgrade_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCleaned>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/CleanAndUpgradeShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn delete_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/DeleteShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn list_shards(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardIds>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/ListShards",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn gc(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static("/nodewriter.NodeWriter/GC");
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn set_resource(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::Resource>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/SetResource",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn delete_relation_nodes(
+            &mut self,
+            request: impl tonic::IntoRequest<super::DeleteGraphNodes>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/DeleteRelationNodes",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn join_graph(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SetGraph>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/JoinGraph",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn remove_resource(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ResourceId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/RemoveResource",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn add_vector_set(
+            &mut self,
+            request: impl tonic::IntoRequest<super::NewVectorSetRequest>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/AddVectorSet",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn remove_vector_set(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::VectorSetId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/RemoveVectorSet",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn list_vector_sets(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::VectorSetList>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/ListVectorSets",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn get_metadata(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::NodeMetadata>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/GetMetadata",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+    }
+}
+/// Generated server implementations.
+pub mod node_writer_server {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    ///Generated trait containing gRPC methods that should be implemented for use with NodeWriterServer.
+    #[async_trait]
+    pub trait NodeWriter: Send + Sync + 'static {
+        async fn new_shard(
+            &self,
+            request: tonic::Request<super::NewShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCreated>,
+            tonic::Status,
+        >;
+        async fn clean_and_upgrade_shard(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCleaned>,
+            tonic::Status,
+        >;
+        async fn delete_shard(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        >;
+        async fn list_shards(
+            &self,
+            request: tonic::Request<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardIds>,
+            tonic::Status,
+        >;
+        async fn gc(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        >;
+        async fn set_resource(
+            &self,
+            request: tonic::Request<super::super::noderesources::Resource>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn delete_relation_nodes(
+            &self,
+            request: tonic::Request<super::DeleteGraphNodes>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn join_graph(
+            &self,
+            request: tonic::Request<super::SetGraph>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn remove_resource(
+            &self,
+            request: tonic::Request<super::super::noderesources::ResourceId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn add_vector_set(
+            &self,
+            request: tonic::Request<super::NewVectorSetRequest>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn remove_vector_set(
+            &self,
+            request: tonic::Request<super::super::noderesources::VectorSetId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn list_vector_sets(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::VectorSetList>,
+            tonic::Status,
+        >;
+        async fn get_metadata(
+            &self,
+            request: tonic::Request<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::NodeMetadata>,
+            tonic::Status,
+        >;
+    }
+    #[derive(Debug)]
+    pub struct NodeWriterServer<T: NodeWriter> {
+        inner: _Inner<T>,
+        accept_compression_encodings: (),
+        send_compression_encodings: (),
+    }
+    struct _Inner<T>(Arc<T>);
+    impl<T: NodeWriter> NodeWriterServer<T> {
+        pub fn new(inner: T) -> Self {
+            Self::from_arc(Arc::new(inner))
+        }
+        pub fn from_arc(inner: Arc<T>) -> Self {
+            let inner = _Inner(inner);
+            Self {
+                inner,
+                accept_compression_encodings: Default::default(),
+                send_compression_encodings: Default::default(),
+            }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> InterceptedService<Self, F>
+        where
+            F: tonic::service::Interceptor,
+        {
+            InterceptedService::new(Self::new(inner), interceptor)
+        }
+    }
+    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeWriterServer<T>
+    where
+        T: NodeWriter,
+        B: Body + Send + 'static,
+        B::Error: Into<StdError> + Send + 'static,
+    {
+        type Response = http::Response<tonic::body::BoxBody>;
+        type Error = std::convert::Infallible;
+        type Future = BoxFuture<Self::Response, Self::Error>;
+        fn poll_ready(
+            &mut self,
+            _cx: &mut Context<'_>,
+        ) -> Poll<Result<(), Self::Error>> {
+            Poll::Ready(Ok(()))
+        }
+        fn call(&mut self, req: http::Request<B>) -> Self::Future {
+            let inner = self.inner.clone();
+            match req.uri().path() {
+                "/nodewriter.NodeWriter/NewShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct NewShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::NewShardRequest>
+                    for NewShardSvc<T> {
+                        type Response = super::super::noderesources::ShardCreated;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::NewShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).new_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = NewShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/CleanAndUpgradeShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct CleanAndUpgradeShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for CleanAndUpgradeShardSvc<T> {
+                        type Response = super::super::noderesources::ShardCleaned;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).clean_and_upgrade_shard(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = CleanAndUpgradeShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/DeleteShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct DeleteShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for DeleteShardSvc<T> {
+                        type Response = super::super::noderesources::ShardId;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).delete_shard(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DeleteShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/ListShards" => {
+                    #[allow(non_camel_case_types)]
+                    struct ListShardsSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::EmptyQuery,
+                    > for ListShardsSvc<T> {
+                        type Response = super::super::noderesources::ShardIds;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::EmptyQuery,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).list_shards(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ListShardsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/GC" => {
+                    #[allow(non_camel_case_types)]
+                    struct GCSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for GCSvc<T> {
+                        type Response = super::super::noderesources::EmptyResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).gc(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GCSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/SetResource" => {
+                    #[allow(non_camel_case_types)]
+                    struct SetResourceSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::Resource>
+                    for SetResourceSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::Resource,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).set_resource(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SetResourceSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/DeleteRelationNodes" => {
+                    #[allow(non_camel_case_types)]
+                    struct DeleteRelationNodesSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::DeleteGraphNodes>
+                    for DeleteRelationNodesSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::DeleteGraphNodes>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).delete_relation_nodes(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DeleteRelationNodesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/JoinGraph" => {
+                    #[allow(non_camel_case_types)]
+                    struct JoinGraphSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<T: NodeWriter> tonic::server::UnaryService<super::SetGraph>
+                    for JoinGraphSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SetGraph>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).join_graph(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = JoinGraphSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/RemoveResource" => {
+                    #[allow(non_camel_case_types)]
+                    struct RemoveResourceSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::ResourceId,
+                    > for RemoveResourceSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::ResourceId,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).remove_resource(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RemoveResourceSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/AddVectorSet" => {
+                    #[allow(non_camel_case_types)]
+                    struct AddVectorSetSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::NewVectorSetRequest>
+                    for AddVectorSetSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::NewVectorSetRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).add_vector_set(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = AddVectorSetSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/RemoveVectorSet" => {
+                    #[allow(non_camel_case_types)]
+                    struct RemoveVectorSetSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::VectorSetId,
+                    > for RemoveVectorSetSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::VectorSetId,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).remove_vector_set(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RemoveVectorSetSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/ListVectorSets" => {
+                    #[allow(non_camel_case_types)]
+                    struct ListVectorSetsSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for ListVectorSetsSvc<T> {
+                        type Response = super::super::noderesources::VectorSetList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).list_vector_sets(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ListVectorSetsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/GetMetadata" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetMetadataSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::EmptyQuery,
+                    > for GetMetadataSvc<T> {
+                        type Response = super::super::noderesources::NodeMetadata;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::EmptyQuery,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).get_metadata(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetMetadataSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                _ => {
+                    Box::pin(async move {
+                        Ok(
+                            http::Response::builder()
+                                .status(200)
+                                .header("grpc-status", "12")
+                                .header("content-type", "application/grpc")
+                                .body(empty_body())
+                                .unwrap(),
+                        )
+                    })
+                }
+            }
+        }
+    }
+    impl<T: NodeWriter> Clone for NodeWriterServer<T> {
+        fn clone(&self) -> Self {
+            let inner = self.inner.clone();
+            Self {
+                inner,
+                accept_compression_encodings: self.accept_compression_encodings,
+                send_compression_encodings: self.send_compression_encodings,
+            }
+        }
+    }
+    impl<T: NodeWriter> Clone for _Inner<T> {
+        fn clone(&self) -> Self {
+            Self(self.0.clone())
+        }
+    }
+    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            write!(f, "{:?}", self.0)
+        }
+    }
+    impl<T: NodeWriter> tonic::transport::NamedService for NodeWriterServer<T> {
+        const NAME: &'static str = "nodewriter.NodeWriter";
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.9/local_dependencies/nucliadb_protos/src/utils.rs` & `nucliadb_node_binding-0.8.0/local_dependencies/nucliadb_protos/src/utils.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-/// Relations are connexions between nodes in the relation index.
-/// They are tuplets (Source, Relation Type, Relation Label, To).
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Relation {
-    #[prost(message, optional, tag="6")]
-    pub source: ::core::option::Option<RelationNode>,
-    #[prost(message, optional, tag="7")]
-    pub to: ::core::option::Option<RelationNode>,
-    #[prost(enumeration="relation::RelationType", tag="5")]
-    pub relation: i32,
-    #[prost(string, tag="8")]
-    pub relation_label: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="9")]
-    pub metadata: ::core::option::Option<RelationMetadata>,
-}
-/// Nested message and enum types in `Relation`.
-pub mod relation {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum RelationType {
-        /// Child resource
-        Child = 0,
-        /// related with label (GENERATED)
-        About = 1,
-        /// related with an entity (GENERATED)
-        Entity = 2,
-        /// related with user (GENERATED)
-        Colab = 3,
-        /// Synonym relation
-        Synonym = 4,
-        /// related with something
-        Other = 5,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationMetadata {
-    #[prost(string, optional, tag="1")]
-    pub paragraph_id: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(int32, optional, tag="2")]
-    pub source_start: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="3")]
-    pub source_end: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="4")]
-    pub to_start: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="5")]
-    pub to_end: ::core::option::Option<i32>,
-}
-/// Nodes are tuplets (Value, Type, Subtype) and they are the main element in the relation index.
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationNode {
-    /// Value of the node.
-    #[prost(string, tag="4")]
-    pub value: ::prost::alloc::string::String,
-    /// The type of the node.
-    #[prost(enumeration="relation_node::NodeType", tag="5")]
-    pub ntype: i32,
-    /// A node may have a subtype (the string should be empty in case it does not).
-    #[prost(string, tag="6")]
-    pub subtype: ::prost::alloc::string::String,
-}
-/// Nested message and enum types in `RelationNode`.
-pub mod relation_node {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum NodeType {
-        Entity = 0,
-        Label = 1,
-        Resource = 2,
-        User = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct JoinGraphEdge {
-    #[prost(int32, tag="4")]
-    pub source: i32,
-    #[prost(int32, tag="1")]
-    pub target: i32,
-    #[prost(enumeration="relation::RelationType", tag="2")]
-    pub rtype: i32,
-    #[prost(string, tag="3")]
-    pub rsubtype: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub metadata: ::core::option::Option<RelationMetadata>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct JoinGraph {
-    #[prost(map="int32, message", tag="1")]
-    pub nodes: ::std::collections::HashMap<i32, RelationNode>,
-    #[prost(message, repeated, tag="2")]
-    pub edges: ::prost::alloc::vec::Vec<JoinGraphEdge>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ExtractedText {
-    #[prost(string, tag="1")]
-    pub text: ::prost::alloc::string::String,
-    #[prost(map="string, string", tag="2")]
-    pub split_text: ::std::collections::HashMap<::prost::alloc::string::String, ::prost::alloc::string::String>,
-    #[prost(string, repeated, tag="3")]
-    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Vector {
-    #[prost(int32, tag="1")]
-    pub start: i32,
-    #[prost(int32, tag="2")]
-    pub end: i32,
-    #[prost(int32, tag="3")]
-    pub start_paragraph: i32,
-    #[prost(int32, tag="4")]
-    pub end_paragraph: i32,
-    #[prost(float, repeated, tag="5")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Vectors {
-    #[prost(message, repeated, tag="1")]
-    pub vectors: ::prost::alloc::vec::Vec<Vector>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorObject {
-    #[prost(message, optional, tag="1")]
-    pub vectors: ::core::option::Option<Vectors>,
-    #[prost(map="string, message", tag="2")]
-    pub split_vectors: ::std::collections::HashMap<::prost::alloc::string::String, Vectors>,
-    #[prost(string, repeated, tag="3")]
-    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVector {
-    #[prost(float, repeated, tag="1")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    #[prost(string, repeated, tag="2")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(int32, tag="3")]
-    pub start: i32,
-    #[prost(int32, tag="4")]
-    pub end: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectors {
-    /// vector's id
-    #[prost(map="string, message", tag="1")]
-    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVector>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectorSet {
-    /// vectorsets
-    #[prost(map="string, message", tag="1")]
-    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVectors>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectorsList {
-    #[prost(string, repeated, tag="1")]
-    pub vectors: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-#[repr(i32)]
-pub enum VectorSimilarity {
-    Cosine = 0,
-    Dot = 1,
-}
+/// Relations are connexions between nodes in the relation index.
+/// They are tuplets (Source, Relation Type, Relation Label, To).
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Relation {
+    #[prost(message, optional, tag="6")]
+    pub source: ::core::option::Option<RelationNode>,
+    #[prost(message, optional, tag="7")]
+    pub to: ::core::option::Option<RelationNode>,
+    #[prost(enumeration="relation::RelationType", tag="5")]
+    pub relation: i32,
+    #[prost(string, tag="8")]
+    pub relation_label: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="9")]
+    pub metadata: ::core::option::Option<RelationMetadata>,
+}
+/// Nested message and enum types in `Relation`.
+pub mod relation {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum RelationType {
+        /// Child resource
+        Child = 0,
+        /// related with label (GENERATED)
+        About = 1,
+        /// related with an entity (GENERATED)
+        Entity = 2,
+        /// related with user (GENERATED)
+        Colab = 3,
+        /// Synonym relation
+        Synonym = 4,
+        /// related with something
+        Other = 5,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationMetadata {
+    #[prost(string, optional, tag="1")]
+    pub paragraph_id: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(int32, optional, tag="2")]
+    pub source_start: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="3")]
+    pub source_end: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="4")]
+    pub to_start: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="5")]
+    pub to_end: ::core::option::Option<i32>,
+}
+/// Nodes are tuplets (Value, Type, Subtype) and they are the main element in the relation index.
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationNode {
+    /// Value of the node.
+    #[prost(string, tag="4")]
+    pub value: ::prost::alloc::string::String,
+    /// The type of the node.
+    #[prost(enumeration="relation_node::NodeType", tag="5")]
+    pub ntype: i32,
+    /// A node may have a subtype (the string should be empty in case it does not).
+    #[prost(string, tag="6")]
+    pub subtype: ::prost::alloc::string::String,
+}
+/// Nested message and enum types in `RelationNode`.
+pub mod relation_node {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum NodeType {
+        Entity = 0,
+        Label = 1,
+        Resource = 2,
+        User = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct JoinGraphEdge {
+    #[prost(int32, tag="4")]
+    pub source: i32,
+    #[prost(int32, tag="1")]
+    pub target: i32,
+    #[prost(enumeration="relation::RelationType", tag="2")]
+    pub rtype: i32,
+    #[prost(string, tag="3")]
+    pub rsubtype: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub metadata: ::core::option::Option<RelationMetadata>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct JoinGraph {
+    #[prost(map="int32, message", tag="1")]
+    pub nodes: ::std::collections::HashMap<i32, RelationNode>,
+    #[prost(message, repeated, tag="2")]
+    pub edges: ::prost::alloc::vec::Vec<JoinGraphEdge>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ExtractedText {
+    #[prost(string, tag="1")]
+    pub text: ::prost::alloc::string::String,
+    #[prost(map="string, string", tag="2")]
+    pub split_text: ::std::collections::HashMap<::prost::alloc::string::String, ::prost::alloc::string::String>,
+    #[prost(string, repeated, tag="3")]
+    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Vector {
+    #[prost(int32, tag="1")]
+    pub start: i32,
+    #[prost(int32, tag="2")]
+    pub end: i32,
+    #[prost(int32, tag="3")]
+    pub start_paragraph: i32,
+    #[prost(int32, tag="4")]
+    pub end_paragraph: i32,
+    #[prost(float, repeated, tag="5")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Vectors {
+    #[prost(message, repeated, tag="1")]
+    pub vectors: ::prost::alloc::vec::Vec<Vector>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorObject {
+    #[prost(message, optional, tag="1")]
+    pub vectors: ::core::option::Option<Vectors>,
+    #[prost(map="string, message", tag="2")]
+    pub split_vectors: ::std::collections::HashMap<::prost::alloc::string::String, Vectors>,
+    #[prost(string, repeated, tag="3")]
+    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVector {
+    #[prost(float, repeated, tag="1")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    #[prost(string, repeated, tag="2")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(int32, tag="3")]
+    pub start: i32,
+    #[prost(int32, tag="4")]
+    pub end: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectors {
+    /// vector's id
+    #[prost(map="string, message", tag="1")]
+    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVector>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectorSet {
+    /// vectorsets
+    #[prost(map="string, message", tag="1")]
+    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVectors>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectorsList {
+    #[prost(string, repeated, tag="1")]
+    pub vectors: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+#[repr(i32)]
+pub enum VectorSimilarity {
+    Cosine = 0,
+    Dot = 1,
+}
```

### Comparing `nucliadb_node_binding-0.7.9/Cargo.toml` & `nucliadb_node_binding-0.8.0/Cargo.toml`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 [package]
 name = "nucliadb_node_binding"
-version = "0.7.9"
+version = "0.8.0"
 edition = "2021"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 [lib]
 name = "nucliadb_node_binding"
 crate-type = ["cdylib"]
 
 [dependencies]
-pyo3 = { version = "0.17.1", features = ["extension-module"] }
+pyo3 = { version = "0.19.1", features = ["extension-module"] }
 nucliadb_node = { path = "local_dependencies/nucliadb_node" }
 nucliadb_core= { path = "local_dependencies/nucliadb_core" }
 nucliadb_telemetry = { path = "local_dependencies/nucliadb_telemetry" }
 serde = { version = "1.0", features = ["derive"] }
 tokio = { version = "1", features = ["full"] }
 log = "0.4"
 bincode = "1.3.3"
 
 openssl = { version = "0.10", features = ["vendored"] }
 prost = "0.10"
 prost-types = "0.10"
 tracing = { version = "0.1.29" }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
 ] }
```

### Comparing `nucliadb_node_binding-0.7.9/test.py` & `nucliadb_node_binding-0.8.0/test.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from datetime import datetime
-
-import nucliadb_node_binding  # type: ignore
-from nucliadb_protos.nodereader_pb2 import SearchRequest, SearchResponse
-from nucliadb_protos.noderesources_pb2 import Resource
-from nucliadb_protos.nodewriter_pb2 import ShardCreated
-
-
-async def main():
-    writer = nucliadb_node_binding.NodeWriter.new()
-    reader = nucliadb_node_binding.NodeReader.new()
-    shard = await writer.new_shard("test-kbid")
-    pb = ShardCreated()
-    pb.ParseFromString(bytearray(shard))
-
-    resourcepb = Resource()
-    resourcepb.resource.uuid = "001"
-    resourcepb.resource.shard_id = pb.id
-    resourcepb.texts["field1"].text = "My lovely text"
-    resourcepb.status = Resource.ResourceStatus.PROCESSED
-    resourcepb.shard_id = pb.id
-    resourcepb.metadata.created.FromDatetime(datetime.now())
-    resourcepb.metadata.modified.FromDatetime(datetime.now())
-    await writer.set_resource(resourcepb.SerializeToString())
-
-    searchpb = SearchRequest()
-    searchpb.shard = pb.id
-    searchpb.body = "text"
-    pbresult = await reader.search(searchpb.SerializeToString())
-    pb = SearchResponse()
-    pb.ParseFromString(bytearray(pbresult))
-
-    print(pb)
-
-
-asyncio.run(main())
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from datetime import datetime
+
+import nucliadb_node_binding  # type: ignore
+from nucliadb_protos.nodereader_pb2 import SearchRequest, SearchResponse
+from nucliadb_protos.noderesources_pb2 import Resource
+from nucliadb_protos.nodewriter_pb2 import ShardCreated
+
+
+async def main():
+    writer = nucliadb_node_binding.NodeWriter.new()
+    reader = nucliadb_node_binding.NodeReader.new()
+    shard = await writer.new_shard("test-kbid")
+    pb = ShardCreated()
+    pb.ParseFromString(bytearray(shard))
+
+    resourcepb = Resource()
+    resourcepb.resource.uuid = "001"
+    resourcepb.resource.shard_id = pb.id
+    resourcepb.texts["field1"].text = "My lovely text"
+    resourcepb.status = Resource.ResourceStatus.PROCESSED
+    resourcepb.shard_id = pb.id
+    resourcepb.metadata.created.FromDatetime(datetime.now())
+    resourcepb.metadata.modified.FromDatetime(datetime.now())
+    await writer.set_resource(resourcepb.SerializeToString())
+
+    searchpb = SearchRequest()
+    searchpb.shard = pb.id
+    searchpb.body = "text"
+    pbresult = await reader.search(searchpb.SerializeToString())
+    pb = SearchResponse()
+    pb.ParseFromString(bytearray(pbresult))
+
+    print(pb)
+
+
+asyncio.run(main())
```

