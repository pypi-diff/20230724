# Comparing `tmp/mb_pytorch-1.0.202304270110-py3-none-any.whl.zip` & `tmp/mb_pytorch-1.0.202307232329-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,12 +1,12 @@
-Zip file size: 38421 bytes, number of entries: 39
+Zip file size: 38550 bytes, number of entries: 39
 -rw-rw-r--  2.0 unx     7166 b- defN 23-Apr-27 00:22 mb_pytorch/classification/training.py
 -rw-rw-r--  2.0 unx       44 b- defN 23-Mar-16 11:39 mb_pytorch/dataloader/__init__.py
 -rw-rw-r--  2.0 unx    12981 b- defN 23-Apr-19 01:00 mb_pytorch/dataloader/loader.py
--rw-rw-r--  2.0 unx     6487 b- defN 23-Apr-26 23:34 mb_pytorch/detection/training.py
+-rw-rw-r--  2.0 unx     7162 b- defN 23-May-05 22:37 mb_pytorch/detection/training.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-23 13:56 mb_pytorch/metalearning/__init__.py
 -rw-rw-r--  2.0 unx     1385 b- defN 23-Mar-02 03:28 mb_pytorch/metalearning/meta_utils.py
 -rw-rw-r--  2.0 unx     1030 b- defN 23-Mar-15 02:58 mb_pytorch/metalearning/proto_dataloader.py
 -rw-rw-r--  2.0 unx     2861 b- defN 23-Mar-03 23:55 mb_pytorch/metalearning/prototypical.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 19:54 mb_pytorch/models/__init__.py
 -rw-rw-r--  2.0 unx    21087 b- defN 23-Apr-27 01:09 mb_pytorch/models/extra_models.py
 -rw-rw-r--  2.0 unx      920 b- defN 23-Mar-12 03:33 mb_pytorch/models/lenet.py
@@ -28,14 +28,14 @@
 -rw-rw-r--  2.0 unx      257 b- defN 23-Mar-01 22:55 mb_pytorch/utils/dist.py
 -rw-rw-r--  2.0 unx     3391 b- defN 23-Apr-11 23:28 mb_pytorch/utils/extra_utils.py
 -rw-rw-r--  2.0 unx     7178 b- defN 23-Mar-15 02:58 mb_pytorch/utils/generate_emb.py
 -rw-rw-r--  2.0 unx     2582 b- defN 23-Apr-03 19:30 mb_pytorch/utils/losses.py
 -rw-rw-r--  2.0 unx     1199 b- defN 23-Apr-04 20:14 mb_pytorch/utils/metrics.py
 -rw-rw-r--  2.0 unx    10502 b- defN 23-Apr-25 00:33 mb_pytorch/utils/viewer.py
 -rw-rw-r--  2.0 unx      994 b- defN 23-Mar-06 13:11 mb_pytorch/utils/yaml_reader.py
--rwxrwxr-x  2.0 unx     1304 b- defN 23-Apr-27 01:37 mb_pytorch-1.0.202304270110.data/scripts/dataload_results.py
--rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202304270110.data/scripts/emb.py
--rw-rw-r--  2.0 unx      329 b- defN 23-Apr-27 01:37 mb_pytorch-1.0.202304270110.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-27 01:37 mb_pytorch-1.0.202304270110.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-Apr-27 01:37 mb_pytorch-1.0.202304270110.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3529 b- defN 23-Apr-27 01:37 mb_pytorch-1.0.202304270110.dist-info/RECORD
-39 files, 125410 bytes uncompressed, 32673 bytes compressed:  73.9%
+-rwxrwxr-x  2.0 unx     1304 b- defN 23-Jul-23 23:29 mb_pytorch-1.0.202307232329.data/scripts/dataload_results.py
+-rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202307232329.data/scripts/emb.py
+-rw-rw-r--  2.0 unx      348 b- defN 23-Jul-23 23:29 mb_pytorch-1.0.202307232329.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-23 23:29 mb_pytorch-1.0.202307232329.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-Jul-23 23:29 mb_pytorch-1.0.202307232329.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3529 b- defN 23-Jul-23 23:29 mb_pytorch-1.0.202307232329.dist-info/RECORD
+39 files, 126104 bytes uncompressed, 32802 bytes compressed:  74.0%
```

## zipnote {}

```diff
@@ -93,26 +93,26 @@
 
 Filename: mb_pytorch/utils/viewer.py
 Comment: 
 
 Filename: mb_pytorch/utils/yaml_reader.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.data/scripts/dataload_results.py
+Filename: mb_pytorch-1.0.202307232329.data/scripts/dataload_results.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.data/scripts/emb.py
+Filename: mb_pytorch-1.0.202307232329.data/scripts/emb.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.dist-info/METADATA
+Filename: mb_pytorch-1.0.202307232329.dist-info/METADATA
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.dist-info/WHEEL
+Filename: mb_pytorch-1.0.202307232329.dist-info/WHEEL
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.dist-info/top_level.txt
+Filename: mb_pytorch-1.0.202307232329.dist-info/top_level.txt
 Comment: 
 
-Filename: mb_pytorch-1.0.202304270110.dist-info/RECORD
+Filename: mb_pytorch-1.0.202307232329.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mb_pytorch/detection/training.py

```diff
@@ -1,37 +1,57 @@
 import torch
+from typing import Optional
 import tqdm
-from torch.utils.tensorboard import SummaryWriter
 import os
 from mb_utils.src.logging import logger
 import numpy as np
 from ..utils.viewer import gradcam_viewer,create_img_grid,plot_classes_pred
+from mb_pytorch.models.modelloader import ModelLoader
+from mb_pytorch.training.train_params import train_helper
 
-__all__ = ['detection_train_loop']
+__all__ = ['segmentation_train_loop']
 
-def detection_train_loop(k_data,data_model,model,train_loader,val_loader,loss_attr,optimizer,scheduler=None,writer=None,logger=None,gradcam=None,gradcam_rgb=False,device='cpu'):
+def segmentation_train_loop( k_yaml: dict,scheduler: Optional[object] =None,writer: Optional[object] =None,
+                              logger: Optional[object] =None,gradcam: Optional[object] =None,
+                              gradcam_rgb: str =False,device: str ='cpu'):
     """
     Function to train the model
     Args:
-        data: data dictionary YAML of DataLoader
-        data_model: model parameters - data.data_dict['model']
-        model: model to be trained
-        train_loader: train dataloader
-        val_loader: validation dataloader
-        loss_attr: loss function
-        optimizer: optimizer
+        k_yaml: data dictionary YAML of DataLoader
         scheduler: scheduler
         writer: tensorboard writer
         logger: logger
         gradcam: gradcam layers to be visulized
         device: default is cpu
     output:
         None
     """
     
+    if logger:
+        logger.info('Training loop Starting')
+    k_data = k_yaml.data_dict['data']
+    data_model = k_yaml.data_dict['model']
+    model_data_load = ModelLoader(k_yaml.data_dict['model'])
+    model =  model_data_load.get_model()
+    
+    if logger:
+        logger.info('Model Loaded')
+    
+    train_loader,val_loader,_,_ = k_yaml.data_load()
+    loss_attr,optimizer_attr,optimizer_dict,scheduler_attr,scheduler_dict = train_helper(data_model) 
+    optimizer = optimizer_attr(model.parameters(),**optimizer_dict)
+    if scheduler is not None:
+        scheduler = scheduler_attr(optimizer,**scheduler_dict)
+
+    if logger:
+        logger.info('Optimizer and Scheduler Loaded')
+        logger.info(f'Loss: {loss_attr}')
+        logger.info(f'Optimizer: {optimizer}')
+        logger.info(f'Scheduler: {scheduler}')
+    
     model.to(device)
 
     for i in tqdm.tqdm(range(data_model['model_epochs'])):
         
         ##train loop
         
         model.train(True)
@@ -52,17 +72,16 @@
             if logger:
                 logger.info(f'Epoch {i+1} - Batch {j+1} - Train Loss: {current_loss.item()}')
             
 
         avg_train_loss = train_loss / len(train_loader)
         if logger:
             logger.info(f'Epoch {i+1} - Train Loss: {avg_train_loss}')
-
-
-        print('lr = ',optimizer.param_groups[0]['lr'])
+            logger.info(f"lr = {optimizer.param_groups[0]['lr']}")
+        
         model.train(False)
     
         if writer is not None:
             writer.add_graph(model, x)
             writer.add_scalar('Loss/train', avg_train_loss, global_step=i)
             for name, param in model.named_parameters():
                 writer.add_histogram(name, param, global_step=i)
@@ -85,39 +104,35 @@
                     if grad_img is not None:
                         grad_img = np.transpose(grad_img,(2,0,1))
                         writer.add_image(f'Gradcam training/{cam_layers}',grad_img,global_step=i)
                     if j == 0:
                         if grad_img is None:
                             if logger:
                                 logger.info(f'Gradcam not supported for {cam_layers}')            
-
-            
-            
+                        
         #validation loop
 
         val_loss = 0
         val_acc = 0
         new_val_loss = 0
     
         with torch.no_grad():
             for l,(x_val, y_val) in enumerate(val_loader):
                 x_val, y_val = x_val.to(device), y_val.to(device)
                 output = model(x_val)
                 val_loss += loss_attr(output, y_val).item() * x_val.size(0)
                 _, preds = torch.max(output, 1) #no need of softmax. max returns the index of the max value
                 val_acc += torch.sum(preds == y_val.data)
                 new_val_loss = val_loss/x_val.size(0)
-                #num_samples += x_val.size(0)
                 if logger: 
                     logger.info(f'Epoch {i+1} - Batch {l+1} - Val Loss: {new_val_loss:.3f}')
             
             avg_val_loss = val_loss / len(val_loader.dataset)
             val_acc = val_acc/len(val_loader.dataset)
-            #val_loss /= num_samples
-            #val_acc = val_acc / num_samples
+
             if logger:
                 logger.info(f'Epoch {i+1} -Avg Val Loss: {avg_val_loss:.3f}')
                 logger.info(f'Epoch {i+1} - Val Accuracy: {val_acc:.3f}')
     
     
         if writer is not None:
             writer.add_scalar('Loss/val', val_loss, global_step=i)
```

## Comparing `mb_pytorch-1.0.202304270110.data/scripts/dataload_results.py` & `mb_pytorch-1.0.202307232329.data/scripts/dataload_results.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304270110.data/scripts/emb.py` & `mb_pytorch-1.0.202307232329.data/scripts/emb.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304270110.dist-info/RECORD` & `mb_pytorch-1.0.202307232329.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 mb_pytorch/classification/training.py,sha256=Xuykvv3ege9fKFrSzbsXTotEJQVEy8yq5coZqPP_noY,7166
 mb_pytorch/dataloader/__init__.py,sha256=nB0xPAHbI91Ra1dDkWR1l4td5A4k9xko-I5Jdgv5apI,44
 mb_pytorch/dataloader/loader.py,sha256=Cn6N9MYVRxpm-fZznk6uu0nzdWCPQWKyK93DNVlKUvk,12981
-mb_pytorch/detection/training.py,sha256=4ptrDukFabYypq3gva4vetAWEzCy9OGQ-RTBzBhdDmg,6487
+mb_pytorch/detection/training.py,sha256=Pp9f9KR0MoKe7Afa0UHLsYvU7A7OAVUTZQRrNtS3v70,7162
 mb_pytorch/metalearning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/metalearning/meta_utils.py,sha256=mgHYiQIIcYQ1pVTJcrjquSXpQstdYD1q8iXO09Zao1s,1385
 mb_pytorch/metalearning/proto_dataloader.py,sha256=WvrfZYkYMxorocCkR_zHS_AC8W_ML9YndB-P6evkdcc,1030
 mb_pytorch/metalearning/prototypical.py,sha256=qFVf6VF3s8zskGqbM3geJV-dfkdO3tRaf3P8U_KR-cE,2861
 mb_pytorch/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/models/extra_models.py,sha256=rNZjn8DeS9dYjk6mlrABDyDSLsZlUOx2l2pT03eaGTw,21087
 mb_pytorch/models/lenet.py,sha256=vZpN0LsfVGNI6z91YO3GLPCVB9Sv0EAxaTwEavRSKKo,920
@@ -27,13 +27,13 @@
 mb_pytorch/utils/dist.py,sha256=7-ZdntmiugRWYnT5wileo8mYTuV1dbjVl4ffJsfnfAw,257
 mb_pytorch/utils/extra_utils.py,sha256=-MaT-x3gwgEOVLpg-tWm5yLFtI0CxpV0QUlXx-rqu08,3391
 mb_pytorch/utils/generate_emb.py,sha256=2iK8wRIrYfaLpEgjdbFnDqGU5ux-1JhncQoeboW_6LQ,7178
 mb_pytorch/utils/losses.py,sha256=OLCPLkJH46IofSSVly2xdcklVv7Q5OFFEGtVrJcV7V0,2582
 mb_pytorch/utils/metrics.py,sha256=Kqmdu9llSjR8aRp3IVlmy6PqeQexf0ZXjTJUcEtvcfI,1199
 mb_pytorch/utils/viewer.py,sha256=oXOdgE7GNeg3aC6tTYq0-Lz47LO7ELWY-z-fQUgbUaE,10502
 mb_pytorch/utils/yaml_reader.py,sha256=Azgr_5qttsH_BBVsCtfccFMvK6IEjTRYhd5qp4S5uzk,994
-mb_pytorch-1.0.202304270110.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
-mb_pytorch-1.0.202304270110.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
-mb_pytorch-1.0.202304270110.dist-info/METADATA,sha256=DjFUP3BlpNeqn2ocItITNZbS1m6_340UGabJnxbQza0,329
-mb_pytorch-1.0.202304270110.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mb_pytorch-1.0.202304270110.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
-mb_pytorch-1.0.202304270110.dist-info/RECORD,,
+mb_pytorch-1.0.202307232329.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
+mb_pytorch-1.0.202307232329.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
+mb_pytorch-1.0.202307232329.dist-info/METADATA,sha256=habMZLRYXXQ0Rynu2VAeJibDmK057gn0bke43nGgQuo,348
+mb_pytorch-1.0.202307232329.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mb_pytorch-1.0.202307232329.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
+mb_pytorch-1.0.202307232329.dist-info/RECORD,,
```

