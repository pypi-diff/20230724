# Comparing `tmp/paddlesci-1.0.0-py3-none-any.whl.zip` & `tmp/paddlesci-1.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,103 +1,107 @@
-Zip file size: 180859 bytes, number of entries: 101
+Zip file size: 195193 bytes, number of entries: 105
 -rw-r--r--  2.0 unx     1334 b- defN 23-May-30 08:04 ppsci/__init__.py
--rw-r--r--  2.0 unx     1623 b- defN 23-May-30 08:04 ppsci/arch/__init__.py
--rw-r--r--  2.0 unx     1378 b- defN 23-May-30 08:04 ppsci/arch/activation.py
+-rw-r--r--  2.0 unx     1767 b- defN 23-Jul-24 17:13 ppsci/arch/__init__.py
+-rw-r--r--  2.0 unx     2793 b- defN 23-Jul-24 17:13 ppsci/arch/activation.py
 -rw-r--r--  2.0 unx    23667 b- defN 23-May-30 08:23 ppsci/arch/afno.py
--rw-r--r--  2.0 unx     4580 b- defN 23-May-30 08:04 ppsci/arch/base.py
+-rw-r--r--  2.0 unx     4569 b- defN 23-Jul-24 17:13 ppsci/arch/base.py
 -rw-r--r--  2.0 unx    17540 b- defN 23-May-30 08:04 ppsci/arch/embedding_koopman.py
--rw-r--r--  2.0 unx     4254 b- defN 23-May-30 08:04 ppsci/arch/mlp.py
+-rw-r--r--  2.0 unx    14276 b- defN 23-Jul-24 17:13 ppsci/arch/gan.py
+-rw-r--r--  2.0 unx     5458 b- defN 23-Jul-24 17:13 ppsci/arch/mlp.py
 -rw-r--r--  2.0 unx     1853 b- defN 23-May-30 08:04 ppsci/arch/model_list.py
 -rw-r--r--  2.0 unx    12830 b- defN 23-May-30 08:23 ppsci/arch/physx_transformer.py
 -rw-r--r--  2.0 unx      721 b- defN 23-May-30 08:04 ppsci/autodiff/__init__.py
--rw-r--r--  2.0 unx     6981 b- defN 23-May-30 08:23 ppsci/autodiff/ad.py
+-rw-r--r--  2.0 unx     7524 b- defN 23-Jul-24 17:13 ppsci/autodiff/ad.py
 -rw-r--r--  2.0 unx     3017 b- defN 23-May-30 08:04 ppsci/constraint/__init__.py
--rw-r--r--  2.0 unx     1797 b- defN 23-May-30 08:23 ppsci/constraint/base.py
--rw-r--r--  2.0 unx     6561 b- defN 23-May-30 08:23 ppsci/constraint/boundary_constraint.py
--rw-r--r--  2.0 unx     6546 b- defN 23-May-30 08:04 ppsci/constraint/initial_constraint.py
--rw-r--r--  2.0 unx     7033 b- defN 23-May-30 08:04 ppsci/constraint/integral_constraint.py
--rw-r--r--  2.0 unx     6713 b- defN 23-May-30 08:04 ppsci/constraint/interior_constraint.py
--rw-r--r--  2.0 unx     6794 b- defN 23-May-30 08:04 ppsci/constraint/periodic_constraint.py
--rw-r--r--  2.0 unx     2852 b- defN 23-May-30 08:04 ppsci/constraint/supervised_constraint.py
--rw-r--r--  2.0 unx     3509 b- defN 23-May-30 08:04 ppsci/data/__init__.py
+-rw-r--r--  2.0 unx     1855 b- defN 23-Jul-24 17:13 ppsci/constraint/base.py
+-rw-r--r--  2.0 unx     6619 b- defN 23-Jul-24 17:13 ppsci/constraint/boundary_constraint.py
+-rw-r--r--  2.0 unx     6864 b- defN 23-Jul-24 17:13 ppsci/constraint/initial_constraint.py
+-rw-r--r--  2.0 unx     7091 b- defN 23-Jul-24 17:13 ppsci/constraint/integral_constraint.py
+-rw-r--r--  2.0 unx     6981 b- defN 23-Jul-24 17:13 ppsci/constraint/interior_constraint.py
+-rw-r--r--  2.0 unx     7068 b- defN 23-Jul-24 17:13 ppsci/constraint/periodic_constraint.py
+-rw-r--r--  2.0 unx     2910 b- defN 23-Jul-24 17:13 ppsci/constraint/supervised_constraint.py
+-rw-r--r--  2.0 unx     3762 b- defN 23-Jul-24 17:13 ppsci/data/__init__.py
 -rw-r--r--  2.0 unx     1567 b- defN 23-May-30 08:04 ppsci/data/dataloader.py
--rw-r--r--  2.0 unx     2162 b- defN 23-May-30 08:04 ppsci/data/dataset/__init__.py
--rw-r--r--  2.0 unx     3899 b- defN 23-May-30 08:23 ppsci/data/dataset/array_dataset.py
+-rw-r--r--  2.0 unx     2162 b- defN 23-Jul-24 17:13 ppsci/data/dataset/__init__.py
+-rw-r--r--  2.0 unx     3962 b- defN 23-Jul-24 17:13 ppsci/data/dataset/array_dataset.py
 -rw-r--r--  2.0 unx    10257 b- defN 23-May-30 08:23 ppsci/data/dataset/csv_dataset.py
--rw-r--r--  2.0 unx     8589 b- defN 23-May-30 08:23 ppsci/data/dataset/era5_dataset.py
+-rw-r--r--  2.0 unx     8593 b- defN 23-Jul-24 17:13 ppsci/data/dataset/era5_dataset.py
 -rw-r--r--  2.0 unx    10348 b- defN 23-May-30 08:23 ppsci/data/dataset/mat_dataset.py
 -rw-r--r--  2.0 unx    10710 b- defN 23-May-30 08:04 ppsci/data/dataset/trphysx_dataset.py
 -rw-r--r--  2.0 unx     3216 b- defN 23-May-30 08:04 ppsci/data/dataset/vtu_dataset.py
 -rw-r--r--  2.0 unx      751 b- defN 23-May-30 08:04 ppsci/data/process/__init__.py
 -rw-r--r--  2.0 unx     2581 b- defN 23-May-30 08:04 ppsci/data/process/batch_transform/__init__.py
 -rw-r--r--  2.0 unx      607 b- defN 23-May-30 08:04 ppsci/data/process/batch_transform/preprocess.py
--rw-r--r--  2.0 unx     1584 b- defN 23-May-30 08:04 ppsci/data/process/transform/__init__.py
--rw-r--r--  2.0 unx     7637 b- defN 23-May-30 08:23 ppsci/data/process/transform/preprocess.py
--rw-r--r--  2.0 unx     1671 b- defN 23-May-30 08:04 ppsci/equation/__init__.py
--rw-r--r--  2.0 unx     1088 b- defN 23-May-30 08:04 ppsci/equation/pde/__init__.py
+-rw-r--r--  2.0 unx     1683 b- defN 23-Jul-24 17:13 ppsci/data/process/transform/__init__.py
+-rw-r--r--  2.0 unx     8710 b- defN 23-Jul-24 17:13 ppsci/data/process/transform/preprocess.py
+-rw-r--r--  2.0 unx     1743 b- defN 23-Jul-24 17:13 ppsci/equation/__init__.py
+-rw-r--r--  2.0 unx     1178 b- defN 23-Jul-24 17:13 ppsci/equation/pde/__init__.py
 -rw-r--r--  2.0 unx     2720 b- defN 23-May-30 08:04 ppsci/equation/pde/base.py
--rw-r--r--  2.0 unx     1514 b- defN 23-May-30 08:04 ppsci/equation/pde/biharmonic.py
--rw-r--r--  2.0 unx     1313 b- defN 23-May-30 08:04 ppsci/equation/pde/laplace.py
--rw-r--r--  2.0 unx     3974 b- defN 23-May-30 08:04 ppsci/equation/pde/navier_stokes.py
--rw-r--r--  2.0 unx     1416 b- defN 23-May-30 08:04 ppsci/equation/pde/normal_dot_vec.py
--rw-r--r--  2.0 unx     1261 b- defN 23-May-30 08:04 ppsci/equation/pde/poisson.py
--rw-r--r--  2.0 unx     2072 b- defN 23-May-30 08:04 ppsci/equation/pde/viv.py
+-rw-r--r--  2.0 unx     1566 b- defN 23-Jul-24 17:13 ppsci/equation/pde/biharmonic.py
+-rw-r--r--  2.0 unx     1354 b- defN 23-Jul-24 17:13 ppsci/equation/pde/laplace.py
+-rw-r--r--  2.0 unx    13372 b- defN 23-Jul-24 17:13 ppsci/equation/pde/linear_elasticity.py
+-rw-r--r--  2.0 unx     5578 b- defN 23-Jul-24 17:13 ppsci/equation/pde/navier_stokes.py
+-rw-r--r--  2.0 unx     1468 b- defN 23-Jul-24 17:13 ppsci/equation/pde/normal_dot_vec.py
+-rw-r--r--  2.0 unx     1302 b- defN 23-Jul-24 17:13 ppsci/equation/pde/poisson.py
+-rw-r--r--  2.0 unx     2194 b- defN 23-Jul-24 17:13 ppsci/equation/pde/viv.py
 -rw-r--r--  2.0 unx     2485 b- defN 23-May-30 08:04 ppsci/geometry/__init__.py
 -rw-r--r--  2.0 unx    10346 b- defN 23-May-30 08:04 ppsci/geometry/csg.py
--rw-r--r--  2.0 unx     7922 b- defN 23-May-30 08:23 ppsci/geometry/geometry.py
--rw-r--r--  2.0 unx     3479 b- defN 23-May-30 08:04 ppsci/geometry/geometry_1d.py
--rw-r--r--  2.0 unx    19590 b- defN 23-May-30 08:23 ppsci/geometry/geometry_2d.py
--rw-r--r--  2.0 unx     5360 b- defN 23-May-30 08:04 ppsci/geometry/geometry_3d.py
+-rw-r--r--  2.0 unx     8222 b- defN 23-Jul-24 17:13 ppsci/geometry/geometry.py
+-rw-r--r--  2.0 unx     4424 b- defN 23-Jul-24 17:13 ppsci/geometry/geometry_1d.py
+-rw-r--r--  2.0 unx    25989 b- defN 23-Jul-24 17:13 ppsci/geometry/geometry_2d.py
+-rw-r--r--  2.0 unx     7147 b- defN 23-Jul-24 17:13 ppsci/geometry/geometry_3d.py
 -rw-r--r--  2.0 unx     7432 b- defN 23-May-30 08:04 ppsci/geometry/geometry_nd.py
 -rw-r--r--  2.0 unx     7003 b- defN 23-May-30 08:04 ppsci/geometry/inflation.py
--rw-r--r--  2.0 unx    18361 b- defN 23-May-30 08:23 ppsci/geometry/mesh.py
+-rw-r--r--  2.0 unx    22127 b- defN 23-Jul-24 17:13 ppsci/geometry/mesh.py
 -rw-r--r--  2.0 unx     6506 b- defN 23-May-30 08:23 ppsci/geometry/pointcloud.py
 -rw-r--r--  2.0 unx     3312 b- defN 23-May-30 08:04 ppsci/geometry/sampler.py
--rw-r--r--  2.0 unx    21126 b- defN 23-May-30 08:04 ppsci/geometry/timedomain.py
--rw-r--r--  2.0 unx     1464 b- defN 23-May-30 08:04 ppsci/loss/__init__.py
+-rw-r--r--  2.0 unx    21095 b- defN 23-Jul-24 17:13 ppsci/geometry/timedomain.py
+-rw-r--r--  2.0 unx     1529 b- defN 23-Jul-24 17:13 ppsci/loss/__init__.py
 -rw-r--r--  2.0 unx     1158 b- defN 23-May-30 08:23 ppsci/loss/base.py
--rw-r--r--  2.0 unx     2656 b- defN 23-May-30 08:23 ppsci/loss/integral.py
--rw-r--r--  2.0 unx     4640 b- defN 23-May-30 08:23 ppsci/loss/l1.py
--rw-r--r--  2.0 unx     6332 b- defN 23-May-30 08:23 ppsci/loss/l2.py
--rw-r--r--  2.0 unx     6642 b- defN 23-May-30 08:23 ppsci/loss/mse.py
--rw-r--r--  2.0 unx     1573 b- defN 23-May-30 08:04 ppsci/metric/__init__.py
--rw-r--r--  2.0 unx     4286 b- defN 23-May-30 08:04 ppsci/metric/anomaly_coef.py
+-rw-r--r--  2.0 unx     2330 b- defN 23-Jul-24 17:13 ppsci/loss/func.py
+-rw-r--r--  2.0 unx     2751 b- defN 23-Jul-24 17:13 ppsci/loss/integral.py
+-rw-r--r--  2.0 unx     4425 b- defN 23-Jul-24 17:13 ppsci/loss/l1.py
+-rw-r--r--  2.0 unx     6308 b- defN 23-Jul-24 17:13 ppsci/loss/l2.py
+-rw-r--r--  2.0 unx     6630 b- defN 23-Jul-24 17:13 ppsci/loss/mse.py
+-rw-r--r--  2.0 unx     1644 b- defN 23-Jul-24 17:13 ppsci/metric/__init__.py
+-rw-r--r--  2.0 unx     4288 b- defN 23-Jul-24 17:13 ppsci/metric/anomaly_coef.py
 -rw-r--r--  2.0 unx      804 b- defN 23-May-30 08:04 ppsci/metric/base.py
--rw-r--r--  2.0 unx     1488 b- defN 23-May-30 08:04 ppsci/metric/l2_rel.py
--rw-r--r--  2.0 unx     1480 b- defN 23-May-30 08:04 ppsci/metric/mae.py
--rw-r--r--  2.0 unx     1480 b- defN 23-May-30 08:04 ppsci/metric/mse.py
--rw-r--r--  2.0 unx     4760 b- defN 23-May-30 08:04 ppsci/metric/rmse.py
--rw-r--r--  2.0 unx     2565 b- defN 23-May-30 08:23 ppsci/optimizer/__init__.py
--rw-r--r--  2.0 unx    21140 b- defN 23-May-30 08:23 ppsci/optimizer/lr_scheduler.py
--rw-r--r--  2.0 unx    19501 b- defN 23-May-30 08:23 ppsci/optimizer/optimizer.py
+-rw-r--r--  2.0 unx     1872 b- defN 23-Jul-24 17:13 ppsci/metric/func.py
+-rw-r--r--  2.0 unx     1836 b- defN 23-Jul-24 17:13 ppsci/metric/l2_rel.py
+-rw-r--r--  2.0 unx     1548 b- defN 23-Jul-24 17:13 ppsci/metric/mae.py
+-rw-r--r--  2.0 unx     1548 b- defN 23-Jul-24 17:13 ppsci/metric/mse.py
+-rw-r--r--  2.0 unx     4830 b- defN 23-Jul-24 17:13 ppsci/metric/rmse.py
+-rw-r--r--  2.0 unx     2570 b- defN 23-Jul-24 17:13 ppsci/optimizer/__init__.py
+-rw-r--r--  2.0 unx    25047 b- defN 23-Jul-24 17:13 ppsci/optimizer/lr_scheduler.py
+-rw-r--r--  2.0 unx    20161 b- defN 23-Jul-24 17:13 ppsci/optimizer/optimizer.py
 -rw-r--r--  2.0 unx      804 b- defN 23-May-30 08:04 ppsci/solver/__init__.py
--rw-r--r--  2.0 unx    10759 b- defN 23-May-30 08:23 ppsci/solver/eval.py
--rw-r--r--  2.0 unx     3861 b- defN 23-May-30 08:04 ppsci/solver/printer.py
--rw-r--r--  2.0 unx    26660 b- defN 23-May-30 09:36 ppsci/solver/solver.py
--rw-r--r--  2.0 unx     8817 b- defN 23-May-30 08:23 ppsci/solver/train.py
+-rw-r--r--  2.0 unx    10762 b- defN 23-Jul-24 17:13 ppsci/solver/eval.py
+-rw-r--r--  2.0 unx     4013 b- defN 23-Jul-24 17:13 ppsci/solver/printer.py
+-rw-r--r--  2.0 unx    27567 b- defN 23-Jul-24 17:13 ppsci/solver/solver.py
+-rw-r--r--  2.0 unx     8820 b- defN 23-Jul-24 17:13 ppsci/solver/train.py
 -rw-r--r--  2.0 unx     3316 b- defN 23-May-30 08:23 ppsci/solver/visu.py
--rw-r--r--  2.0 unx     1720 b- defN 23-May-30 08:04 ppsci/utils/__init__.py
+-rw-r--r--  2.0 unx     1720 b- defN 23-Jul-24 17:13 ppsci/utils/__init__.py
 -rw-r--r--  2.0 unx     4823 b- defN 23-May-30 08:23 ppsci/utils/checker.py
--rw-r--r--  2.0 unx     6365 b- defN 23-May-30 08:23 ppsci/utils/config.py
--rw-r--r--  2.0 unx     8944 b- defN 23-May-30 08:23 ppsci/utils/download.py
--rw-r--r--  2.0 unx     6659 b- defN 23-May-30 08:23 ppsci/utils/expression.py
+-rw-r--r--  2.0 unx     6365 b- defN 23-Jul-24 17:13 ppsci/utils/config.py
+-rw-r--r--  2.0 unx     8946 b- defN 23-Jul-24 17:13 ppsci/utils/download.py
+-rw-r--r--  2.0 unx     6788 b- defN 23-Jul-24 17:13 ppsci/utils/expression.py
 -rw-r--r--  2.0 unx    14420 b- defN 23-May-30 08:23 ppsci/utils/initializer.py
--rw-r--r--  2.0 unx     5906 b- defN 23-May-30 08:23 ppsci/utils/logger.py
--rw-r--r--  2.0 unx     7720 b- defN 23-May-30 08:23 ppsci/utils/misc.py
+-rw-r--r--  2.0 unx     6124 b- defN 23-Jul-24 17:13 ppsci/utils/logger.py
+-rw-r--r--  2.0 unx     7721 b- defN 23-Jul-24 17:13 ppsci/utils/misc.py
 -rw-r--r--  2.0 unx     4517 b- defN 23-May-30 08:23 ppsci/utils/profiler.py
--rw-r--r--  2.0 unx     5830 b- defN 23-May-30 08:23 ppsci/utils/reader.py
--rw-r--r--  2.0 unx     5655 b- defN 23-May-30 08:23 ppsci/utils/save_load.py
+-rw-r--r--  2.0 unx     5830 b- defN 23-Jul-24 17:13 ppsci/utils/reader.py
+-rw-r--r--  2.0 unx     5620 b- defN 23-Jul-24 17:13 ppsci/utils/save_load.py
 -rw-r--r--  2.0 unx     2777 b- defN 23-May-30 08:04 ppsci/validate/__init__.py
--rw-r--r--  2.0 unx     2017 b- defN 23-May-30 08:04 ppsci/validate/base.py
+-rw-r--r--  2.0 unx     2081 b- defN 23-Jul-24 17:13 ppsci/validate/base.py
 -rw-r--r--  2.0 unx     6788 b- defN 23-May-30 08:04 ppsci/validate/geo_validator.py
 -rw-r--r--  2.0 unx     3501 b- defN 23-May-30 08:04 ppsci/validate/sup_validator.py
 -rw-r--r--  2.0 unx     2618 b- defN 23-May-30 08:04 ppsci/visualize/__init__.py
 -rw-r--r--  2.0 unx     2074 b- defN 23-May-30 08:23 ppsci/visualize/base.py
--rw-r--r--  2.0 unx    17667 b- defN 23-May-30 08:23 ppsci/visualize/plot.py
+-rw-r--r--  2.0 unx    17665 b- defN 23-Jul-24 17:13 ppsci/visualize/plot.py
 -rw-r--r--  2.0 unx    14863 b- defN 23-May-30 08:04 ppsci/visualize/visualizer.py
--rw-r--r--  2.0 unx     5569 b- defN 23-May-30 08:04 ppsci/visualize/vtu.py
--rw-r--r--  2.0 unx    11438 b- defN 23-May-30 11:44 paddlesci-1.0.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     4505 b- defN 23-May-30 11:44 paddlesci-1.0.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-30 11:44 paddlesci-1.0.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 23-May-30 11:44 paddlesci-1.0.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     8450 b- defN 23-May-30 11:44 paddlesci-1.0.0.dist-info/RECORD
-101 files, 606552 bytes uncompressed, 167683 bytes compressed:  72.4%
+-rw-r--r--  2.0 unx     5582 b- defN 23-Jul-24 17:13 ppsci/visualize/vtu.py
+-rw-r--r--  2.0 unx    11438 b- defN 23-Jul-24 17:18 paddlesci-1.1.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5223 b- defN 23-Jul-24 17:18 paddlesci-1.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-24 17:18 paddlesci-1.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       13 b- defN 23-Jul-24 17:18 paddlesci-1.1.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     8775 b- defN 23-Jul-24 17:18 paddlesci-1.1.0.dist-info/RECORD
+105 files, 667134 bytes uncompressed, 181525 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -12,14 +12,17 @@
 
 Filename: ppsci/arch/base.py
 Comment: 
 
 Filename: ppsci/arch/embedding_koopman.py
 Comment: 
 
+Filename: ppsci/arch/gan.py
+Comment: 
+
 Filename: ppsci/arch/mlp.py
 Comment: 
 
 Filename: ppsci/arch/model_list.py
 Comment: 
 
 Filename: ppsci/arch/physx_transformer.py
@@ -108,14 +111,17 @@
 
 Filename: ppsci/equation/pde/biharmonic.py
 Comment: 
 
 Filename: ppsci/equation/pde/laplace.py
 Comment: 
 
+Filename: ppsci/equation/pde/linear_elasticity.py
+Comment: 
+
 Filename: ppsci/equation/pde/navier_stokes.py
 Comment: 
 
 Filename: ppsci/equation/pde/normal_dot_vec.py
 Comment: 
 
 Filename: ppsci/equation/pde/poisson.py
@@ -162,14 +168,17 @@
 
 Filename: ppsci/loss/__init__.py
 Comment: 
 
 Filename: ppsci/loss/base.py
 Comment: 
 
+Filename: ppsci/loss/func.py
+Comment: 
+
 Filename: ppsci/loss/integral.py
 Comment: 
 
 Filename: ppsci/loss/l1.py
 Comment: 
 
 Filename: ppsci/loss/l2.py
@@ -183,14 +192,17 @@
 
 Filename: ppsci/metric/anomaly_coef.py
 Comment: 
 
 Filename: ppsci/metric/base.py
 Comment: 
 
+Filename: ppsci/metric/func.py
+Comment: 
+
 Filename: ppsci/metric/l2_rel.py
 Comment: 
 
 Filename: ppsci/metric/mae.py
 Comment: 
 
 Filename: ppsci/metric/mse.py
@@ -282,23 +294,23 @@
 
 Filename: ppsci/visualize/visualizer.py
 Comment: 
 
 Filename: ppsci/visualize/vtu.py
 Comment: 
 
-Filename: paddlesci-1.0.0.dist-info/LICENSE
+Filename: paddlesci-1.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: paddlesci-1.0.0.dist-info/METADATA
+Filename: paddlesci-1.1.0.dist-info/METADATA
 Comment: 
 
-Filename: paddlesci-1.0.0.dist-info/WHEEL
+Filename: paddlesci-1.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: paddlesci-1.0.0.dist-info/top_level.txt
+Filename: paddlesci-1.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: paddlesci-1.0.0.dist-info/RECORD
+Filename: paddlesci-1.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppsci/arch/__init__.py

```diff
@@ -14,26 +14,30 @@
 
 import copy
 
 from ppsci.arch.mlp import MLP  # isort:skip
 from ppsci.arch.embedding_koopman import LorenzEmbedding  # isort:skip
 from ppsci.arch.embedding_koopman import RosslerEmbedding  # isort:skip
 from ppsci.arch.embedding_koopman import CylinderEmbedding  # isort:skip
+from ppsci.arch.gan import Generator  # isort:skip
+from ppsci.arch.gan import Discriminator  # isort:skip
 from ppsci.arch.physx_transformer import PhysformerGPT2  # isort:skip
 from ppsci.arch.model_list import ModelList  # isort:skip
 from ppsci.arch.afno import AFNONet  # isort:skip
 from ppsci.arch.afno import PrecipNet  # isort:skip
 from ppsci.utils import logger  # isort:skip
 
 
 __all__ = [
     "MLP",
     "LorenzEmbedding",
     "RosslerEmbedding",
     "CylinderEmbedding",
+    "Generator",
+    "Discriminator",
     "PhysformerGPT2",
     "ModelList",
     "AFNONet",
     "PrecipNet",
     "build_model",
 ]
```

## ppsci/arch/activation.py

```diff
@@ -14,26 +14,86 @@
 
 from typing import Callable
 
 import paddle
 import paddle.nn.functional as F
 from paddle import nn
 
+
+class Stan(nn.Layer):
+    """Self-scalable Tanh.
+    paper: https://arxiv.org/abs/2204.12589v1
+
+    Args:
+        out_features (int, optional): Output features. Defaults to 1.
+    """
+
+    def __init__(self, out_features: int = 1):
+        super().__init__()
+        self.beta = self.create_parameter(
+            shape=(out_features,),
+            default_initializer=nn.initializer.Constant(1),
+        )
+
+    def forward(self, x):
+        # TODO: manually broadcast beta to x.shape for preventing backward error yet.
+        return F.tanh(x) * (1 + paddle.broadcast_to(self.beta, x.shape) * x)
+        # return F.tanh(x) * (1 + self.beta * x)
+
+
+class Swish(nn.Layer):
+    def __init__(self, beta: float = 1.0):
+        super().__init__()
+        self.beta = self.create_parameter(
+            shape=[],
+            default_initializer=paddle.nn.initializer.Constant(beta),
+        )
+
+    def forward(self, x):
+        return x * F.sigmoid(self.beta * x)
+
+
+class Cos(nn.Layer):
+    def __init__(self):
+        super().__init__()
+
+    def forward(self, x):
+        return paddle.cos(x)
+
+
+class Sin(nn.Layer):
+    def __init__(self):
+        super().__init__()
+
+    def forward(self, x):
+        return paddle.sin(x)
+
+
+class Silu(nn.Layer):
+    def __init__(self):
+        super().__init__()
+
+    def forward(self, x):
+        return x * F.sigmoid(x)
+
+
 act_func_dict = {
-    "elu": F.elu,
-    "relu": F.relu,
-    "selu": F.selu,
-    "gelu": F.gelu,
-    "sigmoid": F.sigmoid,
-    "silu": F.silu,
-    "sin": paddle.sin,
-    "cos": paddle.cos,
-    "swish": F.silu,
-    "tanh": F.tanh,
+    "elu": nn.ELU(),
+    "relu": nn.ReLU(),
+    "selu": nn.SELU(),
+    "gelu": nn.GELU(),
+    "leaky_relu": nn.LeakyReLU(),
+    "sigmoid": nn.Sigmoid(),
+    "silu": Silu(),
+    "sin": Sin(),
+    "cos": Cos(),
+    "swish": Swish(),
+    "tanh": nn.Tanh(),
     "identity": nn.Identity(),
+    "stan": Stan,
 }
 
 
 def get_activation(act_name: str) -> Callable:
     """Get activation function according to act_name.
 
     Args:
```

## ppsci/arch/base.py

```diff
@@ -69,15 +69,15 @@
 
     def split_to_dict(
         self, data_tensor: paddle.Tensor, keys: Tuple[str, ...], axis=-1
     ) -> Dict[str, paddle.Tensor]:
         """Split tensor and wrap into a dict by given keys.
 
         Args:
-            data_tensor (Dict[str, paddle.Tensor]): Tensor to be split.
+            data_tensor (paddle.Tensor): Tensor to be split.
             keys (Tuple[str, ...]): Keys tensor mapping to.
             axis (int, optional): Axis split at. Defaults to -1.
 
         Returns:
             Dict[str, paddle.Tensor]: Dict contains tensor.
         """
         if len(keys) == 1:
```

## ppsci/arch/mlp.py

```diff
@@ -12,33 +12,59 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import Optional
 from typing import Tuple
 from typing import Union
 
-from paddle import nn
+import paddle.nn as nn
 
 from ppsci.arch import activation as act_mod
 from ppsci.arch import base
+from ppsci.utils import initializer
+
+
+class WeightNormLinear(nn.Layer):
+    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        self.weight_v = self.create_parameter((in_features, out_features))
+        self.weight_g = self.create_parameter((out_features,))
+        if bias:
+            self.bias = self.create_parameter((out_features,))
+        else:
+            self.bias = None
+        self._init_weights()
+
+    def _init_weights(self) -> None:
+        initializer.xavier_uniform_(self.weight_v)
+        initializer.constant_(self.weight_g, 1.0)
+        if self.bias is not None:
+            initializer.constant_(self.bias, 0.0)
+
+    def forward(self, input):
+        norm = self.weight_v.norm(p=2, axis=0, keepdim=True)
+        weight = self.weight_g * self.weight_v / norm
+        return nn.functional.linear(input, weight, self.bias)
 
 
 class MLP(base.Arch):
     """Multi layer perceptron network.
 
     Args:
         input_keys (Tuple[str, ...]): Name of input keys, such as ("x", "y", "z").
         output_keys (Tuple[str, ...]): Name of output keys, such as ("u", "v", "w").
         num_layers (int): Number of hidden layers.
         hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size.
             An integer for all layers, or list of integer specify each layer's size.
         activation (str, optional): Name of activation function. Defaults to "tanh".
         skip_connection (bool, optional): Whether to use skip connection. Defaults to False.
         weight_norm (bool, optional): Whether to apply weight norm on parameter(s). Defaults to False.
-        input_dim (Optional[int], optional): Number of input's dimension. Defaults to None.
+        input_dim (Optional[int]): Number of input's dimension. Defaults to None.
 
     Examples:
         >>> import ppsci
         >>> model = ppsci.arch.MLP(("x", "y"), ("u", "v"), 5, 128)
     """
 
     def __init__(
@@ -52,14 +78,15 @@
         weight_norm: bool = False,
         input_dim: Optional[int] = None,
     ):
         super().__init__()
         self.input_keys = input_keys
         self.output_keys = output_keys
         self.linears = []
+        self.acts = []
         if isinstance(hidden_size, (tuple, list)):
             if num_layers is not None:
                 raise ValueError(
                     "num_layers should be None when hidden_size is specified"
                 )
         elif isinstance(hidden_size, int):
             if not isinstance(num_layers, int):
@@ -72,39 +99,45 @@
                 f"hidden_size should be list of int or int"
                 f"but got {type(hidden_size)}"
             )
 
         # initialize FC layer(s)
         cur_size = len(self.input_keys) if input_dim is None else input_dim
         for _size in hidden_size:
-            self.linears.append(nn.Linear(cur_size, _size))
-            if weight_norm:
-                self.linears[-1] = nn.utils.weight_norm(self.linears[-1], dim=1)
+            self.linears.append(
+                WeightNormLinear(cur_size, _size)
+                if weight_norm
+                else nn.Linear(cur_size, _size)
+            )
+            # initialize activation function
+            self.acts.append(
+                act_mod.get_activation(activation)
+                if activation != "stan"
+                else act_mod.get_activation(activation)(_size)
+            )
             cur_size = _size
-        self.linears = nn.LayerList(self.linears)
 
+        self.linears = nn.LayerList(self.linears)
+        self.acts = nn.LayerList(self.acts)
         self.last_fc = nn.Linear(cur_size, len(self.output_keys))
 
-        # initialize activation function
-        self.act = act_mod.get_activation(activation)
-
         self.skip_connection = skip_connection
 
     def forward_tensor(self, x):
         y = x
         skip = None
         for i, linear in enumerate(self.linears):
             y = linear(y)
             if self.skip_connection and i % 2 == 0:
                 if skip is not None:
                     skip = y
                     y = y + skip
                 else:
                     skip = y
-            y = self.act(y)
+            y = self.acts[i](y)
 
         y = self.last_fc(y)
 
         return y
 
     def forward(self, x):
         if self._input_transform is not None:
```

## ppsci/autodiff/ad.py

```diff
@@ -17,194 +17,214 @@
 """
 
 from typing import Optional
 
 import paddle
 
 
-class Jacobian:
+class _Jacobian:
     """Compute Jacobian matrix J: J[i][j] = dy_i/dx_j, where i = 0, ..., dim_y-1 and
     j = 0, ..., dim_x - 1.
 
     It is lazy evaluation, i.e., it only computes J[i][j] when needed, and will cache
     by output tensor(row index in jacobian matrix).
 
     Args:
         ys (paddle.Tensor): Output Tensor of shape [batch_size, dim_y].
         xs (paddle.Tensor): Input Tensor of shape [batch_size, dim_x].
     """
 
-    def __init__(self, ys: paddle.Tensor, xs: paddle.Tensor):
+    def __init__(self, ys: "paddle.Tensor", xs: "paddle.Tensor"):
         self.ys = ys
         self.xs = xs
 
         self.dim_y = ys.shape[1]
         self.dim_x = xs.shape[1]
 
         self.J = {}
 
-    def __call__(self, i: int = 0, j: Optional[int] = None) -> paddle.Tensor:
+    def __call__(self, i: int = 0, j: Optional[int] = None) -> "paddle.Tensor":
         """Returns J[`i`][`j`]. If `j` is ``None``, returns the gradient of y_i, i.e.,
         J[i].
         """
         if not 0 <= i < self.dim_y:
-            raise ValueError(f"i={i} is not valid.")
+            raise ValueError(f"i({i}) should in range [0, {self.dim_y}).")
         if j is not None and not 0 <= j < self.dim_x:
-            raise ValueError(f"j={j} is not valid.")
+            raise ValueError(f"j({j}) should in range [0, {self.dim_x}).")
         # Compute J[i]
         if i not in self.J:
             y = self.ys[:, i : i + 1] if self.dim_y > 1 else self.ys
             self.J[i] = paddle.grad(y, self.xs, create_graph=True)[0]
 
-        return self.J[i] if j is None or self.dim_x == 1 else self.J[i][:, j : j + 1]
+        return self.J[i] if (j is None or self.dim_x == 1) else self.J[i][:, j : j + 1]
 
 
 class Jacobians:
-    """Compute multiple Jacobians.
+    r"""Compute multiple Jacobians.
+
+    $$
+    \rm Jacobian(ys, xs, i, j) = \dfrac{\partial ys_i}{\partial xs_j}
+    $$
 
     A new instance will be created for a new pair of (output, input). For the (output,
     input) pair that has been computed before, it will reuse the previous instance,
     rather than creating a new one.
     """
 
     def __init__(self):
         self.Js = {}
 
     def __call__(
-        self, ys: paddle.Tensor, xs: paddle.Tensor, i: int = 0, j: Optional[int] = None
-    ) -> paddle.Tensor:
+        self,
+        ys: "paddle.Tensor",
+        xs: "paddle.Tensor",
+        i: int = 0,
+        j: Optional[int] = None,
+    ) -> "paddle.Tensor":
         """Compute jacobians for given ys and xs.
 
         Args:
             ys (paddle.Tensor): Output tensor.
             xs (paddle.Tensor): Input tensor.
             i (int, optional): i-th output variable. Defaults to 0.
             j (Optional[int]): j-th input variable. Defaults to None.
 
         Returns:
             paddle.Tensor: Jacobian matrix of ys[i] to xs[j].
 
         Examples:
+            >>> import paddle
             >>> import ppsci
             >>> x = paddle.randn([4, 1])
             >>> x.stop_gradient = False
             >>> y = x * x
             >>> dy_dx = ppsci.autodiff.jacobian(y, x)
         """
         key = (ys, xs)
         if key not in self.Js:
-            self.Js[key] = Jacobian(ys, xs)
+            self.Js[key] = _Jacobian(ys, xs)
         return self.Js[key](i, j)
 
-    def clear(self):
+    def _clear(self):
         """Clear cached Jacobians."""
         self.Js = {}
 
 
-class Hessian:
+# Use high-order differentiation with singleton pattern for convenient
+jacobian = Jacobians()
+
+
+class _Hessian:
     """Compute Hessian matrix H: H[i][j] = d^2y / dx_i dx_j, where i,j = 0,..., dim_x-1.
 
     It is lazy evaluation, i.e., it only computes H[i][j] when needed.
 
     Args:
-        y: Output Tensor of shape (batch_size, 1) or (batch_size, dim_y > 1).
+        ys: Output Tensor of shape (batch_size, 1) or (batch_size, dim_y > 1).
         xs: Input Tensor of shape (batch_size, dim_x).
         component: If `y` has the shape (batch_size, dim_y > 1), then `y[:, component]`
             is used to compute the Hessian. Do not use if `y` has the shape (batch_size,
             1).
         grad_y: The gradient of `y` w.r.t. `xs`. Provide `grad_y` if known to avoid
             duplicate computation. `grad_y` can be computed from ``Jacobian``.
     """
 
     def __init__(
         self,
-        y: paddle.Tensor,
-        xs: paddle.Tensor,
+        ys: "paddle.Tensor",
+        xs: "paddle.Tensor",
         component: Optional[int] = None,
-        grad_y: Optional[paddle.Tensor] = None,
+        grad_y: Optional["paddle.Tensor"] = None,
     ):
-        dim_y = y.shape[1]
+        dim_y = ys.shape[1]
 
         if dim_y > 1:
             if component is None:
-                raise ValueError("The component of y is missing.")
+                raise ValueError(
+                    f"component({component}) can not be None when dim_y({dim_y})>1."
+                )
             if component >= dim_y:
                 raise ValueError(
-                    f"The component of y={component} cannot be larger than the "
-                    f"dimension={dim_y}."
+                    f"component({component}) should be smaller than dim_y({dim_y})."
                 )
         else:
             if component is not None:
-                raise ValueError("Do not use component for 1D y.")
+                raise ValueError(
+                    f"component{component} should be set to None when dim_y({dim_y})=1."
+                )
             component = 0
 
         if grad_y is None:
-            grad_y = jacobian(y, xs, i=component, j=None)
-        self.H = Jacobian(grad_y, xs)
+            grad_y = jacobian(ys, xs, i=component, j=None)
+        self.H = _Jacobian(grad_y, xs)
 
     def __call__(self, i: int = 0, j: int = 0):
         """Returns H[`i`][`j`]."""
         return self.H(i, j)
 
 
 class Hessians:
-    """Compute multiple Hessians.
+    r"""Compute multiple Hessians.
+
+    $$
+    \rm Hessian(ys, xs, component, i, j) = \dfrac{\partial ys_{component}}{\partial xs_i \partial xs_j}
+    $$
 
     A new instance will be created for a new pair of (output, input). For the (output,
     input) pair that has been computed before, it will reuse the previous instance,
     rather than creating a new one.
     """
 
     def __init__(self):
         self.Hs = {}
 
     def __call__(
         self,
-        ys: paddle.Tensor,
-        xs: paddle.Tensor,
+        ys: "paddle.Tensor",
+        xs: "paddle.Tensor",
         component: Optional[int] = None,
         i: int = 0,
         j: int = 0,
-        grad_y: Optional[paddle.Tensor] = None,
-    ) -> paddle.Tensor:
+        grad_y: Optional["paddle.Tensor"] = None,
+    ) -> "paddle.Tensor":
         """Compute hessian matrix for given ys and xs.
 
         Args:
             ys (paddle.Tensor): Output tensor.
             xs (paddle.Tensor): Input tensor.
-            component (Optional[int], optional): If `y` has the shape (batch_size, dim_y > 1), then `y[:, component]`
+            component (Optional[int]): If `y` has the shape (batch_size, dim_y > 1), then `y[:, component]`
                 is used to compute the Hessian. Do not use if `y` has the shape (batch_size,
                 1). Defaults to None.
             i (int, optional): i-th input variable. Defaults to 0.
             j (int, optional): j-th input variable. Defaults to 0.
-            grad_y (Optional[paddle.Tensor], optional): The gradient of `y` w.r.t. `xs`. Provide `grad_y` if known to avoid
+            grad_y (Optional[paddle.Tensor]): The gradient of `y` w.r.t. `xs`. Provide `grad_y` if known to avoid
                 duplicate computation. Defaults to None.
 
         Returns:
             paddle.Tensor: Hessian matrix.
 
         Examples:
+            >>> import paddle
             >>> import ppsci
             >>> x = paddle.randn([4, 3])
             >>> x.stop_gradient = False
             >>> y = (x * x).sin()
             >>> dy_dxx = ppsci.autodiff.hessian(y, x, component=0)
         """
         key = (ys, xs, component)
         if key not in self.Hs:
-            self.Hs[key] = Hessian(ys, xs, component=component, grad_y=grad_y)
+            self.Hs[key] = _Hessian(ys, xs, component=component, grad_y=grad_y)
         return self.Hs[key](i, j)
 
-    def clear(self):
+    def _clear(self):
         """Clear cached Hessians."""
         self.Hs = {}
 
 
 # Use high-order differentiation with singleton pattern for convenient
-jacobian = Jacobians()
 hessian = Hessians()
 
 
 def clear():
     """Clear cached Jacobians and Hessians."""
-    jacobian.clear()
-    hessian.clear()
+    jacobian._clear()
+    hessian._clear()
```

## ppsci/constraint/base.py

```diff
@@ -8,21 +8,24 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Dict
 
 from paddle import io
 
 from ppsci import data
-from ppsci import loss
+
+if TYPE_CHECKING:
+    from ppsci import loss
 
 
 class Constraint:
     """Base class for constraint.
 
     Args:
         dataset (io.Dataset): Dataset.
@@ -31,15 +34,15 @@
         name (str): Name of constraint.
     """
 
     def __init__(
         self,
         dataset: io.Dataset,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         name: str,
     ):
         self.data_loader = data.build_dataloader(dataset, dataloader_cfg)
         self.data_loader = data.dataloader.InfiniteDataLoader(self.data_loader)
         self.data_iter = iter(self.data_loader)
         self.loss = loss
         self.name = name
```

## ppsci/constraint/boundary_constraint.py

```diff
@@ -8,30 +8,33 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import sympy
 from sympy.parsing import sympy_parser as sp_parser
 from typing_extensions import Literal
 
 from ppsci import geometry
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class BoundaryConstraint(base.Constraint):
     """Class for boundary constraint.
 
     Args:
         output_expr (Dict[str, Callable]): Function in dict for computing output.
             e.g. {"u_mul_v": lambda out: out["u"] * out["v"]} means the model output u
@@ -70,15 +73,15 @@
 
     def __init__(
         self,
         output_expr: Dict[str, Callable],
         label_dict: Dict[str, Union[float, Callable]],
         geom: geometry.Geometry,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         random: Literal["pseudo", "LHS"] = "pseudo",
         criteria: Optional[Callable] = None,
         evenly: bool = False,
         weight_dict: Optional[Dict[str, Union[float, Callable]]] = None,
         name: str = "BC",
     ):
         self.output_expr = output_expr
```

## ppsci/constraint/initial_constraint.py

```diff
@@ -8,41 +8,44 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import sympy
 from sympy.parsing import sympy_parser as sp_parser
 from typing_extensions import Literal
 
 from ppsci import geometry
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class InitialConstraint(base.Constraint):
     """Class for initial constraint.
 
     Args:
         output_expr (Dict[str, Callable]): Function in dict for computing output.
             e.g. {"u_mul_v": lambda out: out["u"] * out["v"]} means the model output u
             will be multiplied by model output v and the result will be named "u_mul_v".
         label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing
             label, which will be a reference value to participate in the loss calculation.
-        geom (geometry.Geometry): Geometry where data sampled from.
+        geom (geometry.TimeXGeometry): Geometry where data sampled from.
         dataloader_cfg (Dict[str, Any]): Dataloader config.
         loss (loss.Loss): Loss functor.
         random (Literal["pseudo", "LHS"], optional): Random method for sampling data in
             geometry. Defaults to "pseudo".
         criteria (Optional[Callable]): Criteria for refining specified boundaries.
             Defaults to None.
         evenly (bool, optional): Whether to use evenly distribution sampling.
@@ -71,41 +74,47 @@
         ... )
     """
 
     def __init__(
         self,
         output_expr: Dict[str, Callable],
         label_dict: Dict[str, Union[float, Callable]],
-        geom: geometry.Geometry,
+        geom: geometry.TimeXGeometry,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         random: Literal["pseudo", "LHS"] = "pseudo",
         criteria: Optional[Callable] = None,
         evenly: bool = False,
         weight_dict: Optional[Dict[str, Callable]] = None,
         name: str = "IC",
     ):
         self.output_expr = output_expr
         for label_name, expr in self.output_expr.items():
             if isinstance(expr, str):
                 self.output_expr[label_name] = sp_parser.parse_expr(expr)
 
         self.label_dict = label_dict
         self.input_keys = geom.dim_keys
         self.output_keys = list(label_dict.keys())
+        # "area" will be kept in "output_dict" for computation.
+        if isinstance(geom.geometry, geometry.Mesh):
+            self.output_keys += ["area"]
+
         if isinstance(criteria, str):
             criteria = eval(criteria)
 
         # prepare input
         input = geom.sample_initial_interior(
             dataloader_cfg["batch_size"] * dataloader_cfg["iters_per_epoch"],
             random,
             criteria,
             evenly,
         )
+        if "area" in input:
+            input["area"] *= dataloader_cfg["iters_per_epoch"]
 
         # prepare label
         label = {}
         for key, value in label_dict.items():
             if isinstance(value, str):
                 value = sp_parser.parse_expr(value)
             if isinstance(value, (int, float)):
```

## ppsci/constraint/integral_constraint.py

```diff
@@ -8,32 +8,35 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import paddle
 import sympy
 from sympy.parsing import sympy_parser as sp_parser
 from typing_extensions import Literal
 
 from ppsci import geometry
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 from ppsci.utils import misc
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class IntegralConstraint(base.Constraint):
     """Class for integral constraint.
 
     Args:
         output_expr (Dict[str, Callable]): Function in dict for computing output.
             e.g. {"u_mul_v": lambda out: out["u"] * out["v"]} means the model output u
@@ -71,15 +74,15 @@
 
     def __init__(
         self,
         output_expr: Dict[str, Callable],
         label_dict: Dict[str, Union[float, Callable]],
         geom: geometry.Geometry,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         random: Literal["pseudo", "LHS"] = "pseudo",
         criteria: Optional[Callable] = None,
         weight_dict: Optional[Dict[str, Callable]] = None,
         name: str = "IgC",
     ):
         self.output_expr = output_expr
         for label_name, expr in self.output_expr.items():
```

## ppsci/constraint/interior_constraint.py

```diff
@@ -8,30 +8,33 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import sympy
 from sympy.parsing import sympy_parser as sp_parser
 from typing_extensions import Literal
 
 from ppsci import geometry
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class InteriorConstraint(base.Constraint):
     """Class for integral constraint.
 
     Args:
         output_expr (Dict[str, Callable]): Function in dict for computing output.
             e.g. {"u_mul_v": lambda out: out["u"] * out["v"]} means the model output u
@@ -70,15 +73,15 @@
 
     def __init__(
         self,
         output_expr: Dict[str, Callable],
         label_dict: Dict[str, Union[float, Callable]],
         geom: geometry.Geometry,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         random: Literal["pseudo", "LHS"] = "pseudo",
         criteria: Optional[Callable] = None,
         evenly: bool = False,
         weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,
         name: str = "EQ",
     ):
         self.output_expr = output_expr
@@ -131,18 +134,20 @@
                 raise NotImplementedError(f"type of {type(value)} is invalid yet.")
 
         # prepare weight
         weight = {key: np.ones_like(next(iter(label.values()))) for key in label}
         if weight_dict is not None:
             for key, value in weight_dict.items():
                 if isinstance(value, str):
-                    value = sp_parser.parse_expr(value)
-
-                if isinstance(value, (int, float)):
-                    weight[key] = np.full_like(next(iter(label.values())), value)
+                    if value == "sdf":
+                        weight[key] = input["sdf"]
+                    else:
+                        raise NotImplementedError(f"string {value} is invalid yet.")
+                elif isinstance(value, (int, float)):
+                    weight[key] = np.full_like(next(iter(label.values())), float(value))
                 elif isinstance(value, sympy.Basic):
                     func = sympy.lambdify(
                         sympy.symbols(geom.dim_keys),
                         value,
                         [{"amax": lambda xy, _: np.maximum(xy[0], xy[1])}, "numpy"],
                     )
                     weight[key] = func(
@@ -154,12 +159,15 @@
                     if isinstance(weight[key], (int, float)):
                         weight[key] = np.full_like(
                             next(iter(input.values())), weight[key]
                         )
                 else:
                     raise NotImplementedError(f"type of {type(value)} is invalid yet.")
 
+        if "sdf" in input:
+            input.pop("sdf")
+
         # wrap input, label, weight into a dataset
         _dataset = getattr(dataset, dataloader_cfg["dataset"])(input, label, weight)
 
         # construct dataloader with dataset and dataloader_cfg
         super().__init__(_dataset, dataloader_cfg, loss, name)
```

## ppsci/constraint/periodic_constraint.py

```diff
@@ -8,31 +8,34 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import paddle
 import sympy
 from sympy.parsing import sympy_parser as sp_parser
 from typing_extensions import Literal
 
 from ppsci import geometry
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class PeriodicConstraint(base.Constraint):
     """Class for periodic constraint.
 
     Args:
         output_expr (Dict[str, Callable]): Function in dict for computing output.
             e.g. {"u_mul_v": lambda out: out["u"] * out["v"]} means the model output u
@@ -57,51 +60,57 @@
     def __init__(
         self,
         output_expr: Dict[str, Callable],
         label_dict: Dict[str, Union[float, Callable]],
         geom: geometry.Geometry,
         periodic_key: str,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         random: Literal["pseudo", "LHS"] = "pseudo",
         criteria: Optional[Callable] = None,
         evenly: bool = False,
         weight_dict: Optional[Dict[str, Callable]] = None,
         name: str = "PeriodicBC",
     ):
         self.output_expr = output_expr
         for label_name, expr in self.output_expr.items():
             if isinstance(expr, str):
                 self.output_expr[label_name] = sp_parser.parse_expr(expr)
 
         self.input_keys = geom.dim_keys
         self.output_keys = list(output_expr.keys())
+        # "area" will be kept in "output_dict" for computation.
+        if isinstance(geom, geometry.Mesh):
+            self.output_keys += ["area"]
 
         if isinstance(criteria, str):
             criteria = eval(criteria)
 
-        if dataloader_cfg["sampler"]["batch_size"] % 2 > 0:
+        if dataloader_cfg["batch_size"] % 2 > 0:
             raise ValueError(
                 f"batch_size({dataloader_cfg['sampler']['batch_size']}) "
-                f"should be positive and even when using PeriodicConstraint"
+                "should be positive and even when using PeriodicConstraint"
             )
-        if dataloader_cfg["sampler"]["shuffle"]:
+        if dataloader_cfg.get("shuffle", False):
             raise ValueError(
                 f"shuffle({dataloader_cfg['sampler']['batch_size']}) "
-                f"should be False when using PeriodicConstraint "
+                "should be False when using PeriodicConstraint"
             )
 
         # prepare input
-        _bs_half = dataloader_cfg["sampler"]["batch_size"] // 2
+        _bs_half = dataloader_cfg["batch_size"] // 2
         input = geom.sample_boundary(
             _bs_half * dataloader_cfg["iters_per_epoch"],
             random,
             criteria,
             evenly,
         )
+        if "area" in input:
+            input["area"] *= dataloader_cfg["iters_per_epoch"]
+
         input_periodic = geom.periodic_point(
             input,
             geom.geometry.dim_keys.index(periodic_key)
             if isinstance(geom, geometry.TimeXGeometry)
             else geom.dim_keys.index(periodic_key),
         )
         # concatenate original data next to periodic data, i.e.
```

## ppsci/constraint/supervised_constraint.py

```diff
@@ -8,23 +8,26 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import Optional
 
-from ppsci import loss
 from ppsci.constraint import base
 from ppsci.data import dataset
 
+if TYPE_CHECKING:
+    from ppsci import loss
+
 
 class SupervisedConstraint(base.Constraint):
     """Class for supervised constraint.
 
     Args:
         dataloader_cfg (Dict[str, Any]): Dataloader config.
         loss (loss.Loss): Loss functor.
@@ -47,15 +50,15 @@
         ...     name="bc_sup",
         ... )  # doctest: +SKIP
     """
 
     def __init__(
         self,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
+        loss: "loss.Loss",
         output_expr: Optional[Dict[str, Callable]] = None,
         name: str = "Sup",
     ):
         self.output_expr = output_expr
 
         # build dataset
         _dataset = dataset.build_dataset(dataloader_cfg["dataset"])
```

## ppsci/data/__init__.py

```diff
@@ -102,8 +102,14 @@
         batch_sampler=sampler,
         collate_fn=collate_fn,
         num_workers=cfg.get("num_workers", 0),
         use_shared_memory=cfg.get("use_shared_memory", False),
         worker_init_fn=init_fn,
     )
 
+    if len(dataloader_) == 0:
+        raise ValueError(
+            f"batch_size({sampler_cfg['batch_size']}) should not bigger than number of "
+            f"samples({len(_dataset)}) when drop_last is {sampler_cfg.get('drop_last', False)}."
+        )
+
     return dataloader_
```

## ppsci/data/dataset/array_dataset.py

```diff
@@ -23,39 +23,39 @@
 
 class NamedArrayDataset(io.Dataset):
     """Class for Named Array Dataset.
 
     Args:
         input (Dict[str, np.ndarray]): Input dict.
         label (Dict[str, np.ndarray]): Label dict.
-        weight (Dict[str, np.ndarray], optional): Weight dict.
-        transforms (Optional[vision.Compose]): Compose object contains sample wise
+        weight (Optional[Dict[str, np.ndarray]], optional): Weight dict.
+        transforms (Optional[vision.Compose], optional): Compose object contains sample wise
             transform(s).
 
     Examples:
         >>> import ppsci
         >>> input = {"x": np.random.randn(100, 1)}
         >>> output = {"u": np.random.randn(100, 1)}
         >>> weight = {"u": np.random.randn(100, 1)}
         >>> dataset = ppsci.data.dataset.NamedArrayDataset(input, output, weight)
     """
 
     def __init__(
         self,
         input: Dict[str, np.ndarray],
         label: Dict[str, np.ndarray],
-        weight: Dict[str, np.ndarray],
+        weight: Optional[Dict[str, np.ndarray]] = None,
         transforms: Optional[vision.Compose] = None,
     ):
         super().__init__()
         self.input = input
         self.label = label
         self.input_keys = tuple(input.keys())
         self.label_keys = tuple(label.keys())
-        self.weight = weight
+        self.weight = {} if weight is None else weight
         self.transforms = transforms
         self._len = len(next(iter(input.values())))
 
     def __getitem__(self, idx):
         input_item = {key: value[idx] for key, value in self.input.items()}
         label_item = {key: value[idx] for key, value in self.label.items()}
         weight_item = {key: value[idx] for key, value in self.weight.items()}
```

## ppsci/data/dataset/era5_dataset.py

```diff
@@ -133,15 +133,15 @@
                     label_file[label_idx + i], 0
                 )
             else:
                 label_item[self.label_keys[i]] = label_file[
                     label_idx + i, self.vars_channel
                 ]
 
-        weight_shape = [1] * len(next(iter(label_item.values)).shape)
+        weight_shape = [1] * len(next(iter(label_item.values())).shape)
         weight_item = {
             key: np.full(weight_shape, value, paddle.get_default_dtype())
             for key, value in self.weight_dict.items()
         }
 
         if self.transforms is not None:
             input_item, label_item, weight_item = self.transforms(
@@ -216,15 +216,15 @@
 
         label_item = {}
         for key in _file["label_dict"]:
             label_item[key] = np.asarray(
                 _file["label_dict"][key], paddle.get_default_dtype()
             )
 
-        weight_shape = [1] * len(next(iter(label_item.values)).shape)
+        weight_shape = [1] * len(next(iter(label_item.values())).shape)
         weight_item = {
             key: np.full(weight_shape, value, paddle.get_default_dtype())
             for key, value in self.weight_dict.items()
         }
 
         if self.transforms is not None:
             input_item, label_item, weight_item = self.transforms(
```

## ppsci/data/process/transform/__init__.py

```diff
@@ -14,22 +14,24 @@
 
 # from ppsci.data.process.postprocess import *
 import copy
 
 from paddle import vision
 
 from ppsci.data.process.transform.preprocess import CropData
+from ppsci.data.process.transform.preprocess import FunctionalTransform
 from ppsci.data.process.transform.preprocess import Log1p
 from ppsci.data.process.transform.preprocess import Normalize
 from ppsci.data.process.transform.preprocess import Scale
 from ppsci.data.process.transform.preprocess import SqueezeData
 from ppsci.data.process.transform.preprocess import Translate
 
 __all__ = [
     "CropData",
+    "FunctionalTransform",
     "Log1p",
     "Normalize",
     "Scale",
     "SqueezeData",
     "Translate",
     "build_transforms",
 ]
```

## ppsci/data/process/transform/preprocess.py

```diff
@@ -8,14 +8,15 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Callable
 from typing import Dict
 from typing import Tuple
 from typing import Union
 
 import numpy as np
 
 
@@ -213,7 +214,42 @@
                     B, C, H, W = value.shape
                     label_item[key] = value.reshape((B * C, H, W))
                 if value.ndim != 3:
                     raise ValueError(
                         f"Only support squeeze data to ndim=3 now, but got ndim={value.ndim}"
                     )
         return input_item, label_item, weight_item
+
+
+class FunctionalTransform:
+    """Functional data transform class, which allows to use custom data transform function from given transform_func for special cases.
+
+    Args:
+        transform_func (Callable): Function of data transform.
+
+    Examples:
+        >>> import ppsci
+        >>> import numpy as np
+        >>> def transform_func(data_dict):
+        ...     rand_ratio = np.random.rand()
+        ...     for key in data_dict:
+        ...         data_dict[key] = data_dict[key] * rand_ratio
+        ...     return data_dict
+        >>> transform_cfg = {
+        ...     "transforms": (
+        ...         {
+        ...             "FunctionalTransform": {
+        ...                 "transform_func": transform_func,
+        ...             },
+        ...         },
+        ...     ),
+        ... }
+    """
+
+    def __init__(
+        self,
+        transform_func: Callable,
+    ):
+        self.transform_func = transform_func
+
+    def __call__(self, data_dict: Dict[str, np.ndarray]):
+        return self.transform_func(data_dict)
```

## ppsci/equation/__init__.py

```diff
@@ -13,25 +13,27 @@
 # limitations under the License.
 
 import copy
 
 from ppsci.equation.pde import PDE
 from ppsci.equation.pde import Biharmonic
 from ppsci.equation.pde import Laplace
+from ppsci.equation.pde import LinearElasticity
 from ppsci.equation.pde import NavierStokes
 from ppsci.equation.pde import NormalDotVec
 from ppsci.equation.pde import Poisson
 from ppsci.equation.pde import Vibration
 from ppsci.utils import logger
 from ppsci.utils import misc
 
 __all__ = [
     "PDE",
     "Biharmonic",
     "Laplace",
+    "LinearElasticity",
     "NavierStokes",
     "NormalDotVec",
     "Poisson",
     "Vibration",
     "build_equation",
 ]
```

## ppsci/equation/pde/__init__.py

```diff
@@ -11,21 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from ppsci.equation.pde.base import PDE
 from ppsci.equation.pde.biharmonic import Biharmonic
 from ppsci.equation.pde.laplace import Laplace
+from ppsci.equation.pde.linear_elasticity import LinearElasticity
 from ppsci.equation.pde.navier_stokes import NavierStokes
 from ppsci.equation.pde.normal_dot_vec import NormalDotVec
 from ppsci.equation.pde.poisson import Poisson
 from ppsci.equation.pde.viv import Vibration
 
 __all__ = [
     "PDE",
     "Biharmonic",
     "Laplace",
+    "LinearElasticity",
     "NavierStokes",
     "NormalDotVec",
     "Poisson",
     "Vibration",
 ]
```

## ppsci/equation/pde/biharmonic.py

```diff
@@ -13,15 +13,19 @@
 # limitations under the License.
 
 from ppsci.autodiff import hessian
 from ppsci.equation.pde import base
 
 
 class Biharmonic(base.PDE):
-    """Class for biharmonic equation.
+    r"""Class for biharmonic equation.
+
+    $$
+    \nabla^4 \varphi = \dfrac{q}{D}
+    $$
 
     Args:
         dim (int): Dimension of equation.
         q (float): Load.
         D (float): Rigidity.
 
     Examples:
```

## ppsci/equation/pde/laplace.py

```diff
@@ -13,15 +13,19 @@
 # limitations under the License.
 
 from ppsci.autodiff import hessian
 from ppsci.equation.pde import base
 
 
 class Laplace(base.PDE):
-    """Class for laplace equation.
+    r"""Class for laplace equation.
+
+    $$
+    \nabla^2 \varphi = 0
+    $$
 
     Args:
         dim (int): Dimension of equation.
 
     Examples:
         >>> import ppsci
         >>> pde = ppsci.equation.Laplace(2)
```

## ppsci/equation/pde/navier_stokes.py

```diff
@@ -8,99 +8,129 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Callable
+from typing import Union
+
 from ppsci.autodiff import hessian
 from ppsci.autodiff import jacobian
 from ppsci.equation.pde import base
 
 
 class NavierStokes(base.PDE):
-    """Class for navier-stokes equation.
+    r"""Class for navier-stokes equation.
+
+    $$
+    \begin{cases}
+        \dfrac{\partial u}{\partial x} + \dfrac{\partial v}{\partial y} + \dfrac{\partial w}{\partial z} = 0 \\
+        \dfrac{\partial u}{\partial t} + u\dfrac{\partial u}{\partial x} + v\dfrac{\partial u}{\partial y} + w\dfrac{\partial w}{\partial z} =
+            - \dfrac{1}{\rho}\dfrac{\partial p}{\partial x}
+            + \nu(
+                \dfrac{\partial ^2 u}{\partial x ^2}
+                + \dfrac{\partial ^2 u}{\partial y ^2}
+                + \dfrac{\partial ^2 u}{\partial z ^2}
+            ) \\
+        \dfrac{\partial v}{\partial t} + u\dfrac{\partial v}{\partial x} + v\dfrac{\partial v}{\partial y} + w\dfrac{\partial w}{\partial z} =
+            - \dfrac{1}{\rho}\dfrac{\partial p}{\partial y}
+            + \nu(
+                \dfrac{\partial ^2 v}{\partial x ^2}
+                + \dfrac{\partial ^2 v}{\partial y ^2}
+                + \dfrac{\partial ^2 v}{\partial z ^2}
+            ) \\
+        \dfrac{\partial w}{\partial t} + u\dfrac{\partial w}{\partial x} + v\dfrac{\partial w}{\partial y} + w\dfrac{\partial w}{\partial z} =
+            - \dfrac{1}{\rho}\dfrac{\partial p}{\partial z}
+            + \nu(
+                \dfrac{\partial ^2 w}{\partial x ^2}
+                + \dfrac{\partial ^2 w}{\partial y ^2}
+                + \dfrac{\partial ^2 w}{\partial z ^2}
+            ) \\
+    \end{cases}
+    $$
 
     Args:
-        nu (float): Dynamic viscosity.
+        nu (Union[float, Callable]): Dynamic viscosity.
         rho (float): Density.
         dim (int): Dimension of equation.
         time (bool): Whether the euqation is time-dependent.
 
     Examples:
         >>> import ppsci
         >>> pde = ppsci.equation.NavierStokes(0.1, 1.0, 3, False)
     """
 
-    def __init__(self, nu: float, rho: float, dim: int, time: bool):
+    def __init__(self, nu: Union[float, Callable], rho: float, dim: int, time: bool):
         super().__init__()
         self.nu = nu
         self.rho = rho
         self.dim = dim
         self.time = time
 
         def continuity_compute_func(out):
             x, y = out["x"], out["y"]
             u, v = out["u"], out["v"]
             continuity = jacobian(u, x) + jacobian(v, y)
             if self.dim == 3:
-                z = out["z"]
-                w = out["w"]
+                z, w = out["z"], out["w"]
                 continuity += jacobian(w, z)
             return continuity
 
         self.add_equation("continuity", continuity_compute_func)
 
         def momentum_x_compute_func(out):
+            nu = self.nu(out) if callable(self.nu) else self.nu
             x, y = out["x"], out["y"]
             u, v, p = out["u"], out["v"], out["p"]
             momentum_x = (
                 u * jacobian(u, x)
                 + v * jacobian(u, y)
                 - nu / rho * hessian(u, x)
                 - nu / rho * hessian(u, y)
                 + 1 / rho * jacobian(p, x)
             )
             if self.time:
                 t = out["t"]
                 momentum_x += jacobian(u, t)
             if self.dim == 3:
-                z = out["z"]
-                w = out["w"]
+                z, w = out["z"], out["w"]
                 momentum_x += w * jacobian(u, z)
                 momentum_x -= nu / rho * hessian(u, z)
             return momentum_x
 
         self.add_equation("momentum_x", momentum_x_compute_func)
 
         def momentum_y_compute_func(out):
+            nu = self.nu(out) if callable(self.nu) else self.nu
             x, y = out["x"], out["y"]
             u, v, p = out["u"], out["v"], out["p"]
             momentum_y = (
                 u * jacobian(v, x)
                 + v * jacobian(v, y)
                 - nu / rho * hessian(v, x)
                 - nu / rho * hessian(v, y)
                 + 1 / rho * jacobian(p, y)
             )
             if self.time:
                 t = out["t"]
                 momentum_y += jacobian(v, t)
             if self.dim == 3:
-                z = out["z"]
-                w = out["w"]
+                z, w = out["z"], out["w"]
                 momentum_y += w * jacobian(v, z)
                 momentum_y -= nu / rho * hessian(v, z)
             return momentum_y
 
         self.add_equation("momentum_y", momentum_y_compute_func)
 
         if self.dim == 3:
 
             def momentum_z_compute_func(out):
+                nu = self.nu(out) if callable(self.nu) else self.nu
                 x, y, z = out["x"], out["y"], out["z"]
                 u, v, w, p = out["u"], out["v"], out["w"], out["p"]
                 momentum_z = (
                     u * jacobian(w, x)
                     + v * jacobian(w, y)
                     + w * jacobian(w, z)
                     - nu / rho * hessian(w, x)
```

## ppsci/equation/pde/normal_dot_vec.py

```diff
@@ -14,15 +14,19 @@
 
 from typing import Tuple
 
 from ppsci.equation.pde import base
 
 
 class NormalDotVec(base.PDE):
-    """NormalDotVec.
+    r"""NormalDotVec.
+
+    $$
+    \mathbf{n} \cdot \mathbf{v} = 0
+    $$
 
     Args:
         velocity_keys (Tuple[str, ...]): Keys for velocity(ies).
 
     Examples:
         >>> import ppsci
         >>> pde = ppsci.equation.NormalDotVec(("u", "v", "w"))
```

## ppsci/equation/pde/poisson.py

```diff
@@ -13,15 +13,19 @@
 # limitations under the License.
 
 from ppsci.autodiff import hessian
 from ppsci.equation.pde import base
 
 
 class Poisson(base.PDE):
-    """Class for poisson equation.
+    r"""Class for poisson equation.
+
+    $$
+    \nabla^2 \varphi = C
+    $$
 
     Args:
         dim (int): Dimension of equation.
 
     Examples:
         >>> import ppsci
         >>> pde = ppsci.equation.Poisson(2)
```

## ppsci/equation/pde/viv.py

```diff
@@ -17,36 +17,40 @@
 
 from ppsci.autodiff import hessian
 from ppsci.autodiff import jacobian
 from ppsci.equation.pde import base
 
 
 class Vibration(base.PDE):
-    """Vortex induced vibration equation.
+    r"""Vortex induced vibration equation.
+
+    $$
+    \rho \dfrac{\partial^2 \eta}{\partial t^2} + e^{k1} \dfrac{\partial \eta}{\partial t} + e^{k2} \eta = f
+    $$
 
     Args:
         rho (float): Generalized mass.
-        k1 (float): Learnable paremters for modal damping.
-        k2 (float): Learnable paremters for generalized stiffness.
+        k1 (float): Learnable parameter for modal damping.
+        k2 (float): Learnable parameter for generalized stiffness.
 
     Examples:
         >>> import ppsci
         >>> pde = ppsci.equation.Vibration(1.0, 4.0, -1.0)
     """
 
     def __init__(self, rho: float, k1: float, k2: float):
         super().__init__()
         self.rho = rho
         self.k1 = paddle.create_parameter(
-            shape=[1],
+            shape=[],
             dtype=paddle.get_default_dtype(),
             default_initializer=initializer.Constant(k1),
         )
         self.k2 = paddle.create_parameter(
-            shape=[1],
+            shape=[],
             dtype=paddle.get_default_dtype(),
             default_initializer=initializer.Constant(k2),
         )
         self.learnable_parameters.append(self.k1)
         self.learnable_parameters.append(self.k2)
 
         def f_compute_func(out):
```

## ppsci/geometry/geometry.py

```diff
@@ -90,15 +90,23 @@
                 _nsuc += 1
 
             if _ntry >= 1000 and _nsuc == 0:
                 raise ValueError(
                     "Sample interior points failed, "
                     "please check correctness of geometry and given creteria."
                 )
-        return misc.convert_to_dict(x, self.dim_keys)
+
+        # if sdf_func added, return x_dict and sdf_dict, else, only return the x_dict
+        if hasattr(self, "sdf_func"):
+            sdf = -self.sdf_func(x)
+            sdf_dict = misc.convert_to_dict(sdf, ("sdf",))
+        else:
+            sdf_dict = {}
+        x_dict = misc.convert_to_dict(x, self.dim_keys)
+        return {**x_dict, **sdf_dict}
 
     def sample_boundary(self, n, random="pseudo", criteria=None, evenly=False):
         """Compute the random points in the geometry and return those meet criteria."""
         x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())
         _size, _ntry, _nsuc = 0, 0, 0
         while _size < n:
             if evenly:
@@ -133,15 +141,15 @@
             _ntry += 1
             if len(points) > 0:
                 _nsuc += 1
 
             if _ntry >= 1000 and _nsuc == 0:
                 raise ValueError(
                     "Sample boundary points failed, "
-                    "please check correctness of geometry and given creteria."
+                    "please check correctness of geometry and given criteria."
                 )
 
         if not (
             misc.typename(self) == "TimeXGeometry"
             and misc.typename(self.geometry) == "Mesh"
         ):
             normal = self.boundary_normal(x)
```

## ppsci/geometry/geometry_1d.py

```diff
@@ -89,7 +89,28 @@
         periodic_x_normal = self.boundary_normal(periodic_x)
 
         periodic_x = misc.convert_to_dict(periodic_x, self.dim_keys)
         periodic_x_normal = misc.convert_to_dict(
             periodic_x_normal, [f"normal_{k}" for k in self.dim_keys]
         )
         return {**periodic_x, **periodic_x_normal}
+
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape is [N, 1]
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+
+        For interval with [l, r], the sdf is defined by:
+            sdf(x) = -min(x-l, r-x) = ((r-l)/2 - abs(x-(l+r)/2))/2
+        """
+        return ((self.r - self.l) / 2 - np.abs(points - (self.l + self.r) / 2)) / 2
```

## ppsci/geometry/geometry_2d.py

```diff
@@ -74,14 +74,34 @@
         return self.radius * X + self.center
 
     def random_boundary_points(self, n, random="pseudo"):
         theta = 2 * np.pi * sampler.sample(n, 1, random)
         X = np.concatenate((np.cos(theta), np.sin(theta)), axis=1)
         return self.radius * X + self.center
 
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape is [N, 2]
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        sdf = self.radius - np.linalg.norm(points - self.center, axis=1)
+        sdf = -sdf[..., np.newaxis]
+        return sdf
+
 
 class Rectangle(geometry_nd.Hypercube):
     """Class for rectangle geometry
 
     Args:
         xmin (Tuple[float, float]): Bottom left corner point, [x0, y0].
         xmax (Tuple[float, float]): Top right corner point, [x1, y1].
@@ -173,14 +193,43 @@
             len(vertices) == 4
             and np.isclose(np.prod(vertices[1] - vertices[0]), 0)
             and np.isclose(np.prod(vertices[2] - vertices[1]), 0)
             and np.isclose(np.prod(vertices[3] - vertices[2]), 0)
             and np.isclose(np.prod(vertices[0] - vertices[3]), 0)
         )
 
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape of the array is [N, 2].
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        if points.shape[1] != 2:
+            raise ValueError(
+                f"Shape of given points should be [*, 2], but got {points.shape}"
+            )
+        center = (self.xmin + self.xmax) / 2
+        dist_from_center = (
+            np.abs(points - center) - np.array([self.xmax - self.xmin]) / 2
+        )
+        return -(
+            np.linalg.norm(np.maximum(dist_from_center, 0), axis=1)
+            + np.minimum(np.max(dist_from_center, axis=1), 0)
+        ).reshape(-1, 1)
+
 
 class Triangle(geometry.Geometry):
     """Class for Triangle
 
     The order of vertices can be in a clockwise or counterclockwise direction. The
     vertices will be re-ordered in counterclockwise (right hand rule).
 
@@ -345,14 +394,61 @@
                 x.append(l * self.n12 + self.x1)
             elif l < self.l12 + self.l23:
                 x.append((l - self.l12) * self.n23 + self.x2)
             else:
                 x.append((l - self.l12 - self.l23) * self.n31 + self.x3)
         return np.vstack(x)
 
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape of the array is [N, 2].
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        if points.shape[1] != 2:
+            raise ValueError(
+                f"Shape of given points should be [*, 2], but got {points.shape}"
+            )
+        v1p = points - self.x1  # v1p: vector from x1 to points
+        v2p = points - self.x2
+        v3p = points - self.x3
+        # vv12_p: vertical vector of points to v12(If the vertical point is in the extension of v12,
+        # the vector will be the vector from x1 to points)
+        vv12_p = (
+            self.v12
+            * np.clip(np.dot(v1p, self.v12.reshape(2, -1)) / self.l12**2, 0, 1)
+            - v1p
+        )
+        vv23_p = (
+            self.v23
+            * np.clip(np.dot(v2p, self.v23.reshape(2, -1)) / self.l23**2, 0, 1)
+            - v2p
+        )
+        vv31_p = (
+            self.v31
+            * np.clip(np.dot(v3p, self.v31.reshape(2, -1)) / self.l31**2, 0, 1)
+            - v3p
+        )
+        is_inside = self.is_inside(points).reshape(-1, 1) * 2 - 1
+        len_vv12_p = np.linalg.norm(vv12_p, axis=1, keepdims=True)
+        len_vv23_p = np.linalg.norm(vv23_p, axis=1, keepdims=True)
+        len_vv31_p = np.linalg.norm(vv31_p, axis=1, keepdims=True)
+        mini_dist = np.minimum(np.minimum(len_vv12_p, len_vv23_p), len_vv31_p)
+        return is_inside * mini_dist
+
 
 class Polygon(geometry.Geometry):
     """Class for simple polygon.
 
     Args:
         vertices (Tuple[Tuple[float, float], ...]): The order of vertices can be in a
             clockwise or counterclockwisedirection. The vertices will be re-ordered in
@@ -499,14 +595,59 @@
             if l > l1:
                 i += 1
                 l0, l1 = l1, l1 + self.diagonals[i, i + 1]
                 v = (self.vertices[i + 1] - self.vertices[i]) / self.diagonals[i, i + 1]
             x.append((l - l0) * v + self.vertices[i])
         return np.vstack(x)
 
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape is [N, 2]
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        sdf_value = np.empty((points.shape[0], 1), dtype=paddle.get_default_dtype())
+        for n in range(points.shape[0]):
+            distance = np.dot(
+                points[n] - self.vertices[0], points[n] - self.vertices[0]
+            )
+            inside_tag = 1.0
+            for i in range(self.vertices.shape[0]):
+                j = (self.vertices.shape[0] - 1) if i == 0 else (i - 1)
+                # Calculate the shortest distance from point P to each edge.
+                vector_ij = self.vertices[j] - self.vertices[i]
+                vector_in = points[n] - self.vertices[i]
+                distance_vector = vector_in - vector_ij * np.clip(
+                    np.dot(vector_in, vector_ij) / np.dot(vector_ij, vector_ij),
+                    0.0,
+                    1.0,
+                )
+                distance = np.minimum(
+                    distance, np.dot(distance_vector, distance_vector)
+                )
+                # Calculate the inside and outside using the Odd-even rule
+                odd_even_rule_number = np.array(
+                    [
+                        points[n][1] >= self.vertices[i][1],
+                        points[n][1] < self.vertices[j][1],
+                        vector_ij[0] * vector_in[1] > vector_ij[1] * vector_in[0],
+                    ]
+                )
+                if odd_even_rule_number.all() or np.all(~odd_even_rule_number):
+                    inside_tag *= -1.0
+            sdf_value[n] = inside_tag * np.sqrt(distance)
+        return -sdf_value
+
 
 def polygon_signed_area(vertices):
     """The (signed) area of a simple polygon.
 
     If the vertices are in the counterclockwise direction, then the area is positive; if
     they are in the clockwise direction, the area is negative.
```

## ppsci/geometry/geometry_3d.py

```diff
@@ -126,14 +126,36 @@
                     )
                 )
         pts = np.vstack(pts)
         if len(pts) > n:
             return pts[np.random.choice(len(pts), size=n, replace=False)]
         return pts
 
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape is [N, 3]
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        sdf = (
+            ((self.xmax - self.xmin) / 2 - abs(points - (self.xmin + self.xmax) / 2))
+        ).min(axis=1)
+        sdf = -sdf[..., np.newaxis]
+        return sdf
+
 
 class Sphere(geometry_nd.Hypersphere):
     """Class for Sphere
 
     Args:
         center (Tuple[float, float, float]): Center of the sphere [x0, y0, z0].
         radius (float): Radius of the sphere.
@@ -145,7 +167,27 @@
     def uniform_boundary_points(self, n: int):
         nl = np.arange(1, n + 1).astype(paddle.get_default_dtype())
         g = (np.sqrt(5) - 1) / 2
         z = (2 * nl - 1) / n - 1
         x = np.sqrt(1 - z**2) * np.cos(2 * np.pi * nl * g)
         y = np.sqrt(1 - z**2) * np.sin(2 * np.pi * nl * g)
         return np.stack((x, y, z), axis=-1)
+
+    def sdf_func(self, points: np.ndarray) -> np.ndarray:
+        """Compute signed distance field.
+
+        Args:
+            points (np.ndarray): The coordinate points used to calculate the SDF value,
+                the shape is [N, 3]
+
+        Returns:
+            np.ndarray: Unsquared SDF values of input points, the shape is [N, 1].
+
+        NOTE: This function usually returns ndarray with negative values, because
+        according to the definition of SDF, the SDF value of the coordinate point inside
+        the object(interior points) is negative, the outside is positive, and the edge
+        is 0. Therefore, when used for weighting, a negative sign is often added before
+        the result of this function.
+        """
+        sdf = self.radius - (((points - self.center) ** 2).sum(axis=1)) ** 0.5
+        sdf = -sdf[..., np.newaxis]
+        return sdf
```

## ppsci/geometry/mesh.py

```diff
@@ -8,27 +8,32 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Callable
+from typing import Dict
 from typing import Optional
 from typing import Union
 
 import numpy as np
 import paddle
 
 from ppsci.geometry import geometry
 from ppsci.geometry import geometry_3d
 from ppsci.geometry import sampler
 from ppsci.utils import checker
 from ppsci.utils import misc
 
+if TYPE_CHECKING:
+    import pymesh
+
 
 class Mesh(geometry.Geometry):
     """Class for mesh geometry.
 
     Args:
         mesh (Union[str, Mesh]): Mesh file path or mesh object, such as "/path/to/mesh.stl".
 
@@ -36,15 +41,18 @@
         >>> import ppsci
         >>> geom = ppsci.geometry.Mesh("/path/to/mesh.stl")  # doctest: +SKIP
     """
 
     def __init__(self, mesh: Union["pymesh.Mesh", str]):
         # check if pymesh is installed when using Mesh Class
         if not checker.dynamic_import_to_globals(["pymesh"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pymesh python package."
+                "Please install it as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import pymesh
 
         if isinstance(mesh, str):
             self.py_mesh = pymesh.meshio.load_mesh(mesh)
         elif isinstance(mesh, pymesh.Mesh):
             self.py_mesh = mesh
         else:
@@ -54,20 +62,19 @@
 
     def init_mesh(self):
         """Initialize necessary variables for mesh"""
         if "face_normal" not in self.py_mesh.get_attribute_names():
             self.py_mesh.add_attribute("face_normal")
         self.face_normal = self.py_mesh.get_attribute("face_normal").reshape([-1, 3])
 
-        if "face_area" not in self.py_mesh.get_attribute_names():
-            self.py_mesh.add_attribute("face_area")
-        self.face_area = self.py_mesh.get_attribute("face_area").reshape([-1])
-
         if not checker.dynamic_import_to_globals(["open3d"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import open3d python package. "
+                "Please install it with `pip install open3d`."
+            )
         import open3d
 
         self.open3d_mesh = open3d.geometry.TriangleMesh(
             open3d.utility.Vector3dVector(np.array(self.py_mesh.vertices)),
             open3d.utility.Vector3iVector(np.array(self.py_mesh.faces)),
         )
         self.open3d_mesh.compute_vertex_normals()
@@ -83,15 +90,18 @@
         self.v0 = self.vectors[:, 0]
         self.v1 = self.vectors[:, 1]
         self.v2 = self.vectors[:, 2]
         self.num_vertices = self.py_mesh.num_vertices
         self.num_faces = self.py_mesh.num_faces
 
         if not checker.dynamic_import_to_globals(["pysdf"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pysdf python package. "
+                "Please install open3d with `pip install pysdf`."
+            )
         import pysdf
 
         self.pysdf = pysdf.SDF(self.vertices, self.faces)
         self.bounds = (
             ((np.min(self.vectors[:, :, 0])), np.max(self.vectors[:, :, 0])),
             ((np.min(self.vectors[:, :, 1])), np.max(self.vectors[:, :, 1])),
             ((np.min(self.vectors[:, :, 2])), np.max(self.vectors[:, :, 2])),
@@ -110,15 +120,18 @@
         NOTE: This function usually returns ndarray with negative values, because
         according to the definition of SDF, the SDF value of the coordinate point inside
         the object(interior points) is negative, the outside is positive, and the edge
         is 0. Therefore, when used for weighting, a negative sign is often added before
         the result of this function.
         """
         if not checker.dynamic_import_to_globals(["pymesh"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pymesh python package."
+                "Please install it as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import pymesh
 
         sdf, _, _, _ = pymesh.signed_distance_to_mesh(self.py_mesh, points)
         sdf = sdf[..., np.newaxis]
         return sdf
 
     def is_inside(self, x):
@@ -129,15 +142,19 @@
         return np.isclose(self.sdf_func(x), 0.0).flatten()
 
     def translate(self, translation, relative=True):
         vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())
         faces = np.array(self.faces)
 
         if not checker.dynamic_import_to_globals(("open3d", "pymesh")):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import open3d and pymesh python package. "
+                "Please install open3d with `pip install open3d` and "
+                "pymesh as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import open3d
         import pymesh
 
         open3d_mesh = open3d.geometry.TriangleMesh(
             open3d.utility.Vector3dVector(vertices),
             open3d.utility.Vector3iVector(faces),
         )
@@ -149,23 +166,27 @@
         return self
 
     def scale(self, scale, center=(0, 0, 0)):
         vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())
         faces = np.array(self.faces, dtype=paddle.get_default_dtype())
 
         if not checker.dynamic_import_to_globals(("open3d", "pymesh")):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import open3d and pymesh python package. "
+                "Please install open3d with `pip install open3d` and "
+                "pymesh as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import open3d
         import pymesh
 
         open3d_mesh = open3d.geometry.TriangleMesh(
             open3d.utility.Vector3dVector(vertices),
             open3d.utility.Vector3iVector(faces),
         )
-        open3d_mesh.scale(scale, center)
+        open3d_mesh = open3d_mesh.scale(scale, center)
         self.py_mesh = pymesh.form_mesh(
             np.asarray(open3d_mesh.vertices, dtype=paddle.get_default_dtype()), faces
         )
         self.init_mesh()
         return self
 
     def uniform_boundary_points(self, n: int):
@@ -192,137 +213,189 @@
             all_points.append(points)
             all_areas.append(areas)
 
         all_points = np.concatenate(all_points, axis=0)
         all_areas = np.concatenate(all_areas, axis=0)
         return all_points, all_areas
 
-    def inflated_random_boundary_points(
-        self, n, distance, random="pseudo", criteria=None
-    ):
-        if not isinstance(n, (tuple, list)):
-            n = [n]
-        if not isinstance(distance, (tuple, list)):
-            distance = [distance]
-        if len(n) != len(distance):
-            raise ValueError(
-                f"len(n)({len(n)}) should be equal to len(distance)({len(distance)})"
-            )
-        all_points = []
-        all_normal = []
-        all_area = []
-
-        from ppsci.geometry import inflation
-
-        for _n, _dist in zip(n, distance):
-            inflated_mesh = Mesh(inflation.pymesh_inflation(self.py_mesh, _dist))
-            points, normal, area = inflated_mesh.random_boundary_points(
-                _n, random, criteria
-            )
-            all_points.append(points)
-            all_points.append(normal)
-            all_points.append(area)
-
-        all_points = np.concatenate(all_points, axis=0)
-        all_normal = np.concatenate(all_normal, axis=0)
-        all_area = np.concatenate(all_area, axis=0)
-        return all_points, all_normal, all_area
-
     def _approximate_area(
         self,
         random: str = "pseudo",
         criteria: Optional[Callable] = None,
         n_appr: int = 10000,
-    ) -> np.ndarray:
+    ) -> float:
         """Approximate area with given `criteria` and `n_appr` points by Monte Carlo
         algorithm.
 
         Args:
             random (str, optional): Random method. Defaults to "pseudo".
             criteria (Optional[Callable]): Criteria function. Defaults to None.
             n_appr (int): Number of points for approximating area. Defaults to 10000.
 
         Returns:
             np.ndarray: Approximated areas with shape of [n_faces, ].
         """
+        triangle_areas = area_of_triangles(self.v0, self.v1, self.v2)
+        triangle_probabilities = triangle_areas / np.linalg.norm(triangle_areas, ord=1)
+        triangle_index = np.arange(triangle_probabilities.shape[0])
+        npoint_per_triangle = np.random.choice(
+            triangle_index, n_appr, p=triangle_probabilities
+        )
+        npoint_per_triangle, _ = np.histogram(
+            npoint_per_triangle,
+            np.arange(triangle_probabilities.shape[0] + 1) - 0.5,
+        )
+
         appr_areas = []
-        for i in range(self.num_faces):
-            sampled_points = sample_in_triangle(
-                self.v0[i], self.v1[i], self.v2[i], n_appr, random
-            )
-            appr_area = self.face_area[i]
-            if criteria is not None:
-                criteria_mask = criteria(
-                    *np.split(sampled_points, self.ndim, 1)
-                ).flatten()
-                appr_area *= criteria_mask.mean()
+        if criteria is not None:
+            aux_points = []
 
-            appr_areas.append(appr_area)
+        for i, npoint in enumerate(npoint_per_triangle):
+            if npoint == 0:
+                continue
+            # sample points for computing criteria mask if criteria is given
+            if criteria is not None:
+                points_at_triangle_i = sample_in_triangle(
+                    self.v0[i], self.v1[i], self.v2[i], npoint, random
+                )
+                aux_points.append(points_at_triangle_i)
 
-        return np.asarray(appr_areas, paddle.get_default_dtype())
+            appr_areas.append(
+                np.full(
+                    (npoint, 1), triangle_areas[i] / npoint, paddle.get_default_dtype()
+                )
+            )
+        appr_areas = np.concatenate(appr_areas, axis=0)  # [n_appr, 1]
 
-    def random_boundary_points(self, n, random="pseudo", criteria=None):
-        valid_areas = self._approximate_area(random, criteria)
-        triangle_prob = valid_areas / np.linalg.norm(valid_areas, ord=1)
+        # set invalid area to 0 by computing criteria mask with auxiliary points
+        if criteria is not None:
+            aux_points = np.concatenate(aux_points, axis=0)  # [n_appr, 3]
+            criteria_mask = criteria(*np.split(aux_points, self.ndim, 1))
+            appr_areas *= criteria_mask
+        return appr_areas.sum()
+
+    def random_boundary_points(self, n, random="pseudo"):
+        triangle_area = area_of_triangles(self.v0, self.v1, self.v2)
+        triangle_prob = triangle_area / np.linalg.norm(triangle_area, ord=1)
         npoint_per_triangle = np.random.choice(
             np.arange(len(triangle_prob)), n, p=triangle_prob
         )
         npoint_per_triangle, _ = np.histogram(
             npoint_per_triangle, np.arange(len(triangle_prob) + 1) - 0.5
         )
 
-        all_points = []
-        all_normal = []
-        all_area = []
+        points = []
+        normal = []
+        areas = []
         for i, npoint in enumerate(npoint_per_triangle):
             if npoint == 0:
                 continue
-            face_points = sample_in_triangle(
-                self.v0[i], self.v1[i], self.v2[i], npoint, random, criteria
+            points_at_triangle_i = sample_in_triangle(
+                self.v0[i], self.v1[i], self.v2[i], npoint, random
             )
-            face_normal = np.tile(self.face_normal[i], [npoint, 1])
-            valid_area = np.full(
-                [npoint, 1],
-                valid_areas[i] / npoint,
+            normal_at_triangle_i = np.tile(self.face_normal[i], (npoint, 1)).astype(
+                paddle.get_default_dtype()
+            )
+            areas_at_triangle_i = np.full(
+                (npoint, 1),
+                triangle_area[i] / npoint,
                 dtype=paddle.get_default_dtype(),
             )
 
-            all_points.append(face_points)
-            all_normal.append(face_normal)
-            all_area.append(valid_area)
-
-        all_points = np.concatenate(all_points, axis=0)
-        all_normal = np.concatenate(all_normal, axis=0)
-        all_area = np.concatenate(all_area, axis=0)
+            points.append(points_at_triangle_i)
+            normal.append(normal_at_triangle_i)
+            areas.append(areas_at_triangle_i)
+
+        points = np.concatenate(points, axis=0)
+        normal = np.concatenate(normal, axis=0)
+        areas = np.concatenate(areas, axis=0)
 
-        # NOTE: use global mean area instead of local mean area
-        all_area = np.full_like(all_area, all_area.mean())
-        return all_points, all_normal, all_area
+        return points, normal, areas
 
     def sample_boundary(
         self, n, random="pseudo", criteria=None, evenly=False, inflation_dist=None
-    ):
+    ) -> Dict[str, np.ndarray]:
         # TODO(sensen): support for time-dependent points(repeat data in time)
         if inflation_dist is not None:
-            points, normals, areas = self.inflated_random_boundary_points(
-                n, inflation_dist, random
-            )
+            if not isinstance(n, (tuple, list)):
+                n = [n]
+            if not isinstance(inflation_dist, (tuple, list)):
+                inflation_dist = [inflation_dist]
+            if len(n) != len(inflation_dist):
+                raise ValueError(
+                    f"len(n)({len(n)}) should be equal to len(inflation_dist)({len(inflation_dist)})"
+                )
+
+            from ppsci.geometry import inflation
+
+            inflated_data_dict = {}
+            for _n, _dist in zip(n, inflation_dist):
+                # 1. manually inflate mesh at first
+                inflated_mesh = Mesh(inflation.pymesh_inflation(self.py_mesh, _dist))
+                # 2. compute all data by sample_boundary with `inflation_dist=None`
+                data_dict = inflated_mesh.sample_boundary(
+                    _n,
+                    random,
+                    criteria,
+                    evenly,
+                    inflation_dist=None,
+                )
+                for key, value in data_dict:
+                    if key not in inflated_data_dict:
+                        inflated_data_dict[key] = value
+                    else:
+                        inflated_data_dict[key] = np.concatenate(
+                            (inflated_data_dict[key], value), axis=0
+                        )
+            return inflated_data_dict
         else:
             if evenly:
                 raise ValueError(
                     "Can't sample evenly on mesh now, please set evenly=False."
                 )
-            # points, normal, area = self.uniform_boundary_points(n, False)
-            points, normals, areas = self.random_boundary_points(n, random, criteria)
+            _size, _ntry, _nsuc = 0, 0, 0
+            all_points = []
+            all_normal = []
+            while _size < n:
+                points, normal, _ = self.random_boundary_points(n, random)
+                if criteria is not None:
+                    criteria_mask = criteria(
+                        *np.split(points, self.ndim, axis=1)
+                    ).flatten()
+                    points = points[criteria_mask]
+                    normal = normal[criteria_mask]
+
+                if len(points) > n - _size:
+                    points = points[: n - _size]
+                    normal = normal[: n - _size]
+
+                all_points.append(points)
+                all_normal.append(normal)
+
+                _size += len(points)
+                _ntry += 1
+                if len(points) > 0:
+                    _nsuc += 1
+
+                if _ntry >= 1000 and _nsuc == 0:
+                    raise ValueError(
+                        "Sample boundary points failed, "
+                        "please check correctness of geometry and given creteria."
+                    )
+
+            all_points = np.concatenate(all_points, axis=0)
+            all_normal = np.concatenate(all_normal, axis=0)
+            appr_area = self._approximate_area(random, criteria)
+            all_areas = np.full((n, 1), appr_area / n, paddle.get_default_dtype())
 
-        x_dict = misc.convert_to_dict(points, self.dim_keys)
+        x_dict = misc.convert_to_dict(all_points, self.dim_keys)
         normal_dict = misc.convert_to_dict(
-            normals, [f"normal_{key}" for key in self.dim_keys if key != "t"]
+            all_normal, [f"normal_{key}" for key in self.dim_keys if key != "t"]
         )
-        area_dict = misc.convert_to_dict(areas, ["area"])
+        area_dict = misc.convert_to_dict(all_areas, ["area"])
         return {**x_dict, **normal_dict, **area_dict}
 
     def random_points(self, n, random="pseudo", criteria=None):
         _size = 0
         all_points = []
         cuboid = geometry_3d.Cuboid(
             [bound[0] for bound in self.bounds],
@@ -368,15 +441,18 @@
         sdf = -self.sdf_func(points)
         sdf_dict = misc.convert_to_dict(sdf, ["sdf"])
 
         return {**x_dict, **area_dict, **sdf_dict}
 
     def union(self, other: "Mesh"):
         if not checker.dynamic_import_to_globals(["pymesh"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pymesh python package. "
+                "Please install it as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import pymesh
 
         csg = pymesh.CSGTree(
             {"union": [{"mesh": self.py_mesh}, {"mesh": other.py_mesh}]}
         )
         return Mesh(csg.mesh)
 
@@ -384,28 +460,34 @@
         return self.union(other)
 
     def __add__(self, other: "Mesh"):
         return self.union(other)
 
     def difference(self, other: "Mesh"):
         if not checker.dynamic_import_to_globals(["pymesh"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pymesh python package. "
+                "Please install it as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import pymesh
 
         csg = pymesh.CSGTree(
             {"difference": [{"mesh": self.py_mesh}, {"mesh": other.py_mesh}]}
         )
         return Mesh(csg.mesh)
 
     def __sub__(self, other: "Mesh"):
         return self.difference(other)
 
     def intersection(self, other: "Mesh"):
         if not checker.dynamic_import_to_globals(["pymesh"]):
-            raise ModuleNotFoundError
+            raise ImportError(
+                "Could not import pymesh python package. "
+                "Please install it as https://pymesh.readthedocs.io/en/latest/installation.html."
+            )
         import pymesh
 
         csg = pymesh.CSGTree(
             {"intersection": [{"mesh": self.py_mesh}, {"mesh": other.py_mesh}]}
         )
         return Mesh(csg.mesh)
```

## ppsci/geometry/timedomain.py

```diff
@@ -196,15 +196,15 @@
                 _ntry += 1
                 if len(_x) > 0:
                     _nsuc += 1
 
                 if _ntry >= 1000 and _nsuc == 0:
                     raise ValueError(
                         "Sample points failed, "
-                        "please check correctness of geometry and given creteria."
+                        "please check correctness of geometry and given criteria."
                     )
 
             # 2. repeat spatial points along time
             tx = []
             for ti in t:
                 tx.append(
                     np.hstack(
@@ -530,17 +530,17 @@
 
     def random_initial_points(self, n, random="pseudo"):
         x = self.geometry.random_points(n, random=random)
         t = self.timedomain.t0
         return np.hstack((np.full([n, 1], t, dtype=paddle.get_default_dtype()), x))
 
     def periodic_point(self, x, component):
-        t, _x = x[:, :1], x[:, 1:]
-        xp = self.geometry.periodic_point(_x, component)
-        return np.vstack((x, np.hstack(t, xp)))
+        xp = self.geometry.periodic_point(x, component)
+        txp = {"t": x["t"], **xp}
+        return txp
 
     def sample_initial_interior(
         self, n: int, random: str = "pseudo", criteria=None, evenly=False
     ):
         """Sample random points in the time-geometry and return those meet criteria."""
         x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())
         _size, _ntry, _nsuc = 0, 0, 0
```

## ppsci/loss/__init__.py

```diff
@@ -11,26 +11,28 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 
 from ppsci.loss.base import Loss
+from ppsci.loss.func import FunctionalLoss
 from ppsci.loss.integral import IntegralLoss
 from ppsci.loss.l1 import L1Loss
 from ppsci.loss.l1 import PeriodicL1Loss
 from ppsci.loss.l2 import L2Loss
 from ppsci.loss.l2 import L2RelLoss
 from ppsci.loss.l2 import PeriodicL2Loss
 from ppsci.loss.mse import MSELoss
 from ppsci.loss.mse import MSELossWithL2Decay
 from ppsci.loss.mse import PeriodicMSELoss
 
 __all__ = [
     "Loss",
+    "FunctionalLoss",
     "IntegralLoss",
     "L1Loss",
     "PeriodicL1Loss",
     "L2Loss",
     "L2RelLoss",
     "PeriodicL2Loss",
     "MSELoss",
```

## ppsci/loss/integral.py

```diff
@@ -24,21 +24,21 @@
 
 class IntegralLoss(base.Loss):
     r"""Class for integral loss with Monte-Carlo integration algorithm.
 
     $$
     L =
     \begin{cases}
-        \dfrac{1}{N} \Vert \mathbf{s} \circ \mathbf{x} - \mathbf{y} \Vert_2^2, & \text{if reduction='mean'} \\
-         \Vert \mathbf{s} \circ \mathbf{x} - \mathbf{y} \Vert_2^2, & \text{if reduction='sum'}
+        \dfrac{1}{N} \Vert \displaystyle\sum_{i=1}^{M}{\mathbf{s}_i \cdot \mathbf{x}_i} - \mathbf{y} \Vert_2^2, & \text{if reduction='mean'} \\
+         \Vert \displaystyle\sum_{i=0}^{M}{\mathbf{s}_i \cdot \mathbf{x}_i} - \mathbf{y} \Vert_2^2, & \text{if reduction='sum'}
     \end{cases}
     $$
 
     $$
-    \mathbf{x}, \mathbf{y}, \mathbf{s} \in \mathcal{R}^{N}
+    \mathbf{x}, \mathbf{s} \in \mathcal{R}^{M \times N}, \mathbf{y} \in \mathcal{R}^{N}
     $$
 
     Args:
         reduction (Literal["mean", "sum"], optional): Reduction method. Defaults to "mean".
         weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.
 
     Examples:
```

## ppsci/loss/l1.py

```diff
@@ -56,19 +56,14 @@
     def forward(self, output_dict, label_dict, weight_dict=None):
         losses = 0.0
         for key in label_dict:
             loss = F.l1_loss(output_dict[key], label_dict[key], "none")
             if weight_dict:
                 loss *= weight_dict[key]
 
-            if isinstance(self.weight, (float, int)):
-                loss *= self.weight
-            elif isinstance(self.weight, dict) and key in self.weight:
-                loss *= self.weight[key]
-
             if "area" in output_dict:
                 loss *= output_dict["area"]
 
             loss = loss.sum(axis=1)
 
             if self.reduction == "sum":
                 loss = loss.sum()
@@ -123,15 +118,15 @@
                     f"Length of output({n_output}) of key({key}) should be even."
                 )
 
             n_output //= 2
             loss = F.l1_loss(
                 output_dict[key][:n_output], output_dict[key][n_output:], "none"
             )
-            if weight_dict is not None:
+            if weight_dict:
                 loss *= weight_dict[key]
             if "area" in output_dict:
                 loss *= output_dict["area"]
 
             loss = loss.sum(axis=1)
 
             if self.reduction == "sum":
```

## ppsci/loss/l2.py

```diff
@@ -54,15 +54,15 @@
             )
         super().__init__(reduction, weight)
 
     def forward(self, output_dict, label_dict, weight_dict=None):
         losses = 0.0
         for key in label_dict:
             loss = F.mse_loss(output_dict[key], label_dict[key], "none")
-            if weight_dict is not None:
+            if weight_dict:
                 loss *= weight_dict[key]
 
             if "area" in output_dict:
                 loss *= output_dict["area"]
 
             loss = loss.sum(axis=1).sqrt()
 
@@ -184,15 +184,15 @@
         y_norms = paddle.norm(y_, p=2, axis=1)
         return diff_norms / y_norms
 
     def forward(self, output_dict, label_dict, weight_dict=None):
         losses = 0
         for key in label_dict:
             loss = self.rel_loss(output_dict[key], label_dict[key])
-            if weight_dict is not None:
+            if weight_dict:
                 loss *= weight_dict[key]
 
             if self.reduction == "sum":
                 loss = loss.sum()
             elif self.reduction == "mean":
                 loss = loss.mean()
```

## ppsci/loss/mse.py

```diff
@@ -173,15 +173,15 @@
                     f"Length of output({n_output}) of key({key}) should be even."
                 )
 
             n_output //= 2
             loss = F.mse_loss(
                 output_dict[key][:n_output], output_dict[key][n_output:], "none"
             )
-            if weight_dict is not None:
+            if weight_dict:
                 loss *= weight_dict[key]
             if "area" in output_dict:
                 loss *= output_dict["area"]
 
             if self.reduction == "sum":
                 loss = loss.sum()
             elif self.reduction == "mean":
```

## ppsci/metric/__init__.py

```diff
@@ -12,24 +12,26 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 
 from ppsci.metric.anomaly_coef import LatitudeWeightedACC
 from ppsci.metric.base import Metric
+from ppsci.metric.func import FunctionalMetric
 from ppsci.metric.l2_rel import L2Rel
 from ppsci.metric.mae import MAE
 from ppsci.metric.mse import MSE
 from ppsci.metric.rmse import RMSE
 from ppsci.metric.rmse import LatitudeWeightedRMSE
 from ppsci.utils import misc
 
 __all__ = [
     "LatitudeWeightedACC",
     "Metric",
+    "FunctionalMetric",
     "L2Rel",
     "MAE",
     "MSE",
     "RMSE",
     "LatitudeWeightedRMSE",
     "build_metric",
 ]
```

## ppsci/metric/anomaly_coef.py

```diff
@@ -28,15 +28,15 @@
 
     $$
     metric =
         \dfrac{\sum\limits_{m,n}{L_mX_{mn}Y_{mn}}}{\sqrt{\sum\limits_{m,n}{L_mX_{mn}^{2}}\sum\limits_{m,n}{L_mY_{mn}^{2}}}}
     $$
 
     $$
-    L_m = N_{lat}\dfrac{cos(lat_m)}{\sum\limits_{j=1}^{N_{lat}}cos(lat_j)}
+    L_m = N_{lat}\dfrac{\cos(lat_m)}{\sum\limits_{j=1}^{N_{lat}}\cos(lat_j)}
     $$
 
     $lat_m$ is the latitude at m.
     $N_{lat}$ is the number of latitude set by `num_lat`.
 
     Args:
         num_lat (int): Number of latitude.
```

## ppsci/metric/l2_rel.py

```diff
@@ -8,42 +8,52 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import paddle
 
 from ppsci.metric import base
 
 
 class L2Rel(base.Metric):
     r"""Class for l2 relative error.
 
     $$
-    metric = \dfrac{\Vert x-y \Vert_2}{\Vert y \Vert_2}
+    metric = \dfrac{\Vert \mathbf{x} - \mathbf{y} \Vert_2}{\Vert \max(\mathbf{y}, \epsilon) \Vert_2}
+    $$
+
+    $$
+    \mathbf{x}, \mathbf{y} \in \mathcal{R}^{N}
     $$
 
     Args:
         keep_batch (bool, optional): Whether keep batch axis. Defaults to False.
 
     Examples:
         >>> import ppsci
         >>> metric = ppsci.metric.L2Rel()
     """
 
+    # NOTE: Avoid divide by zero in result
+    # see https://github.com/scikit-learn/scikit-learn/pull/15007
+    EPS: float = np.finfo(np.float32).eps
+
     def __init__(self, keep_batch: bool = False):
         if keep_batch:
             raise ValueError(f"keep_batch should be False, but got {keep_batch}.")
         super().__init__(keep_batch)
 
     @paddle.no_grad()
     def forward(self, output_dict, label_dict):
         metric_dict = {}
         for key in label_dict:
-            rel_l2 = paddle.norm(label_dict[key] - output_dict[key]) / paddle.norm(
-                label_dict[key]
-            )
+            rel_l2 = (
+                paddle.norm(label_dict[key] - output_dict[key], p=2, axis=1)
+                / paddle.norm(label_dict[key], p=2, axis=1).clip(min=self.EPS)
+            ).mean()
             metric_dict[key] = rel_l2
 
         return metric_dict
```

## ppsci/metric/mae.py

```diff
@@ -18,15 +18,19 @@
 from ppsci.metric import base
 
 
 class MAE(base.Metric):
     r"""Mean absolute error.
 
     $$
-    metric = \dfrac{1}{N}\sum\limits_{i=1}^{N}{|x_i-y_i|}
+    metric = \dfrac{1}{N} \Vert \mathbf{x} - \mathbf{y} \Vert_1
+    $$
+
+    $$
+    \mathbf{x}, \mathbf{y} \in \mathcal{R}^{N}
     $$
 
     Args:
         keep_batch (bool, optional): Whether keep batch axis. Defaults to False.
 
     Examples:
         >>> import ppsci
```

## ppsci/metric/mse.py

```diff
@@ -18,15 +18,19 @@
 from ppsci.metric import base
 
 
 class MSE(base.Metric):
     r"""Mean square error
 
     $$
-    metric = \dfrac{1}{N}\sum\limits_{i=1}^{N}{(x_i-y_i)^2}
+    metric = \dfrac{1}{N} \Vert \mathbf{x} - \mathbf{y} \Vert_2^2
+    $$
+
+    $$
+    \mathbf{x}, \mathbf{y} \in \mathcal{R}^{N}
     $$
 
     Args:
         keep_batch (bool, optional): Whether keep batch axis. Defaults to False.
 
     Examples:
         >>> import ppsci
```

## ppsci/metric/rmse.py

```diff
@@ -24,15 +24,19 @@
 from ppsci.metric import base
 
 
 class RMSE(base.Metric):
     r"""Root mean square error
 
     $$
-    metric = \sqrt{\dfrac{1}{N}\sum\limits_{i=1}^{N}{(x_i-y_i)^2}}
+    metric = \sqrt{\dfrac{1}{N} \Vert \mathbf{x} - \mathbf{y} \Vert_2^2}
+    $$
+
+    $$
+    \mathbf{x}, \mathbf{y} \in \mathcal{R}^{N}
     $$
 
     Args:
         keep_batch (bool, optional): Whether keep batch axis. Defaults to False.
 
     Examples:
         >>> import ppsci
@@ -58,15 +62,15 @@
     r"""Latitude weighted root mean square error.
 
     $$
     metric =\sqrt{\dfrac{1}{MN}\sum\limits_{m=1}^{M}\sum\limits_{n=1}^{N}L_m(X_{mn}-Y_{mn})^{2}}
     $$
 
     $$
-    L_m = N_{lat}\dfrac{cos(lat_m)}{\sum\limits_{j=1}^{N_{lat}}cos(lat_j)}
+    L_m = N_{lat}\dfrac{\cos(lat_m)}{\sum\limits_{j=1}^{N_{lat}}\cos(lat_j)}
     $$
 
     $lat_m$ is the latitude at m.
     $N_{lat}$ is the number of latitude set by `num_lat`.
 
     Args:
         num_lat (int): Number of latitude.
```

## ppsci/optimizer/__init__.py

```diff
@@ -35,39 +35,39 @@
 ]
 
 
 def build_lr_scheduler(cfg, epochs, iters_per_epoch):
     """Build learning rate scheduler.
 
     Args:
-        cfg (AttrDict): Learing rate scheduler config.
+        cfg (AttrDict): Learning rate scheduler config.
         epochs (int): Total epochs.
         iters_per_epoch (int): Number of iterations of one epoch.
 
     Returns:
-        LRScheduler: Learing rate scheduler.
+        LRScheduler: Learning rate scheduler.
     """
     cfg = copy.deepcopy(cfg)
     cfg.update({"epochs": epochs, "iters_per_epoch": iters_per_epoch})
     lr_scheduler_cls = cfg.pop("name")
     lr_scheduler_ = eval(lr_scheduler_cls)(**cfg)
     return lr_scheduler_()
 
 
 def build_optimizer(cfg, model_list, epochs, iters_per_epoch):
-    """Build optimizer and learing rate scheduler
+    """Build optimizer and learning rate scheduler
 
     Args:
-        cfg (AttrDict): Learing rate scheduler config.
+        cfg (AttrDict): Learning rate scheduler config.
         model_list (Tuple[nn.Layer, ...]): Tuple of model(s).
         epochs (int): Total epochs.
         iters_per_epoch (int): Number of iterations of one epoch.
 
     Returns:
-        Optimizer, LRScheduler: Optimizer and learing rate scheduler.
+        Optimizer, LRScheduler: Optimizer and learning rate scheduler.
     """
     # build lr_scheduler
     cfg = copy.deepcopy(cfg)
     lr_cfg = cfg.pop("lr")
     if isinstance(lr_cfg, float):
         lr_scheduler = lr_cfg
     else:
```

## ppsci/optimizer/lr_scheduler.py

```diff
@@ -25,14 +25,15 @@
     "Linear",
     "Cosine",
     "Step",
     "Piecewise",
     "MultiStepDecay",
     "ExponentialDecay",
     "CosineWarmRestarts",
+    "OneCycleLR",
 ]
 
 
 class LRBase:
     """Base class for custom learning rates.
 
     Args:
@@ -638,9 +639,92 @@
             last_epoch=self.last_epoch,
             verbose=self.verbose,
         )
 
         if self.warmup_steps > 0:
             learning_rate = self.linear_warmup(learning_rate)
 
+        setattr(learning_rate, "by_epoch", self.by_epoch)
+        return learning_rate
+
+
+class OneCycleLR(LRBase):
+    """Sets the learning rate according to the one cycle learning rate scheduler.
+    The scheduler adjusts the learning rate from an initial learning rate to the maximum learning rate and then
+    from that maximum learning rate to the minimum learning rate, which is much less than the initial learning rate.
+
+    It has been proposed in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120).
+
+    Please note that the default behaviour of this scheduler follows the fastai implementation of one cycle,
+    which claims that **"unpublished work has shown even better results by using only two phases"**.
+    If you want the behaviour of this scheduler to be consistent with the paper, please set `three_phase=True`.
+
+    Args:
+        epochs (int): Total epoch(s).
+        iters_per_epoch (int): Number of iterations within an epoch.
+        max_learning_rate (float): The maximum learning rate. It is a python float number. Functionally, it defines the initial learning rate by `divide_factor` .
+        divide_factor (float, optional): Initial learning rate will be determined by initial_learning_rate = max_learning_rate / divide_factor. Defaults to 25.0.
+        end_learning_rate (float, optional): The minimum learning rate during training, it should be much less than initial learning rate. Defaults to 0.0001.
+        phase_pct (float): The percentage of total steps which used to increasing learning rate. Defaults to 0.3.
+        anneal_strategy (str, optional): Strategy of adjusting learning rate. "cos" for cosine annealing, "linear" for linear annealing. Defaults to "cos".
+        three_phase (bool, optional): Whether to use three phase. Defaults to False.
+        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.
+        warmup_start_lr (float, optional): start learning rate within warmup. Defaults to 0.0.
+        last_epoch (int, optional): Last epoch. Defaults to -1.
+        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.
+
+    Examples:
+        >>> import ppsci
+        >>> lr = ppsci.optimizer.lr_scheduler.OneCycleLR(1e-3, 100)
+    """
+
+    def __init__(
+        self,
+        epochs: int,
+        iters_per_epoch: int,
+        max_learning_rate: float,
+        divide_factor: float = 25.0,
+        end_learning_rate: float = 0.0001,
+        phase_pct: float = 0.3,
+        anneal_strategy: str = "cos",
+        three_phase: bool = False,
+        warmup_epoch: int = 0,
+        warmup_start_lr: float = 0.0,
+        last_epoch: int = -1,
+        by_epoch: bool = False,
+    ):
+        super().__init__(
+            epochs,
+            iters_per_epoch,
+            max_learning_rate,
+            warmup_epoch,
+            warmup_start_lr,
+            last_epoch,
+            by_epoch,
+        )
+        self.total_steps = epochs
+        if not by_epoch:
+            self.total_steps *= iters_per_epoch
+        self.divide_factor = divide_factor
+        self.end_learning_rate = end_learning_rate
+        self.phase_pct = phase_pct
+        self.anneal_strategy = anneal_strategy
+        self.three_phase = three_phase
+
+    def __call__(self):
+        learning_rate = lr.OneCycleLR(
+            max_learning_rate=self.learning_rate,
+            total_steps=self.total_steps,
+            divide_factor=self.divide_factor,
+            end_learning_rate=self.end_learning_rate,
+            phase_pct=self.phase_pct,
+            anneal_strategy=self.anneal_strategy,
+            three_phase=self.three_phase,
+            last_epoch=self.last_epoch,
+            verbose=self.verbose,
+        )
+
+        if self.warmup_steps > 0:
+            learning_rate = self.linear_warmup(learning_rate)
+
         setattr(learning_rate, "by_epoch", self.by_epoch)
         return learning_rate
```

## ppsci/optimizer/optimizer.py

```diff
@@ -61,16 +61,18 @@
             Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]
         ] = None,
     ):
         self.learning_rate = learning_rate
         self.weight_decay = weight_decay
         self.grad_clip = grad_clip
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = (
             sum([m.parameters() for m in model_list], []) if model_list else None
         )
         opt = optim.SGD(
             learning_rate=self.learning_rate,
             parameters=parameters,
             weight_decay=self.weight_decay,
@@ -118,16 +120,18 @@
         self.weight_decay = weight_decay
         self.grad_clip = grad_clip
         self.use_nesterov = use_nesterov
         self.no_weight_decay_name_list = (
             no_weight_decay_name.split() if no_weight_decay_name else []
         )
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = None
         if len(self.no_weight_decay_name_list) > 0:
             params_with_decay = []
             params_without_decay = []
             for m in model_list:
                 params = [
                     p
@@ -208,16 +212,18 @@
         self.beta2 = beta2
         self.epsilon = epsilon
         self.learning_rate = learning_rate
         self.weight_decay = weight_decay
         self.grad_clip = grad_clip
         self.lazy_mode = lazy_mode
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = (
             sum([m.parameters() for m in model_list], []) if model_list else None
         )
         opt = optim.Adam(
             learning_rate=self.learning_rate,
             beta1=self.beta1,
             beta2=self.beta2,
@@ -269,16 +275,18 @@
         self.max_iter = max_iter
         self.max_eval = max_eval
         self.tolerance_grad = tolerance_grad
         self.tolerance_change = tolerance_change
         self.history_size = history_size
         self.line_search_fn = line_search_fn
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = (
             sum([m.parameters() for m in model_list], []) if model_list else None
         )
         try:
             opt = getattr(optim, "LBFGS")(
                 learning_rate=self.lr,
                 max_iter=self.max_iter,
@@ -340,16 +348,18 @@
         self.learning_rate = learning_rate
         self.momentum = momentum
         self.rho = rho
         self.epsilon = epsilon
         self.weight_decay = weight_decay
         self.grad_clip = grad_clip
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = (
             sum([m.parameters() for m in model_list], []) if model_list else None
         )
         opt = optim.RMSProp(
             learning_rate=self.learning_rate,
             momentum=self.momentum,
             rho=self.rho,
@@ -402,16 +412,18 @@
         self.grad_clip = grad_clip
         self.weight_decay = weight_decay
         self.no_weight_decay_name_list = (
             no_weight_decay_name.split() if no_weight_decay_name else []
         )
         self.one_dim_param_no_weight_decay = one_dim_param_no_weight_decay
 
-    def __call__(self, model_list: Tuple[nn.Layer, ...]):
+    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):
         # model_list is None in static graph
+        if not isinstance(model_list, (tuple, list)):
+            model_list = (model_list,)
         parameters = (
             sum([m.parameters() for m in model_list], []) if model_list else None
         )
 
         # TODO(gaotingquan): model_list is None when in static graph, "no_weight_decay" not work.
         if model_list is None:
             if (
```

## ppsci/solver/eval.py

```diff
@@ -16,15 +16,16 @@
 from typing import TYPE_CHECKING
 
 import paddle
 from paddle import io
 
 from ppsci.solver import printer
 from ppsci.utils import misc
-from ppsci.utils import profiler
+
+# from ppsci.utils import profiler
 
 if TYPE_CHECKING:
     from ppsci import solver
 
 
 def _eval_by_dataset(solver: "solver.Solver", epoch_id: int, log_freq: int) -> float:
     """Evaluate with computing metric on total samples.
```

## ppsci/solver/printer.py

```diff
@@ -66,23 +66,25 @@
         f"{metric_msg}, {time_msg}, {ips_msg}, {eta_msg}"
     )
 
     logger.scaler(
         name="lr",
         value=trainer.optimizer.get_lr(),
         step=trainer.global_step,
-        writer=trainer.vdl_writer,
+        vdl_writer=trainer.vdl_writer,
+        wandb_writer=trainer.wandb_writer,
     )
 
     for key in trainer.train_output_info:
         logger.scaler(
             name=f"train_{key}",
             value=trainer.train_output_info[key].avg,
             step=trainer.global_step,
-            writer=trainer.vdl_writer,
+            vdl_writer=trainer.vdl_writer,
+            wandb_writer=trainer.wandb_writer,
         )
 
 
 def log_eval_info(trainer, batch_size, epoch_id, iters_per_epoch, iter_id):
     metric_msg = ", ".join(
         [
             f"{key}: {trainer.eval_output_info[key].avg:.5f}"
@@ -106,9 +108,10 @@
     )
 
     for key in trainer.eval_output_info:
         logger.scaler(
             name=f"eval_{key}",
             value=trainer.eval_output_info[key].avg,
             step=trainer.global_step,
-            writer=trainer.vdl_writer,
+            vdl_writer=trainer.vdl_writer,
+            wandb_writer=trainer.wandb_writer,
         )
```

## ppsci/solver/solver.py

```diff
@@ -58,15 +58,17 @@
         update_freq (int, optional): Update frequency of parameters. Defaults to 1.
         save_freq (int, optional): Saving frequency for checkpoint. Defaults to 0.
         log_freq (int, optional): Logging frequency. Defaults to 10.
         eval_during_train (bool, optional): Whether evaluate model during training. Defaults to False.
         start_eval_epoch (int, optional): Epoch number evaluation applied begin after. Defaults to 1.
         eval_freq (int, optional): Evaluation frequency. Defaults to 1.
         seed (int, optional): Random seed. Defaults to 42.
-        vdl_writer (Optional[vdl.LogWriter]): VisualDL writer object. Defaults to None.
+        use_vdl (Optional[bool]): Whether use VisualDL to log scalars. Defaults to False.
+        use_wandb (Optional[bool]): Whether use wandb to log data. Defaults to False.
+        wandb_cfg (Optional[Dict[str, str]]): Config dict of wandb. Defaults to None.
         device (Literal["cpu", "gpu", "xpu"], optional): Runtime device. Defaults to "gpu".
         equation (Optional[Dict[str, ppsci.equation.PDE]]): Equation dict. Defaults to None.
         geom (Optional[Dict[str, ppsci.geometry.Geometry]]): Geometry dict. Defaults to None.
         validator (Optional[Dict[str, ppsci.validate.Validator]]): Validator dict. Defaults to None.
         visualizer (Optional[Dict[str, ppsci.visualize.Visualizer]]): Visualizer dict. Defaults to None.
         use_amp (bool, optional): Whether use AMP. Defaults to False.
         amp_level (Literal["O1", "O2", "O0"], optional): AMP level. Defaults to "O0".
@@ -115,15 +117,17 @@
         update_freq: int = 1,
         save_freq: int = 0,
         log_freq: int = 10,
         eval_during_train: bool = False,
         start_eval_epoch: int = 1,
         eval_freq: int = 1,
         seed: int = 42,
-        vdl_writer: Optional[vdl.LogWriter] = None,
+        use_vdl: bool = False,
+        use_wandb: bool = False,
+        wandb_config: Optional[Dict[str, str]] = None,
         device: Literal["cpu", "gpu", "xpu"] = "gpu",
         equation: Optional[Dict[str, ppsci.equation.PDE]] = None,
         geom: Optional[Dict[str, ppsci.geometry.Geometry]] = None,
         validator: Optional[Dict[str, ppsci.validate.Validator]] = None,
         visualizer: Optional[Dict[str, ppsci.visualize.Visualizer]] = None,
         use_amp: bool = False,
         amp_level: Literal["O1", "O2", "O0"] = "O0",
@@ -174,20 +178,38 @@
             "reader_cost": misc.AverageMeter("reader_cost", ".5f", postfix="s"),
         }
 
         # fix seed for reproducibility
         self.seed = seed
 
         # set VisualDL tool
-        self.vdl_writer = vdl_writer
+        self.vdl_writer = None
+        if use_vdl:
+            self.vdl_writer = vdl.LogWriter(f"{output_dir}/vdl")
+        # set WandB tool
+        self.wandb_writer = None
+        if use_wandb:
+            try:
+                import wandb
+            except ModuleNotFoundError:
+                raise ModuleNotFoundError(
+                    "Please install 'wandb' with `pip install wandb` first."
+                )
+            wandb.init(**wandb_config)
+            self.wandb_writer = wandb
 
         # set running device
+        if device != "cpu" and paddle.device.get_device() == "cpu":
+            logger.warning(f"Set device({device}) to 'cpu' for only cpu available.")
+            device = "cpu"
         self.device = paddle.set_device(device)
+
         # set equations for physics-driven or data-physics hybrid driven task, such as PINN
         self.equation = equation
+
         # set geometry for generating data
         self.geom = {} if geom is None else geom
 
         # set validator
         self.validator = validator
 
         # set visualizer
@@ -213,14 +235,20 @@
                         f"{misc.typename(metric)}.keep_batch should be "
                         f"{compute_metric_by_batch} when compute_metric_by_batch="
                         f"{compute_metric_by_batch}."
                     )
         # whether set `stop_gradient=True` for every Tensor if no differentiation involved during computation
         self.eval_with_no_grad = eval_with_no_grad
 
+        # decorate model(s) and optimizer(s) for AMP
+        if self.use_amp:
+            self.model, self.optimizer = amp.decorate(
+                self.model, self.optimizer, self.amp_level
+            )
+
         # initialize an dict for tracking best metric during training
         self.best_metric = {
             "metric": float("inf"),
             "epoch": 0,
         }
         # load model checkpoint, usually used for resume training
         if checkpoint_path is not None:
@@ -239,20 +267,14 @@
             self.train_epoch_func = ppsci.solver.train.train_LBFGS_epoch_func
             if self.update_freq != 1:
                 self.update_freq = 1
                 logger.warning("Set update_freq to to 1 when using L-BFGS optimizer.")
         else:
             self.train_epoch_func = ppsci.solver.train.train_epoch_func
 
-        # decorate model(s) and optimizer(s) for AMP
-        if self.use_amp:
-            self.model, self.optimizer = amp.decorate(
-                self.model, self.optimizer, self.amp_level
-            )
-
         # wrap model and optimizer to parallel object
         self.rank = dist.get_rank()
         self.world_size = dist.get_world_size()
         if self.world_size > 1:
             # TODO(sensen): support different kind of DistributedStrategy
             fleet.init(is_collective=True)
             self.model = fleet.distributed_model(self.model)
@@ -492,14 +514,15 @@
         Args:
             input_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Input data in dict.
             expr_dict (Optional[Dict[str, Callable]]): Expression dict, which guide to
                 compute equation variable with callable function. Defaults to None.
             batch_size (int, optional): Predicting by batch size. Defaults to 64.
             no_grad (bool): Whether set stop_gradient=True for entire prediction, mainly
                 for memory-efficiency. Defaults to True.
+
         Returns:
             Dict[str, paddle.Tensor]: Prediction in dict.
         """
         num_samples = len(next(iter(input_dict.values())))
         num_pad = (self.world_size - num_samples % self.world_size) % self.world_size
         # pad with last element if `num_samples` is not divisible by `world_size`
         # ensuring every device get same number of data.
```

## ppsci/solver/train.py

```diff
@@ -15,15 +15,16 @@
 import time
 from typing import TYPE_CHECKING
 
 from paddle.distributed.fleet.utils import hybrid_parallel_util as hpu
 
 from ppsci.solver import printer
 from ppsci.utils import misc
-from ppsci.utils import profiler
+
+# from ppsci.utils import profiler
 
 if TYPE_CHECKING:
     from ppsci import solver
 
 
 def train_epoch_func(solver: "solver.Solver", epoch_id: int, log_freq: int):
     """Train program for one epoch
@@ -66,18 +67,18 @@
             total_batch_size += next(iter(input_dict.values())).shape[0]
             reader_tic = time.perf_counter()
 
         with solver.no_sync_context_manager(solver.world_size > 1, solver.model):
             # forward for every constraint, including model and equation expression
             with solver.autocast_context_manager(solver.use_amp, solver.amp_level):
                 constraint_losses = solver.forward_helper.train_forward(
-                    [
+                    (
                         _constraint.output_expr
                         for _constraint in solver.constraint.values()
-                    ],
+                    ),
                     input_dicts,
                     solver.model,
                     solver.constraint,
                     label_dicts,
                     weight_dicts,
                 )
                 # accumulate all losses
@@ -167,18 +168,18 @@
                 Tensor: Computed loss.
             """
             total_loss = 0
             with solver.no_sync_context_manager(solver.world_size > 1, solver.model):
                 with solver.autocast_context_manager(solver.use_amp, solver.amp_level):
                     # forward for every constraint, including model and equation expression
                     constraint_losses = solver.forward_helper.train_forward(
-                        [
+                        (
                             _constraint.output_expr
                             for _constraint in solver.constraint.values()
-                        ],
+                        ),
                         input_dicts,
                         solver.model,
                         solver.constraint,
                         label_dicts,
                         weight_dicts,
                     )
                     # accumulate all losses
```

## ppsci/utils/config.py

```diff
@@ -121,15 +121,15 @@
                 raise ValueError(f"index({k}) out of range({dl})")
             dl[k] = str2num(v)
         else:
             override(dl[k], ks[1:], v)
     else:
         if len(ks) == 1:
             # assert ks[0] in dl, (f"{ks[0]} is not exist in {dl}")
-            if not ks[0] in dl:
+            if ks[0] not in dl:
                 print(f"A new field ({ks[0]}) detected!")
             dl[ks[0]] = str2num(v)
         else:
             if ks[0] not in dl.keys():
                 dl[ks[0]] = {}
                 print(f"A new Series field ({ks[0]}) detected!")
             override(dl[ks[0]], ks[1:], v)
```

## ppsci/utils/download.py

```diff
@@ -137,15 +137,15 @@
             continue
 
         if req.status_code != 200:
             raise RuntimeError(
                 f"Downloading from {url} failed with code " f"{req.status_code}!"
             )
 
-        # For protecting download interupted, download to
+        # For protecting download interrupted, download to
         # tmp_fullname firstly, move tmp_fullname to fullname
         # after download finished
         tmp_fullname = fullname + "_tmp"
         total_size = req.headers.get("content-length")
         with open(tmp_fullname, "wb") as f:
             if total_size:
                 with tqdm.tqdm(total=(int(total_size) + 1023) // 1024) as pbar:
@@ -183,15 +183,15 @@
 
 def _decompress(fname):
     """
     Decompress for zip and tar file
     """
     logger.info(f"Decompressing {fname}...")
 
-    # For protecting decompressing interupted,
+    # For protecting decompressing interrupted,
     # decompress to fpath_tmp directory firstly, if decompress
     # successed, move decompress files to fpath and delete
     # fpath_tmp and remove download compress file.
 
     if tarfile.is_tarfile(fname):
         uncompressed_path = _uncompress_file_tar(fname)
     elif zipfile.is_zipfile(fname):
```

## ppsci/utils/expression.py

```diff
@@ -40,15 +40,15 @@
     """
 
     def __init__(self):
         super().__init__()
 
     def forward(self, *args, **kwargs):
         raise NotImplementedError(
-            f"Use train_forward/eval_forward/visu_forward instead of forward."
+            "Use train_forward/eval_forward/visu_forward instead of forward."
         )
 
     @jit.to_static
     def train_forward(
         self,
         expr_dicts: Tuple[Dict[str, Callable], ...],
         input_dicts: Tuple[Dict[str, "paddle.Tensor"], ...],
@@ -140,14 +140,18 @@
             if name not in label_dict:
                 continue
             if callable(expr):
                 output_dict[name] = expr({**output_dict, **input_dict})
             else:
                 raise TypeError(f"expr type({type(expr)}) is invalid")
 
+        # put field 'area' into output_dict
+        if "area" in input_dict:
+            output_dict["area"] = input_dict["area"]
+
         # clear differentiation cache
         clear()
 
         # compute loss for each validator according to its' own output, label and weight
         validator_loss = validator.loss(
             output_dict,
             label_dict,
@@ -162,15 +166,15 @@
         model: nn.Layer,
     ) -> Dict[str, "paddle.Tensor"]:
         """Forward computation for visualization, including model forward and equation
         forward.
 
         Args:
             expr_dict (Optional[Dict[str, Callable]]): Expression dict.
-            input_dict (Dict[str, paddle.Tensor]]): Input dict.
+            input_dict (Dict[str, paddle.Tensor]): Input dict.
             model (nn.Layer): NN model.
 
         Returns:
             Dict[str, paddle.Tensor]: Result dict for given expression dict.
         """
         # model forward
         output_dict = model(input_dict)
```

## ppsci/utils/logger.py

```diff
@@ -8,14 +8,15 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import functools
 import logging
 import os
 import sys
 
 import paddle.distributed as dist
 
 _logger = None
@@ -94,25 +95,26 @@
     """Set log level."""
     if dist.get_rank() == 0:
         _logger.setLevel(log_level)
     else:
         _logger.setLevel(logging.ERROR)
 
 
-def log_at_trainer0(log):
+def log_at_trainer0(log_func):
     """
     Logs will print multi-times when calling Fleet API.
     Only display single log and ignore the others.
     """
 
-    def wrapper(fmt, *args):
+    @functools.wraps(log_func)
+    def wrapped_log_func(fmt, *args):
         if dist.get_rank() == 0:
-            log(fmt, *args)
+            log_func(fmt, *args)
 
-    return wrapper
+    return wrapped_log_func
 
 
 @log_at_trainer0
 def info(fmt, *args):
     _logger.info(fmt, *args)
 
 
@@ -127,25 +129,26 @@
 
 
 @log_at_trainer0
 def error(fmt, *args):
     _logger.error(fmt, *args)
 
 
-def scaler(name, value, step, writer):
+def scaler(name, value, step, vdl_writer=None, wandb_writer=None):
     """
     This function will draw a scalar curve generated by the visualdl.
     Usage: Install visualdl: pip3 install visualdl==2.0.0b4
            and then:
            visualdl --logdir ./scalar --host 0.0.0.0 --port 8830
            to preview loss corve in real time.
     """
-    if writer is None:
-        return
-    writer.add_scalar(tag=name, step=step, value=value)
+    if vdl_writer is not None:
+        vdl_writer.add_scalar(tag=name, step=step, value=value)
+    if wandb_writer is not None:
+        wandb_writer.log({"step": step, f"{name.replace('_', '/')}": value})
 
 
 def advertise():
     """
     Show the advertising message like the following:
 
     ===========================================================
```

## ppsci/utils/misc.py

```diff
@@ -199,15 +199,15 @@
     return obj.__class__.__name__
 
 
 def combine_array_with_time(x: np.ndarray, t: Tuple[int, ...]) -> np.ndarray:
     """Combine given data x with time sequence t.
     Given x with shape (N, D) and t with shape (T, ),
     this function will repeat t_i for N times and will concat it with data x for each t_i in t,
-    finally return the stacked result, whic is of shape (NxT, D+1).
+    finally return the stacked result, which is of shape (NxT, D+1).
 
     Args:
         x (np.ndarray): Points data with shape (N, D).
         t (Tuple[int, ...]): Time sequence with shape (T, ).
 
     Returns:
         np.ndarray: Combined data with shape of (NxT, D+1).
```

## ppsci/utils/save_load.py

```diff
@@ -117,24 +117,22 @@
     """Save checkpoint, including model params, optimizer params, metric information.
 
     Args:
         model (nn.Layer): Model with parameters.
         optimizer (optimizer.Optimizer): Optimizer for model.
         grad_scaler (Optional[amp.GradScaler]): GradScaler for AMP. Defaults to None.
         metric (Dict[str, float]): Metric information, such as {"RMSE": ...}.
-        model_dir (str): Directory for chekpoint storage.
+        model_dir (str): Directory for checkpoint storage.
         prefix (str, optional): Prefix for storage. Defaults to "ppsci".
         equation (Optional[Dict[str, ppsci.equation.PDE]]): Equations. Defaults to None.
     """
     if paddle.distributed.get_rank() != 0:
         return
     if model_dir is None:
-        logger.warning(
-            f"model_dir({model_dir}) is set to None, skip save_checkpoint..."
-        )
+        logger.warning("model_dir is set to None, skip save_checkpoint...")
         return
     model_dir = os.path.join(model_dir, "checkpoints")
     os.makedirs(model_dir, exist_ok=True)
     model_path = os.path.join(model_dir, prefix)
 
     paddle.save(model.state_dict(), f"{model_path}.pdparams")
     paddle.save(optimizer.state_dict(), f"{model_path}.pdopt")
```

## ppsci/validate/base.py

```diff
@@ -8,22 +8,25 @@
 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import TYPE_CHECKING
 from typing import Any
 from typing import Dict
 
 from paddle import io
 
 from ppsci import data
-from ppsci import loss
-from ppsci import metric
+
+if TYPE_CHECKING:
+    from ppsci import loss
+    from ppsci import metric
 
 
 class Validator:
     """Base class for validators.
 
     Args:
         dataset (io.Dataset): Dataset for validator.
@@ -33,16 +36,16 @@
         name (str): Name of validator.
     """
 
     def __init__(
         self,
         dataset: io.Dataset,
         dataloader_cfg: Dict[str, Any],
-        loss: loss.Loss,
-        metric: Dict[str, metric.Metric],
+        loss: "loss.Loss",
+        metric: Dict[str, "metric.Metric"],
         name: str,
     ):
         self.data_loader = data.build_dataloader(dataset, dataloader_cfg)
         self.data_iter = iter(self.data_loader)
         self.loss = loss
         self.metric = metric
         self.name = name
```

## ppsci/visualize/plot.py

```diff
@@ -161,15 +161,15 @@
     yticks: Optional[Tuple[float, ...]] = None,
 ):
     """Save plot from given 2D data.
 
     Args:
         filename (str): Filename.
         visu_data (Tuple[np.ndarray, ...]): Data that requires visualization.
-        visu_keys (Tuple[str, ...]]): Keys for visualizing data. such as ("u", "v").
+        visu_keys (Tuple[str, ...]): Keys for visualizing data. such as ("u", "v").
         num_timestamps (int, optional): Number of timestamps coord/value contains. Defaults to 1.
         stride (int, optional): The time stride of visualization. Defaults to 1.
         xticks (Optional[Tuple[float, ...]]): Tuple of xtick locations. Defaults to None.
         yticks (Optional[Tuple[float, ...]]): Tuple of ytick locations. Defaults to None.
     """
 
     plt.close("all")
@@ -310,15 +310,15 @@
     num_timestamps: int = 1,
 ):
     """Save plot from given 3D data.
 
     Args:
         filename (str): Filename.
         visu_data (Tuple[np.ndarray, ...]): Data that requires visualization.
-        visu_keys (Tuple[str, ...]]): Keys for visualizing data. such as ("u", "v").
+        visu_keys (Tuple[str, ...]): Keys for visualizing data. such as ("u", "v").
         num_timestamps (int, optional): Number of timestamps coord/value contains. Defaults to 1.
     """
 
     fig = plt.figure(figsize=(10, 10))
     len_ts = len(visu_data[0]) // num_timestamps
     for t in range(num_timestamps):
         ax = fig.add_subplot(1, num_timestamps, t + 1, projection="3d")
@@ -330,15 +330,15 @@
             cmap = plt.get_cmap(CMAPS[i % len(CMAPS)])
             _colorline3d(data[:, 0], data[:, 1], data[:, 2], cmap=cmap, ax=ax)
             cmaps.append(cmap)
         cmap_handles = [Rectangle((0, 0), 1, 1) for _ in visu_keys]
         handler_map = dict(
             zip(cmap_handles, [HandlerColormap(cm, num_stripes=8) for cm in cmaps])
         )
-        # Create custom legend with color map rectangels
+        # Create custom legend with color map rectangles
         ax.legend(
             handles=cmap_handles,
             labels=visu_keys,
             handler_map=handler_map,
             loc="upper right",
             framealpha=0.95,
         )
```

## ppsci/visualize/vtu.py

```diff
@@ -107,15 +107,15 @@
         coord_keys (Tuple[str, ...]): Tuple of coord key. such as ("x", "y").
         value_keys (Tuple[str, ...]): Tuple of value key. such as ("u", "v").
         num_timestamps (int, optional): Number of timestamp in data_dict. Defaults to 1.
     """
     if len(coord_keys) not in [2, 3, 4]:
         raise ValueError(f"ndim of coord ({len(coord_keys)}) should be 2, 3 or 4")
 
-    coord = [data_dict[k] for k in coord_keys if k != "t"]
+    coord = [data_dict[k] for k in coord_keys if k not in ("t", "sdf")]
     value = [data_dict[k] for k in value_keys] if value_keys else None
 
     coord = np.concatenate(coord, axis=1)
 
     if value is not None:
         value = np.concatenate(value, axis=1)
```

## Comparing `paddlesci-1.0.0.dist-info/LICENSE` & `paddlesci-1.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `paddlesci-1.0.0.dist-info/METADATA` & `paddlesci-1.1.0.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: paddlesci
-Version: 1.0.0
+Version: 1.1.0
 Summary: A library for scientific machine learning
 Home-page: https://github.com/PaddlePaddle/PaddleScience
 Author: PaddlePaddle
 License: Apache-2.0
 Project-URL: Homepage, https://github.com/PaddlePaddle/PaddleScience
 Project-URL: Bug Tracker, https://github.com/PaddlePaddle/PaddleScience/issues
 Project-URL: Changelog, https://github.com/PaddlePaddle/PaddleScience/releases
@@ -40,16 +40,18 @@
 Requires-Dist: tqdm
 Requires-Dist: imageio
 
 # PaddleScience
 
 > *Developed with [PaddlePaddle](https://www.paddlepaddle.org.cn/)*
 
-[![Doc](https://img.shields.io/readthedocs/paddlescience-docs/latest)](https://paddlescience-docs.readthedocs.io/zh/latest/)
+[![Version](https://img.shields.io/pypi/v/paddlesci)](https://pypi.org/project/paddlesci/)
+[![Python Version](https://img.shields.io/pypi/pyversions/paddlesci)](https://pypi.org/project/paddlesci/)
 [![License](https://img.shields.io/github/license/PaddlePaddle/PaddleScience)](./LICENSE)
+[![Doc](https://img.shields.io/readthedocs/paddlescience-docs/latest)](https://paddlescience-docs.readthedocs.io/zh/latest/)
 
 [**PaddleScience使用文档**](https://paddlescience-docs.readthedocs.io/zh/latest/)
 
 ## 简介
 
 PaddleScience 是一个基于深度学习框架 PaddlePaddle 开发的科学计算套件，利用深度神经网络的学习能力和 PaddlePaddle 框架的自动(高阶)微分机制，解决物理、化学、气象等领域的问题。支持物理机理驱动、数据驱动、数理融合三种求解方式，并提供了基础 API 和详尽文档供用户使用与二次开发。
 
@@ -68,15 +70,19 @@
 
 ## 安装使用
 
 1. 执行以下命令，从 github 上克隆 PaddleScience 项目，进入 PaddleScience 目录，并将该目录添加到系统环境变量中
 
     ``` shell
     git clone https://github.com/PaddlePaddle/PaddleScience.git
+    # 若 github clone 速度比较慢，可以使用 gitee clone
+    # git clone https://gitee.com/paddlepaddle/PaddleScience.git
+
     cd PaddleScience
+    git checkout release/1.1
     export PYTHONPATH=$PWD:$PYTHONPATH
     ```
 
 2. 安装必要的依赖包
 
     ``` shell
     pip install -r requirements.txt
@@ -110,10 +116,14 @@
 
 如使用过程中遇到问题或想提出开发建议，欢迎在 [**Issue**](https://github.com/PaddlePaddle/PaddleScience/issues/new/choose) 页面新建 issue。
 
 ## 贡献代码
 
 PaddleScience 项目欢迎并依赖开发人员和开源社区中的用户，请参阅 [**贡献指南**](https://paddlescience-docs.readthedocs.io/zh/latest/zh/contribute/)。
 
+## 致谢
+
+PaddleScience 的部分模块和案例设计受 [NVIDIA-Modulus](https://github.com/NVIDIA/modulus/tree/main)、[DeepXDE](https://github.com/lululxvi/deepxde/tree/master)、[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP/tree/develop)、[PaddleClas](https://github.com/PaddlePaddle/PaddleClas/tree/develop) 等优秀开源套件的启发。
+
 ## 证书
 
 [Apache License 2.0](https://github.com/PaddlePaddle/PaddleScience/blob/develop/LICENSE)
```

## Comparing `paddlesci-1.0.0.dist-info/RECORD` & `paddlesci-1.1.0.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,101 +1,105 @@
 ppsci/__init__.py,sha256=zJZgsHjqod46dpaPC_CmMrKdOE9yG5sUm0KBEv0VBu8,1334
-ppsci/arch/__init__.py,sha256=H5kD-uZ39jYOAh1cc1a8G2yrCEjLX5q4KuAUH6-hVHI,1623
-ppsci/arch/activation.py,sha256=2wdhlWMv0iC0r4fbG7VVuXN1SmPCmb4srPuLUI3C8po,1378
+ppsci/arch/__init__.py,sha256=DFJalWP3FpeO0ZiZcWLlMn5B6wWLXqI4omrQvJAExr4,1767
+ppsci/arch/activation.py,sha256=Xm6co6EW52ICn0yaSyoYQz202lnFyCAElC3Bhvp_9q0,2793
 ppsci/arch/afno.py,sha256=SwV7ngs2-2GADcPh2UlwwU7t3p0zi4c9PNPha7HdRjQ,23667
-ppsci/arch/base.py,sha256=FusQCn6cJQZJIhqIcnn0WzKJZhzecwrGI1eMKtG3484,4580
+ppsci/arch/base.py,sha256=qsa3OaCng_WaYADp1D3fpefGYq-kPYvUDC049z8SX9I,4569
 ppsci/arch/embedding_koopman.py,sha256=VyUwt7sZm6oCCmpB4vpDsPLPPBlit89Z5pONQyo-9aI,17540
-ppsci/arch/mlp.py,sha256=fkncdd_MiSscoiekOsGQcpWs2q-9StO8hBOkzO2hkB8,4254
+ppsci/arch/gan.py,sha256=PKQADJxy3UMMJ5gC7g0PlCogLQwFZCwVvCOGZGGadlc,14276
+ppsci/arch/mlp.py,sha256=XjYOxyPTsoz7ifg2PNCd0BttmjUhbEVP1t2UNRPBTVQ,5458
 ppsci/arch/model_list.py,sha256=wQB6xIzIoQt3sT0sNjwGJF_USqXdcqUv7P9RhCMCb9s,1853
 ppsci/arch/physx_transformer.py,sha256=mTsPlO9e3COpCPCQN5F9Bz2nLcysMyf8aBcbIS_YaFQ,12830
 ppsci/autodiff/__init__.py,sha256=aUddQd8WXMEuk9mIyUCgMIYGuG-kdIDP50ipgf3L-Xg,721
-ppsci/autodiff/ad.py,sha256=DBphlckWJklDL7iVEmKxKWWf2bL5kr41mDJhvBEBNeU,6981
+ppsci/autodiff/ad.py,sha256=fq7FpAV4OwyJ-Ct6RIjMmRFQNJmQD74T9x_Gys8S_L8,7524
 ppsci/constraint/__init__.py,sha256=-IaNgmoxjMDu0K_LKmymVaC6Azxg3ICuau8Ut93wONU,3017
-ppsci/constraint/base.py,sha256=C-Hi4tSLeYdQ9GEtoDam-BHtLOxFSCTu-0kpLZVikKU,1797
-ppsci/constraint/boundary_constraint.py,sha256=tQAUZf3GhXMojq0V0X5A_Bbzdhpe30NPDENKgFwmb5k,6561
-ppsci/constraint/initial_constraint.py,sha256=1X5NP84JcTA2GA9-BHXNv9XntGJT4dbNtHJnvkl_ymE,6546
-ppsci/constraint/integral_constraint.py,sha256=ZAouMyhr3hb7z9Kl-wtQoiUrORniqWFKm6jmQKG7AsQ,7033
-ppsci/constraint/interior_constraint.py,sha256=Dlxwpq22fLpEWLEZRPUMA-pLtMuydE9NjpOnorw31FU,6713
-ppsci/constraint/periodic_constraint.py,sha256=SK3w7QNXqTllpPOKgJw6td2vdbEDZrjlmZ7idmsBEn4,6794
-ppsci/constraint/supervised_constraint.py,sha256=5kvEfyd1wUgktx6_OX6naOM-nP2npt-cyo3QrrRHFH8,2852
-ppsci/data/__init__.py,sha256=pILH3RrV-At-m5col6-YZWvPlZ2j-TWkMSp_YH1CqD4,3509
+ppsci/constraint/base.py,sha256=n2yx_tVYOe3nqTjMZ5eyOBBE_UmCMpcRVZWyzHfvrAQ,1855
+ppsci/constraint/boundary_constraint.py,sha256=5cFI3FKAfN0VZF179FG-DCDeoBVSDB8kLMQKqyMHuAA,6619
+ppsci/constraint/initial_constraint.py,sha256=bk7UHYofFzLpEkeqh7Xavrx6pe5SvYcmUKPbZYviCK4,6864
+ppsci/constraint/integral_constraint.py,sha256=gs8WDLAY6HWWd6BMQBNUi4Vjyp4Z8REJcQkYI6RX5zg,7091
+ppsci/constraint/interior_constraint.py,sha256=RPi6x5AEbfThKukyNbfWJQq4xvnkATd563loCvvTDvw,6981
+ppsci/constraint/periodic_constraint.py,sha256=Rc3JtPpnMsyGFZ7MLSCqrSZ4Zh8M2TiewCTmjGKZwT0,7068
+ppsci/constraint/supervised_constraint.py,sha256=1cRt59pNNEzcCZPoT_DM0kFbnnjaQrQLqeBI99UBVik,2910
+ppsci/data/__init__.py,sha256=v5AS4LSVvZ7-4BC2YKqQxYaKiTPXrRB6gbRhDKx4PV4,3762
 ppsci/data/dataloader.py,sha256=cM76kmz3tkl2d2yjEZVgP0jyQnjSxaaw34ikfa6Q2AE,1567
 ppsci/data/dataset/__init__.py,sha256=gouA0KH3CEN0oHKlwBWW80hfwVGGNSXGZqpTBKEB1vM,2162
-ppsci/data/dataset/array_dataset.py,sha256=hUW2F0PQEGc-M6JrbBbJ7hxQVR7rLGJrdTWRyF5iyW8,3899
+ppsci/data/dataset/array_dataset.py,sha256=DRCeteFLygN_iAMt5KBWQIzZCgDz4JRgQyqhzwGYJxM,3962
 ppsci/data/dataset/csv_dataset.py,sha256=cwE1rvNOyx2JgsUGeC3NjeXPsm27S9T6UKCmip-E_GA,10257
-ppsci/data/dataset/era5_dataset.py,sha256=qAIv9MUdFE06IWx0Px6v4HjYsvdh5zsNHq9--Dc0brU,8589
+ppsci/data/dataset/era5_dataset.py,sha256=p_UCvh2MF_oyGzqjRMcHEH0Iyk5AMyHc2C94qoweqhk,8593
 ppsci/data/dataset/mat_dataset.py,sha256=dbvXn4j0DVv61aWuxthWTmTHhS8BYQ47JxczBfsJseM,10348
 ppsci/data/dataset/trphysx_dataset.py,sha256=nkoZVTT2-ZwSLfV5eyaeJSrmceUaTcofYNQAwRct4L8,10710
 ppsci/data/dataset/vtu_dataset.py,sha256=OcImWbQTt9hAf38rOW6mpTWHwV7lVVA68CzFnsTF4-4,3216
 ppsci/data/process/__init__.py,sha256=3zUYPMZU0GTCfNACh94ualuNMPE3YjHkbWYFY8XaXX4,751
 ppsci/data/process/batch_transform/__init__.py,sha256=5AiN_sxGoDz1K4sHrj4gAT_aXxdBjpFwGlxluXed1pw,2581
 ppsci/data/process/batch_transform/preprocess.py,sha256=3q3Z8DUeUhXxByF6SZWLVneL1Tnpg9sl_YMjye4M2Bo,607
-ppsci/data/process/transform/__init__.py,sha256=tFDmdfxj6Sq9aUzQwQ_wJbj8T70gBQDym9bz_XuSR7c,1584
-ppsci/data/process/transform/preprocess.py,sha256=8fpRqM0ELAw1d2Loz0NAEC_seqFJqnXZosjnxni8fUU,7637
-ppsci/equation/__init__.py,sha256=c2t3HReu6Xyzk2bOPjxqJFhlOXwl5k3yWakGQAoOhLM,1671
-ppsci/equation/pde/__init__.py,sha256=rFl77Qnv2NexIgSvjAwXCVJlHLCDrviIgl-oZZ8QY9Y,1088
+ppsci/data/process/transform/__init__.py,sha256=my9O-ouTVQkems6vb6w7rJakL86MXpY6t2Z_FiAtfT0,1683
+ppsci/data/process/transform/preprocess.py,sha256=St7uVtB_zM4n7xv-r20m0tM-fW-qrsq6zwl8NsDsLfM,8710
+ppsci/equation/__init__.py,sha256=-zcmOXMVX4hiFZ_EaA8udlzS7yS2176K_ZQqns8759c,1743
+ppsci/equation/pde/__init__.py,sha256=RsYxvs8Q2lOt_pmaVZNSrPXTJmuiN_EJVu5peJhCmfc,1178
 ppsci/equation/pde/base.py,sha256=LbXXoqAJfnonIdaTHmJ4sEuG5zVi79Z0NImFQmCi6mY,2720
-ppsci/equation/pde/biharmonic.py,sha256=Bb5dLSVkVuKtW78suW6BD9A81junHRKmRJ09O5dVeXg,1514
-ppsci/equation/pde/laplace.py,sha256=NzFn75px4tzmjKUoQT5BuwW15TSEeF9vMHrAFVuBXrc,1313
-ppsci/equation/pde/navier_stokes.py,sha256=Cr99oDVc0NLe8ejqI2AI3T6jJSQGn3BoLdlE1H8ahoE,3974
-ppsci/equation/pde/normal_dot_vec.py,sha256=k5zZHYEyTDJAYSyZopkLdDo5z4UqbFrMU1aawmIFMlA,1416
-ppsci/equation/pde/poisson.py,sha256=NQZV7OtjoKjZ8MGSfD0ezdRDriXFyethL2HuWAuYzVI,1261
-ppsci/equation/pde/viv.py,sha256=zpjVL1nAYHU6TYYurUlBI89285ijULd-QUkTIJpwvjw,2072
+ppsci/equation/pde/biharmonic.py,sha256=B_jUq26LJ7XUOBn7Djon4TkN38UmEiV46katI2OHg10,1566
+ppsci/equation/pde/laplace.py,sha256=o4KVjNC9xlVkwZ2YCQETK9b_y19MIQQUPUdxx_wfBDg,1354
+ppsci/equation/pde/linear_elasticity.py,sha256=XTtv33HlALLymbaTwB5bd56W2ORjAMESeY-eAg3jVo0,13372
+ppsci/equation/pde/navier_stokes.py,sha256=baOfyGRUrCZppscYObF2its7vnRSny6ZWEN8Ma0JjUE,5578
+ppsci/equation/pde/normal_dot_vec.py,sha256=EyswFNK6UVqKWcIjHRJ3VnmEmIc7ehl6bz89jpkGiE4,1468
+ppsci/equation/pde/poisson.py,sha256=DA2EtiEklIHXOs0Zgb5S1gI5pSSAEiwTD7o0_wLdCTY,1302
+ppsci/equation/pde/viv.py,sha256=7Q_gRl56drLJBBA2XU8xrgbJ_ubM3eRtRDvqRJnArcQ,2194
 ppsci/geometry/__init__.py,sha256=NwEiApFpxA_xeizhcgjoqBU7DhZGJ493P_ftcBKnIj0,2485
 ppsci/geometry/csg.py,sha256=bcZC8fIImwzRUeFkJ9wHbFowUmmtVY8SluWYaUDemUM,10346
-ppsci/geometry/geometry.py,sha256=bRLk8Gfwgkv1tTkaflhEV26Itt2sKVlIYa-waVm0ebg,7922
-ppsci/geometry/geometry_1d.py,sha256=sma_qP7TE3HySRD0EXMAK6DRgS8spxEexo8aG0T_7Fg,3479
-ppsci/geometry/geometry_2d.py,sha256=I-Hxcp_xtb1LpFbJupWeVxHkkQpN0TNsY2nF0U6Wdtc,19590
-ppsci/geometry/geometry_3d.py,sha256=BZIGzLv-ZU7zwv66-okSFpu8wYWHwhDFhpzc22wa9MQ,5360
+ppsci/geometry/geometry.py,sha256=8whx0Mhsz479DPDQU1jS3MsOPCXQ7fFw9WQVaVKnq1s,8222
+ppsci/geometry/geometry_1d.py,sha256=ROUTMYaV6Ib4uW-Djjc_vL5lUV4PRv09mnuBxBCWSvM,4424
+ppsci/geometry/geometry_2d.py,sha256=OMsJvmjrcs7OJ3VTjtNNkVH0s3OFXHQQHz6G54WQhNo,25989
+ppsci/geometry/geometry_3d.py,sha256=1UIHVWcyeZpOVAeVN4ncTcp0X2FagD-xUpdwWdm-o20,7147
 ppsci/geometry/geometry_nd.py,sha256=Sfpi_-vEqusOdrFyUxrkl1rbRd4E9j4pe3BGx2rOrbA,7432
 ppsci/geometry/inflation.py,sha256=m4YGf9F8ZlBN7NFAMm3QlxuY9A1lMebA3ih9DM_4B0U,7003
-ppsci/geometry/mesh.py,sha256=UnbRQl62kFdqb2xBNtjj_ObckExIOfqYSXx1y5xbrgw,18361
+ppsci/geometry/mesh.py,sha256=bxIDJIakxRgaZJxrzGTluJPef1jJjOM87t5AwstTsgw,22127
 ppsci/geometry/pointcloud.py,sha256=-HMMn8bRmV6m9EdAqeHlrRZPYUltO3wcud_SPMKcKsE,6506
 ppsci/geometry/sampler.py,sha256=xfTlCxMJYCj_2SyruGGngp-qFb2OHdQypmCT5HeVqbs,3312
-ppsci/geometry/timedomain.py,sha256=AqMMS3NzZ3OcWYiFi7kNhak9tvpqlhedV_l6rBRXEec,21126
-ppsci/loss/__init__.py,sha256=rMvMMXSOw0-HWbd_X-YdmdP_5DpgLbwEbFrRQbIJLk8,1464
+ppsci/geometry/timedomain.py,sha256=VVC-kvtXvyJH9cuJFbFiYTJgx-TgLxQiwt159ybANvI,21095
+ppsci/loss/__init__.py,sha256=aELUZkVIV61vlsWlBiFqH-t7huDpfoEzGvUs21yCyU4,1529
 ppsci/loss/base.py,sha256=kZJkQnVqBOSzpwfqG7E-7nuHoPQv0jeWnABGhSPfDQU,1158
-ppsci/loss/integral.py,sha256=8EtmcSqcfHEAkbYdF53XqHASF0OrBsKbpboZTfA2ySE,2656
-ppsci/loss/l1.py,sha256=Yj741WViNe2jjhSUN3KFgECV-60ssiMiRFdC5gW7cTI,4640
-ppsci/loss/l2.py,sha256=sWOvGFQHse-F4Chyo964ga9OmPj4vE3vkaB1SnyRA4c,6332
-ppsci/loss/mse.py,sha256=UDO5zpI-mQH7m0RPQUl1lgj20qOU3QZ6bepzen7o1AA,6642
-ppsci/metric/__init__.py,sha256=u5lNdmDXQcIzT7zOGnAapVTqinnkEozOrm8amltJNYI,1573
-ppsci/metric/anomaly_coef.py,sha256=UoNmvDlQ8L444mpxkD-ggILOtkB574lDmPUAyZ7yHWc,4286
+ppsci/loss/func.py,sha256=F1OCysShzG5q8fOSksBzWB92iEXNPJNSVmuXKUNGBEY,2330
+ppsci/loss/integral.py,sha256=SGKI-H0Vmop5iNktRowv9aADigO4JNSEZkycVbqRroI,2751
+ppsci/loss/l1.py,sha256=_sKpqAYchpN4AXCi0lJdQnmTOTyAkOCgDd63mW9xWT8,4425
+ppsci/loss/l2.py,sha256=5-XmT96oJAg8RGegAXx0QZwOHyiIe1CsPl4zSLyIDzs,6308
+ppsci/loss/mse.py,sha256=twi0k5OY-wkJNcKs-PuITbqkFjRnW_bDKEbfjw891MY,6630
+ppsci/metric/__init__.py,sha256=6w0fQiZrrjYa4VTgg9j1apnCdUusJT85egLY_Dve-7s,1644
+ppsci/metric/anomaly_coef.py,sha256=EvrEkwr4I7KLt510P0NTDak0Gf4TfBV-M_bW4iSA7vE,4288
 ppsci/metric/base.py,sha256=UvuNgCl-Y9ODDs4NXn-2s0KcJYmW0Gocwy3coosyVms,804
-ppsci/metric/l2_rel.py,sha256=LIMngmu34pu9awCCKySfTnUFWPep_xPkPLaRha6EbHM,1488
-ppsci/metric/mae.py,sha256=Edw2Sjc8rta3Y_PoBfZ0mxJEz8rokDTeF8muQT0IzUs,1480
-ppsci/metric/mse.py,sha256=XPevcnYI-6_PH3pWhkl45TVF81Z-DN3ni7oUbJbGSi8,1480
-ppsci/metric/rmse.py,sha256=SD8gaNqiI1RfyoQP25uL9G3hpUCDvQe2Xynwhgx-hzo,4760
-ppsci/optimizer/__init__.py,sha256=o7WZda630B3hlQrGvuNcqvVHdz5UEXQ9l7LsrEjxa2I,2565
-ppsci/optimizer/lr_scheduler.py,sha256=IMQoAcfgXkeWOaKX4m1txdVenEodvFoP3IYZ7l4Nk2k,21140
-ppsci/optimizer/optimizer.py,sha256=HkQyazJ8z69SIkBS88HemlBWrqXBRaaqg5SouUdHdiM,19501
+ppsci/metric/func.py,sha256=AZ8jl42fROcg57YNoG3Wq8TypQg1b-2WoWSsnhSg9Qw,1872
+ppsci/metric/l2_rel.py,sha256=8kohIAYTAeBkcYN3h2IJPHbt4d06RyLRShaMrm0ALF4,1836
+ppsci/metric/mae.py,sha256=rO1oRibEPG6GPRrcW5oZhzx7eKdIT0CQlFvMtf5g0Gw,1548
+ppsci/metric/mse.py,sha256=MBtGqALDcJe3D-JqkLKr1iHIUPgku0_L5x3eDoa9zjs,1548
+ppsci/metric/rmse.py,sha256=qdUHUF91qWTtRblcYbfjB0SKgSDX3v7cgNdMVHZ2maU,4830
+ppsci/optimizer/__init__.py,sha256=F3jDa00FPt1UoFGzM2LiTKWfJbaa9XoD04x7XwnTcTo,2570
+ppsci/optimizer/lr_scheduler.py,sha256=eL0s9UcdZHya0N_kcl-F_daQ3fVFmOBsugHOzALdB14,25047
+ppsci/optimizer/optimizer.py,sha256=d54GBIrU52MZWKAuSO_5sgOBh4OoHeRfKAfl9YBgwF4,20161
 ppsci/solver/__init__.py,sha256=Z22E8gn1K2OPOc1k7X2bNVEPeSiSzKN3-cc3DpsEwME,804
-ppsci/solver/eval.py,sha256=kryf8r11U-IuTZIY6UNN1QGagW3fP5j9TmZ3tsDdZCU,10759
-ppsci/solver/printer.py,sha256=9gQ2PrDwmqfyW4e77kZcbPBEC4wJg9ruzsS3YdG6D_0,3861
-ppsci/solver/solver.py,sha256=oR_ffmcbm-lHYPsEGKI8a3lWgLYmmECyS5Ncey02lYY,26660
-ppsci/solver/train.py,sha256=yfYcaCznB5FEkEPJiyYBdL1qKUd_oQzVLaYpFdOtdvk,8817
+ppsci/solver/eval.py,sha256=KPijpIQBn_zPOZ66mUqbfaH_gvOjWJRfAqNe-TY-dbE,10762
+ppsci/solver/printer.py,sha256=borCtzSLFQ6pK4len_mwBtHF6eTh7J1IVuwQgtXDam4,4013
+ppsci/solver/solver.py,sha256=sLJhotDX1MYo3gxYISMdDjF-ArRJhRWVHGYtIOlDiDc,27567
+ppsci/solver/train.py,sha256=oXFuu20iGlYteCunniAtK9UoOJeuOsj0AGP3LBVNVKM,8820
 ppsci/solver/visu.py,sha256=KWJUWKO57RUhcVa9z3qFe8IVa8tTVCub50J7xinPKhI,3316
 ppsci/utils/__init__.py,sha256=yWg2C8KM_-1eqc2y2yUg1ee2y55ckqhehKbutZYijp0,1720
 ppsci/utils/checker.py,sha256=B7U_98NsSSBdr8Mk88AQmD9tkzFPbzZxu5dQEZ2ldr8,4823
-ppsci/utils/config.py,sha256=84nG7r5fQV9XpZvZB8hYuNB4jp3GJlGvAHivvc_iWKU,6365
-ppsci/utils/download.py,sha256=5c_QyA3mqb9l9YBzE5jeY23kv1UFJNsvDvrncw_m6Sg,8944
-ppsci/utils/expression.py,sha256=Bk1tAMkyBJJaLKP0GaycCMlT33TOk3CIBHb64BSzGt8,6659
+ppsci/utils/config.py,sha256=HwmI1CQqrAedpukG2tEr8chgi7Df0vJulRbsCnrr2J4,6365
+ppsci/utils/download.py,sha256=SZPHsGhttn3z47nTyRBn0NUtL21viqzNla44Qu74m7U,8946
+ppsci/utils/expression.py,sha256=5-BuZmgNqVpbaD6eKQkUTJcDfXhXDQgMonnI4us_Ww0,6788
 ppsci/utils/initializer.py,sha256=tCJXRY1U7YQcAhSx9ZJuL_mnjJBTtBWgT-WzYGO3ZU0,14420
-ppsci/utils/logger.py,sha256=kCP0kadn53ozK3JpUDmOtdNN9jUYx8ph1MvNgA1W6e0,5906
-ppsci/utils/misc.py,sha256=tutdSiXUGtJHaikvYhuliCWHclw-lq1LngwT10yIcPk,7720
+ppsci/utils/logger.py,sha256=wIAXsQTmafq8xB_2fgaZq2WKN9uQXRLRcyz5wTtPvf0,6124
+ppsci/utils/misc.py,sha256=Tk-ef3u1kla8t9cEVkljDg_yePosBt5eSjGh3WFuJhk,7721
 ppsci/utils/profiler.py,sha256=RLHvCSeAZ2BgSK0kUw8R48bWzNjJax-uZHhXhf_Rbx8,4517
 ppsci/utils/reader.py,sha256=xd6EWF1mP1H6NPm7gEBWp8-hdaT5anqD8jNOb9doNuM,5830
-ppsci/utils/save_load.py,sha256=5r915DFIZJEI2IusWln2jhK2xF2ihTvwq0RCvL_ICgw,5655
+ppsci/utils/save_load.py,sha256=p5yrGiV6wuGrh035epccqTVZ0WjotyNoSsLg1QF3cog,5620
 ppsci/validate/__init__.py,sha256=r-j0CZfnTujI2Q0y4AAm05hGpLxDTJVlCiklDV89eOs,2777
-ppsci/validate/base.py,sha256=JTMif6HPom4Dc9giI9qUkQ31O26mS6NBg3aXC0RKDSY,2017
+ppsci/validate/base.py,sha256=rRArg6q1p-sZ-d-SLf5PUdl78WhwARNmhWNrP1a7olI,2081
 ppsci/validate/geo_validator.py,sha256=r3mDA4kUWvERmV77bzZiJ8j9pAB3dmvEAE61QKuMKQ8,6788
 ppsci/validate/sup_validator.py,sha256=eYMtKlZjNspGhSCJr-ZcFi0VR49Ul1rhbKl0NhTFCq0,3501
 ppsci/visualize/__init__.py,sha256=GCU21cNrt8sgE9MgJlyiQdLMtLShkiqhsVPTanVAx0I,2618
 ppsci/visualize/base.py,sha256=gERkDuPLzFyLynLsSCQ2o1hGfXe_Itn8Pj9sZFC1bs0,2074
-ppsci/visualize/plot.py,sha256=7_QbanFJaXoA5d3tJ9z2ZFBLZYZIH56h7u0qy-WM2PA,17667
+ppsci/visualize/plot.py,sha256=zc_rsrGOvSFOFTz3sHwt2nOSNilXbyWWxfFDd704VuA,17665
 ppsci/visualize/visualizer.py,sha256=zmwShKAOXGu-TOhXw-j-sW5hvu4p72Xg6LRu-6vezTc,14863
-ppsci/visualize/vtu.py,sha256=rZSNloltqt5ok3ordupmhPtSBMB3s3pzHJIAi1Vgwsc,5569
-paddlesci-1.0.0.dist-info/LICENSE,sha256=pe9UXIno4EQ2ng4ngy9UGFL2C87Qp76YRORhIBfkuKg,11438
-paddlesci-1.0.0.dist-info/METADATA,sha256=KS58o_WRGqfS_TB97zgOqGTuHRyu8qwM-F8wa8IwsoQ,4505
-paddlesci-1.0.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-paddlesci-1.0.0.dist-info/top_level.txt,sha256=z4SM49gDNE7M_VPs-qPKa7iTfSr_ExkFNPXphTGDqxY,6
-paddlesci-1.0.0.dist-info/RECORD,,
+ppsci/visualize/vtu.py,sha256=HlhnPvZRVYxy-XfX1KhYPfAu2XfuZG2EJkQG3sF7I8k,5582
+paddlesci-1.1.0.dist-info/LICENSE,sha256=pe9UXIno4EQ2ng4ngy9UGFL2C87Qp76YRORhIBfkuKg,11438
+paddlesci-1.1.0.dist-info/METADATA,sha256=uIpb65o9JaREmPwReBtA-XXl90S-VbOVhmZAit66C4Q,5223
+paddlesci-1.1.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+paddlesci-1.1.0.dist-info/top_level.txt,sha256=XzEDAxVW_Yk4Ut8ZvOQbKzgiFhUqBhtszGOQnAYytSs,13
+paddlesci-1.1.0.dist-info/RECORD,,
```

