# Comparing `tmp/pywr-1.8.0.tar.gz` & `tmp/pywr-1.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist\pywr-1.8.0.tar", last modified: Fri Jul 17 00:40:47 2020, max compression
+gzip compressed data, was "dist/pywr-1.9.0.tar", last modified: Tue Sep 15 21:27:30 2020, max compression
```

## Comparing `pywr-1.8.0.tar` & `pywr-1.9.0.tar`

### file list

```diff
@@ -1,271 +1,279 @@
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.979120 pywr-1.8.0/
--rw-rw-rw-   0        0        0       87 2020-02-21 22:35:05.000000 pywr-1.8.0/.coveragerc
--rw-rw-rw-   0        0        0     1026 2020-02-21 22:35:05.000000 pywr-1.8.0/.gitignore
--rw-rw-rw-   0        0        0     1639 2020-06-20 21:08:56.000000 pywr-1.8.0/.travis.yml
--rw-rw-rw-   0        0        0    12557 2020-07-17 00:36:16.000000 pywr-1.8.0/CHANGELOG.md
--rw-rw-rw-   0        0        0    35821 2020-02-21 22:35:05.000000 pywr-1.8.0/LICENSE.txt
--rw-rw-rw-   0        0        0     5320 2020-07-17 00:40:47.979120 pywr-1.8.0/PKG-INFO
--rw-rw-rw-   0        0        0     4264 2020-03-15 18:59:15.000000 pywr-1.8.0/README.rst
--rw-rw-rw-   0        0        0     3933 2020-06-20 21:08:56.000000 pywr-1.8.0/appveyor.yml
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.194444 pywr-1.8.0/docs/
--rw-rw-rw-   0        0        0     7605 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/Makefile
--rwxrwxrwx   0        0        0     7249 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/make.bat
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.206880 pywr-1.8.0/docs/presentations/
--rw-rw-rw-   0        0        0   567766 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/presentations/Pywr introduction - September 2018.pdf
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.245295 pywr-1.8.0/docs/source/
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.251300 pywr-1.8.0/docs/source/_static/
--rw-rw-rw-   0        0        0        0 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/_static/.gitignore
--rw-rw-rw-   0        0        0     5843 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/_static/pywr_d3.png
--rw-rw-rw-   0        0        0       55 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/_static/style.css
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.251300 pywr-1.8.0/docs/source/_templates/
--rw-rw-rw-   0        0        0        0 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/_templates/.gitignore
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.279326 pywr-1.8.0/docs/source/api/
--rw-rw-rw-   0        0        0      193 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.core.rst
--rw-rw-rw-   0        0        0      582 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.nodes.rst
--rw-rw-rw-   0        0        0      313 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/api/pywr.notebook.rst
--rw-rw-rw-   0        0        0      548 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.optimisation.rst
--rw-rw-rw-   0        0        0     2795 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.parameters.rst
--rw-rw-rw-   0        0        0     2206 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.recorders.rst
--rw-rw-rw-   0        0        0      224 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.rst
--rw-rw-rw-   0        0        0      527 2020-07-17 00:36:16.000000 pywr-1.8.0/docs/source/api/pywr.solvers.rst
--rw-rw-rw-   0        0        0    10094 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/conf.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.306042 pywr-1.8.0/docs/source/cookbook/
--rw-rw-rw-   0        0        0     3487 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/aggregated_node.rst
--rw-rw-rw-   0        0        0     4946 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/aggregated_parameter.rst
--rw-rw-rw-   0        0        0     2147 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/control_curves.rst
--rw-rw-rw-   0        0        0      298 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/cookbook.rst
--rw-rw-rw-   0        0        0     7093 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/dataframes.rst
--rw-rw-rw-   0        0        0     5510 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/cookbook/demand_saving.rst
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.322797 pywr-1.8.0/docs/source/extending_pywr/
--rw-rw-rw-   0        0        0     6010 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/extending_pywr/extending_pywr_nodes.rst
--rw-rw-rw-   0        0        0    10934 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/extending_pywr/extending_pywr_parameters.rst
--rw-rw-rw-   0        0        0      116 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/extending_pywr/extending_pywr_recorders.rst
--rw-rw-rw-   0        0        0     1034 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/extending_pywr/index.rst
--rw-rw-rw-   0        0        0      430 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/formulation.rst
--rw-rw-rw-   0        0        0      451 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/index.rst
--rw-rw-rw-   0        0        0     6965 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/install.rst
--rw-rw-rw-   0        0        0     9094 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/json.rst
--rw-rw-rw-   0        0        0     1838 2020-03-15 18:59:15.000000 pywr-1.8.0/docs/source/license.rst
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.330309 pywr-1.8.0/docs/source/pyplots/
--rw-rw-rw-   0        0        0     1113 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/pyplots/demand_saving_levels.py
--rw-rw-rw-   0        0        0      487 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/pyplots/top_hat.py
--rw-rw-rw-   0        0        0      704 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/tutorial.json
--rw-rw-rw-   0        0        0     2899 2020-02-21 22:35:05.000000 pywr-1.8.0/docs/source/tutorial.rst
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.342319 pywr-1.8.0/examples/
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.348358 pywr-1.8.0/examples/data/
--rw-rw-rw-   0        0        0  1172434 2020-02-21 22:35:05.000000 pywr-1.8.0/examples/data/thames_stochastic_flow.gz
--rw-rw-rw-   0        0        0     3410 2020-02-21 22:35:05.000000 pywr-1.8.0/examples/hydropower_example.json
--rw-rw-rw-   0        0        0      466 2020-02-21 22:35:05.000000 pywr-1.8.0/examples/hydropower_example.py
--rw-rw-rw-   0        0        0      959 2020-02-21 22:35:05.000000 pywr-1.8.0/examples/simple_model.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.358534 pywr-1.8.0/examples/thames/
--rw-rw-rw-   0        0        0     4200 2020-06-20 21:08:56.000000 pywr-1.8.0/examples/thames/thames.json
--rw-rw-rw-   0        0        0     9116 2020-06-20 21:08:56.000000 pywr-1.8.0/examples/thames/thames.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.371518 pywr-1.8.0/examples/two_reservoir/
--rw-rw-rw-   0        0        0     4022 2020-06-20 21:08:56.000000 pywr-1.8.0/examples/two_reservoir/two_reservoir.json
--rw-rw-rw-   0        0        0     4843 2020-06-20 21:08:56.000000 pywr-1.8.0/examples/two_reservoir/two_reservoir_moea.py
--rw-rw-rw-   0        0        0      690 2020-06-20 21:08:56.000000 pywr-1.8.0/examples/two_reservoir/two_reservoir_plot.py
--rw-rw-rw-   0        0        0     4408 2020-02-21 22:35:05.000000 pywr-1.8.0/github_deploy_key.enc
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.389535 pywr-1.8.0/pywr/
--rw-rw-rw-   0        0        0      667 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/__init__.py
--rw-rw-rw-   0        0        0      302 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/_component.pxd
--rw-rw-rw-   0        0        0     3151 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/_component.pyx
--rw-rw-rw-   0        0        0     3949 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/_core.pxd
--rw-rw-rw-   0        0        0    41231 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/_core.pyx
--rw-rw-rw-   0        0        0    36897 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/_model.pyx
--rw-rw-rw-   0        0        0      293 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/core.py
--rw-rw-rw-   0        0        0    10271 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/dataframe_tools.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.404481 pywr-1.8.0/pywr/domains/
--rw-rw-rw-   0        0        0      203 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/domains/__init__.py
--rw-rw-rw-   0        0        0     4799 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/domains/groundwater.py
--rw-rw-rw-   0        0        0     9755 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/domains/river.py
--rw-rw-rw-   0        0        0     1989 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/h5tools.py
--rw-rw-rw-   0        0        0      941 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/hashes.py
--rw-rw-rw-   0        0        0       21 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/model.py
--rw-rw-rw-   0        0        0    36184 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/nodes.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.421238 pywr-1.8.0/pywr/notebook/
--rw-rw-rw-   0        0        0    12366 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/notebook/__init__.py
--rw-rw-rw-   0        0        0     6740 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/notebook/draw_graph.js
--rw-rw-rw-   0        0        0     3607 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/notebook/figures.py
--rw-rw-rw-   0        0        0      609 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/notebook/graph.css
--rw-rw-rw-   0        0        0     2378 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/notebook/sankey.py
--rw-rw-rw-   0        0        0     2547 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/notebook/save_graph.js
--rw-rw-rw-   0        0        0      565 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/notebook/template.html
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.429143 pywr-1.8.0/pywr/optimisation/
--rw-rw-rw-   0        0        0     3254 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/optimisation/__init__.py
--rw-rw-rw-   0        0        0     8922 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/optimisation/platypus.py
--rw-rw-rw-   0        0        0     3380 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/optimisation/pygmo.py
--rw-rw-rw-   0        0        0      762 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameter_property.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.444954 pywr-1.8.0/pywr/parameters/
--rw-rw-rw-   0        0        0       25 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/__init__.py
--rw-rw-rw-   0        0        0     1468 2020-03-15 18:59:15.000000 pywr-1.8.0/pywr/parameters/_control_curves.pxd
--rw-rw-rw-   0        0        0    29024 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/parameters/_control_curves.pyx
--rw-rw-rw-   0        0        0      574 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_hydropower.pxd
--rw-rw-rw-   0        0        0     9368 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_hydropower.pyx
--rw-rw-rw-   0        0        0     6998 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/parameters/_parameters.pxd
--rw-rw-rw-   0        0        0    86785 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/parameters/_parameters.pyx
--rw-rw-rw-   0        0        0      811 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_polynomial.pxd
--rw-rw-rw-   0        0        0     7876 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_polynomial.pyx
--rw-rw-rw-   0        0        0     1216 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_thresholds.pxd
--rw-rw-rw-   0        0        0    11123 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/_thresholds.pyx
--rw-rw-rw-   0        0        0      340 2020-03-15 18:59:15.000000 pywr-1.8.0/pywr/parameters/control_curves.py
--rw-rw-rw-   0        0        0     2275 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/groundwater.py
--rw-rw-rw-   0        0        0     7450 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/parameters/licenses.py
--rw-rw-rw-   0        0        0    13134 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/parameters/parameters.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.465974 pywr-1.8.0/pywr/recorders/
--rw-rw-rw-   0        0        0       26 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/recorders/__init__.py
--rw-rw-rw-   0        0        0      805 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/_hydropower.pxd
--rw-rw-rw-   0        0        0    11799 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/_hydropower.pyx
--rw-rw-rw-   0        0        0     5606 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/recorders/_recorders.pxd
--rw-rw-rw-   0        0        0    86489 2020-07-17 00:36:16.000000 pywr-1.8.0/pywr/recorders/_recorders.pyx
--rw-rw-rw-   0        0        0      518 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/_thresholds.pxd
--rw-rw-rw-   0        0        0     4055 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/recorders/_thresholds.pyx
--rw-rw-rw-   0        0        0     3200 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/calibration.py
--rw-rw-rw-   0        0        0    13281 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/events.py
--rw-rw-rw-   0        0        0     3286 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/recorders/progress.py
--rw-rw-rw-   0        0        0    22286 2020-06-20 21:08:56.000000 pywr-1.8.0/pywr/recorders/recorders.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.469979 pywr-1.8.0/pywr/solvers/
--rw-rw-rw-   0        0        0     5728 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/solvers/__init__.py
--rw-rw-rw-   0        0        0    57227 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/solvers/cython_glpk.pyx
--rw-rw-rw-   0        0        0    23101 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/solvers/cython_lpsolve.pyx
--rw-rw-rw-   0        0        0     3284 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/solvers/libglpk.pxd
--rw-rw-rw-   0        0        0     5174 2020-02-21 22:35:05.000000 pywr-1.8.0/pywr/timestepper.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.398474 pywr-1.8.0/pywr.egg-info/
--rw-rw-rw-   0        0        0     5320 2020-07-17 00:40:46.000000 pywr-1.8.0/pywr.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     7457 2020-07-17 00:40:47.000000 pywr-1.8.0/pywr.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2020-07-17 00:40:46.000000 pywr-1.8.0/pywr.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      208 2020-07-17 00:40:46.000000 pywr-1.8.0/pywr.egg-info/requires.txt
--rw-rw-rw-   0        0        0        5 2020-07-17 00:40:46.000000 pywr-1.8.0/pywr.egg-info/top_level.txt
--rw-rw-rw-   0        0        0      157 2020-07-17 00:40:47.980120 pywr-1.8.0/setup.cfg
--rw-rw-rw-   0        0        0     6554 2020-06-20 21:08:56.000000 pywr-1.8.0/setup.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.619875 pywr-1.8.0/tests/
--rw-rw-rw-   0        0        0      222 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/conftest.py
--rw-rw-rw-   0        0        0     2462 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/fixtures.py
--rw-rw-rw-   0        0        0     1164 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/helpers.py
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.928915 pywr-1.8.0/tests/models/
--rw-rw-rw-   0        0        0      101 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/.gitattributes
--rw-rw-rw-   0        0        0     2262 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/agg_recorder_nesting.json
--rw-rw-rw-   0        0        0      937 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/aggregated1.json
--rw-rw-rw-   0        0        0      901 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/annual_license.json
--rw-rw-rw-   0        0        0     1055 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/bottleneck.json
--rw-rw-rw-   0        0        0      600 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/breaklink.json
--rw-rw-rw-   0        0        0     3885 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/control_curve.csv
--rw-rw-rw-   0        0        0      801 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/cost1.json
--rw-rw-rw-   0        0        0      793 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/csv_recorder.json
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.934921 pywr-1.8.0/tests/models/data/
--rw-rw-rw-   0        0        0  1172434 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/data/thames_stochastic_flow.gz
--rw-rw-rw-   0        0        0     1470 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/deficit.json
--rw-rw-rw-   0        0        0     2870 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/demand_saving2.json
--rw-rw-rw-   0        0        0     3424 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/models/demand_saving2_with_variables.json
--rw-rw-rw-   0        0        0     2341 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/demand_saving_hdf.json
--rw-rw-rw-   0        0        0    10120 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/demand_saving_level.h5
--rw-rw-rw-   0        0        0     4383 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/demand_saving_with_tables_recorder.json
--rw-rw-rw-   0        0        0      783 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/models/discount.json
--rw-rw-rw-   0        0        0      753 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/extra1.json
--rw-rw-rw-   0        0        0      340 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/extra2.json
--rw-rw-rw-   0        0        0     1028 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/flow_interpolation.json
--rw-rw-rw-   0        0        0     1478 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/flow_parameter.json
--rw-rw-rw-   0        0        0     3203 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/hydropower_example.json
--rw-rw-rw-   0        0        0     3044 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/hydropower_target_example.json
--rw-rw-rw-   0        0        0       51 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/initial_volumes.csv
--rw-rw-rw-   0        0        0       34 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/invalid.json
--rw-rw-rw-   0        0        0      714 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/invalid_include.json
--rw-rw-rw-   0        0        0     1667 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/mean_flow_recorder.json
--rw-rw-rw-   0        0        0      158 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/monthly_profiles.csv
--rw-rw-rw-   0        0        0      100 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/multiindex_data.csv
--rw-rw-rw-   0        0        0     1180 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/multiindex_df.json
--rw-rw-rw-   0        0        0      124 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/my_parameter.py
--rw-rw-rw-   0        0        0     1202 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/parameter_recorder.json
--rw-rw-rw-   0        0        0     1250 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/parameter_reference.json
--rw-rw-rw-   0        0        0      798 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/piecewise1.json
--rw-rw-rw-   0        0        0     1158 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/piecewise1_with_parameters.json
--rw-rw-rw-   0        0        0      891 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/python_include.json
--rw-rw-rw-   0        0        0      775 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir1.json
--rw-rw-rw-   0        0        0      779 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir1_pc.json
--rw-rw-rw-   0        0        0     1451 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/models/reservoir2.json
--rw-rw-rw-   0        0        0     1654 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_evaporation.json
--rw-rw-rw-   0        0        0     1615 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_evaporation_without_area_property.json
--rw-rw-rw-   0        0        0     1439 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_initial_vol_from_table.json
--rw-rw-rw-   0        0        0     1118 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_with_cc.json
--rw-rw-rw-   0        0        0     1300 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_with_cc_param_values.json
--rw-rw-rw-   0        0        0     1340 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/reservoir_with_circular_cc.json
--rw-rw-rw-   0        0        0     1080 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river1.json
--rw-rw-rw-   0        0        0     1760 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river2.json
--rw-rw-rw-   0        0        0     1262 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river_discharge1.json
--rw-rw-rw-   0        0        0     1262 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river_discharge2.json
--rw-rw-rw-   0        0        0     1149 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river_mrf1.json
--rw-rw-rw-   0        0        0     1212 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/river_split_with_gauge1.json
--rw-rw-rw-   0        0        0     1540 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/scenario_monthly_profile.json
--rw-rw-rw-   0        0        0      626 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/scenario_with_slices.json
--rw-rw-rw-   0        0        0      716 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/scenario_with_user_combinations.json
--rw-rw-rw-   0        0        0      703 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple1.json
--rw-rw-rw-   0        0        0      718 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple1_monthly.json
--rw-rw-rw-   0        0        0       64 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple_data.csv
--rw-rw-rw-   0        0        0     1112 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple_df.json
--rw-rw-rw-   0        0        0     1227 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple_df_shared.json
--rw-rw-rw-   0        0        0     1435 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple_with_scenario.json
--rw-rw-rw-   0        0        0     1993 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/simple_with_scenario_wrapper.json
--rw-rw-rw-   0        0        0    10198 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/test_data1.xlsx
--rw-rw-rw-   0        0        0      510 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1.csv
--rw-rw-rw-   0        0        0     1202 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1.json
--rw-rw-rw-   0        0        0      106 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1_weekly.csv
--rw-rw-rw-   0        0        0     7344 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1_weekly.h5
--rw-rw-rw-   0        0        0     1215 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1_weekly.json
--rw-rw-rw-   0        0        0     1208 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1_weekly_hdf.json
--rw-rw-rw-   0        0        0     1174 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries1_xlsx.json
--rw-rw-rw-   0        0        0     2231 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2.csv
--rw-rw-rw-   0        0        0     9672 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2.h5
--rw-rw-rw-   0        0        0     1567 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2.json
--rw-rw-rw-   0        0        0     1447 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2_hdf.json
--rw-rw-rw-   0        0        0     1438 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2_hdf_wrong_hash.json
--rw-rw-rw-   0        0        0     3174 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries2_with_fdc.json
--rw-rw-rw-   0        0        0     8740 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries3.csv
--rw-rw-rw-   0        0        0     1442 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries3.json
--rw-rw-rw-   0        0        0     1548 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/timeseries4.json
--rw-rw-rw-   0        0        0     3721 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/two_reservoir.json
--rw-rw-rw-   0        0        0     3920 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/models/two_reservoir_constrained.json
--rw-rw-rw-   0        0        0      595 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/version1.json
--rw-rw-rw-   0        0        0     1284 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/virtual_storage1.json
--rw-rw-rw-   0        0        0     1879 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/models/virtual_storage2.json
--rw-rw-rw-   0        0        0     1036 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/notebook.ipynb
--rw-rw-rw-   0        0        0       77 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/pytest.ini
--rw-rw-rw-   0        0        0     8908 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_agg_constraints.py
--rw-rw-rw-   0        0        0     7044 2020-03-15 18:59:15.000000 pywr-1.8.0/tests/test_aggregated_nodes.py
--rw-rw-rw-   0        0        0     3083 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_aggregator.py
--rw-rw-rw-   0        0        0    12524 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_analytical.py
--rw-rw-rw-   0        0        0     3271 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_components.py
--rw-rw-rw-   0        0        0    19837 2020-03-15 18:59:15.000000 pywr-1.8.0/tests/test_control_curves.py
--rw-rw-rw-   0        0        0    18507 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/test_core.py
--rw-rw-rw-   0        0        0     3643 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/test_delay.py
--rw-rw-rw-   0        0        0     9452 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_df_resampling.py
--rw-rw-rw-   0        0        0     3350 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_groundwater.py
--rw-rw-rw-   0        0        0     1058 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_hashes.py
--rw-rw-rw-   0        0        0     7880 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_license.py
--rw-rw-rw-   0        0        0     1172 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_named_iterator.py
--rw-rw-rw-   0        0        0     1181 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_node_iterator.py
--rw-rw-rw-   0        0        0     3238 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_notebook.py
--rw-rw-rw-   0        0        0     1292 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_optimisation.py
--rw-rw-rw-   0        0        0    64683 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/test_parameters.py
--rw-rw-rw-   0        0        0     2268 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_piecewise.py
--rw-rw-rw-   0        0        0     4700 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_platypus.py
--rw-rw-rw-   0        0        0     1560 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_pygmo.py
--rw-rw-rw-   0        0        0    76227 2020-07-17 00:36:16.000000 pywr-1.8.0/tests/test_recorders.py
--rw-rw-rw-   0        0        0     6992 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_river.py
--rw-rw-rw-   0        0        0    20968 2020-06-20 21:08:56.000000 pywr-1.8.0/tests/test_run.py
--rw-rw-rw-   0        0        0    13841 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/test_scenarios.py
--rw-rw-rw-   0        0        0    34537 2020-02-21 22:35:05.000000 pywr-1.8.0/tests/timeseries1.xlsx
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.940926 pywr-1.8.0/travis/
--rw-rw-rw-   0        0        0     2282 2020-06-20 21:08:56.000000 pywr-1.8.0/travis/build-wheels.sh
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.963105 pywr-1.8.0/travis/manylinux1_x86_64-glpk/
--rw-rw-rw-   0        0        0      236 2020-02-21 22:35:05.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/Dockerfile
--rw-rw-rw-   0        0        0      182 2020-02-21 22:35:05.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/Makefile
--rw-rw-rw-   0        0        0      514 2020-02-21 22:35:05.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/build_glpk.sh
--rw-rw-rw-   0        0        0      526 2020-02-21 22:35:05.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/build_lpsolve.sh
--rw-rw-rw-   0        0        0     2630 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/fix-lpsolve-compilation.patch
--rw-rw-rw-   0        0        0      210 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux1_x86_64-glpk/lpsolveConfig.cmake.in
-drwxrwxrwx   0        0        0        0 2020-07-17 00:40:47.978119 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/
--rw-rw-rw-   0        0        0      239 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/Dockerfile
--rw-rw-rw-   0        0        0      185 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/Makefile
--rw-rw-rw-   0        0        0      514 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/build_glpk.sh
--rw-rw-rw-   0        0        0      677 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/build_lpsolve.sh
--rw-rw-rw-   0        0        0     2630 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/fix-lpsolve-compilation.patch
--rw-rw-rw-   0        0        0      210 2020-02-21 22:35:06.000000 pywr-1.8.0/travis/manylinux2010_x86_64-glpk/lpsolveConfig.cmake.in
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.649613 pywr-1.9.0/
+-rw-rw-r--   0 james     (1000) james     (1000)       78 2020-09-15 21:11:56.000000 pywr-1.9.0/.coveragerc
+-rw-rw-r--   0 james     (1000) james     (1000)      958 2020-09-15 21:11:56.000000 pywr-1.9.0/.gitignore
+-rw-rw-r--   0 james     (1000) james     (1000)     1582 2020-09-15 21:11:56.000000 pywr-1.9.0/.travis.yml
+-rw-rw-r--   0 james     (1000) james     (1000)    13243 2020-09-15 21:11:56.000000 pywr-1.9.0/CHANGELOG.md
+-rw-rw-r--   0 james     (1000) james     (1000)    35147 2020-09-15 21:11:56.000000 pywr-1.9.0/LICENSE.txt
+-rw-rw-r--   0 james     (1000) james     (1000)     5221 2020-09-15 21:27:30.649613 pywr-1.9.0/PKG-INFO
+-rw-rw-r--   0 james     (1000) james     (1000)     4195 2020-09-15 21:11:56.000000 pywr-1.9.0/README.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     3806 2020-09-15 21:11:56.000000 pywr-1.9.0/appveyor.yml
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.633613 pywr-1.9.0/docs/
+-rw-rw-r--   0 james     (1000) james     (1000)     7413 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/Makefile
+-rw-rw-r--   0 james     (1000) james     (1000)     7249 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/make.bat
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.633613 pywr-1.9.0/docs/presentations/
+-rw-rw-r--   0 james     (1000) james     (1000)   567766 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/presentations/Pywr introduction - September 2018.pdf
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.634613 pywr-1.9.0/docs/source/
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.634613 pywr-1.9.0/docs/source/_static/
+-rw-rw-r--   0 james     (1000) james     (1000)        0 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/_static/.gitignore
+-rw-rw-r--   0 james     (1000) james     (1000)     5843 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/_static/pywr_d3.png
+-rw-rw-r--   0 james     (1000) james     (1000)       53 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/_static/style.css
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.634613 pywr-1.9.0/docs/source/_templates/
+-rw-rw-r--   0 james     (1000) james     (1000)        0 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/_templates/.gitignore
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.634613 pywr-1.9.0/docs/source/api/
+-rw-rw-r--   0 james     (1000) james     (1000)      179 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.core.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      586 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.nodes.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      301 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.notebook.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      517 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.optimisation.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     2645 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.parameters.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     2099 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.recorders.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      224 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      497 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.solvers.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      188 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/api/pywr.utils.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     9783 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/conf.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.635613 pywr-1.9.0/docs/source/cookbook/
+-rw-rw-r--   0 james     (1000) james     (1000)     3398 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/aggregated_node.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     4823 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/aggregated_parameter.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     2102 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/control_curves.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      287 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/cookbook.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     6925 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/dataframes.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     5389 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/cookbook/demand_saving.rst
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.635613 pywr-1.9.0/docs/source/extending_pywr/
+-rw-rw-r--   0 james     (1000) james     (1000)     5885 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/extending_pywr/extending_pywr_nodes.rst
+-rw-rw-r--   0 james     (1000) james     (1000)    10699 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/extending_pywr/extending_pywr_parameters.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      110 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/extending_pywr/extending_pywr_recorders.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     1012 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/extending_pywr/index.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      418 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/formulation.rst
+-rw-rw-r--   0 james     (1000) james     (1000)      429 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/index.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     6782 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/install.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     8830 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/json.rst
+-rw-rw-r--   0 james     (1000) james     (1000)     1807 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/license.rst
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.635613 pywr-1.9.0/docs/source/pyplots/
+-rw-rw-r--   0 james     (1000) james     (1000)     1080 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/pyplots/demand_saving_levels.py
+-rw-rw-r--   0 james     (1000) james     (1000)      466 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/pyplots/top_hat.py
+-rw-rw-r--   0 james     (1000) james     (1000)      669 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/tutorial.json
+-rw-rw-r--   0 james     (1000) james     (1000)     2805 2020-09-15 21:11:56.000000 pywr-1.9.0/docs/source/tutorial.rst
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.635613 pywr-1.9.0/examples/
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.635613 pywr-1.9.0/examples/data/
+-rw-rw-r--   0 james     (1000) james     (1000)  1172434 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/data/thames_stochastic_flow.gz
+-rw-rw-r--   0 james     (1000) james     (1000)     3290 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/hydropower_example.json
+-rw-rw-r--   0 james     (1000) james     (1000)      445 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/hydropower_example.py
+-rwxrwxr-x   0 james     (1000) james     (1000)      920 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/simple_model.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.636613 pywr-1.9.0/examples/thames/
+-rw-rw-r--   0 james     (1000) james     (1000)     4055 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/thames/thames.json
+-rw-rw-r--   0 james     (1000) james     (1000)     8806 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/thames/thames.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.636613 pywr-1.9.0/examples/two_reservoir/
+-rw-rw-r--   0 james     (1000) james     (1000)     3873 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/two_reservoir/two_reservoir.json
+-rw-rw-r--   0 james     (1000) james     (1000)     4697 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/two_reservoir/two_reservoir_moea.py
+-rw-rw-r--   0 james     (1000) james     (1000)      664 2020-09-15 21:11:56.000000 pywr-1.9.0/examples/two_reservoir/two_reservoir_plot.py
+-rw-rw-r--   0 james     (1000) james     (1000)     4408 2020-09-15 21:11:56.000000 pywr-1.9.0/github_deploy_key.enc
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.637613 pywr-1.9.0/pywr/
+-rw-rw-r--   0 james     (1000) james     (1000)      649 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)      290 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/_component.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)     3043 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/_component.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     4333 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/_core.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    44916 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/_core.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)    36135 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/_model.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)      287 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/core.py
+-rw-rw-r--   0 james     (1000) james     (1000)    10006 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/dataframe_tools.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.638613 pywr-1.9.0/pywr/domains/
+-rw-rw-r--   0 james     (1000) james     (1000)      197 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/domains/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)     4688 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/domains/groundwater.py
+-rw-rw-r--   0 james     (1000) james     (1000)     9507 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/domains/river.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1941 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/h5tools.py
+-rw-rw-r--   0 james     (1000) james     (1000)      907 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/hashes.py
+-rw-rw-r--   0 james     (1000) james     (1000)       21 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/model.py
+-rw-rw-r--   0 james     (1000) james     (1000)    41899 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/nodes.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.638613 pywr-1.9.0/pywr/notebook/
+-rw-rw-r--   0 james     (1000) james     (1000)    12006 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)     6531 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/draw_graph.js
+-rw-rw-r--   0 james     (1000) james     (1000)     3488 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/figures.py
+-rw-rw-r--   0 james     (1000) james     (1000)      570 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/graph.css
+-rw-rw-r--   0 james     (1000) james     (1000)     2302 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/sankey.py
+-rw-rw-r--   0 james     (1000) james     (1000)     2469 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/save_graph.js
+-rw-rw-r--   0 james     (1000) james     (1000)      537 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/notebook/template.html
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.638613 pywr-1.9.0/pywr/optimisation/
+-rw-rw-r--   0 james     (1000) james     (1000)     3221 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/optimisation/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)     8726 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/optimisation/platypus.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3292 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/optimisation/pygmo.py
+-rw-rw-r--   0 james     (1000) james     (1000)      737 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameter_property.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.639613 pywr-1.9.0/pywr/parameters/
+-rw-rw-r--   0 james     (1000) james     (1000)       25 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1426 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_control_curves.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    28390 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_control_curves.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)      558 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_hydropower.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)     9140 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_hydropower.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     6779 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_parameters.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    84665 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_parameters.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)      787 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_polynomial.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)     7700 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_polynomial.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     1182 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_thresholds.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    10838 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/_thresholds.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)      335 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/control_curves.py
+-rw-rw-r--   0 james     (1000) james     (1000)     2219 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/groundwater.py
+-rw-rw-r--   0 james     (1000) james     (1000)     7240 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/licenses.py
+-rw-rw-r--   0 james     (1000) james     (1000)    12811 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/parameters/parameters.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.640613 pywr-1.9.0/pywr/recorders/
+-rw-rw-r--   0 james     (1000) james     (1000)       25 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)      785 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_hydropower.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    11529 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_hydropower.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     5430 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_recorders.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)    84465 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_recorders.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)      505 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_thresholds.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)     3930 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/_thresholds.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     3116 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/calibration.py
+-rw-rw-r--   0 james     (1000) james     (1000)    12969 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/events.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3200 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/progress.py
+-rw-rw-r--   0 james     (1000) james     (1000)    21726 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/recorders/recorders.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.640613 pywr-1.9.0/pywr/solvers/
+-rw-rw-r--   0 james     (1000) james     (1000)     5876 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/solvers/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)    59683 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/solvers/cython_glpk.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)    23356 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/solvers/cython_lpsolve.pyx
+-rw-rw-r--   0 james     (1000) james     (1000)     3190 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/solvers/libglpk.pxd
+-rw-rw-r--   0 james     (1000) james     (1000)     4999 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/timestepper.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.640613 pywr-1.9.0/pywr/utils/
+-rw-rw-r--   0 james     (1000) james     (1000)        0 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/utils/__init__.py
+-rw-rw-r--   0 james     (1000) james     (1000)     5368 2020-09-15 21:11:56.000000 pywr-1.9.0/pywr/utils/bisect.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.637613 pywr-1.9.0/pywr.egg-info/
+-rw-r--r--   0 james     (1000) james     (1000)     5221 2020-09-15 21:27:19.000000 pywr-1.9.0/pywr.egg-info/PKG-INFO
+-rw-r--r--   0 james     (1000) james     (1000)     7673 2020-09-15 21:27:30.000000 pywr-1.9.0/pywr.egg-info/SOURCES.txt
+-rw-r--r--   0 james     (1000) james     (1000)        1 2020-09-15 21:27:19.000000 pywr-1.9.0/pywr.egg-info/dependency_links.txt
+-rw-r--r--   0 james     (1000) james     (1000)      208 2020-09-15 21:27:19.000000 pywr-1.9.0/pywr.egg-info/requires.txt
+-rw-r--r--   0 james     (1000) james     (1000)        5 2020-09-15 21:27:19.000000 pywr-1.9.0/pywr.egg-info/top_level.txt
+-rw-rw-r--   0 james     (1000) james     (1000)      148 2020-09-15 21:27:30.649613 pywr-1.9.0/setup.cfg
+-rw-rw-r--   0 james     (1000) james     (1000)     6385 2020-09-15 21:11:56.000000 pywr-1.9.0/setup.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.642613 pywr-1.9.0/tests/
+-rw-rw-r--   0 james     (1000) james     (1000)      213 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/conftest.py
+-rw-rw-r--   0 james     (1000) james     (1000)     2371 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/fixtures.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1209 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/helpers.py
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.647613 pywr-1.9.0/tests/models/
+-rw-rw-r--   0 james     (1000) james     (1000)       99 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/.gitattributes
+-rw-rw-r--   0 james     (1000) james     (1000)     2179 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/agg_recorder_nesting.json
+-rw-rw-r--   0 james     (1000) james     (1000)      895 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/aggregated1.json
+-rw-rw-r--   0 james     (1000) james     (1000)      862 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/annual_license.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1008 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/bottleneck.json
+-rw-rw-r--   0 james     (1000) james     (1000)      569 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/breaklink.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3885 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/control_curve.csv
+-rw-rw-r--   0 james     (1000) james     (1000)      765 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/cost1.json
+-rw-rw-r--   0 james     (1000) james     (1000)      758 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/csv_recorder.json
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.648613 pywr-1.9.0/tests/models/data/
+-rw-rw-r--   0 james     (1000) james     (1000)  1172434 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/data/thames_stochastic_flow.gz
+-rw-rw-r--   0 james     (1000) james     (1000)     1411 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/deficit.json
+-rw-rw-r--   0 james     (1000) james     (1000)     2772 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/demand_saving2.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3307 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/demand_saving2_with_variables.json
+-rw-rw-r--   0 james     (1000) james     (1000)     2255 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/demand_saving_hdf.json
+-rw-rw-r--   0 james     (1000) james     (1000)    10120 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/demand_saving_level.h5
+-rw-rw-r--   0 james     (1000) james     (1000)     4224 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/demand_saving_with_tables_recorder.json
+-rw-rw-r--   0 james     (1000) james     (1000)      748 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/discount.json
+-rw-rw-r--   0 james     (1000) james     (1000)      717 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/extra1.json
+-rw-rw-r--   0 james     (1000) james     (1000)      322 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/extra2.json
+-rw-rw-r--   0 james     (1000) james     (1000)      982 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/flow_interpolation.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1418 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/flow_parameter.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3089 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/hydropower_example.json
+-rw-rw-r--   0 james     (1000) james     (1000)     2939 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/hydropower_target_example.json
+-rw-rw-r--   0 james     (1000) james     (1000)       49 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/initial_volumes.csv
+-rw-rw-r--   0 james     (1000) james     (1000)       33 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/invalid.json
+-rw-rw-r--   0 james     (1000) james     (1000)      679 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/invalid_include.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1602 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/mean_flow_recorder.json
+-rw-rw-r--   0 james     (1000) james     (1000)      155 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/monthly_profiles.csv
+-rw-rw-r--   0 james     (1000) james     (1000)       96 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/multiindex_data.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     1135 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/multiindex_df.json
+-rw-rw-r--   0 james     (1000) james     (1000)      118 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/my_parameter.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1153 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/parameter_recorder.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1198 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/parameter_reference.json
+-rw-rw-r--   0 james     (1000) james     (1000)      763 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/piecewise1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1109 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/piecewise1_with_parameters.json
+-rw-rw-r--   0 james     (1000) james     (1000)      852 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/python_include.json
+-rw-rw-r--   0 james     (1000) james     (1000)      740 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir1.json
+-rw-rw-r--   0 james     (1000) james     (1000)      744 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir1_pc.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1390 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir2.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1594 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_evaporation.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1556 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_evaporation_without_area_property.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1381 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_initial_vol_from_table.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1077 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_with_cc.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1255 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_with_cc_param_values.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1296 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/reservoir_with_circular_cc.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1031 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1685 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river2.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1207 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river_discharge1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1207 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river_discharge2.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1101 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river_mrf1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1167 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/river_split_with_gauge1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1482 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/scenario_monthly_profile.json
+-rw-rw-r--   0 james     (1000) james     (1000)      593 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/scenario_with_slices.json
+-rw-rw-r--   0 james     (1000) james     (1000)      678 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/scenario_with_user_combinations.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1399 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/seasonal_virtual_storage.json
+-rw-rw-r--   0 james     (1000) james     (1000)      670 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1568 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple1_bisect.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1531 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple1_infeasible_bisect.json
+-rw-rw-r--   0 james     (1000) james     (1000)      685 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple1_monthly.json
+-rw-rw-r--   0 james     (1000) james     (1000)       61 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple_data.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     1067 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple_df.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1178 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple_df_shared.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1376 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple_with_scenario.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1920 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/simple_with_scenario_wrapper.json
+-rw-rw-r--   0 james     (1000) james     (1000)    10198 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/test_data1.xlsx
+-rw-rw-r--   0 james     (1000) james     (1000)      510 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     1151 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1.json
+-rw-rw-r--   0 james     (1000) james     (1000)      100 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1_weekly.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     7168 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1_weekly.h5
+-rw-rw-r--   0 james     (1000) james     (1000)     1130 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1_weekly.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1054 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1_weekly_hdf.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1124 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries1_xlsx.json
+-rw-rw-r--   0 james     (1000) james     (1000)     2231 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     9672 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2.h5
+-rw-rw-r--   0 james     (1000) james     (1000)     1505 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1387 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2_hdf.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1378 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2_hdf_wrong_hash.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3084 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries2_with_fdc.json
+-rw-rw-r--   0 james     (1000) james     (1000)     8374 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries3.csv
+-rw-rw-r--   0 james     (1000) james     (1000)     1382 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries3.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1487 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/timeseries4.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3582 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/two_reservoir.json
+-rw-rw-r--   0 james     (1000) james     (1000)     3775 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/two_reservoir_constrained.json
+-rw-rw-r--   0 james     (1000) james     (1000)      567 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/version1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1230 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/virtual_storage1.json
+-rw-rw-r--   0 james     (1000) james     (1000)     1804 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/models/virtual_storage2.json
+-rw-rw-r--   0 james     (1000) james     (1000)      983 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/notebook.ipynb
+-rw-rw-r--   0 james     (1000) james     (1000)       75 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/pytest.ini
+-rw-rw-r--   0 james     (1000) james     (1000)    10208 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_agg_constraints.py
+-rw-rw-r--   0 james     (1000) james     (1000)     8121 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_aggregated_nodes.py
+-rw-rw-r--   0 james     (1000) james     (1000)     2981 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_aggregator.py
+-rw-rw-r--   0 james     (1000) james     (1000)    12158 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_analytical.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1943 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_bisect.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3155 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_components.py
+-rw-rw-r--   0 james     (1000) james     (1000)    19291 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_control_curves.py
+-rw-rw-r--   0 james     (1000) james     (1000)    17941 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_core.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3540 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_delay.py
+-rw-rw-r--   0 james     (1000) james     (1000)     9239 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_df_resampling.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3259 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_groundwater.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1034 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_hashes.py
+-rw-rw-r--   0 james     (1000) james     (1000)     7674 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_license.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1121 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_named_iterator.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1146 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_node_iterator.py
+-rw-rw-r--   0 james     (1000) james     (1000)     3152 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_notebook.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1253 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_optimisation.py
+-rw-rw-r--   0 james     (1000) james     (1000)    62817 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_parameters.py
+-rw-rw-r--   0 james     (1000) james     (1000)     2200 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_piecewise.py
+-rw-rw-r--   0 james     (1000) james     (1000)     4583 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_platypus.py
+-rw-rw-r--   0 james     (1000) james     (1000)     1510 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_pygmo.py
+-rw-rw-r--   0 james     (1000) james     (1000)    74172 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_recorders.py
+-rw-rw-r--   0 james     (1000) james     (1000)     6783 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_river.py
+-rw-rw-r--   0 james     (1000) james     (1000)    26248 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_run.py
+-rw-rw-r--   0 james     (1000) james     (1000)    13461 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/test_scenarios.py
+-rw-rw-r--   0 james     (1000) james     (1000)    34537 2020-09-15 21:11:56.000000 pywr-1.9.0/tests/timeseries1.xlsx
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.648613 pywr-1.9.0/travis/
+-rwxrwxr-x   0 james     (1000) james     (1000)     2216 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/build-wheels.sh
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.649613 pywr-1.9.0/travis/manylinux1_x86_64-glpk/
+-rw-rw-r--   0 james     (1000) james     (1000)      226 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/Dockerfile
+-rw-rw-r--   0 james     (1000) james     (1000)      174 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/Makefile
+-rw-rw-r--   0 james     (1000) james     (1000)      489 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/build_glpk.sh
+-rwxrwxr-x   0 james     (1000) james     (1000)      504 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/build_lpsolve.sh
+-rw-rw-r--   0 james     (1000) james     (1000)     2564 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/fix-lpsolve-compilation.patch
+-rw-rw-r--   0 james     (1000) james     (1000)      203 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux1_x86_64-glpk/lpsolveConfig.cmake.in
+drwxrwxr-x   0 james     (1000) james     (1000)        0 2020-09-15 21:27:30.649613 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/
+-rw-rw-r--   0 james     (1000) james     (1000)      229 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/Dockerfile
+-rw-rw-r--   0 james     (1000) james     (1000)      177 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/Makefile
+-rw-rw-r--   0 james     (1000) james     (1000)      489 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/build_glpk.sh
+-rwxrwxr-x   0 james     (1000) james     (1000)      651 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/build_lpsolve.sh
+-rw-rw-r--   0 james     (1000) james     (1000)     2564 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/fix-lpsolve-compilation.patch
+-rw-rw-r--   0 james     (1000) james     (1000)      203 2020-09-15 21:11:56.000000 pywr-1.9.0/travis/manylinux2010_x86_64-glpk/lpsolveConfig.cmake.in
```

### Comparing `pywr-1.8.0/.travis.yml` & `pywr-1.9.0/.travis.yml`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-language: python
-dist: xenial
-python: 3.6
-
-notifications:
-  email: false
-
-services:
-  - docker
-
-sudo: required
-
-env:
-  global:
-    - secure: "bF1Ljix0pDNBBjgewrOxxNHJDEvlWDT5v5KckSq/Wl0v9QMAG5Fiilj0hPHVQrWoGz7F3xPU5KeprjQcfTo2UB7qPGDOVsovafbUTZ+0o6kc2mVLApYsRvlhUPX8tsRCrq6Zo874buMzbDPyAo3nnl3lkkWQ2Dh+gA7b7er9xS0="
-    - DOCKER_IMAGE=pywr/manylinux2010_x86_64-glpk
-  matrix:
-    - PYBIN=/opt/python/cp36-cp36m/bin BUILD_DOC=1
-    - PYBIN=/opt/python/cp36-cp36m/bin PYWR_BUILD_TRACE=true
-    - PYBIN=/opt/python/cp37-cp37m/bin
-    - PYBIN=/opt/python/cp38-cp38/bin
-
-install:
-  - docker pull $DOCKER_IMAGE
-  - |
-    if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
-      # Cython is needed for the coverage plugin.
-      pip install codecov cython 'coverage<5.0'
-    fi
-
-script:
-  - |
-    docker run --rm -v `pwd`:/io \
-      -e PYBIN=$PYBIN \
-      -e PYWR_BUILD_TRACE=$PYWR_BUILD_TRACE \
-      -e BUILD_DOC=$BUILD_DOC \
-      $DOCKER_IMAGE $PRE_CMD /io/travis/build-wheels.sh
-  # Fix permissions of files created in docker
-  - sudo chown -R travis:travis .
-  - |
-    if [ "$BUILD_DOC" -eq "1" ]; then
-      pip install doctr
-      if [[ "${TRAVIS_TAG}" == "v"* ]]; then
-        doctr deploy --deploy-repo pywr/pywr-docs --build-tags --built-docs pywr-docs/html ${TRAVIS_TAG}
-      elif [[ "${TRAVIS_BRANCH}" == "master" ]]; then
-        doctr deploy --deploy-repo pywr/pywr-docs --build-tags --built-docs pywr-docs/html master
-      fi
-    fi
-
-after_success:
-  - |
-    if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
-      coverage combine
-      coverage report
-      codecov
-    fi
-
+language: python
+dist: xenial
+python: 3.6
+
+notifications:
+  email: false
+
+services:
+  - docker
+
+sudo: required
+
+env:
+  global:
+    - secure: "bF1Ljix0pDNBBjgewrOxxNHJDEvlWDT5v5KckSq/Wl0v9QMAG5Fiilj0hPHVQrWoGz7F3xPU5KeprjQcfTo2UB7qPGDOVsovafbUTZ+0o6kc2mVLApYsRvlhUPX8tsRCrq6Zo874buMzbDPyAo3nnl3lkkWQ2Dh+gA7b7er9xS0="
+    - DOCKER_IMAGE=pywr/manylinux2010_x86_64-glpk
+  matrix:
+    - PYBIN=/opt/python/cp36-cp36m/bin BUILD_DOC=1
+    - PYBIN=/opt/python/cp36-cp36m/bin PYWR_BUILD_TRACE=true
+    - PYBIN=/opt/python/cp37-cp37m/bin
+    - PYBIN=/opt/python/cp38-cp38/bin
+
+install:
+  - docker pull $DOCKER_IMAGE
+  - |
+    if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
+      # Cython is needed for the coverage plugin.
+      pip install codecov cython 'coverage<5.0'
+    fi
+
+script:
+  - |
+    docker run --rm -v `pwd`:/io \
+      -e PYBIN=$PYBIN \
+      -e PYWR_BUILD_TRACE=$PYWR_BUILD_TRACE \
+      -e BUILD_DOC=$BUILD_DOC \
+      $DOCKER_IMAGE $PRE_CMD /io/travis/build-wheels.sh
+  # Fix permissions of files created in docker
+  - sudo chown -R travis:travis .
+  - |
+    if [ "$BUILD_DOC" -eq "1" ]; then
+      pip install doctr
+      if [[ "${TRAVIS_TAG}" == "v"* ]]; then
+        doctr deploy --deploy-repo pywr/pywr-docs --build-tags --built-docs pywr-docs/html ${TRAVIS_TAG}
+      elif [[ "${TRAVIS_BRANCH}" == "master" ]]; then
+        doctr deploy --deploy-repo pywr/pywr-docs --build-tags --built-docs pywr-docs/html master
+      fi
+    fi
+
+after_success:
+  - |
+    if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
+      coverage combine
+      coverage report
+      codecov
+    fi
+
```

### Comparing `pywr-1.8.0/CHANGELOG.md` & `pywr-1.9.0/CHANGELOG.md`

 * *Files 12% similar despite different names*

```diff
@@ -1,298 +1,320 @@
-# Changelog
-
-All issue numbers are relative to https://github.com/pywr/pywr/issues unless otherwise stated.
-
-## v1.8.0
-
-### New Features
-
-- A change to the behaviour of Storage nodes with parameters for `max_volume`. Such nodes must
-now have both `initial_volume` and `initial_volume_pc` specified. This allows using arbitrary parameters
-for `max_volume` provided the initial condition is explicitly defined in absolute and percentage terms. (#690)
-- Added `DelayNode` and `FlowDelayParameter` to allow emulation of time-of-travel. (#904)
-- Added `DiscountFactorParameter`. (#901)
-- Added support for optimising days of the year in `RbfProfileParameter`. (#908)
-- Added `NumpyArrayDailyProfileParameterRecorder` for recording calculated annual profiles, and
-refactored the internal calculations around day of the year. `Timestep` now includes properties
-for `dayofyear_index`, `week_index` and `is_leap_year`. (#903)
-- Added error logging in `Model`'s `load`, `setup`, `reset` and `finish` methods. Handling of unknown
-component types now raises a custom `TypeNotFoundError`. (#896)
-
-### Miscellaneous
-
-- Improvements to API documentation. (#905)
-
-## v1.7.2
-
-### Miscellaneous
-
-- Release to fix packaging mistake in v1.7.1.
-
-## v1.7.1
-
-### Bug Fixes
-
-- Fixed a bug when using `draw_graph` with a dictionary. (#899)
-- Fixed a bug when giving non-float values to `RbfProfileParameter`. (#897)
-- Improved handling of isolated nodes in `draw_graph` when using JSON data. (#895)
-
-## v1.7.0
-
-### New Features
-
-- Improve Jupyter Notebook graphing functionality. (#868, #885)
-- Added `OffsetParameter`. (#874)
-- Added `PywrRandomGenerator` for use with the Platypus optimisation library. (#867, #892)
-- Added `RbfProfileParameter`. (#873)
-- Changed the signature of recorder aggregation functions to better catch exceptions. (#879)
-
-### Bug Fixes
-
-- Removed unreachable code when CSV dataframes. (#880)
-- Fixed incorrect parsing of `position` keyword in several nodes. (#884)
-
-### Miscellaneous
-
-- Added `IPython` to install dependencies. (#870)
-- Removed optimisation wrapper for `inspyred`. (#878)
-- Exposed run statistics to optimisation wrapper. (#877)
-- Added optional dependencies for docs and dev installs. (#882)
-
-### Documentation
-
-- Added threshold parameters to API documentation. (#881)
-- Corrected `MeanParameterRecorder`'s docstring. (#772)
-- Improved docstrings and made consistent argument names for `InterpolatedVolumeParameter` and `InterpolatedFlowParameter`. (#890)
-
-## v1.6.0
-
-### New Features
-
-- `AnnualTotalFlowRecorder` now accepts an optional list of factors to scale the flow by. (#837)
-- `NumpyArrayNodeRecorder` now accepts an optional factor (default=1.0) to scale the flow by. (#838, #840)
-- Added `UniformDrawdownProfileParameter` (#836)
-- Added `ControlCurvePiecewiseInterpolatedParameter` as a more general replacement for `PiecewiseLinearControlCurve`. (#857)
-- Added 'count_nonzero' as an aggregation function for recorders. (#866)
-
-### Bug Fixes
-
-- Fix bug draw_graph modifying model data when a data dict is given. (#832)
-- Fix the `__init__` method of `BreakLink`. (#850)
-- Fix reset of `AbstractNode._prev_flow`. (#855)
-- Fix a bug calculating of `AggregatedStorage`'s initial volume in multiple scenarios. (#854)
-- Fix resetting of `AnnualVirtualStorage` volume to maximum volume instead of initial volume. (#860)
-- Fix cdef type issue in some control curve parameters allowing use with any `AbstractStorage` (instead of just `Storage`). (#861) 
-- Fix registering of `ArrayIndexedScenarioParameter` (#863)
-
-### Miscellaneous
-
-- Fixed documentation building on tags. (#831)
-- Updated notebook graph drawing to use d3.v5 and removed IE specific code. (#834)
-- Add reference to published academic paper. (#846)
-- `PiecewiseLinearControlCurve` has been marked as deprecated and will be removed in a future version. (#857) 
-- Added examples from the recently published paper. (#852)
-
-## v1.5.0
-
-### New Features
-
-- Added `ScenarioDailyProfileParameter` and `ScenarioWeeklyProfileParameter` to provide different profiles per scenario. (#802)
-- Added `TimestepCountIndexParameterRecorder`, `AnnualCountIndexThresholdRecorder` and `AnnualTotalFlowRecorder` (#784)
-- Added daily interpolation support to `MonthlyProfileParameter`. (#807)
-- Added `__contains__` method to `NamedIterator` and `NodeIterator` (#813) 
-
-### Bug fixes
-
-- Fix resetting progress of `ProgressRecorder` (#816)
-- Fix for `draw_graph` issue error when loading model object that has schematic positions (#821)
-
-### Miscellaneous
-
-- Removed `FutureWarning` and `UnicodeWarning` warning filters (#803)
-- Refactored `setup.py` to improve build time dependency handling and specifying build arguments (#811)
-- Fix deprecated use of `.labels` in the tests (#814) 
-- Fix test warning when incrementing timestep by integers (#815)
-- Fix duplicated test function names (#818)
-- Support Python 3.8 (#796)
-- Refactored the GLPK solvers in to a single extension module. (#822)
-- Add `.pxd` files to the Pywr's package data so they are distributed. (#824)
-
-### Documentation
-
-- Fixed some warning and deployment issues with the documentation (#771) 
-- Add missing new line from code-block sections in `json.rst` (#817)
-
-## v1.4.0
-
-### New Features
-
-- Added support time-steps based on Pandas offsets (#675)
-- Added `InterpolatedFlowParameter` (#740)
-- Added support for `percentile` and `percentileofscore` aggregation functions (#777)
-- Added `PiecewiseIntegralParameter` (#772)
-- Added support for including references to Python modules in JSON format (#765)
-- Added `CurrentYearThresholdParameter` and `CurrentOrdinalDayThresholdParameter` parameters (#789)
-
-### Bug fixes
-
-- Ensure `comment` key doesn't get passed to Pandas `read_xxx` functions (#788)
-
-### Documentation
-
-- Added some docs for AggregatedNode (#756)
-
-## v1.3.0
-
-### New Features
-
-- Allow use of parameters as values in `ControlCurveInterpolatedParameter` (#750)
-- Added `ScenarioWrapper` parameter (#763)
-
-### Bug fixes
-
-- Fix loading PiecewiseLink with parameters from JSON (#749)
-- Fixed a bug with `CSVRecorder` not saving volumes correctly (#767)
-
-### Miscellaneous
-
-- Removed `six` as dependency (#745)
-- Removed `pywr.__git_hash__` (#752)
-- Removed `Blender` node (#757)
-
-## v1.2.0
-
-### New Features
-
-- Support for embedding dataframe data directly in to JSON (#700)
-- Added `NumpyArrayAreaRecorder` and refactored storage recorders (#684)
-- Added getter for Recorder.agg_func (#719)
-- Add `DivisionParameter` and tests (#722)
-- Add `FlowParameter` for tracking yesterday's flow (#724)
-- Add `InterpolatedQuadratureParameter` (#714)
-- Add new array deficit recorders (#729):
-  - `NumpyArrayNodeDeficitRecorder` - timeseries of deficit.
-  - `NumpyArrayNodeSuppliedRatioRecorder` - timeseries of supply / demand
-  - `NumpyArrayNodeCurtailmentRatioRecorder` - timeseries of 1 - supply / demand
-
-### Bug fixes
-
-- Fix a bug with hydropower parameters & recorders not applying efficiency factor (#737)
-- Refactor of the code used to load named parameters and recorders to use shared functions (as they are both components) (#720)
-- Fix a bug with AggregatedRecorder not returning the instance on load (#723)
-- Use `flow` instead of `max_flow` in two_reservoirs test and example (#721)
-
-### Documentation
-
-- Added API documentation for nodes (#668)
-- Fix `PiecewiseLink` docstring's ASCII diagram (#668)
-
-### Miscellaneous
-
-- Clean-up various warnings in tests (#695)
-- Removed conda-recipe (Pywr is now in conda-forge) (#692)
-- Added codecov based coverage reporting (#705)
-- Updated test builds to use manylinux2010 to build wheels (#710)
-- Updated installation instructions to reflect wheels in Pypi and conda-forge installation.
-
-
-## v1.1.0
-
-### New features
-
-- New "edge based" GLPK solver. (#672)
-- Improved `FlowDurationDeviationRecorder` with JSON support and bug fixes when no scenario is given. (#677)
-
-### Bug fixes
-
-- Replace deprecated calls time time.clock with time.perf_counter. (#683)
-- Type optimisation in AggregatedIndexParameter. (#662)
-
-### Documentation 
-
-- Updated documentation: control curves, extending Pywr. (#652)
-
-### Miscellaneous
-
-- Variable renames and clean up for GLPK path solver. (#672)
-- Spport for Python 3.7. (#662)
-- Updated url in setup.py metadata to GitHub project (#661)
-- Additional cython compiler directives (#645)
-
-## v1.0.0
-
-### New features
-
-- Added ratchet support to threshold parameters. (#655)
-- Added `ConstantScenarioIndexParameter`. (#654)
-- Added support for Platypus and Pygmo optimisation wrappers. Involved a refactor of the existing optimisation support. (#610)
-- Added `HydropowerTargetParameter` to specify a flow target from a hydropower target. (#631)
-- Renamed `HydroPowerRecorder` to `HydropowerRecorder` (#631)
-- Better warning/error messages in `TablesArrayParameter` (#629)
-- Allow solver to be defined by the environment variable `PYWR_SOLVER`. (#619)
-- Added flow weights to `AggregatedNode`. (#603)
-- Added additional labeling functionality to notebook graphing functions. (#612)
-- New and improved variable API for Parameters. (#601, #258, #625)
-
-### Bug fixes
-
-- Fix bug setting the area property of `Storage` nodes. (#657)
-- Fixed bug with finally clause in the optimisation wrapper. (#649)
-- Fix a bug in `AnnualHarmonicSeriesParameter` related to updating the `amplitudes` and `phases` values with `set_double_variables` (#622)
-
-### Miscellaneous
-
-- Refactored several recorders and unified the use of temporal aggregation. This deprecated several keyword arguments in some existing Recorders. See the PR for details. (#635)
-- Removed deprecated (Monthly|Daily)ProfileControlCurve. (#640)
-- Dropped support for Python 2 and <3.6. Pywr is no longer tested against Python versions earlier than 3.6. (#623)
-- Use new `networkx.nodes_with_selfloops` function. (#628)
-- `AbstractProfileControlCurveParameter`, `MonthlyProfileControlCurveParameter` and `DailyProfileControlCurveParameter` have been removed after deprecation. (#231, #640)
-- Improved documentation. (#616, #627)
-
-
-## v0.5.1
-
-### Miscellaneous
-
-- Fixes to the source distribution (sdist) and inclusions of MANIFEST.in
-- Changes to the build systems on travis to enable deploy to Anaconda and Pypi.
-
-## v0.5
-
-### New features
-
-- Added build and testing for OS X via travis. (#588)
-- Added data consumption warnings to DataFrameParamter and TablesArrayParameter. (#562)
-- Added `PiecewiseLinearControlCurve` as a new cython parameter.
-- Pywr now emits some logging calls during a model run. 
-- Improved the event handling code to allow tracking of a `Parameter`'s value during an event.
-- Added support for initialising storage volume by percentage. Can be set in json through the `"initial_volume_pc"` property.
-- Added GZ2 and BZ2 compression support to `CSVRecorder`.
-- Added JSON support for license parameters. (#544)
-- Added a sense check to `TablesArrayParameter` having non-finite values.
-- Added scale and offset to ConstantParameter.
-- Added JSON support for license parameters. (#544)
-- Added `WeeklyProfileParameter` (#537)
-- Added `InterpolationParameter`. Closes #478. (#535)
-- Added surface area property to `Storage` (#525)
-- Added optional checksum when loading DataFrameParameter and TablesArrayParameter using hashlib.
-- Added ProgressRecorder and JupyterProgressRecorder (#520)
-- The threshold in `*ThresholdParameter` can now be a Parameter. (#517)
-- Added `HydroPowerRecorder` and `TotalHydroEnergyRecorder` for hydropower studies (#584)
-
-### Bug fixes
-- Explicitly set the frequency during dataframe resampling (#563)
-- `IndexedArrayParameter` also accepts 'parameters' in JSON. Closes #538. (#539)
-- Recursive deletion of child nodes when deleting compound nodes. (#527)
-- Compatibility with NetworkX 2.x (#529)
-- Changed GLPK log level to remove printing of superfluous messages to stdout. (#523)
-- Fixed loading parameters for `PiecewiseLink` in JSON. (#519)
-
-### Deprecated features
-- `AbstractProfileControlCurveParameter` marked for deprecation.
-
-### Miscellaneous
-- Improved the online documentation including the API reference. 
-- Added a hydropower example. 
-- General tweaks and corrections to class docstrings.
-- Updated conda build recipe to use the conda-forge lp_solve package.
-- Updated the conda build recipe to use MSVC features.
+# Changelog
+
+All issue numbers are relative to https://github.com/pywr/pywr/issues unless otherwise stated.
+
+## v1.9.0
+
+### New Features
+
+- Added `BisectionSearchModel` that performs a bisectional search on a single parameter instead of a 
+standard simulation. (#915)
+- Allow `AggregatedNode` factors to be time-varying using `Parameter`s. (#919)
+- Added `RollingVirtualStorage` node intended for modelling rolling licenses. (#891)
+- Added `SeasonalVirtualStorage` node intended for modelling licenses that apply for limited periods. (#923)
+
+### Bug Fixes
+
+- Ensure `RollingMeanFlowNodeRecorder`'s internal memory pointer is reset correctly. (#893)
+- Fix a bug where `AggregatedNode` would warn about small factors with any negative value. (#921)
+- Fixed `AggreagtedNode` initial volume being incorrectly calculated when its dependent nodes used a proportional
+initial volume. (#922)
+
+### Miscellaneous
+
+- Added `NullSolver` that performs no allocation and is intended for debugging purposes. (#924)
+- Added a small tolerance for equality checking of fixed bounds in the GLPK solvers. (#925)
+
+## v1.8.0
+
+### New Features
+
+- A change to the behaviour of Storage nodes with parameters for `max_volume`. Such nodes must
+now have both `initial_volume` and `initial_volume_pc` specified. This allows using arbitrary parameters
+for `max_volume` provided the initial condition is explicitly defined in absolute and percentage terms. (#690)
+- Added `DelayNode` and `FlowDelayParameter` to allow emulation of time-of-travel. (#904)
+- Added `DiscountFactorParameter`. (#901)
+- Added support for optimising days of the year in `RbfProfileParameter`. (#908)
+- Added `NumpyArrayDailyProfileParameterRecorder` for recording calculated annual profiles, and
+refactored the internal calculations around day of the year. `Timestep` now includes properties
+for `dayofyear_index`, `week_index` and `is_leap_year`. (#903)
+- Added error logging in `Model`'s `load`, `setup`, `reset` and `finish` methods. Handling of unknown
+component types now raises a custom `TypeNotFoundError`. (#896)
+
+### Miscellaneous
+
+- Improvements to API documentation. (#905)
+
+## v1.7.2
+
+### Miscellaneous
+
+- Release to fix packaging mistake in v1.7.1.
+
+## v1.7.1
+
+### Bug Fixes
+
+- Fixed a bug when using `draw_graph` with a dictionary. (#899)
+- Fixed a bug when giving non-float values to `RbfProfileParameter`. (#897)
+- Improved handling of isolated nodes in `draw_graph` when using JSON data. (#895)
+
+## v1.7.0
+
+### New Features
+
+- Improve Jupyter Notebook graphing functionality. (#868, #885)
+- Added `OffsetParameter`. (#874)
+- Added `PywrRandomGenerator` for use with the Platypus optimisation library. (#867, #892)
+- Added `RbfProfileParameter`. (#873)
+- Changed the signature of recorder aggregation functions to better catch exceptions. (#879)
+
+### Bug Fixes
+
+- Removed unreachable code when CSV dataframes. (#880)
+- Fixed incorrect parsing of `position` keyword in several nodes. (#884)
+
+### Miscellaneous
+
+- Added `IPython` to install dependencies. (#870)
+- Removed optimisation wrapper for `inspyred`. (#878)
+- Exposed run statistics to optimisation wrapper. (#877)
+- Added optional dependencies for docs and dev installs. (#882)
+
+### Documentation
+
+- Added threshold parameters to API documentation. (#881)
+- Corrected `MeanParameterRecorder`'s docstring. (#772)
+- Improved docstrings and made consistent argument names for `InterpolatedVolumeParameter` and `InterpolatedFlowParameter`. (#890)
+
+## v1.6.0
+
+### New Features
+
+- `AnnualTotalFlowRecorder` now accepts an optional list of factors to scale the flow by. (#837)
+- `NumpyArrayNodeRecorder` now accepts an optional factor (default=1.0) to scale the flow by. (#838, #840)
+- Added `UniformDrawdownProfileParameter` (#836)
+- Added `ControlCurvePiecewiseInterpolatedParameter` as a more general replacement for `PiecewiseLinearControlCurve`. (#857)
+- Added 'count_nonzero' as an aggregation function for recorders. (#866)
+
+### Bug Fixes
+
+- Fix bug draw_graph modifying model data when a data dict is given. (#832)
+- Fix the `__init__` method of `BreakLink`. (#850)
+- Fix reset of `AbstractNode._prev_flow`. (#855)
+- Fix a bug calculating of `AggregatedStorage`'s initial volume in multiple scenarios. (#854)
+- Fix resetting of `AnnualVirtualStorage` volume to maximum volume instead of initial volume. (#860)
+- Fix cdef type issue in some control curve parameters allowing use with any `AbstractStorage` (instead of just `Storage`). (#861) 
+- Fix registering of `ArrayIndexedScenarioParameter` (#863)
+
+### Miscellaneous
+
+- Fixed documentation building on tags. (#831)
+- Updated notebook graph drawing to use d3.v5 and removed IE specific code. (#834)
+- Add reference to published academic paper. (#846)
+- `PiecewiseLinearControlCurve` has been marked as deprecated and will be removed in a future version. (#857) 
+- Added examples from the recently published paper. (#852)
+
+## v1.5.0
+
+### New Features
+
+- Added `ScenarioDailyProfileParameter` and `ScenarioWeeklyProfileParameter` to provide different profiles per scenario. (#802)
+- Added `TimestepCountIndexParameterRecorder`, `AnnualCountIndexThresholdRecorder` and `AnnualTotalFlowRecorder` (#784)
+- Added daily interpolation support to `MonthlyProfileParameter`. (#807)
+- Added `__contains__` method to `NamedIterator` and `NodeIterator` (#813) 
+
+### Bug fixes
+
+- Fix resetting progress of `ProgressRecorder` (#816)
+- Fix for `draw_graph` issue error when loading model object that has schematic positions (#821)
+
+### Miscellaneous
+
+- Removed `FutureWarning` and `UnicodeWarning` warning filters (#803)
+- Refactored `setup.py` to improve build time dependency handling and specifying build arguments (#811)
+- Fix deprecated use of `.labels` in the tests (#814) 
+- Fix test warning when incrementing timestep by integers (#815)
+- Fix duplicated test function names (#818)
+- Support Python 3.8 (#796)
+- Refactored the GLPK solvers in to a single extension module. (#822)
+- Add `.pxd` files to the Pywr's package data so they are distributed. (#824)
+
+### Documentation
+
+- Fixed some warning and deployment issues with the documentation (#771) 
+- Add missing new line from code-block sections in `json.rst` (#817)
+
+## v1.4.0
+
+### New Features
+
+- Added support time-steps based on Pandas offsets (#675)
+- Added `InterpolatedFlowParameter` (#740)
+- Added support for `percentile` and `percentileofscore` aggregation functions (#777)
+- Added `PiecewiseIntegralParameter` (#772)
+- Added support for including references to Python modules in JSON format (#765)
+- Added `CurrentYearThresholdParameter` and `CurrentOrdinalDayThresholdParameter` parameters (#789)
+
+### Bug fixes
+
+- Ensure `comment` key doesn't get passed to Pandas `read_xxx` functions (#788)
+
+### Documentation
+
+- Added some docs for AggregatedNode (#756)
+
+## v1.3.0
+
+### New Features
+
+- Allow use of parameters as values in `ControlCurveInterpolatedParameter` (#750)
+- Added `ScenarioWrapper` parameter (#763)
+
+### Bug fixes
+
+- Fix loading PiecewiseLink with parameters from JSON (#749)
+- Fixed a bug with `CSVRecorder` not saving volumes correctly (#767)
+
+### Miscellaneous
+
+- Removed `six` as dependency (#745)
+- Removed `pywr.__git_hash__` (#752)
+- Removed `Blender` node (#757)
+
+## v1.2.0
+
+### New Features
+
+- Support for embedding dataframe data directly in to JSON (#700)
+- Added `NumpyArrayAreaRecorder` and refactored storage recorders (#684)
+- Added getter for Recorder.agg_func (#719)
+- Add `DivisionParameter` and tests (#722)
+- Add `FlowParameter` for tracking yesterday's flow (#724)
+- Add `InterpolatedQuadratureParameter` (#714)
+- Add new array deficit recorders (#729):
+  - `NumpyArrayNodeDeficitRecorder` - timeseries of deficit.
+  - `NumpyArrayNodeSuppliedRatioRecorder` - timeseries of supply / demand
+  - `NumpyArrayNodeCurtailmentRatioRecorder` - timeseries of 1 - supply / demand
+
+### Bug fixes
+
+- Fix a bug with hydropower parameters & recorders not applying efficiency factor (#737)
+- Refactor of the code used to load named parameters and recorders to use shared functions (as they are both components) (#720)
+- Fix a bug with AggregatedRecorder not returning the instance on load (#723)
+- Use `flow` instead of `max_flow` in two_reservoirs test and example (#721)
+
+### Documentation
+
+- Added API documentation for nodes (#668)
+- Fix `PiecewiseLink` docstring's ASCII diagram (#668)
+
+### Miscellaneous
+
+- Clean-up various warnings in tests (#695)
+- Removed conda-recipe (Pywr is now in conda-forge) (#692)
+- Added codecov based coverage reporting (#705)
+- Updated test builds to use manylinux2010 to build wheels (#710)
+- Updated installation instructions to reflect wheels in Pypi and conda-forge installation.
+
+
+## v1.1.0
+
+### New features
+
+- New "edge based" GLPK solver. (#672)
+- Improved `FlowDurationDeviationRecorder` with JSON support and bug fixes when no scenario is given. (#677)
+
+### Bug fixes
+
+- Replace deprecated calls time time.clock with time.perf_counter. (#683)
+- Type optimisation in AggregatedIndexParameter. (#662)
+
+### Documentation 
+
+- Updated documentation: control curves, extending Pywr. (#652)
+
+### Miscellaneous
+
+- Variable renames and clean up for GLPK path solver. (#672)
+- Spport for Python 3.7. (#662)
+- Updated url in setup.py metadata to GitHub project (#661)
+- Additional cython compiler directives (#645)
+
+## v1.0.0
+
+### New features
+
+- Added ratchet support to threshold parameters. (#655)
+- Added `ConstantScenarioIndexParameter`. (#654)
+- Added support for Platypus and Pygmo optimisation wrappers. Involved a refactor of the existing optimisation support. (#610)
+- Added `HydropowerTargetParameter` to specify a flow target from a hydropower target. (#631)
+- Renamed `HydroPowerRecorder` to `HydropowerRecorder` (#631)
+- Better warning/error messages in `TablesArrayParameter` (#629)
+- Allow solver to be defined by the environment variable `PYWR_SOLVER`. (#619)
+- Added flow weights to `AggregatedNode`. (#603)
+- Added additional labeling functionality to notebook graphing functions. (#612)
+- New and improved variable API for Parameters. (#601, #258, #625)
+
+### Bug fixes
+
+- Fix bug setting the area property of `Storage` nodes. (#657)
+- Fixed bug with finally clause in the optimisation wrapper. (#649)
+- Fix a bug in `AnnualHarmonicSeriesParameter` related to updating the `amplitudes` and `phases` values with `set_double_variables` (#622)
+
+### Miscellaneous
+
+- Refactored several recorders and unified the use of temporal aggregation. This deprecated several keyword arguments in some existing Recorders. See the PR for details. (#635)
+- Removed deprecated (Monthly|Daily)ProfileControlCurve. (#640)
+- Dropped support for Python 2 and <3.6. Pywr is no longer tested against Python versions earlier than 3.6. (#623)
+- Use new `networkx.nodes_with_selfloops` function. (#628)
+- `AbstractProfileControlCurveParameter`, `MonthlyProfileControlCurveParameter` and `DailyProfileControlCurveParameter` have been removed after deprecation. (#231, #640)
+- Improved documentation. (#616, #627)
+
+
+## v0.5.1
+
+### Miscellaneous
+
+- Fixes to the source distribution (sdist) and inclusions of MANIFEST.in
+- Changes to the build systems on travis to enable deploy to Anaconda and Pypi.
+
+## v0.5
+
+### New features
+
+- Added build and testing for OS X via travis. (#588)
+- Added data consumption warnings to DataFrameParamter and TablesArrayParameter. (#562)
+- Added `PiecewiseLinearControlCurve` as a new cython parameter.
+- Pywr now emits some logging calls during a model run. 
+- Improved the event handling code to allow tracking of a `Parameter`'s value during an event.
+- Added support for initialising storage volume by percentage. Can be set in json through the `"initial_volume_pc"` property.
+- Added GZ2 and BZ2 compression support to `CSVRecorder`.
+- Added JSON support for license parameters. (#544)
+- Added a sense check to `TablesArrayParameter` having non-finite values.
+- Added scale and offset to ConstantParameter.
+- Added JSON support for license parameters. (#544)
+- Added `WeeklyProfileParameter` (#537)
+- Added `InterpolationParameter`. Closes #478. (#535)
+- Added surface area property to `Storage` (#525)
+- Added optional checksum when loading DataFrameParameter and TablesArrayParameter using hashlib.
+- Added ProgressRecorder and JupyterProgressRecorder (#520)
+- The threshold in `*ThresholdParameter` can now be a Parameter. (#517)
+- Added `HydroPowerRecorder` and `TotalHydroEnergyRecorder` for hydropower studies (#584)
+
+### Bug fixes
+- Explicitly set the frequency during dataframe resampling (#563)
+- `IndexedArrayParameter` also accepts 'parameters' in JSON. Closes #538. (#539)
+- Recursive deletion of child nodes when deleting compound nodes. (#527)
+- Compatibility with NetworkX 2.x (#529)
+- Changed GLPK log level to remove printing of superfluous messages to stdout. (#523)
+- Fixed loading parameters for `PiecewiseLink` in JSON. (#519)
+
+### Deprecated features
+- `AbstractProfileControlCurveParameter` marked for deprecation.
+
+### Miscellaneous
+- Improved the online documentation including the API reference. 
+- Added a hydropower example. 
+- General tweaks and corrections to class docstrings.
+- Updated conda build recipe to use the conda-forge lp_solve package.
+- Updated the conda build recipe to use MSVC features.
```

### Comparing `pywr-1.8.0/LICENSE.txt` & `pywr-1.9.0/LICENSE.txt`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,674 +1,674 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
-
-            How to Apply These Terms to Your New Programs
-
-  If you develop a new program, and you want it to be of the greatest
-possible use to the public, the best way to achieve this is to make it
-free software which everyone can redistribute and change under these terms.
-
-  To do so, attach the following notices to the program.  It is safest
-to attach them to the start of each source file to most effectively
-state the exclusion of warranty; and each file should have at least
-the "copyright" line and a pointer to where the full notice is found.
-
-    <one line to give the program's name and a brief idea of what it does.>
-    Copyright (C) <year>  <name of author>
-
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-
-Also add information on how to contact you by electronic and paper mail.
-
-  If the program does terminal interaction, make it output a short
-notice like this when it starts in an interactive mode:
-
-    <program>  Copyright (C) <year>  <name of author>
-    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
-    This is free software, and you are welcome to redistribute it
-    under certain conditions; type `show c' for details.
-
-The hypothetical commands `show w' and `show c' should show the appropriate
-parts of the General Public License.  Of course, your program's commands
-might be different; for a GUI interface, you would use an "about box".
-
-  You should also get your employer (if you work as a programmer) or school,
-if any, to sign a "copyright disclaimer" for the program, if necessary.
-For more information on this, and how to apply and follow the GNU GPL, see
-<http://www.gnu.org/licenses/>.
-
-  The GNU General Public License does not permit incorporating your program
-into proprietary programs.  If your program is a subroutine library, you
-may consider it more useful to permit linking proprietary applications with
-the library.  If this is what you want to do, use the GNU Lesser General
-Public License instead of this License.  But first, please read
-<http://www.gnu.org/philosophy/why-not-lgpl.html>.
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU General Public License is a free, copyleft license for
+software and other kinds of works.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+the GNU General Public License is intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.  We, the Free Software Foundation, use the
+GNU General Public License for most of our software; it applies also to
+any other work released this way by its authors.  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  To protect your rights, we need to prevent others from denying you
+these rights or asking you to surrender the rights.  Therefore, you have
+certain responsibilities if you distribute copies of the software, or if
+you modify it: responsibilities to respect the freedom of others.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must pass on to the recipients the same
+freedoms that you received.  You must make sure that they, too, receive
+or can get the source code.  And you must show them these terms so they
+know their rights.
+
+  Developers that use the GNU GPL protect your rights with two steps:
+(1) assert copyright on the software, and (2) offer you this License
+giving you legal permission to copy, distribute and/or modify it.
+
+  For the developers' and authors' protection, the GPL clearly explains
+that there is no warranty for this free software.  For both users' and
+authors' sake, the GPL requires that modified versions be marked as
+changed, so that their problems will not be attributed erroneously to
+authors of previous versions.
+
+  Some devices are designed to deny users access to install or run
+modified versions of the software inside them, although the manufacturer
+can do so.  This is fundamentally incompatible with the aim of
+protecting users' freedom to change the software.  The systematic
+pattern of such abuse occurs in the area of products for individuals to
+use, which is precisely where it is most unacceptable.  Therefore, we
+have designed this version of the GPL to prohibit the practice for those
+products.  If such problems arise substantially in other domains, we
+stand ready to extend this provision to those domains in future versions
+of the GPL, as needed to protect the freedom of users.
+
+  Finally, every program is threatened constantly by software patents.
+States should not allow patents to restrict development and use of
+software on general-purpose computers, but in those that do, we wish to
+avoid the special danger that patents applied to a free program could
+make it effectively proprietary.  To prevent this, the GPL assures that
+patents cannot be used to render the program non-free.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Use with the GNU Affero General Public License.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU Affero General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the special requirements of the GNU Affero General Public License,
+section 13, concerning interaction through a network will apply to the
+combination as such.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program.  If not, see <http://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If the program does terminal interaction, make it output a short
+notice like this when it starts in an interactive mode:
+
+    <program>  Copyright (C) <year>  <name of author>
+    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, your program's commands
+might be different; for a GUI interface, you would use an "about box".
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU GPL, see
+<http://www.gnu.org/licenses/>.
+
+  The GNU General Public License does not permit incorporating your program
+into proprietary programs.  If your program is a subroutine library, you
+may consider it more useful to permit linking proprietary applications with
+the library.  If this is what you want to do, use the GNU Lesser General
+Public License instead of this License.  But first, please read
+<http://www.gnu.org/philosophy/why-not-lgpl.html>.
```

### Comparing `pywr-1.8.0/PKG-INFO` & `pywr-1.9.0/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,98 +1,98 @@
-Metadata-Version: 2.1
-Name: pywr
-Version: 1.8.0
-Summary: Python Water Resource model
-Home-page: https://github.com/pywr/pywr
-Author: Joshua Arnott
-Author-email: josh@snorfalorpagus.net
-License: UNKNOWN
-Description: ====
-        Pywr
-        ====
-        
-        Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
-        
-        .. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
-           :target: https://travis-ci.org/pywr/pywr
-        
-        .. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh?svg=true
-           :target: https://ci.appveyor.com/project/pywr-admin/pywr
-        
-        .. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
-           :target: https://gitter.im/pywr/pywr
-        
-        .. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
-          :target: https://codecov.io/gh/pywr/pywr
-        
-        Overview
-        ========
-        
-        Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
-        
-        Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
-        
-        .. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
-           :width: 250px
-           :height: 190px
-        
-        New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
-        
-        Design goals
-        ============
-        
-        Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywrâ€™s principle design goals are that it is:
-        
-        - Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
-        - Free to use without restriction â€“ licensed under the GNU General Public Licence;
-        - Extendable â€“ uses the Python programming language to define complex operational rules and control model runs.
-        
-        Installation
-        ============
-        
-        Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
-        
-        See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
-        
-        Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
-        
-        .. code-block:: console
-        
-            python setup.py install --with-glpk --with-lpsolve
-        
-        For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
-        
-        Citation
-        ========
-        
-        Please consider citing the following paper when using Pywr:
-        
-        
-            Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
-        
-        
-        License
-        =======
-        
-        Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
-        
-        
-        This program is free software; you can redistribute it and/or modify
-        it under the terms of the GNU General Public License as published by
-        the Free Software Foundation; either version 1, or (at your option)
-        any later version.
-        
-        This program is distributed in the hope that it will be useful,
-        but WITHOUT ANY WARRANTY; without even the implied warranty of
-        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-        GNU General Public License for more details.
-        
-        You should have received a copy of the GNU General Public License
-        along with this program; if not, write to the Free Software
-        Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
-        
-Platform: UNKNOWN
-Description-Content-Type: text/x-rst
-Provides-Extra: docs
-Provides-Extra: test
-Provides-Extra: dev
-Provides-Extra: optimisation
+Metadata-Version: 2.1
+Name: pywr
+Version: 1.9.0
+Summary: Python Water Resource model
+Home-page: https://github.com/pywr/pywr
+Author: Joshua Arnott
+Author-email: josh@snorfalorpagus.net
+License: UNKNOWN
+Description: ====
+        Pywr
+        ====
+        
+        Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
+        
+        .. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
+           :target: https://travis-ci.org/pywr/pywr
+        
+        .. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh/branch/master?svg=true
+           :target: https://ci.appveyor.com/project/pywr-admin/pywr
+        
+        .. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
+           :target: https://gitter.im/pywr/pywr
+        
+        .. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
+          :target: https://codecov.io/gh/pywr/pywr
+        
+        Overview
+        ========
+        
+        Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
+        
+        Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
+        
+        .. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
+           :width: 250px
+           :height: 190px
+        
+        New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
+        
+        Design goals
+        ============
+        
+        Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywr’s principle design goals are that it is:
+        
+        - Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
+        - Free to use without restriction – licensed under the GNU General Public Licence;
+        - Extendable – uses the Python programming language to define complex operational rules and control model runs.
+        
+        Installation
+        ============
+        
+        Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
+        
+        See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
+        
+        Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
+        
+        .. code-block:: console
+        
+            python setup.py install --with-glpk --with-lpsolve
+        
+        For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
+        
+        Citation
+        ========
+        
+        Please consider citing the following paper when using Pywr:
+        
+        
+            Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
+        
+        
+        License
+        =======
+        
+        Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
+        
+        
+        This program is free software; you can redistribute it and/or modify
+        it under the terms of the GNU General Public License as published by
+        the Free Software Foundation; either version 1, or (at your option)
+        any later version.
+        
+        This program is distributed in the hope that it will be useful,
+        but WITHOUT ANY WARRANTY; without even the implied warranty of
+        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+        GNU General Public License for more details.
+        
+        You should have received a copy of the GNU General Public License
+        along with this program; if not, write to the Free Software
+        Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
+        
+Platform: UNKNOWN
+Description-Content-Type: text/x-rst
+Provides-Extra: docs
+Provides-Extra: test
+Provides-Extra: dev
+Provides-Extra: optimisation
```

### Comparing `pywr-1.8.0/README.rst` & `pywr-1.9.0/README.rst`

 * *Files 12% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-====
-Pywr
-====
-
-Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
-
-.. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
-   :target: https://travis-ci.org/pywr/pywr
-
-.. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh?svg=true
-   :target: https://ci.appveyor.com/project/pywr-admin/pywr
-
-.. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
-   :target: https://gitter.im/pywr/pywr
-
-.. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
-  :target: https://codecov.io/gh/pywr/pywr
-
-Overview
-========
-
-Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
-
-Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
-
-.. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
-   :width: 250px
-   :height: 190px
-
-New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
-
-Design goals
-============
-
-Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywr’s principle design goals are that it is:
-
-- Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
-- Free to use without restriction – licensed under the GNU General Public Licence;
-- Extendable – uses the Python programming language to define complex operational rules and control model runs.
-
-Installation
-============
-
-Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
-
-See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
-
-Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
-
-.. code-block:: console
-
-    python setup.py install --with-glpk --with-lpsolve
-
-For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
-
-Citation
-========
-
-Please consider citing the following paper when using Pywr:
-
-
-    Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
-
-
-License
-=======
-
-Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
-
-
-This program is free software; you can redistribute it and/or modify
-it under the terms of the GNU General Public License as published by
-the Free Software Foundation; either version 1, or (at your option)
-any later version.
-
-This program is distributed in the hope that it will be useful,
-but WITHOUT ANY WARRANTY; without even the implied warranty of
-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-GNU General Public License for more details.
-
-You should have received a copy of the GNU General Public License
-along with this program; if not, write to the Free Software
-Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
+====
+Pywr
+====
+
+Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
+
+.. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
+   :target: https://travis-ci.org/pywr/pywr
+
+.. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh/branch/master?svg=true
+   :target: https://ci.appveyor.com/project/pywr-admin/pywr
+
+.. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
+   :target: https://gitter.im/pywr/pywr
+
+.. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
+  :target: https://codecov.io/gh/pywr/pywr
+
+Overview
+========
+
+Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
+
+Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
+
+.. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
+   :width: 250px
+   :height: 190px
+
+New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
+
+Design goals
+============
+
+Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywr’s principle design goals are that it is:
+
+- Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
+- Free to use without restriction – licensed under the GNU General Public Licence;
+- Extendable – uses the Python programming language to define complex operational rules and control model runs.
+
+Installation
+============
+
+Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
+
+See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
+
+Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
+
+.. code-block:: console
+
+    python setup.py install --with-glpk --with-lpsolve
+
+For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
+
+Citation
+========
+
+Please consider citing the following paper when using Pywr:
+
+
+    Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
+
+
+License
+=======
+
+Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
+
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 1, or (at your option)
+any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
```

### Comparing `pywr-1.8.0/appveyor.yml` & `pywr-1.9.0/appveyor.yml`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-# Based on appveyor.yml from https://github.com/Toblerity/Fiona
-
-environment:
-
-  global:
-    BUILD_HOME: "C:\\build"
-
-    GLPK_VER: "4.65"
-    GLPK_HOME: "C:\\glpk-%GLPK_VER%"
-    GLPK_PKG_LIB: "glpk_4_65.lib"
-    GLPK_LIB: "glpk.lib"
-    GLPK_PKG_DLL: "glpk_4_65.dll"
-    GLPK_DLL: "glpk.dll"
-    GLPK_URL: "https://sourceforge.net/projects/winglpk/files/winglpk/GLPK-%GLPK_VER%/winglpk-%GLPK_VER%.zip/download"
-
-    LPSOLVE_VER: "5.5.2.5"
-    LPSOLVE_HOME: "C:\\lpsolve-%LPSOLVE_VER%"
-    LPSOLVE_LIB: "lpsolve55.lib"
-    LPSOLVE_DLL: "lpsolve55.dll"
-    LPSOLVE_URL: "https://sourceforge.net/projects/lpsolve/files/lpsolve/%LPSOLVE_VER%/lp_solve_%LPSOLVE_VER%_dev_win64.zip/download"
-
-    INCLUDE: "%GLPK_HOME%\\src;%LPSOLVE_HOME%"
-    LIB: "%GLPK_HOME%\\w64;%LPSOLVE_HOME%\\lpsolve"
-
-    PYTHON_ARCH: "64"
-
-  matrix:
-    # Python 3.6
-    - PYTHON: "C:\\Python36-x64"
-
-    # Python 3.7
-    - PYTHON: "C:\\Python37-x64"
-
-    # Python 3.8
-    - PYTHON: "C:\\Python38-x64"
-
-
-# We always use a 64-bit machine, but can build x86 distributions
-# with the TARGET_ARCH variable.
-platform:
-    - x64
-
-install:
-    - ECHO "Filesystem root:"
-    - ps: "ls \"C:/\""
-
-    - "SET PATH=%PYTHON%;%PYTHON%\\Scripts;%PATH%"
-    - "SET PYTHONPATH=%PYTHON%\\Lib\\site-packages;%PYTHONPATH%"
-
-    - "python --version"
-    - "python -c \"import struct; print(struct.calcsize('P') * 8)\""
-
-    # Create working directories
-    - ps: mkdir %BUILD_HOME% | out-null
-    - ps: mkdir %GLPK_HOME% | out-null
-    - ps: mkdir %LPSOLVE_HOME% | out-null
-
-    - curl -L %GLPK_URL% --output glpk.zip
-    - 7z x glpk.zip -oC:\
-    - cmd: echo "Installed GLPK:"
-    - ps: "ls \"C:/\""
-    - ps: "ls %GLPK_HOME%"
-
-    - curl -L --output lpsolve.zip %LPSOLVE_URL%
-    - 7z x lpsolve.zip -o%LPSOLVE_HOME%\lpsolve
-
-    - "SET PACKAGE_DATA=true"
-
-    - cd C:\projects\pywr
-    # Upgrade to the latest version of pip to avoid it displaying warnings
-    # about it being out of date.
-    - cmd: python -m pip install --disable-pip-version-check --user --upgrade pip
-
-    - cmd: pip --version
-    # Install the build dependencies of the project. If some dependencies contain
-    # compiled extensions and are not provided as pre-built wheel packages,
-    # pip will build them from source using the MSVC compiler matching the
-    # target Python version and architecture
-    - "pip install cython packaging numpy jupyter pytest wheel"
-
-    # Install optional dependencies for tests
-    # pygmo is not install because Win32 wheels are not supported.
-    - "pip install platypus-opt"
-
-build_script:
-  # Build the compiled extension
-  - cmd: echo %PATH%
-  - cmd: echo %PYTHONPATH%
-
-  # Rename (remove version number) the packaged library files for linking
-  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_LIB% %GLPK_HOME%\w64\%GLPK_LIB%
-  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_DLL% %GLPK_HOME%\w64\%GLPK_DLL%
-  # Copy GLPK libraries to package
-  - ps: mkdir pywr\.libs
-  - ps: mkdir pywr\.libs\licenses
-  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_DLL% pywr\.libs\%GLPK_PKG_DLL%
-  - cmd: copy %GLPK_HOME%\COPYING pywr\.libs\licenses\COPYING
-
-  - cmd: copy %LPSOLVE_HOME%\lpsolve\%LPSOLVE_DLL% pywr\.libs\%LPSOLVE_DLL%
-
-  # build pywr and create a wheel
-  - "python setup.py build_ext --with-glpk --with-lpsolve bdist_wheel"
-
-  # install the wheel
-  # - ps: python -m pip install --upgrade pip
-  - cmd: FOR %%i in (dist\*.whl) DO pip install --force-reinstall --ignore-installed %%i
-  - cmd: move pywr pywr.build
-
-test_script:
-  # Run the project tests
-  - cmd: SET
-
-  - ps: python -c "import pywr"
-  - ps: python -c "from pywr.solvers import cython_glpk"
-
-  - "SET PYWR_SOLVER=glpk"
-  - "python -m pytest"
-
-  - "SET PYWR_SOLVER=glpk-edge"
-  - "python -m pytest"
-
-  - "SET PYWR_SOLVER=lpsolve"
-  - "python -m pytest"
-
-artifacts:
-  - path: dist\*.whl
-    name: wheel
+# Based on appveyor.yml from https://github.com/Toblerity/Fiona
+
+environment:
+
+  global:
+    BUILD_HOME: "C:\\build"
+
+    GLPK_VER: "4.65"
+    GLPK_HOME: "C:\\glpk-%GLPK_VER%"
+    GLPK_PKG_LIB: "glpk_4_65.lib"
+    GLPK_LIB: "glpk.lib"
+    GLPK_PKG_DLL: "glpk_4_65.dll"
+    GLPK_DLL: "glpk.dll"
+    GLPK_URL: "https://sourceforge.net/projects/winglpk/files/winglpk/GLPK-%GLPK_VER%/winglpk-%GLPK_VER%.zip/download"
+
+    LPSOLVE_VER: "5.5.2.5"
+    LPSOLVE_HOME: "C:\\lpsolve-%LPSOLVE_VER%"
+    LPSOLVE_LIB: "lpsolve55.lib"
+    LPSOLVE_DLL: "lpsolve55.dll"
+    LPSOLVE_URL: "https://sourceforge.net/projects/lpsolve/files/lpsolve/%LPSOLVE_VER%/lp_solve_%LPSOLVE_VER%_dev_win64.zip/download"
+
+    INCLUDE: "%GLPK_HOME%\\src;%LPSOLVE_HOME%"
+    LIB: "%GLPK_HOME%\\w64;%LPSOLVE_HOME%\\lpsolve"
+
+    PYTHON_ARCH: "64"
+
+  matrix:
+    # Python 3.6
+    - PYTHON: "C:\\Python36-x64"
+
+    # Python 3.7
+    - PYTHON: "C:\\Python37-x64"
+
+    # Python 3.8
+    - PYTHON: "C:\\Python38-x64"
+
+
+# We always use a 64-bit machine, but can build x86 distributions
+# with the TARGET_ARCH variable.
+platform:
+    - x64
+
+install:
+    - ECHO "Filesystem root:"
+    - ps: "ls \"C:/\""
+
+    - "SET PATH=%PYTHON%;%PYTHON%\\Scripts;%PATH%"
+    - "SET PYTHONPATH=%PYTHON%\\Lib\\site-packages;%PYTHONPATH%"
+
+    - "python --version"
+    - "python -c \"import struct; print(struct.calcsize('P') * 8)\""
+
+    # Create working directories
+    - ps: mkdir %BUILD_HOME% | out-null
+    - ps: mkdir %GLPK_HOME% | out-null
+    - ps: mkdir %LPSOLVE_HOME% | out-null
+
+    - curl -L %GLPK_URL% --output glpk.zip
+    - 7z x glpk.zip -oC:\
+    - cmd: echo "Installed GLPK:"
+    - ps: "ls \"C:/\""
+    - ps: "ls %GLPK_HOME%"
+
+    - curl -L --output lpsolve.zip %LPSOLVE_URL%
+    - 7z x lpsolve.zip -o%LPSOLVE_HOME%\lpsolve
+
+    - "SET PACKAGE_DATA=true"
+
+    - cd C:\projects\pywr
+    # Upgrade to the latest version of pip to avoid it displaying warnings
+    # about it being out of date.
+    - cmd: python -m pip install --disable-pip-version-check --user --upgrade pip
+
+    - cmd: pip --version
+    # Install the build dependencies of the project. If some dependencies contain
+    # compiled extensions and are not provided as pre-built wheel packages,
+    # pip will build them from source using the MSVC compiler matching the
+    # target Python version and architecture
+    - "pip install cython packaging numpy jupyter pytest wheel"
+
+    # Install optional dependencies for tests
+    # pygmo is not install because Win32 wheels are not supported.
+    - "pip install platypus-opt"
+
+build_script:
+  # Build the compiled extension
+  - cmd: echo %PATH%
+  - cmd: echo %PYTHONPATH%
+
+  # Rename (remove version number) the packaged library files for linking
+  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_LIB% %GLPK_HOME%\w64\%GLPK_LIB%
+  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_DLL% %GLPK_HOME%\w64\%GLPK_DLL%
+  # Copy GLPK libraries to package
+  - ps: mkdir pywr\.libs
+  - ps: mkdir pywr\.libs\licenses
+  - cmd: copy %GLPK_HOME%\w64\%GLPK_PKG_DLL% pywr\.libs\%GLPK_PKG_DLL%
+  - cmd: copy %GLPK_HOME%\COPYING pywr\.libs\licenses\COPYING
+
+  - cmd: copy %LPSOLVE_HOME%\lpsolve\%LPSOLVE_DLL% pywr\.libs\%LPSOLVE_DLL%
+
+  # build pywr and create a wheel
+  - "python setup.py build_ext --with-glpk --with-lpsolve bdist_wheel"
+
+  # install the wheel
+  # - ps: python -m pip install --upgrade pip
+  - cmd: FOR %%i in (dist\*.whl) DO pip install --force-reinstall --ignore-installed %%i
+  - cmd: move pywr pywr.build
+
+test_script:
+  # Run the project tests
+  - cmd: SET
+
+  - ps: python -c "import pywr"
+  - ps: python -c "from pywr.solvers import cython_glpk"
+
+  - "SET PYWR_SOLVER=glpk"
+  - "python -m pytest"
+
+  - "SET PYWR_SOLVER=glpk-edge"
+  - "python -m pytest"
+
+  - "SET PYWR_SOLVER=lpsolve"
+  - "python -m pytest"
+
+artifacts:
+  - path: dist\*.whl
+    name: wheel
```

### Comparing `pywr-1.8.0/docs/make.bat` & `pywr-1.9.0/docs/make.bat`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/docs/presentations/Pywr introduction - September 2018.pdf` & `pywr-1.9.0/docs/presentations/Pywr introduction - September 2018.pdf`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/docs/source/_static/pywr_d3.png` & `pywr-1.9.0/docs/source/_static/pywr_d3.png`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/docs/source/api/pywr.optimisation.rst` & `pywr-1.9.0/docs/source/api/pywr.optimisation.rst`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-Optimisation
-============
-
-Submodules
-----------
-
-Platypus (pywr.optimisation.platypus)
--------------------------------------
-
-.. automodule:: pywr.optimisation.platypus
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
-
-Pygmo (pywr.optimisation.pygmo)
--------------------------------
-
-.. automodule:: pywr.optimisation.pygmo
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
-
-Module contents
----------------
-
-.. automodule:: pywr.optimisation
-    :members:
-    :undoc-members:
-    :show-inheritance:
+Optimisation
+============
+
+Submodules
+----------
+
+Platypus (pywr.optimisation.platypus)
+-------------------------------------
+
+.. automodule:: pywr.optimisation.platypus
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
+
+Pygmo (pywr.optimisation.pygmo)
+-------------------------------
+
+.. automodule:: pywr.optimisation.pygmo
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
+
+Module contents
+---------------
+
+.. automodule:: pywr.optimisation
+    :members:
+    :undoc-members:
+    :show-inheritance:
```

### Comparing `pywr-1.8.0/docs/source/conf.py` & `pywr-1.9.0/docs/source/conf.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,311 +1,311 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-#
-# Pywr documentation build configuration file, created by
-# sphinx-quickstart on Mon Jun  8 20:10:37 2015.
-#
-# This file is execfile()d with the current directory set to its
-# containing dir.
-#
-# Note that not all possible configuration values are present in this
-# autogenerated file.
-#
-# All configuration values have a default; values that are commented out
-# serve to show the default.
-
-import sys
-import os
-import shlex
-import pywr
-
-# If extensions (or modules to document with autodoc) are in another directory,
-# add these directories to sys.path here. If the directory is relative to the
-# documentation root, use os.path.abspath to make it absolute, like shown here.
-#sys.path.insert(0, os.path.abspath('.'))
-
-# -- General configuration ------------------------------------------------
-
-# If your documentation needs a minimal Sphinx version, state it here.
-#needs_sphinx = '1.0'
-
-# Add any Sphinx extension module names here, as strings. They can be
-# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
-# ones.
-extensions = [
-    'sphinx.ext.autodoc',
-    'sphinx.ext.autosummary',
-    'sphinx.ext.mathjax',
-    'matplotlib.sphinxext.plot_directive',
-    'numpydoc'
-]
-
-autosummary_generate = True
-numpydoc_show_class_members=False
-
-# Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
-
-# The suffix(es) of source filenames.
-# You can specify multiple suffix as a list of string:
-# source_suffix = ['.rst', '.md']
-source_suffix = '.rst'
-
-# The encoding of source files.
-#source_encoding = 'utf-8-sig'
-
-# The master toctree document.
-master_doc = 'index'
-
-# General information about the project.
-project = 'Pywr'
-copyright = 'Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester'
-author = 'Joshua Arnott, James E. Tomlinson'
-
-# The version info for the project you're documenting, acts as replacement for
-# |version| and |release|, also used in various other places throughout the
-# built documents.
-#
-# The short X.Y version.
-version = pywr.__version__
-# The full version, including alpha/beta/rc tags.
-release = version
-
-
-# The language for content autogenerated by Sphinx. Refer to documentation
-# for a list of supported languages.
-#
-# This is also used if you do content translation via gettext catalogs.
-# Usually you set "language" from the command line for these cases.
-language = None
-
-# There are two options for replacing |today|: either, you set today to some
-# non-false value, then it is used:
-#today = ''
-# Else, today_fmt is used as the format for a strftime call.
-#today_fmt = '%B %d, %Y'
-
-# List of patterns, relative to source directory, that match files and
-# directories to ignore when looking for source files.
-exclude_patterns = []
-
-# The reST default role (used for this markup: `text`) to use for all
-# documents.
-#default_role = None
-
-# If true, '()' will be appended to :func: etc. cross-reference text.
-#add_function_parentheses = True
-
-# If true, the current module name will be prepended to all description
-# unit titles (such as .. function::).
-#add_module_names = True
-
-# If true, sectionauthor and moduleauthor directives will be shown in the
-# output. They are ignored by default.
-#show_authors = False
-
-# The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
-
-# A list of ignored prefixes for module index sorting.
-#modindex_common_prefix = []
-
-# If true, keep warnings as "system message" paragraphs in the built documents.
-#keep_warnings = False
-
-# If true, `todo` and `todoList` produce output, else they produce nothing.
-todo_include_todos = False
-
-
-# -- Options for HTML output ----------------------------------------------
-#
-# # The theme to use for HTML and HTML Help pages.  See the documentation for
-# # a list of builtin themes.
-# html_theme = 'sphinxdoc'
-
-html_theme = "sphinx_rtd_theme"
-
-#
-# # Theme options are theme-specific and customize the look and feel of a theme
-# # further.  For a list of options available for each theme, see the
-# # documentation.
-# html_theme_options = {
-#     'github_user': 'pywr',
-#     'github_repo': 'pywr',
-# }
-#
-# # Add any paths that contain custom themes here, relative to this directory.
-# html_theme_path = [alabaster.get_path()]
-
-# The name for this set of Sphinx documents.  If None, it defaults to
-# "<project> v<release> documentation".
-#html_title = None
-
-# A shorter title for the navigation bar.  Default is the same as html_title.
-#html_short_title = None
-
-# The name of an image file (relative to this directory) to place at the top
-# of the sidebar.
-#html_logo = None
-
-# The name of an image file (within the static path) to use as favicon of the
-# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
-# pixels large.
-#html_favicon = None
-
-# Add any paths that contain custom static files (such as style sheets) here,
-# relative to this directory. They are copied after the builtin static files,
-# so a file named "default.css" will overwrite the builtin "default.css".
-html_static_path = ['_static']
-
-# Add any extra paths that contain custom files (such as robots.txt or
-# .htaccess) here, relative to this directory. These files are copied
-# directly to the root of the documentation.
-#html_extra_path = []
-
-# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
-# using the given strftime format.
-#html_last_updated_fmt = '%b %d, %Y'
-
-# If true, SmartyPants will be used to convert quotes and dashes to
-# typographically correct entities.
-#html_use_smartypants = True
-
-# Custom sidebar templates, maps document names to template names.
-# Needed for alabaster theme
-# html_sidebars = {
-#     '**': [
-#         'about.html',
-#         'navigation.html',
-#         'relations.html',
-#         'searchbox.html',
-#         'donate.html',
-#     ]
-# }
-
-# Additional templates that should be rendered to pages, maps page names to
-# template names.
-#html_additional_pages = {}
-
-# If false, no module index is generated.
-#html_domain_indices = True
-
-# If false, no index is generated.
-#html_use_index = True
-
-# If true, the index is split into individual pages for each letter.
-#html_split_index = False
-
-# If true, links to the reST sources are added to the pages.
-#html_show_sourcelink = True
-
-# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
-#html_show_sphinx = True
-
-# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
-#html_show_copyright = True
-
-# If true, an OpenSearch description file will be output, and all pages will
-# contain a <link> tag referring to it.  The value of this option must be the
-# base URL from which the finished HTML is served.
-#html_use_opensearch = ''
-
-# This is the file name suffix for HTML files (e.g. ".xhtml").
-#html_file_suffix = None
-
-# Language to be used for generating the HTML full-text search index.
-# Sphinx supports the following languages:
-#   'da', 'de', 'en', 'es', 'fi', 'fr', 'h', 'it', 'ja'
-#   'nl', 'no', 'pt', 'ro', 'r', 'sv', 'tr'
-#html_search_language = 'en'
-
-# A dictionary with options for the search language support, empty by default.
-# Now only 'ja' uses this config value
-#html_search_options = {'type': 'default'}
-
-# The name of a javascript file (relative to the configuration directory) that
-# implements a search results scorer. If empty, the default will be used.
-#html_search_scorer = 'scorer.js'
-
-# Output file base name for HTML help builder.
-htmlhelp_basename = 'Pywrdoc'
-
-# -- Options for LaTeX output ---------------------------------------------
-
-latex_elements = {
-# The paper size ('letterpaper' or 'a4paper').
-#'papersize': 'letterpaper',
-
-# The font size ('10pt', '11pt' or '12pt').
-#'pointsize': '10pt',
-
-# Additional stuff for the LaTeX preamble.
-#'preamble': '',
-
-# Latex figure (float) alignment
-#'figure_align': 'htbp',
-}
-
-# Grouping the document tree into LaTeX files. List of tuples
-# (source start file, target name, title,
-#  author, documentclass [howto, manual, or own class]).
-latex_documents = [
-  (master_doc, 'Pywr.tex', 'Pywr Documentation',
-   'Joshua Arnott', 'manual'),
-]
-
-# The name of an image file (relative to this directory) to place at the top of
-# the title page.
-#latex_logo = None
-
-# For "manual" documents, if this is true, then toplevel headings are parts,
-# not chapters.
-#latex_use_parts = False
-
-# If true, show page references after internal links.
-#latex_show_pagerefs = False
-
-# If true, show URL addresses after external links.
-#latex_show_urls = False
-
-# Documents to append as an appendix to all manuals.
-#latex_appendices = []
-
-# If false, no module index is generated.
-#latex_domain_indices = True
-
-
-# -- Options for manual page output ---------------------------------------
-
-# One entry per manual page. List of tuples
-# (source start file, name, description, authors, manual section).
-man_pages = [
-    (master_doc, 'pywr', 'Pywr Documentation',
-     [author], 1)
-]
-
-# If true, show URL addresses after external links.
-#man_show_urls = False
-
-
-# -- Options for Texinfo output -------------------------------------------
-
-# Grouping the document tree into Texinfo files. List of tuples
-# (source start file, target name, title, author,
-#  dir menu entry, description, category)
-texinfo_documents = [
-  (master_doc, 'Pywr', 'Pywr Documentation',
-   author, 'Pywr', 'One line description of project.',
-   'Miscellaneous'),
-]
-
-# Documents to append as an appendix to all manuals.
-#texinfo_appendices = []
-
-# If false, no module index is generated.
-#texinfo_domain_indices = True
-
-# How to display URL addresses: 'footnote', 'no', or 'inline'.
-#texinfo_show_urls = 'footnote'
-
-# If true, do not generate a @detailmenu in the "Top" node's menu.
-#texinfo_no_detailmenu = False
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+#
+# Pywr documentation build configuration file, created by
+# sphinx-quickstart on Mon Jun  8 20:10:37 2015.
+#
+# This file is execfile()d with the current directory set to its
+# containing dir.
+#
+# Note that not all possible configuration values are present in this
+# autogenerated file.
+#
+# All configuration values have a default; values that are commented out
+# serve to show the default.
+
+import sys
+import os
+import shlex
+import pywr
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+#sys.path.insert(0, os.path.abspath('.'))
+
+# -- General configuration ------------------------------------------------
+
+# If your documentation needs a minimal Sphinx version, state it here.
+#needs_sphinx = '1.0'
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+    'sphinx.ext.autodoc',
+    'sphinx.ext.autosummary',
+    'sphinx.ext.mathjax',
+    'matplotlib.sphinxext.plot_directive',
+    'numpydoc'
+]
+
+autosummary_generate = True
+numpydoc_show_class_members=False
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['_templates']
+
+# The suffix(es) of source filenames.
+# You can specify multiple suffix as a list of string:
+# source_suffix = ['.rst', '.md']
+source_suffix = '.rst'
+
+# The encoding of source files.
+#source_encoding = 'utf-8-sig'
+
+# The master toctree document.
+master_doc = 'index'
+
+# General information about the project.
+project = 'Pywr'
+copyright = 'Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester'
+author = 'Joshua Arnott, James E. Tomlinson'
+
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
+# The short X.Y version.
+version = pywr.__version__
+# The full version, including alpha/beta/rc tags.
+release = version
+
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#
+# This is also used if you do content translation via gettext catalogs.
+# Usually you set "language" from the command line for these cases.
+language = None
+
+# There are two options for replacing |today|: either, you set today to some
+# non-false value, then it is used:
+#today = ''
+# Else, today_fmt is used as the format for a strftime call.
+#today_fmt = '%B %d, %Y'
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+exclude_patterns = []
+
+# The reST default role (used for this markup: `text`) to use for all
+# documents.
+#default_role = None
+
+# If true, '()' will be appended to :func: etc. cross-reference text.
+#add_function_parentheses = True
+
+# If true, the current module name will be prepended to all description
+# unit titles (such as .. function::).
+#add_module_names = True
+
+# If true, sectionauthor and moduleauthor directives will be shown in the
+# output. They are ignored by default.
+#show_authors = False
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = 'sphinx'
+
+# A list of ignored prefixes for module index sorting.
+#modindex_common_prefix = []
+
+# If true, keep warnings as "system message" paragraphs in the built documents.
+#keep_warnings = False
+
+# If true, `todo` and `todoList` produce output, else they produce nothing.
+todo_include_todos = False
+
+
+# -- Options for HTML output ----------------------------------------------
+#
+# # The theme to use for HTML and HTML Help pages.  See the documentation for
+# # a list of builtin themes.
+# html_theme = 'sphinxdoc'
+
+html_theme = "sphinx_rtd_theme"
+
+#
+# # Theme options are theme-specific and customize the look and feel of a theme
+# # further.  For a list of options available for each theme, see the
+# # documentation.
+# html_theme_options = {
+#     'github_user': 'pywr',
+#     'github_repo': 'pywr',
+# }
+#
+# # Add any paths that contain custom themes here, relative to this directory.
+# html_theme_path = [alabaster.get_path()]
+
+# The name for this set of Sphinx documents.  If None, it defaults to
+# "<project> v<release> documentation".
+#html_title = None
+
+# A shorter title for the navigation bar.  Default is the same as html_title.
+#html_short_title = None
+
+# The name of an image file (relative to this directory) to place at the top
+# of the sidebar.
+#html_logo = None
+
+# The name of an image file (within the static path) to use as favicon of the
+# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
+# pixels large.
+#html_favicon = None
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+html_static_path = ['_static']
+
+# Add any extra paths that contain custom files (such as robots.txt or
+# .htaccess) here, relative to this directory. These files are copied
+# directly to the root of the documentation.
+#html_extra_path = []
+
+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
+# using the given strftime format.
+#html_last_updated_fmt = '%b %d, %Y'
+
+# If true, SmartyPants will be used to convert quotes and dashes to
+# typographically correct entities.
+#html_use_smartypants = True
+
+# Custom sidebar templates, maps document names to template names.
+# Needed for alabaster theme
+# html_sidebars = {
+#     '**': [
+#         'about.html',
+#         'navigation.html',
+#         'relations.html',
+#         'searchbox.html',
+#         'donate.html',
+#     ]
+# }
+
+# Additional templates that should be rendered to pages, maps page names to
+# template names.
+#html_additional_pages = {}
+
+# If false, no module index is generated.
+#html_domain_indices = True
+
+# If false, no index is generated.
+#html_use_index = True
+
+# If true, the index is split into individual pages for each letter.
+#html_split_index = False
+
+# If true, links to the reST sources are added to the pages.
+#html_show_sourcelink = True
+
+# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
+#html_show_sphinx = True
+
+# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
+#html_show_copyright = True
+
+# If true, an OpenSearch description file will be output, and all pages will
+# contain a <link> tag referring to it.  The value of this option must be the
+# base URL from which the finished HTML is served.
+#html_use_opensearch = ''
+
+# This is the file name suffix for HTML files (e.g. ".xhtml").
+#html_file_suffix = None
+
+# Language to be used for generating the HTML full-text search index.
+# Sphinx supports the following languages:
+#   'da', 'de', 'en', 'es', 'fi', 'fr', 'h', 'it', 'ja'
+#   'nl', 'no', 'pt', 'ro', 'r', 'sv', 'tr'
+#html_search_language = 'en'
+
+# A dictionary with options for the search language support, empty by default.
+# Now only 'ja' uses this config value
+#html_search_options = {'type': 'default'}
+
+# The name of a javascript file (relative to the configuration directory) that
+# implements a search results scorer. If empty, the default will be used.
+#html_search_scorer = 'scorer.js'
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = 'Pywrdoc'
+
+# -- Options for LaTeX output ---------------------------------------------
+
+latex_elements = {
+# The paper size ('letterpaper' or 'a4paper').
+#'papersize': 'letterpaper',
+
+# The font size ('10pt', '11pt' or '12pt').
+#'pointsize': '10pt',
+
+# Additional stuff for the LaTeX preamble.
+#'preamble': '',
+
+# Latex figure (float) alignment
+#'figure_align': 'htbp',
+}
+
+# Grouping the document tree into LaTeX files. List of tuples
+# (source start file, target name, title,
+#  author, documentclass [howto, manual, or own class]).
+latex_documents = [
+  (master_doc, 'Pywr.tex', 'Pywr Documentation',
+   'Joshua Arnott', 'manual'),
+]
+
+# The name of an image file (relative to this directory) to place at the top of
+# the title page.
+#latex_logo = None
+
+# For "manual" documents, if this is true, then toplevel headings are parts,
+# not chapters.
+#latex_use_parts = False
+
+# If true, show page references after internal links.
+#latex_show_pagerefs = False
+
+# If true, show URL addresses after external links.
+#latex_show_urls = False
+
+# Documents to append as an appendix to all manuals.
+#latex_appendices = []
+
+# If false, no module index is generated.
+#latex_domain_indices = True
+
+
+# -- Options for manual page output ---------------------------------------
+
+# One entry per manual page. List of tuples
+# (source start file, name, description, authors, manual section).
+man_pages = [
+    (master_doc, 'pywr', 'Pywr Documentation',
+     [author], 1)
+]
+
+# If true, show URL addresses after external links.
+#man_show_urls = False
+
+
+# -- Options for Texinfo output -------------------------------------------
+
+# Grouping the document tree into Texinfo files. List of tuples
+# (source start file, target name, title, author,
+#  dir menu entry, description, category)
+texinfo_documents = [
+  (master_doc, 'Pywr', 'Pywr Documentation',
+   author, 'Pywr', 'One line description of project.',
+   'Miscellaneous'),
+]
+
+# Documents to append as an appendix to all manuals.
+#texinfo_appendices = []
+
+# If false, no module index is generated.
+#texinfo_domain_indices = True
+
+# How to display URL addresses: 'footnote', 'no', or 'inline'.
+#texinfo_show_urls = 'footnote'
+
+# If true, do not generate a @detailmenu in the "Top" node's menu.
+#texinfo_no_detailmenu = False
```

### Comparing `pywr-1.8.0/docs/source/cookbook/aggregated_parameter.rst` & `pywr-1.9.0/docs/source/cookbook/aggregated_parameter.rst`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,123 +1,123 @@
-Aggregated parameters
----------------------
-
-Basic usage
-===========
-
-An aggregated parameter returns the values of its child parameters aggregated using an *aggregation function*. The following aggregation functions are available: sum, minimum, maximum, mean (value only), median (value only), product (value only), any (index only), all (index only), custom.
-
-There are two kinds of aggregated parameter: ``AggregatedParameter`` and ``AggregatedIndexParameter``, referred to as "value" and "index" in this document. A value aggregated parameter aggregates the values of it's children, while the index aggregated parameter does the same for the index of ``IndexParameter``.
-
-An example of creating an ``AggregatedParameter`` in Python is given below. The value of the aggregated parameter is the product of the baseline and profile values, e.g. in January the value is 5.0 * 0.8 = 4.0.
-
-.. code-block:: python
-
-    baseline = ConstantParameter(5.0)
-    profile = MonthlyParameter(
-        [0.8, 0.8, 0.8, 0.8, 1.1, 1.1, 1.1, 1.1, 0.8, 0.8, 0.8, 0.8]
-    )
-    agg = AggregatedParameter(
-        parameters=[baseline, profile],
-        agg_func="product"
-    )
-
-An example of creating an ``AggregatedParameter`` in JSON is given below. This parameter aggregates three parameters, one of which is a constant while the other two are references to parameters named in the ``"parameters"`` section.
-
-.. code-block:: javascript
-
-    {
-        "type": "aggregated",
-        "agg_func": "product",
-        "parameters": [
-            104.7
-            "monthly_demand_profile",
-            "demand_saving_factor"
-        ]
-    }
-
-Aggregated parameters can be used to build up complex functions. A more detailed explanation of the above example can be found in :ref:`demand_saving`.
-
-Sum, Min, Max, Product
-======================
-
-The ``"sum"``, ``"min"``, ``"max"`` and ``"product"`` aggregation functions are available to both aggregated value and aggregated index parameters.
-
-Mean and Median
-===============
-
-The ``"mean"`` and ``"median"`` aggregation functions are only available for aggregated value parameters, as they could return non-integer values.
-
-Any and All
-===========
-
-The ``any`` and ``all`` aggregation functions behave like their NumPy equivalents, ``numpy.any`` and ``numpy.all`` , returning 0 or 1 depending on whether any or all of the child values are truthy (i.e. non-zero).
-
-An example use is an index parameter which is "on" when any reservoir in a group is below its control curve.
-
-Custom aggregation functions
-============================
-
-Custom aggregation functions can be used via the Python API only. The function is called with the values of the individual parameters and should return the aggregated value. For example, the following aggregation function returns the 25th percentile of the values:
-
-.. code-block:: python
-
-    func = lambda x: np.percentile(x, 25)
-    parameter = AggregatedParameter(parameters, agg_func=func)
-
-MaxParameter, MinParameter and NegativeParameter, NegativeMaxParameter
-======================================================================
-
-The Max/Min/Negative parameters are optimised aggregation functions for some common operations, which aggregate a single parameter and a constant.
-
-The examples below compare the "max" aggregation function in ``AggregatedParameter`` to the ``MaxParameter``. The JSON required is shorter, arguably more readable and quicker to evaluate.
-
-.. code-block:: javascript
-
-    {
-        "type": "aggregated",
-        "agg_func": "max",
-        "parameters": [
-            "another_parameter",
-            0.0
-        ]
-    }
-
-.. code-block:: javascript
-
-    {
-        "type": "max",
-        "parameter": "another_parameter"
-        "threshold": 0.0
-    }
-
-An example use of these functions is to handle the net inflow timeseries for a reservoir, which includes both positive flows (net gain) and negative flows (net evaporation / leakage). If the original parameter is given as *X*, the positive component can be achieved using ``max(X, 0)`` and attached to an ``Input`` node. The negative component needs to be made positive, as ``Outputs`` require positive flows, using ``max(negative(X))``. This setup is shown in JSON below.
-
-.. code-block:: javascript
-
-    "original": ...
-
-    "inflow": {
-        "type": "max",
-        "parameter": "original"
-        "threshold": 0.0
-    }
-    
-    "evaporation": {
-        "type": "max",
-        "parameter": {
-            "type": "negative",
-            "parameter": "original"
-            "threshold": 0.0
-        }
-        "threshold": 0.0
-    }
-
-The pattern above was common enough to warrant the creation of the ``NegativeMaxParameter``.
-
-.. code-block:: javascript
-
-    "evaporation": {
-        "type": "negativemax",
-        "parameter": "original",
-        "threshold": 0.0
-    }
+Aggregated parameters
+---------------------
+
+Basic usage
+===========
+
+An aggregated parameter returns the values of its child parameters aggregated using an *aggregation function*. The following aggregation functions are available: sum, minimum, maximum, mean (value only), median (value only), product (value only), any (index only), all (index only), custom.
+
+There are two kinds of aggregated parameter: ``AggregatedParameter`` and ``AggregatedIndexParameter``, referred to as "value" and "index" in this document. A value aggregated parameter aggregates the values of it's children, while the index aggregated parameter does the same for the index of ``IndexParameter``.
+
+An example of creating an ``AggregatedParameter`` in Python is given below. The value of the aggregated parameter is the product of the baseline and profile values, e.g. in January the value is 5.0 * 0.8 = 4.0.
+
+.. code-block:: python
+
+    baseline = ConstantParameter(5.0)
+    profile = MonthlyParameter(
+        [0.8, 0.8, 0.8, 0.8, 1.1, 1.1, 1.1, 1.1, 0.8, 0.8, 0.8, 0.8]
+    )
+    agg = AggregatedParameter(
+        parameters=[baseline, profile],
+        agg_func="product"
+    )
+
+An example of creating an ``AggregatedParameter`` in JSON is given below. This parameter aggregates three parameters, one of which is a constant while the other two are references to parameters named in the ``"parameters"`` section.
+
+.. code-block:: javascript
+
+    {
+        "type": "aggregated",
+        "agg_func": "product",
+        "parameters": [
+            104.7
+            "monthly_demand_profile",
+            "demand_saving_factor"
+        ]
+    }
+
+Aggregated parameters can be used to build up complex functions. A more detailed explanation of the above example can be found in :ref:`demand_saving`.
+
+Sum, Min, Max, Product
+======================
+
+The ``"sum"``, ``"min"``, ``"max"`` and ``"product"`` aggregation functions are available to both aggregated value and aggregated index parameters.
+
+Mean and Median
+===============
+
+The ``"mean"`` and ``"median"`` aggregation functions are only available for aggregated value parameters, as they could return non-integer values.
+
+Any and All
+===========
+
+The ``any`` and ``all`` aggregation functions behave like their NumPy equivalents, ``numpy.any`` and ``numpy.all`` , returning 0 or 1 depending on whether any or all of the child values are truthy (i.e. non-zero).
+
+An example use is an index parameter which is "on" when any reservoir in a group is below its control curve.
+
+Custom aggregation functions
+============================
+
+Custom aggregation functions can be used via the Python API only. The function is called with the values of the individual parameters and should return the aggregated value. For example, the following aggregation function returns the 25th percentile of the values:
+
+.. code-block:: python
+
+    func = lambda x: np.percentile(x, 25)
+    parameter = AggregatedParameter(parameters, agg_func=func)
+
+MaxParameter, MinParameter and NegativeParameter, NegativeMaxParameter
+======================================================================
+
+The Max/Min/Negative parameters are optimised aggregation functions for some common operations, which aggregate a single parameter and a constant.
+
+The examples below compare the "max" aggregation function in ``AggregatedParameter`` to the ``MaxParameter``. The JSON required is shorter, arguably more readable and quicker to evaluate.
+
+.. code-block:: javascript
+
+    {
+        "type": "aggregated",
+        "agg_func": "max",
+        "parameters": [
+            "another_parameter",
+            0.0
+        ]
+    }
+
+.. code-block:: javascript
+
+    {
+        "type": "max",
+        "parameter": "another_parameter"
+        "threshold": 0.0
+    }
+
+An example use of these functions is to handle the net inflow timeseries for a reservoir, which includes both positive flows (net gain) and negative flows (net evaporation / leakage). If the original parameter is given as *X*, the positive component can be achieved using ``max(X, 0)`` and attached to an ``Input`` node. The negative component needs to be made positive, as ``Outputs`` require positive flows, using ``max(negative(X))``. This setup is shown in JSON below.
+
+.. code-block:: javascript
+
+    "original": ...
+
+    "inflow": {
+        "type": "max",
+        "parameter": "original"
+        "threshold": 0.0
+    }
+    
+    "evaporation": {
+        "type": "max",
+        "parameter": {
+            "type": "negative",
+            "parameter": "original"
+            "threshold": 0.0
+        }
+        "threshold": 0.0
+    }
+
+The pattern above was common enough to warrant the creation of the ``NegativeMaxParameter``.
+
+.. code-block:: javascript
+
+    "evaporation": {
+        "type": "negativemax",
+        "parameter": "original",
+        "threshold": 0.0
+    }
```

### Comparing `pywr-1.8.0/docs/source/cookbook/control_curves.rst` & `pywr-1.9.0/docs/source/cookbook/control_curves.rst`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-Dynamic behaviour and control curves
-------------------------------------
-
-Control curves are an important concept of many Pywr models. They are the most common way in which complex dynamic
-behaviour is created. The typical application uses control curves to parameterise state dependent rules. For example,
-a reservoir release that is dependent on the current volume. In this section of the documentation we discuss
-different strategies for implementing dynamic behaviour and control curves in Pywr models.
-
-
-Basic concept
-=============
-
-It is very common for resource allocation models to include rules and behaviours that are dependent on the
-current state of the model. The most common example of state in Pywr models is the current volume in ``Storage``
-node(s). During a simulation this state will be updated each timestep. Resource allocation rules and constraints
-can be made dependent on this state.
-
-In Pywr this behaviour is implemented through ``Parameters``. Some ``Parameters``, as discussed below, use information
-(i.e. state) from nodes, recorders or other parameters within the model. When such parameters are used within a model
-the behaviour is said to be state dependent. A key advantage of such an approach is that operational rules are
-parameterised independently of the model's boundary conditions. Such rules should be capable of dynamically responding
-to different boundary conditions (e.g. changes in demand, future flow scenarios, etc.).
-
-
-Storage dependent control curves
-================================
-
-Pywr provides a few different ways to implement dynamic state dependent behaviour. The most common approach compares
-current volume in a ``Storage`` node against one or more curves. These are typically referred to as a *curves*
-because they are often defined as a profile which varies through the year. More complex systems will often contain
-multiple ordered control curves that define progressively changing behaviour.
-
-TBC
-
-Other control curves
-====================
-
-TBC
-
-Custom control curves
-=====================
-
-TBC
-
-See also :ref:`extending-pywr-parameters`.
+Dynamic behaviour and control curves
+------------------------------------
+
+Control curves are an important concept of many Pywr models. They are the most common way in which complex dynamic
+behaviour is created. The typical application uses control curves to parameterise state dependent rules. For example,
+a reservoir release that is dependent on the current volume. In this section of the documentation we discuss
+different strategies for implementing dynamic behaviour and control curves in Pywr models.
+
+
+Basic concept
+=============
+
+It is very common for resource allocation models to include rules and behaviours that are dependent on the
+current state of the model. The most common example of state in Pywr models is the current volume in ``Storage``
+node(s). During a simulation this state will be updated each timestep. Resource allocation rules and constraints
+can be made dependent on this state.
+
+In Pywr this behaviour is implemented through ``Parameters``. Some ``Parameters``, as discussed below, use information
+(i.e. state) from nodes, recorders or other parameters within the model. When such parameters are used within a model
+the behaviour is said to be state dependent. A key advantage of such an approach is that operational rules are
+parameterised independently of the model's boundary conditions. Such rules should be capable of dynamically responding
+to different boundary conditions (e.g. changes in demand, future flow scenarios, etc.).
+
+
+Storage dependent control curves
+================================
+
+Pywr provides a few different ways to implement dynamic state dependent behaviour. The most common approach compares
+current volume in a ``Storage`` node against one or more curves. These are typically referred to as a *curves*
+because they are often defined as a profile which varies through the year. More complex systems will often contain
+multiple ordered control curves that define progressively changing behaviour.
+
+TBC
+
+Other control curves
+====================
+
+TBC
+
+Custom control curves
+=====================
+
+TBC
+
+See also :ref:`extending-pywr-parameters`.
```

### Comparing `pywr-1.8.0/docs/source/cookbook/dataframes.rst` & `pywr-1.9.0/docs/source/cookbook/dataframes.rst`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,168 +1,168 @@
-Using external data
--------------------
-
-It's not always practical or desirable to store all the data required by a model in the JSON document. This is particularly true when working with large amounts of timeseries data, but is also applicable to data better stored in a tabular format.
-
-Many parameter types support loading data from an external file using the ``"url"`` keyword. The data is loaded during the setup/reset process, before the model is run.
-
-Supported formats
-=================
-
-Support for the following data formats is available via the `Pandas <http://pandas.pydata.org/pandas-docs/stable/io.html>`_ module.
-
-    * Comma separated values (.csv)
-    * Excel (.xls, .xlsx)
-    * HDF5 (.h5)
-
-**Warning:** When reading Excel documents formulae are supported but not re-evaluated and links are not updated; the value read is the value present when the document was last saved.
-
-When working with large amounts of timeseries data the `HDF5 format <https://www.hdfgroup.org/why-hdf/>`_ is recommended as it has superior read speeds. Data in an HDF5 file should be in the "fixed" format; this is achieved by passing the ``format="fixed"`` argument to ``DataFrame.to_hdf``. Where data access speed is critical, users are advised to look at the ``TablesArrayParameter`` parameter instead which supports very fast access via `pytables` directly (rather than indirectly via `pandas`).
-
-External data is read using the appropriate ``pandas.read_xxx`` function determined by the file extension (e.g. ``pandas.read_excel`` for xls/xlsx). Keywords that are not recognised by Pywr are passed on to these functions. For example, when reading timeseries data from a CSV you can parse the date strings into pandas timestamps by passing ``parse_dates=True`` (see example below).
-
-Examples
-========
-
-Timeseries
-~~~~~~~~~~
-
-The most common kind of data to store in an external file is timeseries data. For example, the flow used by a ``Catchment`` node.
-
-An example dataset is given below with three columns: a timestamp (used as the index), rainfall and flow.
-
-.. csv-table:: timeseries1.csv
-   :header: "Timestamp", "Rainfall", "Flow"
-   :widths: 15, 10, 10
-
-   "1910-01-01", 0.0, 23.920
-   "1910-01-02", 1.8, 22.140
-   "1910-01-03", 5.2, 22.570
-
-The parameter below references the `timeseries1.csv` file in its ``"url"``. The index column is defined by ``"index_col"`` and the data column is defined by the ``"column"`` keyword (in this case, ``"Flow"``). The ``"parse_dates"`` keyword is required in order to parse the dates from strings in the CSV file into pandas date objects.
-
-.. code-block:: javascript
-
-    "flow": {
-        "type": "dataframe",
-        "url": "timeseries1.csv",
-        "index_col": "Timestamp"
-        "parse_dates": true,
-        "column": "Flow"
-    }
-
-If the index column of the dataframe is a timestamp the parameter will support automatic resampling, if required. For example, if the external data is on a daily timestep the model can still be run on a weekly timestep. In this case the data mean flow for the week is used. Only subsampling is currently supported (e.g. you can go from daily to monthly, but not monthly to daily).
-
-Constants
-~~~~~~~~~
-
-Constant parameters can also load their data from an external source. This is useful when data with a common theme is stored in a table. For example, the demand for different nodes in the model.
-
-An example is given below with the population and demand for three different cities (not real data!).
-
-.. csv-table:: demands.csv
-   :header: "City", "Population", "Demand"
-   :widths: 15, 10, 10
-
-   "Oxford", "30,294", 20.3
-   "Cambridge", "28,403", 19.4
-   "London", "790,930", 520.9
-
-As in the previous example, the filename is passed to the ``"url"`` keyword. The ``"index_col"`` keyword defines which column should be used for the lookups, with the ``"index"`` keyword specifying the lookup key.
-
-.. code-block:: javascript
-
-   "max_flow_oxford": {
-       "type": "constant",
-       "url": "demands.csv",
-       "index_col": "City"
-       "index": "Oxford"
-       "column": "Demand"
-   }
-
-Monthly profiles
-~~~~~~~~~~~~~~~~
-
-Monthly profiles can also be loaded from an external data source. Instead of passing a ``"column"`` keyword, the parameter expects the data source to have 12 columns (plus 1 for the index). The names of the columns are not important.
-
-.. csv-table:: demands_monthly.csv
-   :header: "City", "Jan", "Feb", "Mar", "...", "Dec"
-   :widths: 15, 10, 10, 10, 10, 10
-
-   "Oxford", 23.43, 25.32, 24.24, "...", 21.24
-   "Cambridge", 11.23, 14.34, 13.23, "...", 12.23
-
-.. code-block:: javascript
-
-    "max_flow": {
-        "type": "monthlyprofile",
-        "url": "demands_monthly.csv",
-        "index_col": "City",
-        "index": "Oxford"
-    }
-
-Multi-index
-~~~~~~~~~~~
-
-Multi-indexing of dataframes is supported by passing a list to the ``"index_col"`` keyword. Both numeric and string indexes are valid.
-
-.. csv-table:: multiindex_data.csv
-    :header: "level", "node", "max_flow", "cost"
-    :widths: 10, 15, 10, 10
-
-    0,"demand1",10,-10
-    0,"demand2",20,-20
-    1,"demand1",100,-100
-    1,"demand2",200,-200
-
-.. code-block:: javascript
-
-    {
-        "name": "DC1",
-        "type": "output",
-        "max_flow": {
-            "type": "constant",
-            "url": "multiindex_data.csv",
-            "column": "max_flow",
-            "index": [0, "demand1"],
-            "index_col": ["level", "node"]
-        },
-        "cost": {
-            "type": "constant",
-            "url": "multiindex_data.csv",
-            "column": "cost",
-            "index": [1, "demand1"],
-            "index_col": ["level", "node"]
-        }
-    }
-
-In the example above, *max_flow* evaluates to 10 and *cost* evaluates to -100.
-
-Tables
-======
-
-Each time an external data source is referenced using the ``"url"`` keyword the data is reloaded from disk. If a dataset is going to be used multiple times in a model it can be defined in the ``"tables"`` section of the JSON document. In this way the data will only be loaded once. Parameters can then reference the data using the ``"table"`` keyword instead of the ``"url"`` keyword. Although the index column applied to the data must be defined in the ``"tables"`` section, the index used for each lookup can be different.
-
-An example is given below using the `demands.csv` dataset shown previously. Two constant parameters are defined referencing data in the table.
-
-.. code-block:: javascript
-
-    "parameters": {
-        "oxford_demand": {
-            "type": "constant",
-            "table": "simple_data",
-            "column": "Demand",
-            "index": "Oxford"
-        },
-        "cambridge_demand": {
-            "type": "constant",
-            "table": "simple_data",
-            "column": "Demand",
-            "index": "Cambridge"
-        }
-    },
-    "tables": {
-        "simple_data": {
-            "url": "demands.csv",
-            "index_col": "City"
-        }
-    }
+Using external data
+-------------------
+
+It's not always practical or desirable to store all the data required by a model in the JSON document. This is particularly true when working with large amounts of timeseries data, but is also applicable to data better stored in a tabular format.
+
+Many parameter types support loading data from an external file using the ``"url"`` keyword. The data is loaded during the setup/reset process, before the model is run.
+
+Supported formats
+=================
+
+Support for the following data formats is available via the `Pandas <http://pandas.pydata.org/pandas-docs/stable/io.html>`_ module.
+
+    * Comma separated values (.csv)
+    * Excel (.xls, .xlsx)
+    * HDF5 (.h5)
+
+**Warning:** When reading Excel documents formulae are supported but not re-evaluated and links are not updated; the value read is the value present when the document was last saved.
+
+When working with large amounts of timeseries data the `HDF5 format <https://www.hdfgroup.org/why-hdf/>`_ is recommended as it has superior read speeds. Data in an HDF5 file should be in the "fixed" format; this is achieved by passing the ``format="fixed"`` argument to ``DataFrame.to_hdf``. Where data access speed is critical, users are advised to look at the ``TablesArrayParameter`` parameter instead which supports very fast access via `pytables` directly (rather than indirectly via `pandas`).
+
+External data is read using the appropriate ``pandas.read_xxx`` function determined by the file extension (e.g. ``pandas.read_excel`` for xls/xlsx). Keywords that are not recognised by Pywr are passed on to these functions. For example, when reading timeseries data from a CSV you can parse the date strings into pandas timestamps by passing ``parse_dates=True`` (see example below).
+
+Examples
+========
+
+Timeseries
+~~~~~~~~~~
+
+The most common kind of data to store in an external file is timeseries data. For example, the flow used by a ``Catchment`` node.
+
+An example dataset is given below with three columns: a timestamp (used as the index), rainfall and flow.
+
+.. csv-table:: timeseries1.csv
+   :header: "Timestamp", "Rainfall", "Flow"
+   :widths: 15, 10, 10
+
+   "1910-01-01", 0.0, 23.920
+   "1910-01-02", 1.8, 22.140
+   "1910-01-03", 5.2, 22.570
+
+The parameter below references the `timeseries1.csv` file in its ``"url"``. The index column is defined by ``"index_col"`` and the data column is defined by the ``"column"`` keyword (in this case, ``"Flow"``). The ``"parse_dates"`` keyword is required in order to parse the dates from strings in the CSV file into pandas date objects.
+
+.. code-block:: javascript
+
+    "flow": {
+        "type": "dataframe",
+        "url": "timeseries1.csv",
+        "index_col": "Timestamp"
+        "parse_dates": true,
+        "column": "Flow"
+    }
+
+If the index column of the dataframe is a timestamp the parameter will support automatic resampling, if required. For example, if the external data is on a daily timestep the model can still be run on a weekly timestep. In this case the data mean flow for the week is used. Only subsampling is currently supported (e.g. you can go from daily to monthly, but not monthly to daily).
+
+Constants
+~~~~~~~~~
+
+Constant parameters can also load their data from an external source. This is useful when data with a common theme is stored in a table. For example, the demand for different nodes in the model.
+
+An example is given below with the population and demand for three different cities (not real data!).
+
+.. csv-table:: demands.csv
+   :header: "City", "Population", "Demand"
+   :widths: 15, 10, 10
+
+   "Oxford", "30,294", 20.3
+   "Cambridge", "28,403", 19.4
+   "London", "790,930", 520.9
+
+As in the previous example, the filename is passed to the ``"url"`` keyword. The ``"index_col"`` keyword defines which column should be used for the lookups, with the ``"index"`` keyword specifying the lookup key.
+
+.. code-block:: javascript
+
+   "max_flow_oxford": {
+       "type": "constant",
+       "url": "demands.csv",
+       "index_col": "City"
+       "index": "Oxford"
+       "column": "Demand"
+   }
+
+Monthly profiles
+~~~~~~~~~~~~~~~~
+
+Monthly profiles can also be loaded from an external data source. Instead of passing a ``"column"`` keyword, the parameter expects the data source to have 12 columns (plus 1 for the index). The names of the columns are not important.
+
+.. csv-table:: demands_monthly.csv
+   :header: "City", "Jan", "Feb", "Mar", "...", "Dec"
+   :widths: 15, 10, 10, 10, 10, 10
+
+   "Oxford", 23.43, 25.32, 24.24, "...", 21.24
+   "Cambridge", 11.23, 14.34, 13.23, "...", 12.23
+
+.. code-block:: javascript
+
+    "max_flow": {
+        "type": "monthlyprofile",
+        "url": "demands_monthly.csv",
+        "index_col": "City",
+        "index": "Oxford"
+    }
+
+Multi-index
+~~~~~~~~~~~
+
+Multi-indexing of dataframes is supported by passing a list to the ``"index_col"`` keyword. Both numeric and string indexes are valid.
+
+.. csv-table:: multiindex_data.csv
+    :header: "level", "node", "max_flow", "cost"
+    :widths: 10, 15, 10, 10
+
+    0,"demand1",10,-10
+    0,"demand2",20,-20
+    1,"demand1",100,-100
+    1,"demand2",200,-200
+
+.. code-block:: javascript
+
+    {
+        "name": "DC1",
+        "type": "output",
+        "max_flow": {
+            "type": "constant",
+            "url": "multiindex_data.csv",
+            "column": "max_flow",
+            "index": [0, "demand1"],
+            "index_col": ["level", "node"]
+        },
+        "cost": {
+            "type": "constant",
+            "url": "multiindex_data.csv",
+            "column": "cost",
+            "index": [1, "demand1"],
+            "index_col": ["level", "node"]
+        }
+    }
+
+In the example above, *max_flow* evaluates to 10 and *cost* evaluates to -100.
+
+Tables
+======
+
+Each time an external data source is referenced using the ``"url"`` keyword the data is reloaded from disk. If a dataset is going to be used multiple times in a model it can be defined in the ``"tables"`` section of the JSON document. In this way the data will only be loaded once. Parameters can then reference the data using the ``"table"`` keyword instead of the ``"url"`` keyword. Although the index column applied to the data must be defined in the ``"tables"`` section, the index used for each lookup can be different.
+
+An example is given below using the `demands.csv` dataset shown previously. Two constant parameters are defined referencing data in the table.
+
+.. code-block:: javascript
+
+    "parameters": {
+        "oxford_demand": {
+            "type": "constant",
+            "table": "simple_data",
+            "column": "Demand",
+            "index": "Oxford"
+        },
+        "cambridge_demand": {
+            "type": "constant",
+            "table": "simple_data",
+            "column": "Demand",
+            "index": "Cambridge"
+        }
+    },
+    "tables": {
+        "simple_data": {
+            "url": "demands.csv",
+            "index_col": "City"
+        }
+    }
```

### Comparing `pywr-1.8.0/docs/source/cookbook/demand_saving.rst` & `pywr-1.9.0/docs/source/cookbook/demand_saving.rst`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,122 +1,122 @@
-.. _demand_saving:
-
-Demand restrictions
--------------------
-
-Demand restrictions are a common concept in water resource models. When the state of a water resource is poor (for example, lower than normal reservoir levels) demand restrictions may be imposed on customers in order to reduce the likelihood of a failure to supply.
-
-The demand at a given timestep can be represented using the equation below,
-
-.. math::
-
-    d = d_{mean} * p * (1-s)
-
-where :math:`d` is the final demand to be calculated, :math:`d_{mean}` is the baseline demand, :math:`p` is the annual profile and :math:`s` is the demand restriction (e.g. 5%).
-
-The code snippets that follow belong in the ``"parameters": {}`` section of the model document (except for the ``node`` at the end).
-
-The baseline demand is specified as a constant (this is often the *mean* annual demand).
-
-.. code-block:: javascript
-
-    "demand_baseline": {
-        "type": "constant",
-        "values": 50
-    }
-
-A daily or monthly profile can be used to vary the demand throughout the year. In the example below the demand in May - August is 1.2x the baseline demand, with the rest of the year at 0.9x the baseline, forming the common "top hat" profile (illustrated below).
-
-.. code-block:: javascript
-
-    "demand_profile": {
-        "type": "monthlyprofile",
-        "values": [0.9, 0.9, 0.9, 0.9, 1.2, 1.2, 1.2, 1.2, 0.9, 0.9, 0.9, 0.9]
-    },
-
-.. plot:: pyplots/top_hat.py
-
-The demand restriction level describes the level of restrictions as an integer, where 0 means no demand restrictions, level 1 (L1) is some restriction (or perhaps just publicity), level 2 (L2) more severe restrictions, and so on. The specifics will depend on the system being modelled. This is represented in Pywr using the ``ControlCurveIndexParameter``. ``IndexParameter`` return an integer to be used as an index, rather than a decimal value.
-
-The demand restriction level is often related to the storage of strategic reservoirs in relation to a control curve (the expected reservoir volume at a given time of the year). The example below two demand restriction levels are defined (and implicitly a L0) based on the volume in the ``"Central Reservoir"`` storage node. L1 is defined as 80% of full, L2 as 50% of full. Control curves are commonly more complicated, as the expected level of a reservoir is usually lower in the summer than it is in the winter.
-
-.. code-block:: javascript
-
-    "demand_restriction_level": {
-        "type": "controlcurveindex",
-        "storage_node": "Central Reservoir",
-        "control_curves": [
-            "level1",
-            "level2"
-        ]
-    },
-    "level1": {
-        "type": "constant",
-        "values": 0.8
-    },
-    "level2": {
-        "type": "constant",
-        "values": 0.5
-    },
-
-The demand restriction factor is determined by indexing an array of possible restriction profiles with the demand restriction level (i.e. index). In the example below the list of profiles (``"params"``) corresponds to the L0, L1 and L2 profiles respectively. At L0 a constant factor ``1.0`` is used to represent no restrictions. At L1 there is a 10% reduction in demand (``0.90`` as a factor) during the summer months and a 5% reduction elsewhere (``0.95``). At L2 there are further reductions to 75%/80%.
-
-.. code-block:: javascript
-
-    "demand_restriction_factor": {
-        "type": "indexedarray",
-        "index_parameter": "demand_restriction_level",
-        "params": [
-            {
-                "type": "constant",
-                "values": 1.0
-            },
-            {
-                "type": "monthlyprofile",
-                "values": [0.95, 0.95, 0.95, 0.95, 0.90, 0.90, 0.90, 0.90, 0.95, 0.95, 0.95, 0.95]
-            },
-            {
-                "type": "monthlyprofile",
-                "values": [0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8]
-            }
-        ]
-    },
-
-To understand how the index works, the following equivalent Python code may help:
-
-.. code-block:: python
-
-    month = 5
-    demand_restriction_level = 1
-    demand_factors = [[1.0, ...], [0.95, ...], [0.8, ...]]
-    demand_restriction_factor = demand_factors[demand_restriction_level][month-1]
-
-Finally the demand components can be combined as in the equation at the beginning using an ``AggregatedParameter``. Each timestep the value of each of the components is calculated and the values are multiplied to give the final demand value.
-
-.. code-block:: javascript
-
-    "demand_max_flow": {
-        "type": "aggregated",
-        "agg_func": "product",
-        "parameters": [
-            "demand_baseline",
-            "demand_profile",
-            "demand_restriction_factor"
-        ]
-    },
-
-The final profiles are illustrated in the figure below. The actual demand value will switch between the three profiles depending on the resource state of the reservoir.
-
-.. plot:: pyplots/demand_saving_levels.py
-
-This parameter can then be applied to the ``max_flow`` attribute of the demand node.
-
-.. code-block:: javascript
-
-    {
-        "type": "output",
-        "name": "Demand",
-        "max_flow": "demand_max_flow",
-        "cost": -500
-    },
-
+.. _demand_saving:
+
+Demand restrictions
+-------------------
+
+Demand restrictions are a common concept in water resource models. When the state of a water resource is poor (for example, lower than normal reservoir levels) demand restrictions may be imposed on customers in order to reduce the likelihood of a failure to supply.
+
+The demand at a given timestep can be represented using the equation below,
+
+.. math::
+
+    d = d_{mean} * p * (1-s)
+
+where :math:`d` is the final demand to be calculated, :math:`d_{mean}` is the baseline demand, :math:`p` is the annual profile and :math:`s` is the demand restriction (e.g. 5%).
+
+The code snippets that follow belong in the ``"parameters": {}`` section of the model document (except for the ``node`` at the end).
+
+The baseline demand is specified as a constant (this is often the *mean* annual demand).
+
+.. code-block:: javascript
+
+    "demand_baseline": {
+        "type": "constant",
+        "values": 50
+    }
+
+A daily or monthly profile can be used to vary the demand throughout the year. In the example below the demand in May - August is 1.2x the baseline demand, with the rest of the year at 0.9x the baseline, forming the common "top hat" profile (illustrated below).
+
+.. code-block:: javascript
+
+    "demand_profile": {
+        "type": "monthlyprofile",
+        "values": [0.9, 0.9, 0.9, 0.9, 1.2, 1.2, 1.2, 1.2, 0.9, 0.9, 0.9, 0.9]
+    },
+
+.. plot:: pyplots/top_hat.py
+
+The demand restriction level describes the level of restrictions as an integer, where 0 means no demand restrictions, level 1 (L1) is some restriction (or perhaps just publicity), level 2 (L2) more severe restrictions, and so on. The specifics will depend on the system being modelled. This is represented in Pywr using the ``ControlCurveIndexParameter``. ``IndexParameter`` return an integer to be used as an index, rather than a decimal value.
+
+The demand restriction level is often related to the storage of strategic reservoirs in relation to a control curve (the expected reservoir volume at a given time of the year). The example below two demand restriction levels are defined (and implicitly a L0) based on the volume in the ``"Central Reservoir"`` storage node. L1 is defined as 80% of full, L2 as 50% of full. Control curves are commonly more complicated, as the expected level of a reservoir is usually lower in the summer than it is in the winter.
+
+.. code-block:: javascript
+
+    "demand_restriction_level": {
+        "type": "controlcurveindex",
+        "storage_node": "Central Reservoir",
+        "control_curves": [
+            "level1",
+            "level2"
+        ]
+    },
+    "level1": {
+        "type": "constant",
+        "values": 0.8
+    },
+    "level2": {
+        "type": "constant",
+        "values": 0.5
+    },
+
+The demand restriction factor is determined by indexing an array of possible restriction profiles with the demand restriction level (i.e. index). In the example below the list of profiles (``"params"``) corresponds to the L0, L1 and L2 profiles respectively. At L0 a constant factor ``1.0`` is used to represent no restrictions. At L1 there is a 10% reduction in demand (``0.90`` as a factor) during the summer months and a 5% reduction elsewhere (``0.95``). At L2 there are further reductions to 75%/80%.
+
+.. code-block:: javascript
+
+    "demand_restriction_factor": {
+        "type": "indexedarray",
+        "index_parameter": "demand_restriction_level",
+        "params": [
+            {
+                "type": "constant",
+                "values": 1.0
+            },
+            {
+                "type": "monthlyprofile",
+                "values": [0.95, 0.95, 0.95, 0.95, 0.90, 0.90, 0.90, 0.90, 0.95, 0.95, 0.95, 0.95]
+            },
+            {
+                "type": "monthlyprofile",
+                "values": [0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8]
+            }
+        ]
+    },
+
+To understand how the index works, the following equivalent Python code may help:
+
+.. code-block:: python
+
+    month = 5
+    demand_restriction_level = 1
+    demand_factors = [[1.0, ...], [0.95, ...], [0.8, ...]]
+    demand_restriction_factor = demand_factors[demand_restriction_level][month-1]
+
+Finally the demand components can be combined as in the equation at the beginning using an ``AggregatedParameter``. Each timestep the value of each of the components is calculated and the values are multiplied to give the final demand value.
+
+.. code-block:: javascript
+
+    "demand_max_flow": {
+        "type": "aggregated",
+        "agg_func": "product",
+        "parameters": [
+            "demand_baseline",
+            "demand_profile",
+            "demand_restriction_factor"
+        ]
+    },
+
+The final profiles are illustrated in the figure below. The actual demand value will switch between the three profiles depending on the resource state of the reservoir.
+
+.. plot:: pyplots/demand_saving_levels.py
+
+This parameter can then be applied to the ``max_flow`` attribute of the demand node.
+
+.. code-block:: javascript
+
+    {
+        "type": "output",
+        "name": "Demand",
+        "max_flow": "demand_max_flow",
+        "cost": -500
+    },
+
 When a model has more than one demand node it is OK to re-use the demand restriction level/factor for each demand node. Pywr will only calculate the index once for each parameter. Therefore, it is more efficient to share ``IndexParameter`` where possible.
```

### Comparing `pywr-1.8.0/docs/source/extending_pywr/extending_pywr_nodes.rst` & `pywr-1.9.0/docs/source/extending_pywr/extending_pywr_nodes.rst`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,125 +1,125 @@
-.. _extending-pywr-nodes:
-
-Extending Pywr with custom Nodes
---------------------------------
-
-Nodes and subclasses thereof provide the basic network structure in Pywr. There are several different types
-of node available. Two major categories exist: flow nodes and storage nodes. Flow nodes are the typical nodes
-that represent rivers, pipes and other features from, through or to which a resource can flow. These nodes are
-typically characterised by minimum and maximum flow rates. Storage nodes provide the ability to store resource
-from one time-step to another, and are characterised by minimum, maximum and initial volumes.
-
-There are three fundamental types of node in Pywr:
-
-* ``Input`` nodes add water to the system
-* ``Output`` nodes remove water from the system
-* ``Link`` nodes do not add or remove water from the system
-
-There is a fourth node type, ``Storage``, which can be considered fundamental because there are special rules for
-it's behaviour in the linear programme used to solve the water balance:
-
-* ``Storage`` nodes can carry water from one timestep to the next
-
-All other node types in Pywr are subclasses of these base types. For example, the ``pywr.nodes.Catchment`` node type
-is a special case of ``Input`` where the ``min_flow`` and ``max_flow`` properties are equal.
-
-The most common way to create a new node type is using a compound node. A compound node contains one or more existing
-nodes and is used to manage common or more complex arrangements of the basic node types. An example of a compound node
-is the ``PiecewiseLink``. It is composed of a link (``OUT_1``) which receives water from upstream and an link
-(``IN_1``) which conveys water downstream, connected by a set of links in parallel (``LNK_1`` ... ``LNK_N``) each with
-a different ``max_flow`` and ``cost``, illustrated below::
-
-                               /-->-- LNK_1 -->--\
-    UPSTREAM -->-- OUT_1 -->--|--->--  ...  -->---|-->-- IN_1 -->-- DOWNSTREAM
-                               \-->-- LNK_N -->--/
-
-Let's look at an example to create a new node type that represents a leaky pipe. To remove water from the system we
-need to use an output node (``LEAK``), with two links representing the boundaries of the compound node (``INFLOW`` and
-``OUTFLOW``)::
-
-    UPSTREAM -->-- INFLOW -->-- OUTFLOW -->-- DOWNSTREAM
-                     |
-                     \------>--  LEAK
-
-This is a simple structure which represents leakage as a demand with a maximum value and a benefit to be supplied. It
-is slightly flawed as the leakage volume does not vary proportionally to flow through the link, but is sufficient as
-an example:
-
-.. code-block:: python
-
-    from pywr.nodes import Node, Link, Output
-
-    class LeakyPipe(Node):
-        def __init__(self, leakage, leakage_cost=-99999, *args, **kwargs):
-            self.allow_isolated = True  # Required for compound nodes
-
-            super(LeakyPipe, self).__init__(*args, **kwargs)
-
-            # Define the internal nodes. The parent of the nodes is defined to identify them as sub-nodes.
-            self.inflow = Link(self.model, name='{} In'.format(self.name), parent=self)
-            self.outflow = Link(self.model, name='{} Out'.format(self.name), parent=self)
-            self.leak = Output(self.model, name='{} Leak'.format(self.name), parent=self)
-
-            # Connect the internal nodes
-            self.inflow.connect(self.outflow)
-            self.inflow.connect(self.leak)
-
-            # Define the properties of the leak (demand and benefit)
-            self.leak.max_flow = leakage
-            self.leak.cost = leakage_cost
-
-        def iter_slots(self, slot_name=None, is_connector=True):
-            # This is required so that connecting to this node actually connects to the outflow sub-node, and
-            # connecting from this node actually connects to the input sub-node
-            if is_connector:
-                yield self.outflow
-            else:
-                yield self.inflow
-
-        def after(self, timestep):
-            # Make the flow on the compound node appear as the flow _after_ the leak
-            self.commit_all(self.outflow.flow)
-            # Make sure save is done after setting aggregated flow
-            super(LeakyPipe, self).after(timestep)
-
-        @classmethod
-        def load(cls, data, model):
-            del(data["type"])
-            leakage = data.pop("leakage")
-            leakage_cost = data.pop("leakage_cost", None)
-            return cls(model, leakage, leakage_cost, **data)
-
-
-The custom node does not need to be "registered", unlike ``Parameters``, as this is done automatically using
-metaclasses. The new node type can be referenced from a JSON model provided that the class has been imported before
-the JSON is loaded:
-
-.. code-block:: python
-
-    from pywr.model import Model
-    import leakypipe
-    
-    model = Model.load("leaky_pipe_model.sjon")
-
-
-.. code-block:: yaml
-
-    {
-        "type": "leakypipe",
-        "leakage": "1.0"
-    }
-
-The ``allow_isolated`` attribute identifies nodes of this type as compound nodes. Without this the model would raise
-an error that the node is not connected to the rest of the network, as the connections are actually to its sub-nodes.
-
-The ``after`` method is not required but is useful so that recorders can be attached to the compound node. Without
-this the flow would appear to be zero as the flow doesn't *actually* pass through the compound node.
-
-The ``iter_slots`` method is required so that connecting to/from the node (e.g. ``upstream.connect(leaky)``) creates
-connections to the sub-nodes.
-
-A more advanced representation of the leaky pipe could use an additional ``AggregatedNode`` to constrain the ratio
-of flow through the ``OUTFLOW`` and ``LEAK`` nodes. [*]_
-
-.. [*] ``AggregatedNode`` is actually another fundamental node type, as this behaviour requires special treatment
-       in the linear programme.
+.. _extending-pywr-nodes:
+
+Extending Pywr with custom Nodes
+--------------------------------
+
+Nodes and subclasses thereof provide the basic network structure in Pywr. There are several different types
+of node available. Two major categories exist: flow nodes and storage nodes. Flow nodes are the typical nodes
+that represent rivers, pipes and other features from, through or to which a resource can flow. These nodes are
+typically characterised by minimum and maximum flow rates. Storage nodes provide the ability to store resource
+from one time-step to another, and are characterised by minimum, maximum and initial volumes.
+
+There are three fundamental types of node in Pywr:
+
+* ``Input`` nodes add water to the system
+* ``Output`` nodes remove water from the system
+* ``Link`` nodes do not add or remove water from the system
+
+There is a fourth node type, ``Storage``, which can be considered fundamental because there are special rules for
+it's behaviour in the linear programme used to solve the water balance:
+
+* ``Storage`` nodes can carry water from one timestep to the next
+
+All other node types in Pywr are subclasses of these base types. For example, the ``pywr.nodes.Catchment`` node type
+is a special case of ``Input`` where the ``min_flow`` and ``max_flow`` properties are equal.
+
+The most common way to create a new node type is using a compound node. A compound node contains one or more existing
+nodes and is used to manage common or more complex arrangements of the basic node types. An example of a compound node
+is the ``PiecewiseLink``. It is composed of a link (``OUT_1``) which receives water from upstream and an link
+(``IN_1``) which conveys water downstream, connected by a set of links in parallel (``LNK_1`` ... ``LNK_N``) each with
+a different ``max_flow`` and ``cost``, illustrated below::
+
+                               /-->-- LNK_1 -->--\
+    UPSTREAM -->-- OUT_1 -->--|--->--  ...  -->---|-->-- IN_1 -->-- DOWNSTREAM
+                               \-->-- LNK_N -->--/
+
+Let's look at an example to create a new node type that represents a leaky pipe. To remove water from the system we
+need to use an output node (``LEAK``), with two links representing the boundaries of the compound node (``INFLOW`` and
+``OUTFLOW``)::
+
+    UPSTREAM -->-- INFLOW -->-- OUTFLOW -->-- DOWNSTREAM
+                     |
+                     \------>--  LEAK
+
+This is a simple structure which represents leakage as a demand with a maximum value and a benefit to be supplied. It
+is slightly flawed as the leakage volume does not vary proportionally to flow through the link, but is sufficient as
+an example:
+
+.. code-block:: python
+
+    from pywr.nodes import Node, Link, Output
+
+    class LeakyPipe(Node):
+        def __init__(self, leakage, leakage_cost=-99999, *args, **kwargs):
+            self.allow_isolated = True  # Required for compound nodes
+
+            super(LeakyPipe, self).__init__(*args, **kwargs)
+
+            # Define the internal nodes. The parent of the nodes is defined to identify them as sub-nodes.
+            self.inflow = Link(self.model, name='{} In'.format(self.name), parent=self)
+            self.outflow = Link(self.model, name='{} Out'.format(self.name), parent=self)
+            self.leak = Output(self.model, name='{} Leak'.format(self.name), parent=self)
+
+            # Connect the internal nodes
+            self.inflow.connect(self.outflow)
+            self.inflow.connect(self.leak)
+
+            # Define the properties of the leak (demand and benefit)
+            self.leak.max_flow = leakage
+            self.leak.cost = leakage_cost
+
+        def iter_slots(self, slot_name=None, is_connector=True):
+            # This is required so that connecting to this node actually connects to the outflow sub-node, and
+            # connecting from this node actually connects to the input sub-node
+            if is_connector:
+                yield self.outflow
+            else:
+                yield self.inflow
+
+        def after(self, timestep):
+            # Make the flow on the compound node appear as the flow _after_ the leak
+            self.commit_all(self.outflow.flow)
+            # Make sure save is done after setting aggregated flow
+            super(LeakyPipe, self).after(timestep)
+
+        @classmethod
+        def load(cls, data, model):
+            del(data["type"])
+            leakage = data.pop("leakage")
+            leakage_cost = data.pop("leakage_cost", None)
+            return cls(model, leakage, leakage_cost, **data)
+
+
+The custom node does not need to be "registered", unlike ``Parameters``, as this is done automatically using
+metaclasses. The new node type can be referenced from a JSON model provided that the class has been imported before
+the JSON is loaded:
+
+.. code-block:: python
+
+    from pywr.model import Model
+    import leakypipe
+    
+    model = Model.load("leaky_pipe_model.sjon")
+
+
+.. code-block:: yaml
+
+    {
+        "type": "leakypipe",
+        "leakage": "1.0"
+    }
+
+The ``allow_isolated`` attribute identifies nodes of this type as compound nodes. Without this the model would raise
+an error that the node is not connected to the rest of the network, as the connections are actually to its sub-nodes.
+
+The ``after`` method is not required but is useful so that recorders can be attached to the compound node. Without
+this the flow would appear to be zero as the flow doesn't *actually* pass through the compound node.
+
+The ``iter_slots`` method is required so that connecting to/from the node (e.g. ``upstream.connect(leaky)``) creates
+connections to the sub-nodes.
+
+A more advanced representation of the leaky pipe could use an additional ``AggregatedNode`` to constrain the ratio
+of flow through the ``OUTFLOW`` and ``LEAK`` nodes. [*]_
+
+.. [*] ``AggregatedNode`` is actually another fundamental node type, as this behaviour requires special treatment
+       in the linear programme.
```

### Comparing `pywr-1.8.0/docs/source/extending_pywr/extending_pywr_parameters.rst` & `pywr-1.9.0/docs/source/extending_pywr/extending_pywr_parameters.rst`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,235 +1,235 @@
-.. _extending-pywr-parameters:
-
-Extending Pywr with custom Parameters
--------------------------------------
-
-Pywr provides many existing Parameters that can be used to model complex behaviours. This includes parameters that
-read data in different formats (e.g. the dataframe parameter), parameters that solve specific problems (e.g. reservoir
-control curves) and general purpose parameters (e.g. the ``AggregatedParameter``).
-
-Custom parameters can be used to model specific behaviours otherwise not possible with the existing parameters. To
-write a custom parameter you must (as a minimim) inherit from the base ``pywr.parameter.Parameter`` class and implement
-the ``Parameter.value`` method. The arguments to this method represent the current timestep and scenario allowing the
-value of the parameter to be dynamic.
-
-A simple parameter
-==================
-
-The example below shows a minimal parameter which returns the same value every timestep. The parameter is initialised
-with the value to return, which it stores in the ``_value`` attribute on the instance [*]_. This value is returned
-whenever the ``value`` method is queries. The ``load`` class method is used to create an instance of the parameter
-from JSON.
-
-.. code-block:: python
-
-    from pywr.parameters import Parameter
-
-    class MyParameter(Parameter):
-        def __init__(self, model, value, **kwargs):
-            # called once when the parameter is created
-            super().__init__(model, **kwargs)
-            self._value = value
-
-        def value(self, timestep, scenario_index):
-            # called once per timestep for each scenario
-            return self._value
-
-        @classmethod
-        def load(cls, model, data):
-            # called when the parameter is loaded from a JSON document
-            value = data.pop("value")
-            return cls(model, value, **data)
-
-    MyParameter.register()  # register the name so it can be loaded from JSON
-
-.. [*] ``_value`` is used instead of ``value`` to avoid overloading the method of the same name.
-
-An instance of the parameter can be created from a JSON model as below. The type of the parameter is the class name,
-with the word "parameter" optionally removed (e.g. ``"constant"`` and ``"constantparameter"`` both create an instance
-of ``ConstantParameter``).
-
-.. code-block:: yaml
-
-    "max_flow": {
-        "type": "myparameter",
-        "value": 123.0
-    }
-
-Timesteps and scenarios
-=======================
-
-The ``Parameter.value`` method is called once per timestep for each scenario. The value it returns can be varied using
-the ``timestep`` and ``scenario_index`` arguments. For example, a simple version of a ``MonthlyProfileParameter`` could be
-created using the following:
-
-.. code-block:: python
-
-    class MonthlyProfileParameter(Parameter):
-        def __init__(self, model, profile, **kwargs):
-            super().__init__(model, **kwargs)
-            self.profile = profile  # a 12-element list of floats
-
-        def value(self, timestep, scenario_index):
-            index = timestep.month - 1  # convert to zero-based index
-            value = self.profile[index]
-
-        @dataclass
-        def load(cls, model, data):
-            profile = data.pop(profile)
-            return cls(model, profile, **data)
-
-Tracking state with setup, reset, before and after
-==================================================
-
-The ``Parameter.setup`` and ``Parameter.reset`` methods are called once at the start of a model run before the first
-timestep. The ``reset`` method is called for every run, while the `setup` method is only called if the structure of
-the model has changed [*]_.
-
-The ``Parameter.before`` and ``Parameter.after`` methods are called before and after each timestep, respectively. These
-methods can be used when a parameter needs to track state between timesteps. For example, a licence parameter needs
-to track the volume remaining in the licence. It's important to remember that when using scenarios the model has
-multiple states. It's good practice to write stateful parameters with this in mind, even if you aren't using scenarios
-initially, so that you can in the future without rewriting anything. The example below shows a very simplistic licence
-parameter which has a finite volume.
-
-.. code-block:: python
-
-    class LicenceParameter(Parameter):
-        def __init__(self, model, total_volume, **kwargs):
-            super().__init__(self, model, **kwargs)
-            self.total_volume = total_volume
-
-        def setup(self):
-            # allocate an array to hold the parameter state
-            num_scenarios = len(self.model.scenarios.combinations)
-            self._volume_remaining = np.empty([num_scenarios], np.float64)
-
-        def reset(self):
-            # reset the amount remaining in all states to the initial value
-            self._volume_remaining[...] = self.total_volume
-
-        def value(self, timestep, scenario_index):
-            # return the current volume remaining for the scenario
-            return self._volume_remaining[scenario_index.global_id]
-
-        def after(self):
-            # update the state
-            timestep = self.model.timestepper.current  # get current timestep
-            flow_during_timestep = self._node.flow * timestep.days  # see explanation below
-            self._remaining -= flow_during_timestep
-            self._remaining[self._remaining < 0] = 0  # volume remaining cannot be less than zero
-
-        def load(self, model, data):
-            total_volume = data.pop("total_volume")
-            return cls(model, total_volume, **data)
-
-The example above uses the `_node` attribute of the parameter, which is automatically set when the parameter is attached
-to a node. The `flow` attribute of the node represents the flow (per day) via that node. To get the total flow for the
-timestep it must be multipled by the number of days in the timestep, available as `timestep.days`.
-
-.. [*] The model is said to be "dirty" if nodes or edges are added or removed, resulting in a change to the structure
-       of the linear programme used to solve the model. This usually requires Parameters which track state to
-       reallocate memory, instead of just resetting values.
-
-Dependency on other parameters
-==============================
-
-The value of each parameter is calculated at the start of every timestep. A dependency tree is used to ensure that
-parameters are evaluated in the correct order and that there are no circular dependencies [*]_. For example, the
-``AggregatedParameter`` returns the aggregated value of a set of parameters using a user-defined function. In the
-terminology of the dependency tree the ``AggregatedParameter`` is the parent of the other parameters, which are it's
-children. When writing a parameter these dependencies need to be defined explicitly by modifying the
-``Parameter.parents`` or ``Parameter.children`` attributes.
-
-To get the value of a child parameter use the ``Parameter.get_value`` method, or for the index use
-``Parameter.get_index``. These methods return the value/index for the current timestep and scenario. To access the
-value from previous timesteps you must manually track the state of the child parameters.
-
-The ``pywr.parameters.load_parameter`` function is used to load parameters from JSON. This works with both references to
-parameters and nested parameters.
-
-As an example, see a simplified version of ``AggregatedParameter`` that returns the sum value of it's child parameters.
-
-.. code-block:: python
-
-    class SumParameter(Parameter):
-        def __init__(self, model, parameters, **kwargs):
-            super().__init__(model, **kwargs)
-            self.parameters = parameters
-            for parameter in self.parameters:
-                self.children.add(parameter)
-
-        def value(self, timestep, scenario_index):
-            total_value = sum([parameter.get_value(scenario_index) for parameter in parameters])
-            return total_value
-
-        @classmethod
-        def load(self, model, data):
-            parameters = [load_parameter(parameter_data)
-                          for parameter_data in data.pop("parameters")]
-            return cls(model, parameters, **data)
-
-.. [*] A circular dependency is when two (or more) parameters depend on each other. This can be direct (e.g A depends
-       on B, B depends on A) or indirect (e.g. A depends on B, B depends on C, C depends on A). Pywr is unable to resolve the
-       order in which to calculate these parameters and will raise an error at runtime.
-
-Improving performance with Cython
-=================================
-
-Parameters are evaluated many times and can be a significant part of the model run time. Many of the parameters in the
-core library have been written in Cython to improve performance. Custom parameters can be written in Cython too. Cython
-can also be used to link to external C/C++ libraries.
-
-A full tutorial in Cython is beyond the scope of this documentation - see the
-`Cython Documentation <https://cython.readthedocs.io/en/latest/>`_.
-
-The easiest way to compile and run custom parameters written in Cython is using the ``pyximport`` command, which
-compiles pyx modules at runtime. If the parameter is linking to a foreign library you may need to compile using a
-``setup.py`` in order to pass linker arguments.
-
-The example below demonstrates a custom parameter which uses a function from a foreign library (the ``pow`` function
-from ``libm``). There are a few differences from the Python equivalent:
-
-* Use of the ``cimport`` statement
-* Inherit from ``pywr.parameters._parameters.Parameter``
-* The ``value`` method is defined as a cpdef function. This signature must match exactly.
-
-.. code-block:: cython
-    :caption: custom_parameters.pyx
-
-    from pywr.parameters._parameters cimport Parameter
-    from pywr._core cimport Timestep, ScenarioIndex
-
-    cdef extern from "math.h":
-        double pow(double x, double y)
-
-    cdef class SquaredParameter(Parameter):
-        cdef double _value
-
-        def __init__(self, model, value, **kwargs):
-            super().__init__(model, **kwargs)
-            self._value = value
-
-        cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-            return pow(self._value, 2.0)
-
-        @classmethod
-        def load(cls, model, data):
-            # called when the parameter is loaded from a JSON document
-            value = data.pop("value")
-            return cls(model, value, **data)
-
-    SquaredParameter.register()
-
-
-.. code-block:: python
-    :caption: run_model.py
-
-    import pyximport
-    pyximport.install()
-
-    from pywr.model import Model
-    import custom_parameters
-
-    model = Model.load("simple.json")
-    model.run()
+.. _extending-pywr-parameters:
+
+Extending Pywr with custom Parameters
+-------------------------------------
+
+Pywr provides many existing Parameters that can be used to model complex behaviours. This includes parameters that
+read data in different formats (e.g. the dataframe parameter), parameters that solve specific problems (e.g. reservoir
+control curves) and general purpose parameters (e.g. the ``AggregatedParameter``).
+
+Custom parameters can be used to model specific behaviours otherwise not possible with the existing parameters. To
+write a custom parameter you must (as a minimim) inherit from the base ``pywr.parameter.Parameter`` class and implement
+the ``Parameter.value`` method. The arguments to this method represent the current timestep and scenario allowing the
+value of the parameter to be dynamic.
+
+A simple parameter
+==================
+
+The example below shows a minimal parameter which returns the same value every timestep. The parameter is initialised
+with the value to return, which it stores in the ``_value`` attribute on the instance [*]_. This value is returned
+whenever the ``value`` method is queries. The ``load`` class method is used to create an instance of the parameter
+from JSON.
+
+.. code-block:: python
+
+    from pywr.parameters import Parameter
+
+    class MyParameter(Parameter):
+        def __init__(self, model, value, **kwargs):
+            # called once when the parameter is created
+            super().__init__(model, **kwargs)
+            self._value = value
+
+        def value(self, timestep, scenario_index):
+            # called once per timestep for each scenario
+            return self._value
+
+        @classmethod
+        def load(cls, model, data):
+            # called when the parameter is loaded from a JSON document
+            value = data.pop("value")
+            return cls(model, value, **data)
+
+    MyParameter.register()  # register the name so it can be loaded from JSON
+
+.. [*] ``_value`` is used instead of ``value`` to avoid overloading the method of the same name.
+
+An instance of the parameter can be created from a JSON model as below. The type of the parameter is the class name,
+with the word "parameter" optionally removed (e.g. ``"constant"`` and ``"constantparameter"`` both create an instance
+of ``ConstantParameter``).
+
+.. code-block:: yaml
+
+    "max_flow": {
+        "type": "myparameter",
+        "value": 123.0
+    }
+
+Timesteps and scenarios
+=======================
+
+The ``Parameter.value`` method is called once per timestep for each scenario. The value it returns can be varied using
+the ``timestep`` and ``scenario_index`` arguments. For example, a simple version of a ``MonthlyProfileParameter`` could be
+created using the following:
+
+.. code-block:: python
+
+    class MonthlyProfileParameter(Parameter):
+        def __init__(self, model, profile, **kwargs):
+            super().__init__(model, **kwargs)
+            self.profile = profile  # a 12-element list of floats
+
+        def value(self, timestep, scenario_index):
+            index = timestep.month - 1  # convert to zero-based index
+            value = self.profile[index]
+
+        @dataclass
+        def load(cls, model, data):
+            profile = data.pop(profile)
+            return cls(model, profile, **data)
+
+Tracking state with setup, reset, before and after
+==================================================
+
+The ``Parameter.setup`` and ``Parameter.reset`` methods are called once at the start of a model run before the first
+timestep. The ``reset`` method is called for every run, while the `setup` method is only called if the structure of
+the model has changed [*]_.
+
+The ``Parameter.before`` and ``Parameter.after`` methods are called before and after each timestep, respectively. These
+methods can be used when a parameter needs to track state between timesteps. For example, a licence parameter needs
+to track the volume remaining in the licence. It's important to remember that when using scenarios the model has
+multiple states. It's good practice to write stateful parameters with this in mind, even if you aren't using scenarios
+initially, so that you can in the future without rewriting anything. The example below shows a very simplistic licence
+parameter which has a finite volume.
+
+.. code-block:: python
+
+    class LicenceParameter(Parameter):
+        def __init__(self, model, total_volume, **kwargs):
+            super().__init__(self, model, **kwargs)
+            self.total_volume = total_volume
+
+        def setup(self):
+            # allocate an array to hold the parameter state
+            num_scenarios = len(self.model.scenarios.combinations)
+            self._volume_remaining = np.empty([num_scenarios], np.float64)
+
+        def reset(self):
+            # reset the amount remaining in all states to the initial value
+            self._volume_remaining[...] = self.total_volume
+
+        def value(self, timestep, scenario_index):
+            # return the current volume remaining for the scenario
+            return self._volume_remaining[scenario_index.global_id]
+
+        def after(self):
+            # update the state
+            timestep = self.model.timestepper.current  # get current timestep
+            flow_during_timestep = self._node.flow * timestep.days  # see explanation below
+            self._remaining -= flow_during_timestep
+            self._remaining[self._remaining < 0] = 0  # volume remaining cannot be less than zero
+
+        def load(self, model, data):
+            total_volume = data.pop("total_volume")
+            return cls(model, total_volume, **data)
+
+The example above uses the `_node` attribute of the parameter, which is automatically set when the parameter is attached
+to a node. The `flow` attribute of the node represents the flow (per day) via that node. To get the total flow for the
+timestep it must be multipled by the number of days in the timestep, available as `timestep.days`.
+
+.. [*] The model is said to be "dirty" if nodes or edges are added or removed, resulting in a change to the structure
+       of the linear programme used to solve the model. This usually requires Parameters which track state to
+       reallocate memory, instead of just resetting values.
+
+Dependency on other parameters
+==============================
+
+The value of each parameter is calculated at the start of every timestep. A dependency tree is used to ensure that
+parameters are evaluated in the correct order and that there are no circular dependencies [*]_. For example, the
+``AggregatedParameter`` returns the aggregated value of a set of parameters using a user-defined function. In the
+terminology of the dependency tree the ``AggregatedParameter`` is the parent of the other parameters, which are it's
+children. When writing a parameter these dependencies need to be defined explicitly by modifying the
+``Parameter.parents`` or ``Parameter.children`` attributes.
+
+To get the value of a child parameter use the ``Parameter.get_value`` method, or for the index use
+``Parameter.get_index``. These methods return the value/index for the current timestep and scenario. To access the
+value from previous timesteps you must manually track the state of the child parameters.
+
+The ``pywr.parameters.load_parameter`` function is used to load parameters from JSON. This works with both references to
+parameters and nested parameters.
+
+As an example, see a simplified version of ``AggregatedParameter`` that returns the sum value of it's child parameters.
+
+.. code-block:: python
+
+    class SumParameter(Parameter):
+        def __init__(self, model, parameters, **kwargs):
+            super().__init__(model, **kwargs)
+            self.parameters = parameters
+            for parameter in self.parameters:
+                self.children.add(parameter)
+
+        def value(self, timestep, scenario_index):
+            total_value = sum([parameter.get_value(scenario_index) for parameter in parameters])
+            return total_value
+
+        @classmethod
+        def load(self, model, data):
+            parameters = [load_parameter(parameter_data)
+                          for parameter_data in data.pop("parameters")]
+            return cls(model, parameters, **data)
+
+.. [*] A circular dependency is when two (or more) parameters depend on each other. This can be direct (e.g A depends
+       on B, B depends on A) or indirect (e.g. A depends on B, B depends on C, C depends on A). Pywr is unable to resolve the
+       order in which to calculate these parameters and will raise an error at runtime.
+
+Improving performance with Cython
+=================================
+
+Parameters are evaluated many times and can be a significant part of the model run time. Many of the parameters in the
+core library have been written in Cython to improve performance. Custom parameters can be written in Cython too. Cython
+can also be used to link to external C/C++ libraries.
+
+A full tutorial in Cython is beyond the scope of this documentation - see the
+`Cython Documentation <https://cython.readthedocs.io/en/latest/>`_.
+
+The easiest way to compile and run custom parameters written in Cython is using the ``pyximport`` command, which
+compiles pyx modules at runtime. If the parameter is linking to a foreign library you may need to compile using a
+``setup.py`` in order to pass linker arguments.
+
+The example below demonstrates a custom parameter which uses a function from a foreign library (the ``pow`` function
+from ``libm``). There are a few differences from the Python equivalent:
+
+* Use of the ``cimport`` statement
+* Inherit from ``pywr.parameters._parameters.Parameter``
+* The ``value`` method is defined as a cpdef function. This signature must match exactly.
+
+.. code-block:: cython
+    :caption: custom_parameters.pyx
+
+    from pywr.parameters._parameters cimport Parameter
+    from pywr._core cimport Timestep, ScenarioIndex
+
+    cdef extern from "math.h":
+        double pow(double x, double y)
+
+    cdef class SquaredParameter(Parameter):
+        cdef double _value
+
+        def __init__(self, model, value, **kwargs):
+            super().__init__(model, **kwargs)
+            self._value = value
+
+        cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+            return pow(self._value, 2.0)
+
+        @classmethod
+        def load(cls, model, data):
+            # called when the parameter is loaded from a JSON document
+            value = data.pop("value")
+            return cls(model, value, **data)
+
+    SquaredParameter.register()
+
+
+.. code-block:: python
+    :caption: run_model.py
+
+    import pyximport
+    pyximport.install()
+
+    from pywr.model import Model
+    import custom_parameters
+
+    model = Model.load("simple.json")
+    model.run()
```

### Comparing `pywr-1.8.0/docs/source/extending_pywr/index.rst` & `pywr-1.9.0/docs/source/extending_pywr/index.rst`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-.. _extending_pywr:
-
-Extending Pywr
---------------
-
-Pywr is a Python library and as such is designed in an object oriented fashion. These objects (or classes) form
-the basic structure of Pywr, but much of the functionality in Pywr is provided via specific sub-classes designed
-to do specific instances. For example, there exist many specialised ``Parameter`` classes to provide specific types
-of input data. One of the benefits of being a Python library is that users can extend the base functionality in their
-own projects to provide custom functionality. In this section of the documentation there are some examples and guidance
-for extending Pywr in your projects.
-
-
-.. toctree::
-   :maxdepth: 1
-
-   Extending nodes <extending_pywr_nodes.rst>
-   Extending parameters <extending_pywr_parameters.rst>
-   Extending recorders <extending_pywr_recorders.rst>
-
-If you write a custom node/parameter/recorder that is of potential use to others consider creating a pull request to
-have it included in the core library.
+.. _extending_pywr:
+
+Extending Pywr
+--------------
+
+Pywr is a Python library and as such is designed in an object oriented fashion. These objects (or classes) form
+the basic structure of Pywr, but much of the functionality in Pywr is provided via specific sub-classes designed
+to do specific instances. For example, there exist many specialised ``Parameter`` classes to provide specific types
+of input data. One of the benefits of being a Python library is that users can extend the base functionality in their
+own projects to provide custom functionality. In this section of the documentation there are some examples and guidance
+for extending Pywr in your projects.
+
+
+.. toctree::
+   :maxdepth: 1
+
+   Extending nodes <extending_pywr_nodes.rst>
+   Extending parameters <extending_pywr_parameters.rst>
+   Extending recorders <extending_pywr_recorders.rst>
+
+If you write a custom node/parameter/recorder that is of potential use to others consider creating a pull request to
+have it included in the core library.
```

### Comparing `pywr-1.8.0/docs/source/install.rst` & `pywr-1.9.0/docs/source/install.rst`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,183 +1,183 @@
-Installing Pywr
-===============
-
-Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
-
-Building Pywr from source requires a working C compiler. It has been built successfully with MSVC on Windows, GCC on Linux and clang/LLVM on OS X.
-
-The easiest way to install Pywr is using the `Anaconda Python distribution <https://www.continuum.io/downloads>`_. See instructions below on `Installing binary packages with Anaconda`_.
-
-Dependencies
-------------
-
-Pywr has several external dependencies, listed below.
-
- * Core dependencies (required)
-
-   * `Cython <http://cython.org/>`_
-
-   * `NumPy <http://www.numpy.org/>`_
-
-   * `NetworkX <https://networkx.github.io/>`_
-
-   * `Pandas <http://pandas.pydata.org/>`_
-
-   * `packaging <https://pypi.python.org/pypi/packaging>`_
-
- * Linear programming solvers (at least one required)
-
-   * `GLPK <https://www.gnu.org/software/glpk/>`_ (recommended)
-
-   * `lpsolve <http://lpsolve.sourceforge.net/5.5/>`_
-
- * Optional dependencies (providing additional functionality)
-
-   * `pytables <http://www.pytables.org/>`_
-
-   * `xlrd <https://pypi.python.org/pypi/xlrd>`_
-
-   * `pytest <http://pytest.org/latest/>`_ (for testing only)
-
-   * `SciPy <http://www.scipy.org/>`_
-
-   * `Jupyter <https://jupyter.org/>`_
-
-   * `Matplotlib <http://matplotlib.org/>`_
-
-Installing (in general)
------------------------
-
-When installing Pywr you must specific which solvers to build. This is done by passing ``--with-<solver>`` as an argument to ``setup.py``. The following command will build and install Pywr with both the GLPK and lpsolve solvers:
-
-.. code-block:: shell
-
-  python setup.py install --with-glpk --with-lpsolve
-
-To install Pywr in-place in developer mode, use the ``develop`` command instead of ``install``. This is only useful if you plan to modify the Pywr source code and is not required for general use.
-
-.. code-block:: shell
-
-  python setup.py develop --with-glpk --with-lpsolve
-
-Installing binary wheels with pip
----------------------------------
-
-Binary wheel distributions of Pywr are hosted on `Pypi <https://pypi.org/project/pywr/>`_ for Windows and Linux.
-
-.. code-block:: shell
-
-  pip install pywr
-
-Installing binary packages with Anaconda
-----------------------------------------
-
-A binary distribution of Pywr is provided for 3.6+ (64-bit) on Windows, Linux and OS X for the `Anaconda Python distribution <https://www.continuum.io/downloads>`_. Note that this release may lag behind the development version.
-
-You will need to install and configure Anaconda before proceeding. The `conda 30-minute test drive <http://conda.pydata.org/docs/test-drive.html>`_ is a good place to start.
-
-Pywr is hosted on the conda-forge channel.
-
-.. code-block:: shell
-
-  conda config --add channels conda-forge
-  conda install pywr
-
-Installing from source with Anaconda
-------------------------------------
-
-It's possible to install the dependencies as Anaconda packages, but still build from source. This is only required if you want to keep up with development versions, rather than using the binaries done for releases. In this case you need to specify the include and library paths in your environment as the libraries will be installed in a non-standard location. This can be done by passing the relevant flags to setup.py. As an example, the following batch script should work on Windows (with a similar approach taken on Linux/macOS)
-
-.. code-block:: shell
-
-  set LIBRARY=%CONDA_PREFIX%\Library
-  set LIBRARY_INC=%LIBRARY%\include
-  set LIBRARY_LIB=%LIBRARY%\lib
-  python setup.py build_ext -I"%LIBRARY_INC%" -L"%LIBRARY_LIB%" --inplace --with-glpk --with-lpsolve install
-
-Installing on Windows
----------------------
-
-To build Pywr from source on Windows you must first install and configure the MSVC compiler. See the `instructions on this blog <https://blog.ionelmc.ro/2014/12/21/compiling-python-extensions-on-windows/>`_. It is important that you install the correct version of MSVC to correspond with your Python version.
-
-Binaries for GLPK are available from the `WinGLPK project <http://winglpk.sourceforge.net/>`_. This includes the MSVC solution files if you want to build GLPK yourself on Windows, although the prebuilt binaries are fine.
-
-Binaries for lpsolve are available from the `lpsolve sourceforge website <https://sourceforge.net/projects/lpsolve/>`_.
-
-Installing on Linux
--------------------
-
-No special instructions required. Follow instructions as for `installing (in general)`_ to build from source. A conda package is also available.
-
-Ubuntu
-~~~~~~
-
-The following commands should install the GLPK and lpsolve libraries:
-
-.. code-block:: shell
-
-  sudo apt-get install libgmp3-dev libglpk-dev glpk
-  sudo apt-get install liblpsolve55-dev lp-solve
-
-The Ubuntu package for lpsolve includes a static library which can confuse the compiler. The easiest work-around is to remove it:
-
-.. code-block:: shell
-
-  sudo rm /usr/lib/liblpsolve55.a
-  sudo ln -s /usr/lib/lp_solve/liblpsolve55.so /usr/lib/liblpsolve55.so
-
-Installing on OS X
-------------------
-
-Follow instructions as for `installing (in general)`_ to build from source. A conda package is also available.
-
-If external libraries are located in a non-standard location you either need to set the `DYLD_LIBRARY_PATH` environment variable at runtime:
-
-.. code-block:: shell
-
-  export DYLD_LIBRARY_PATH=/path/to/library/directory
-
-Alternatively (and recommended) set the `rpath` of the extension during compilation.
-
-.. code-block:: shell
-
-  export CFLAGS="-Wl,-rpath,/path/to/library/directory"
-
-You may also need to specify the location of the library headers:
-
-.. code-block:: shell
-
-  export C_INCLUDE_PATH=/path/to/include/directory
-
-Examples of the above can be seen in the conda recipe (see `conda-recipe/build.sh`).
-
-The dependencies (GLPK and/or lpsolve) can be built from source manually, or installed using `Homebrew <http://brew.sh/>`_.
-
-Development and testing
------------------------
-
-The source code for Pywr is managed using Git and is hosted on GitHub: https://github.com/pywr/pywr/ .
-
-There are a collection of unit tests for Pywr written using ``pytest``. These can be run using:
-
-.. code-block:: shell
-
-  py.test tests
-
-This will run all avaialble tests using the default solver. A specific solver can be tested by specifying at the command line:
-
-.. code-block:: shell
-
-  py.test tests --solver=lpsolve
-
-Continuous Integration
-~~~~~~~~~~~~~~~~~~~~~~
-
-Pywr is automatically built and tested on Linux and Windows using Travis-CI and AppVeyor (respectively).
-
-Creating a pull request on GitHub will automatically trigger a build.
-
-https://travis-ci.org/pywr/pywr
-
-https://ci.appveyor.com/project/snorfalorpagus/pywr
-
-Both services install Pywr using the Anaconda Python distribution, as this was the easiest way to install all the dependencies.
+Installing Pywr
+===============
+
+Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
+
+Building Pywr from source requires a working C compiler. It has been built successfully with MSVC on Windows, GCC on Linux and clang/LLVM on OS X.
+
+The easiest way to install Pywr is using the `Anaconda Python distribution <https://www.continuum.io/downloads>`_. See instructions below on `Installing binary packages with Anaconda`_.
+
+Dependencies
+------------
+
+Pywr has several external dependencies, listed below.
+
+ * Core dependencies (required)
+
+   * `Cython <http://cython.org/>`_
+
+   * `NumPy <http://www.numpy.org/>`_
+
+   * `NetworkX <https://networkx.github.io/>`_
+
+   * `Pandas <http://pandas.pydata.org/>`_
+
+   * `packaging <https://pypi.python.org/pypi/packaging>`_
+
+ * Linear programming solvers (at least one required)
+
+   * `GLPK <https://www.gnu.org/software/glpk/>`_ (recommended)
+
+   * `lpsolve <http://lpsolve.sourceforge.net/5.5/>`_
+
+ * Optional dependencies (providing additional functionality)
+
+   * `pytables <http://www.pytables.org/>`_
+
+   * `xlrd <https://pypi.python.org/pypi/xlrd>`_
+
+   * `pytest <http://pytest.org/latest/>`_ (for testing only)
+
+   * `SciPy <http://www.scipy.org/>`_
+
+   * `Jupyter <https://jupyter.org/>`_
+
+   * `Matplotlib <http://matplotlib.org/>`_
+
+Installing (in general)
+-----------------------
+
+When installing Pywr you must specific which solvers to build. This is done by passing ``--with-<solver>`` as an argument to ``setup.py``. The following command will build and install Pywr with both the GLPK and lpsolve solvers:
+
+.. code-block:: shell
+
+  python setup.py install --with-glpk --with-lpsolve
+
+To install Pywr in-place in developer mode, use the ``develop`` command instead of ``install``. This is only useful if you plan to modify the Pywr source code and is not required for general use.
+
+.. code-block:: shell
+
+  python setup.py develop --with-glpk --with-lpsolve
+
+Installing binary wheels with pip
+---------------------------------
+
+Binary wheel distributions of Pywr are hosted on `Pypi <https://pypi.org/project/pywr/>`_ for Windows and Linux.
+
+.. code-block:: shell
+
+  pip install pywr
+
+Installing binary packages with Anaconda
+----------------------------------------
+
+A binary distribution of Pywr is provided for 3.6+ (64-bit) on Windows, Linux and OS X for the `Anaconda Python distribution <https://www.continuum.io/downloads>`_. Note that this release may lag behind the development version.
+
+You will need to install and configure Anaconda before proceeding. The `conda 30-minute test drive <http://conda.pydata.org/docs/test-drive.html>`_ is a good place to start.
+
+Pywr is hosted on the conda-forge channel.
+
+.. code-block:: shell
+
+  conda config --add channels conda-forge
+  conda install pywr
+
+Installing from source with Anaconda
+------------------------------------
+
+It's possible to install the dependencies as Anaconda packages, but still build from source. This is only required if you want to keep up with development versions, rather than using the binaries done for releases. In this case you need to specify the include and library paths in your environment as the libraries will be installed in a non-standard location. This can be done by passing the relevant flags to setup.py. As an example, the following batch script should work on Windows (with a similar approach taken on Linux/macOS)
+
+.. code-block:: shell
+
+  set LIBRARY=%CONDA_PREFIX%\Library
+  set LIBRARY_INC=%LIBRARY%\include
+  set LIBRARY_LIB=%LIBRARY%\lib
+  python setup.py build_ext -I"%LIBRARY_INC%" -L"%LIBRARY_LIB%" --inplace --with-glpk --with-lpsolve install
+
+Installing on Windows
+---------------------
+
+To build Pywr from source on Windows you must first install and configure the MSVC compiler. See the `instructions on this blog <https://blog.ionelmc.ro/2014/12/21/compiling-python-extensions-on-windows/>`_. It is important that you install the correct version of MSVC to correspond with your Python version.
+
+Binaries for GLPK are available from the `WinGLPK project <http://winglpk.sourceforge.net/>`_. This includes the MSVC solution files if you want to build GLPK yourself on Windows, although the prebuilt binaries are fine.
+
+Binaries for lpsolve are available from the `lpsolve sourceforge website <https://sourceforge.net/projects/lpsolve/>`_.
+
+Installing on Linux
+-------------------
+
+No special instructions required. Follow instructions as for `installing (in general)`_ to build from source. A conda package is also available.
+
+Ubuntu
+~~~~~~
+
+The following commands should install the GLPK and lpsolve libraries:
+
+.. code-block:: shell
+
+  sudo apt-get install libgmp3-dev libglpk-dev glpk
+  sudo apt-get install liblpsolve55-dev lp-solve
+
+The Ubuntu package for lpsolve includes a static library which can confuse the compiler. The easiest work-around is to remove it:
+
+.. code-block:: shell
+
+  sudo rm /usr/lib/liblpsolve55.a
+  sudo ln -s /usr/lib/lp_solve/liblpsolve55.so /usr/lib/liblpsolve55.so
+
+Installing on OS X
+------------------
+
+Follow instructions as for `installing (in general)`_ to build from source. A conda package is also available.
+
+If external libraries are located in a non-standard location you either need to set the `DYLD_LIBRARY_PATH` environment variable at runtime:
+
+.. code-block:: shell
+
+  export DYLD_LIBRARY_PATH=/path/to/library/directory
+
+Alternatively (and recommended) set the `rpath` of the extension during compilation.
+
+.. code-block:: shell
+
+  export CFLAGS="-Wl,-rpath,/path/to/library/directory"
+
+You may also need to specify the location of the library headers:
+
+.. code-block:: shell
+
+  export C_INCLUDE_PATH=/path/to/include/directory
+
+Examples of the above can be seen in the conda recipe (see `conda-recipe/build.sh`).
+
+The dependencies (GLPK and/or lpsolve) can be built from source manually, or installed using `Homebrew <http://brew.sh/>`_.
+
+Development and testing
+-----------------------
+
+The source code for Pywr is managed using Git and is hosted on GitHub: https://github.com/pywr/pywr/ .
+
+There are a collection of unit tests for Pywr written using ``pytest``. These can be run using:
+
+.. code-block:: shell
+
+  py.test tests
+
+This will run all avaialble tests using the default solver. A specific solver can be tested by specifying at the command line:
+
+.. code-block:: shell
+
+  py.test tests --solver=lpsolve
+
+Continuous Integration
+~~~~~~~~~~~~~~~~~~~~~~
+
+Pywr is automatically built and tested on Linux and Windows using Travis-CI and AppVeyor (respectively).
+
+Creating a pull request on GitHub will automatically trigger a build.
+
+https://travis-ci.org/pywr/pywr
+
+https://ci.appveyor.com/project/snorfalorpagus/pywr
+
+Both services install Pywr using the Anaconda Python distribution, as this was the easiest way to install all the dependencies.
```

### Comparing `pywr-1.8.0/docs/source/json.rst` & `pywr-1.9.0/docs/source/json.rst`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,264 +1,264 @@
-.. _json-model-format:
-
-JSON model format
------------------
-
-Overview of document structure
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-In addition to creating models programmatically using the Python API, models can be described in a JSON (JavaScript Object Notation) document [#]_.
-
-The overall structure of the model is given below. A description of the contents of each of the first level items is given in this document. The most important are the `nodes`_ and `edges`_ sections.
-
-.. code-block:: json
-
-    {
-        "metadata": {},
-        "timestepper": {},
-        "solver": {},
-        "nodes": {},
-        "edges": {},
-        "parameters": {}
-    }
-
-Some examples of JSON models can be found in the `tests/models <https://github.com/pywr/pywr/tree/master/tests/models>`_ folder.
-
-Metadata
-~~~~~~~~
-
-The metadata section includes information about the model as key-value pairs. It is expected as a minimum to include a ``"title"`` and ``"description"`` and may additionally include keys such as ``"author"``.
-
-.. code-block:: json
-
-    {"metadata": {
-        "title": "Example",
-        "description": "An example for the documentation",
-        "author": "John Smith"
-    }}
-
-Timestepper
-~~~~~~~~~~~
-
-The timestepper defines the period a model is run for and the timestep used. It corresponds directly to the :class:`pywr.core.Timestepper` instance on the model. It has three properties: the start date, end date and timestep.
-
-The example below describes a model that will run from 1st January 2016 to 31st December 2016 using a 7 day timestep.
-
-.. code-block:: json
-
-    {"timestepper": {
-        "start": "2016-01-01",
-        "end": "2016-12-31",
-        "timestep": 7
-    }}
-
-Solver
-~~~~~~
-
-The solver section contains items to be passed to the solver. The only required item is the name of the solver to use. Other items will be specific to the solver.
-
-.. code-block:: json
-
-    {"solver": {
-        "name": "glpk"
-    }}
-
-Nodes
-~~~~~
-
-The nodes section describes the nodes in the model. As a minimum a node must have a ``name`` and a ``type``. There are two fundamental types of node in Pywr (:class:`pywr.core.Node` and :class:`pywr.core.Storage`) which have different properties.
-
-Where a parameter can be described as a simple scalar value it is sufficient to pass the value directly (e.g. ``"cost": 10.0``). See also the `parameters`_ section for details on defining non-scalar parameters.
-
-Non-storage nodes
-=================
-
-The ``Node`` type and it's subtypes have a ``max_flow`` and ``cost`` property, both of which have default values.
-
-.. code-block:: json
-
-    {"nodes": [
-        {
-            "name": "groundwater",
-            "type": "input",
-            "max_flow": 23.0,
-            "cost": 10.0
-        }
-    ]}
-
-In addition to the basic ``input``, ``output`` and ``link`` types, subtypes can be created by specifying the appropriate name. Some subtypes will provide additional properties; often these correspond directly to the keyword arguments of the class. For example, a river gauge which has a soft MRF constraint is demonstrated below. The ``"mrf"`` property is the minimum residual flow required, the ``"mrf_cost"`` is the cost applied to that minimum flow, and the ``"cost"`` property is the cost associated with the residual flow.
-
-.. code-block:: json
-
-    {"nodes": [
-        {
-            "name": "Teddington GS",
-            "type": "rivergauge",
-            "mrf": 200.0,
-            "cost": 0.0,
-            "mrf_cost": -1000.0
-        }
-    ]}
-
-Storage nodes
-=============
-
-The ``Storage`` type and it's subtypes have a ``max_volume``, ``min_volume`` and ``initial_volume``, as well as ``num_inputs`` and ``num_outputs``. The maximum and initial volumes must be specified, whereas the others have default values.
-
-.. code-block:: json
-
-    {"nodes": [
-        {
-            "name": "Big Wet Lake",
-            "type": "storage",
-            "max_volume": 1000,
-            "initial_volume": 700,
-            "min_volume": 0,
-            "num_inputs": 1,
-            "num_outputs": 1,
-            "cost": -10.0
-        }
-    ]}
-
-When defining a storage node with multiple inputs or outputs connections need to be made using the slot notation (discussed in the `edges`_ section).
-
-Edges
-~~~~~
-
-The edges section describes the connections between nodes. As a minimum an edge is defined as a two-item list containing the names of the nodes to connect (given in the order corresponding to the direction of flow), e.g.:
-
-.. code-block:: json
-
-    {"edges": [
-        ["supply", "intermediate"],
-        ["intermediate", "demand"]
-    ]}
-
-Additionally the to and from slots can be specified. For example the code below connects `reservoirA` slot 2 to `reservoirB` slot 3.
-
-.. code-block:: json
-
-    {"edges": [
-        ["reservoirA", "reservoirB", 2, 3]
-    ]}
-
-Parameters
-~~~~~~~~~~
-
-Sometimes it is convenient to define a ``Parameter`` used in the model in the ``"parameters"`` section instead of inside a node, for instance if the parameter is needed by more than one node.
-
-.. code-block:: json
-
-    {
-        "nodes": [
-            {
-                "name": "groundwater",
-                "type": "input",
-                "max_flow": "gw_flow"
-            }
-        ],
-        "parameters": [
-            {
-                "name": "gw_flow",
-                "type": "constant",
-                "value": 23.0
-            }
-        ]
-    }
-
-Parameters can be more complicated than simple scalar values. For instance, a time varying parameter can be defined using a monthly or daily profile which repeats each year.
-
-.. code-block:: json
-
-    {"parameters": [
-        {
-            "name": "mrf_profile",
-            "type": "monthlyprofile",
-            "values": [10, 10, 10, 10, 50, 50, 50, 50, 20, 20, 10, 10]
-        }
-    ]}
-
-Instead of defining the data inline using the ``"values"`` property, external data can be referenced as below. The URL should be relative to the JSON document *not* the current working directory.
-
-.. code-block:: json
-
-    {"parameters": [
-        {
-            "name": "catchment_inflow",
-            "type": "dataframe",
-            "url": "data/catchmod_outputs_v2.csv",
-            "column": "Flow",
-            "index_col": "Date",
-            "parse_dates": true
-        }
-    ]}
-
-
-Loading a JSON document
-~~~~~~~~~~~~~~~~~~~~~~~
-
-A Pywr JSON document can be loaded into a `Model` instance by using the `Model.load` class-method:
-
-.. code-block:: python
-
-    from pywr.model import Model
-    my_model = model.load('/path/to/my_model.json')
-    my_model.run()
-
-Once a model is loaded if a reference to an actual node is required, using .get ...
-
-.. code-block:: python
-
-    node = my_model.nodes.get("River Thames")
-    if node:
-        print(f"max_flow: {node.max_flow}")
-    else:
-        print("Not found")
-
-... or try-except is preferable to avoid searching twice.
-
-.. code-block:: python
-
-    try:
-        node = model.nodes["River Thames"]
-    except KeyError:
-        print("Not found")
-    else:
-        print(f"max_flow: {node.max_flow}")
-
-It is also possible to test for node and component membership using their names:
-
-.. code-block:: python
-
-    assert "River Thames" in model.nodes
-    assert "Demand" in model.parameters
-
-
-Debugging and syntax errors
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The JSON format is not sensitive to white space but is otherwise quite strict. When the `json` module fails to parse a document an exception will be raised. The exception includes a (somewhat cryptic) description of the problem and usefully includes a line number (see example below).
-
-.. code-block:: pycon
-
-    >>> model = Model.loads(data)
-    Traceback (most recent call last):
-      File "<stdin>", line 1, in <module>
-      File "/Users/snorf/Desktop/pywr/pywr/core.py", line 316, in loads
-        data = json.loads(data)
-      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/__init__.py", line 318, in loads
-        return _default_decoder.decode(s)
-      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/decoder.py", line 343, in decode
-        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
-      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/decoder.py", line 359, in raw_decode
-        obj, end = self.scan_once(s, idx)
-    ValueError: Expecting property name enclosed in double quotes: line 17 column 9 (char 372)
-
-Common mistakes when writing JSON documents "by hand" include:
-
- * Trailing commas at the end of a list (``["like", "this",]``)
- * Strings not enclosed in quotes (``name`` instead of ``"name"``)
-
-Footnotes
-~~~~~~~~~
-
-.. [#] In fact the model can be represented as a hierarchy of basic Python types, which can be conveniently parsed from a JSON document. Alternative formats are possible; for example, a YAML (Yet Another Markup Language) document as it can be translated to/from JSON losslessly.
+.. _json-model-format:
+
+JSON model format
+-----------------
+
+Overview of document structure
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+In addition to creating models programmatically using the Python API, models can be described in a JSON (JavaScript Object Notation) document [#]_.
+
+The overall structure of the model is given below. A description of the contents of each of the first level items is given in this document. The most important are the `nodes`_ and `edges`_ sections.
+
+.. code-block:: json
+
+    {
+        "metadata": {},
+        "timestepper": {},
+        "solver": {},
+        "nodes": {},
+        "edges": {},
+        "parameters": {}
+    }
+
+Some examples of JSON models can be found in the `tests/models <https://github.com/pywr/pywr/tree/master/tests/models>`_ folder.
+
+Metadata
+~~~~~~~~
+
+The metadata section includes information about the model as key-value pairs. It is expected as a minimum to include a ``"title"`` and ``"description"`` and may additionally include keys such as ``"author"``.
+
+.. code-block:: json
+
+    {"metadata": {
+        "title": "Example",
+        "description": "An example for the documentation",
+        "author": "John Smith"
+    }}
+
+Timestepper
+~~~~~~~~~~~
+
+The timestepper defines the period a model is run for and the timestep used. It corresponds directly to the :class:`pywr.core.Timestepper` instance on the model. It has three properties: the start date, end date and timestep.
+
+The example below describes a model that will run from 1st January 2016 to 31st December 2016 using a 7 day timestep.
+
+.. code-block:: json
+
+    {"timestepper": {
+        "start": "2016-01-01",
+        "end": "2016-12-31",
+        "timestep": 7
+    }}
+
+Solver
+~~~~~~
+
+The solver section contains items to be passed to the solver. The only required item is the name of the solver to use. Other items will be specific to the solver.
+
+.. code-block:: json
+
+    {"solver": {
+        "name": "glpk"
+    }}
+
+Nodes
+~~~~~
+
+The nodes section describes the nodes in the model. As a minimum a node must have a ``name`` and a ``type``. There are two fundamental types of node in Pywr (:class:`pywr.core.Node` and :class:`pywr.core.Storage`) which have different properties.
+
+Where a parameter can be described as a simple scalar value it is sufficient to pass the value directly (e.g. ``"cost": 10.0``). See also the `parameters`_ section for details on defining non-scalar parameters.
+
+Non-storage nodes
+=================
+
+The ``Node`` type and it's subtypes have a ``max_flow`` and ``cost`` property, both of which have default values.
+
+.. code-block:: json
+
+    {"nodes": [
+        {
+            "name": "groundwater",
+            "type": "input",
+            "max_flow": 23.0,
+            "cost": 10.0
+        }
+    ]}
+
+In addition to the basic ``input``, ``output`` and ``link`` types, subtypes can be created by specifying the appropriate name. Some subtypes will provide additional properties; often these correspond directly to the keyword arguments of the class. For example, a river gauge which has a soft MRF constraint is demonstrated below. The ``"mrf"`` property is the minimum residual flow required, the ``"mrf_cost"`` is the cost applied to that minimum flow, and the ``"cost"`` property is the cost associated with the residual flow.
+
+.. code-block:: json
+
+    {"nodes": [
+        {
+            "name": "Teddington GS",
+            "type": "rivergauge",
+            "mrf": 200.0,
+            "cost": 0.0,
+            "mrf_cost": -1000.0
+        }
+    ]}
+
+Storage nodes
+=============
+
+The ``Storage`` type and it's subtypes have a ``max_volume``, ``min_volume`` and ``initial_volume``, as well as ``num_inputs`` and ``num_outputs``. The maximum and initial volumes must be specified, whereas the others have default values.
+
+.. code-block:: json
+
+    {"nodes": [
+        {
+            "name": "Big Wet Lake",
+            "type": "storage",
+            "max_volume": 1000,
+            "initial_volume": 700,
+            "min_volume": 0,
+            "num_inputs": 1,
+            "num_outputs": 1,
+            "cost": -10.0
+        }
+    ]}
+
+When defining a storage node with multiple inputs or outputs connections need to be made using the slot notation (discussed in the `edges`_ section).
+
+Edges
+~~~~~
+
+The edges section describes the connections between nodes. As a minimum an edge is defined as a two-item list containing the names of the nodes to connect (given in the order corresponding to the direction of flow), e.g.:
+
+.. code-block:: json
+
+    {"edges": [
+        ["supply", "intermediate"],
+        ["intermediate", "demand"]
+    ]}
+
+Additionally the to and from slots can be specified. For example the code below connects `reservoirA` slot 2 to `reservoirB` slot 3.
+
+.. code-block:: json
+
+    {"edges": [
+        ["reservoirA", "reservoirB", 2, 3]
+    ]}
+
+Parameters
+~~~~~~~~~~
+
+Sometimes it is convenient to define a ``Parameter`` used in the model in the ``"parameters"`` section instead of inside a node, for instance if the parameter is needed by more than one node.
+
+.. code-block:: json
+
+    {
+        "nodes": [
+            {
+                "name": "groundwater",
+                "type": "input",
+                "max_flow": "gw_flow"
+            }
+        ],
+        "parameters": [
+            {
+                "name": "gw_flow",
+                "type": "constant",
+                "value": 23.0
+            }
+        ]
+    }
+
+Parameters can be more complicated than simple scalar values. For instance, a time varying parameter can be defined using a monthly or daily profile which repeats each year.
+
+.. code-block:: json
+
+    {"parameters": [
+        {
+            "name": "mrf_profile",
+            "type": "monthlyprofile",
+            "values": [10, 10, 10, 10, 50, 50, 50, 50, 20, 20, 10, 10]
+        }
+    ]}
+
+Instead of defining the data inline using the ``"values"`` property, external data can be referenced as below. The URL should be relative to the JSON document *not* the current working directory.
+
+.. code-block:: json
+
+    {"parameters": [
+        {
+            "name": "catchment_inflow",
+            "type": "dataframe",
+            "url": "data/catchmod_outputs_v2.csv",
+            "column": "Flow",
+            "index_col": "Date",
+            "parse_dates": true
+        }
+    ]}
+
+
+Loading a JSON document
+~~~~~~~~~~~~~~~~~~~~~~~
+
+A Pywr JSON document can be loaded into a `Model` instance by using the `Model.load` class-method:
+
+.. code-block:: python
+
+    from pywr.model import Model
+    my_model = model.load('/path/to/my_model.json')
+    my_model.run()
+
+Once a model is loaded if a reference to an actual node is required, using .get ...
+
+.. code-block:: python
+
+    node = my_model.nodes.get("River Thames")
+    if node:
+        print(f"max_flow: {node.max_flow}")
+    else:
+        print("Not found")
+
+... or try-except is preferable to avoid searching twice.
+
+.. code-block:: python
+
+    try:
+        node = model.nodes["River Thames"]
+    except KeyError:
+        print("Not found")
+    else:
+        print(f"max_flow: {node.max_flow}")
+
+It is also possible to test for node and component membership using their names:
+
+.. code-block:: python
+
+    assert "River Thames" in model.nodes
+    assert "Demand" in model.parameters
+
+
+Debugging and syntax errors
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The JSON format is not sensitive to white space but is otherwise quite strict. When the `json` module fails to parse a document an exception will be raised. The exception includes a (somewhat cryptic) description of the problem and usefully includes a line number (see example below).
+
+.. code-block:: pycon
+
+    >>> model = Model.loads(data)
+    Traceback (most recent call last):
+      File "<stdin>", line 1, in <module>
+      File "/Users/snorf/Desktop/pywr/pywr/core.py", line 316, in loads
+        data = json.loads(data)
+      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/__init__.py", line 318, in loads
+        return _default_decoder.decode(s)
+      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/decoder.py", line 343, in decode
+        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
+      File "/Users/snorf/miniconda3/envs/pywr/lib/python3.4/json/decoder.py", line 359, in raw_decode
+        obj, end = self.scan_once(s, idx)
+    ValueError: Expecting property name enclosed in double quotes: line 17 column 9 (char 372)
+
+Common mistakes when writing JSON documents "by hand" include:
+
+ * Trailing commas at the end of a list (``["like", "this",]``)
+ * Strings not enclosed in quotes (``name`` instead of ``"name"``)
+
+Footnotes
+~~~~~~~~~
+
+.. [#] In fact the model can be represented as a hierarchy of basic Python types, which can be conveniently parsed from a JSON document. Alternative formats are possible; for example, a YAML (Yet Another Markup Language) document as it can be translated to/from JSON losslessly.
```

### Comparing `pywr-1.8.0/docs/source/license.rst` & `pywr-1.9.0/docs/source/license.rst`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-License
-=======
-
-Software
---------
-
-Pywr is licensed under the `GNU General Public License, Version 3.0 or later <http://www.gnu.org/licenses/gpl-3.0.en.html>`_.
-
-Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
-
-This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 1, or (at your option) any later version.
-
-This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
-
-You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA 02110-1301 USA.
-
-Documentation
--------------
-
-This documentation is licensed under the `GNU Free Documentation License, Version 1.3 or later <http://www.gnu.org/licenses/fdl-1.3.en.html>`_.
-
-Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
-
-Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.3 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled "GNU Free Documentation License".
-
-Citation
---------
-
-Please consider citing the following paper when using Pywr:
-
-> Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
+License
+=======
+
+Software
+--------
+
+Pywr is licensed under the `GNU General Public License, Version 3.0 or later <http://www.gnu.org/licenses/gpl-3.0.en.html>`_.
+
+Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
+
+This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 1, or (at your option) any later version.
+
+This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA 02110-1301 USA.
+
+Documentation
+-------------
+
+This documentation is licensed under the `GNU Free Documentation License, Version 1.3 or later <http://www.gnu.org/licenses/fdl-1.3.en.html>`_.
+
+Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
+
+Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.3 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled "GNU Free Documentation License".
+
+Citation
+--------
+
+Please consider citing the following paper when using Pywr:
+
+> Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
```

### Comparing `pywr-1.8.0/docs/source/pyplots/demand_saving_levels.py` & `pywr-1.9.0/docs/source/pyplots/demand_saving_levels.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-import numpy as np
-import matplotlib.pyplot as plt
-import pandas
-
-baseline = 50.0
-
-profile = np.array([0.9, 0.9, 0.9, 0.9, 1.2, 1.2, 1.2, 1.2, 0.9, 0.9, 0.9, 0.9])
-
-L0 = np.array([1.0]*12)
-L1 = np.array([0.95, 0.95, 0.95, 0.95, 0.90, 0.90, 0.90, 0.90, 0.95, 0.95, 0.95, 0.95])
-L2 = np.array([0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8])
-
-dates = pandas.date_range("2015-01-01", "2015-12-31")
-
-L0_values = np.array([baseline*profile[d.month-1]*L0[d.month-1] for d in dates])
-L1_values = np.array([baseline*profile[d.month-1]*L1[d.month-1] for d in dates])
-L2_values = np.array([baseline*profile[d.month-1]*L2[d.month-1] for d in dates])
-
-fig = plt.figure(figsize=(8, 5), dpi=300)
-
-plt.plot(np.arange(0, len(dates)), L0_values, linewidth=2)
-plt.plot(np.arange(0, len(dates)), L1_values, linewidth=2)
-plt.plot(np.arange(0, len(dates)), L2_values, linewidth=2)
-
-plt.xlabel("Day of year")
-plt.ylabel("Demand [Ml/d]")
-
-plt.grid(True)
-plt.ylim([0.0, 70])
-plt.xlim(0, 365)
-plt.legend(["Level 0", "Level 1", "Level 2"], loc="lower right")
-plt.tight_layout()
-plt.show()
+import numpy as np
+import matplotlib.pyplot as plt
+import pandas
+
+baseline = 50.0
+
+profile = np.array([0.9, 0.9, 0.9, 0.9, 1.2, 1.2, 1.2, 1.2, 0.9, 0.9, 0.9, 0.9])
+
+L0 = np.array([1.0]*12)
+L1 = np.array([0.95, 0.95, 0.95, 0.95, 0.90, 0.90, 0.90, 0.90, 0.95, 0.95, 0.95, 0.95])
+L2 = np.array([0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8])
+
+dates = pandas.date_range("2015-01-01", "2015-12-31")
+
+L0_values = np.array([baseline*profile[d.month-1]*L0[d.month-1] for d in dates])
+L1_values = np.array([baseline*profile[d.month-1]*L1[d.month-1] for d in dates])
+L2_values = np.array([baseline*profile[d.month-1]*L2[d.month-1] for d in dates])
+
+fig = plt.figure(figsize=(8, 5), dpi=300)
+
+plt.plot(np.arange(0, len(dates)), L0_values, linewidth=2)
+plt.plot(np.arange(0, len(dates)), L1_values, linewidth=2)
+plt.plot(np.arange(0, len(dates)), L2_values, linewidth=2)
+
+plt.xlabel("Day of year")
+plt.ylabel("Demand [Ml/d]")
+
+plt.grid(True)
+plt.ylim([0.0, 70])
+plt.xlim(0, 365)
+plt.legend(["Level 0", "Level 1", "Level 2"], loc="lower right")
+plt.tight_layout()
+plt.show()
```

### Comparing `pywr-1.8.0/docs/source/tutorial.json` & `pywr-1.9.0/tests/models/extra1.json`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.4895833333333333%*

 * *Differences: {"'edges'": "[['supply1', 'link1'], ['link1', 'demand1']]",*

 * * "'includes'": "['extra2.json']",*

 * * "'metadata'": "{'title': 'Simple 1', 'description': 'A very simple example.'}",*

 * * "'nodes'": "{0: {'name': 'supply1', 'max_flow': 15, delete: ['cost']}, 2: {'name': 'demand1', "*

 * *            "'max_flow': 10, 'cost': -10}, insert: [(1, OrderedDict([('name', 'link1'), ('type', "*

 * *            "'Link')]))]}",*

 * * 'delete': "['recorders']"}*

```diff
@@ -1,37 +1,42 @@
 {
     "edges": [
         [
-            "supply",
-            "demand"
+            "supply1",
+            "link1"
+        ],
+        [
+            "link1",
+            "demand1"
         ]
     ],
+    "includes": [
+        "extra2.json"
+    ],
     "metadata": {
+        "description": "A very simple example.",
         "minimum_version": "0.1",
-        "title": "tutorial"
+        "title": "Simple 1"
     },
     "nodes": [
         {
-            "cost": 3,
-            "max_flow": 10,
-            "name": "supply",
+            "max_flow": 15,
+            "name": "supply1",
             "type": "Input"
         },
         {
-            "cost": -100,
-            "max_flow": 6,
-            "name": "demand",
+            "name": "link1",
+            "type": "Link"
+        },
+        {
+            "cost": -10,
+            "max_flow": 10,
+            "name": "demand1",
             "type": "Output"
         }
     ],
-    "recorders": {
-        "recorder": {
-            "node": "supply",
-            "type": "NumpyArrayNodeRecorder"
-        }
-    },
     "timestepper": {
         "end": "2015-12-31",
         "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/docs/source/tutorial.rst` & `pywr-1.9.0/docs/source/tutorial.rst`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-Tutorial
-========
-
-A minimal example
------------------
-
-The simplest example has two nodes: an :func:`~pywr.core.Input` which adds flow to the network and an :func:`~pywr.core.Output` which removes flow from the network.
-
-.. code-block:: python
-
-    from pywr.core import Model, Input, Output
-
-    # create a model (including an empty network)
-    model = Model()
-
-    # create two nodes: a supply, and a demand
-    supply = Input(model, name='supply')
-    demand = Output(model, name='demand')
-
-    # create a connection from the supply to the demand
-    supply.connect(demand)
-
-While technically valid, this model isn't very interesting because we haven't set any constraints or costs on flows in the network.
-
-Let's add some flow constraints to the problem:
-
-.. code-block:: python
-
-    # set maximum flows
-    supply.max_flow = 10.0
-    demand.max_flow = 6.0
-
-The default minimum flow for a :func:`~pywr.core.BaseNode` is zero, so we don't need to set it explicitly.
-
-The model still doesn't do anything as it's missing costs for flow through the nodes. If the cost of supply is less than the benefit received from satisfying demand, flow in the network will occur (within the models constraints).
-
-.. code-block:: python
-   
-    # set cost (+ve) or benefit (-ve)
-    supply.cost = 3.0
-    demand.cost = -100.0
-
-Next we need to tell the model how long to run for. As an example, we'll use a daily timestep for all of 2015.
-
-.. code-block:: python
-
-    import datetime
-    from pywr.core import Timestepper
-
-    model.timestepper = Timestepper(
-        pandas.to_datetime('2015-01-01'),  # first day
-        pandas.to_datetime('2015-12-31'),  # last day
-        datetime.timedelta(1)  # interval
-    )
-
-In order to capture the output from the model we need to use a recorder, such as the :func:`pywr.recorders.NumpyArrayNodeRecorder`.
-
-.. code-block:: python
-
-    from pywr.recorders import NumpyArrayNodeRecorder
-
-    recorder = NumpyArrayNodeRecorder(model, supply)
-
-Finally we are ready to run our model:
-
-.. code-block:: python
-
-    # lets get this party started!
-    model.run()
-
-We can check the result for the first timestep by accessing the recorder's data property:
-
-.. code-block:: python
-
-    scenario = 0
-    timestep = 0
-    print(recorder.data[scenario][timestep])  # prints 6.0
-
-The result of this example model is trivial: the supply exceeds the demand, so the maximum flow at the demand is the limiting factor.
-
-The model defined about could equally have been defined in a JSON document:
-
-.. literalinclude:: tutorial.json
-
-The model can be loaded using the ``Model.load`` method:
-
-.. code-block:: python
-
-    # load the model
-    model = Model.load("tutorial.json")
-    # run, forest, run!
-    model.run()
-
-See further information in on JSON see the :ref:`json-model-format` section.
+Tutorial
+========
+
+A minimal example
+-----------------
+
+The simplest example has two nodes: an :func:`~pywr.core.Input` which adds flow to the network and an :func:`~pywr.core.Output` which removes flow from the network.
+
+.. code-block:: python
+
+    from pywr.core import Model, Input, Output
+
+    # create a model (including an empty network)
+    model = Model()
+
+    # create two nodes: a supply, and a demand
+    supply = Input(model, name='supply')
+    demand = Output(model, name='demand')
+
+    # create a connection from the supply to the demand
+    supply.connect(demand)
+
+While technically valid, this model isn't very interesting because we haven't set any constraints or costs on flows in the network.
+
+Let's add some flow constraints to the problem:
+
+.. code-block:: python
+
+    # set maximum flows
+    supply.max_flow = 10.0
+    demand.max_flow = 6.0
+
+The default minimum flow for a :func:`~pywr.core.BaseNode` is zero, so we don't need to set it explicitly.
+
+The model still doesn't do anything as it's missing costs for flow through the nodes. If the cost of supply is less than the benefit received from satisfying demand, flow in the network will occur (within the models constraints).
+
+.. code-block:: python
+   
+    # set cost (+ve) or benefit (-ve)
+    supply.cost = 3.0
+    demand.cost = -100.0
+
+Next we need to tell the model how long to run for. As an example, we'll use a daily timestep for all of 2015.
+
+.. code-block:: python
+
+    import datetime
+    from pywr.core import Timestepper
+
+    model.timestepper = Timestepper(
+        pandas.to_datetime('2015-01-01'),  # first day
+        pandas.to_datetime('2015-12-31'),  # last day
+        datetime.timedelta(1)  # interval
+    )
+
+In order to capture the output from the model we need to use a recorder, such as the :func:`pywr.recorders.NumpyArrayNodeRecorder`.
+
+.. code-block:: python
+
+    from pywr.recorders import NumpyArrayNodeRecorder
+
+    recorder = NumpyArrayNodeRecorder(model, supply)
+
+Finally we are ready to run our model:
+
+.. code-block:: python
+
+    # lets get this party started!
+    model.run()
+
+We can check the result for the first timestep by accessing the recorder's data property:
+
+.. code-block:: python
+
+    scenario = 0
+    timestep = 0
+    print(recorder.data[scenario][timestep])  # prints 6.0
+
+The result of this example model is trivial: the supply exceeds the demand, so the maximum flow at the demand is the limiting factor.
+
+The model defined about could equally have been defined in a JSON document:
+
+.. literalinclude:: tutorial.json
+
+The model can be loaded using the ``Model.load`` method:
+
+.. code-block:: python
+
+    # load the model
+    model = Model.load("tutorial.json")
+    # run, forest, run!
+    model.run()
+
+See further information in on JSON see the :ref:`json-model-format` section.
```

### Comparing `pywr-1.8.0/examples/data/thames_stochastic_flow.gz` & `pywr-1.9.0/examples/data/thames_stochastic_flow.gz`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/examples/hydropower_example.json` & `pywr-1.9.0/examples/hydropower_example.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,214 +1,206 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4879 6472 6f70 6f77  itle": "Hydropow
-00000030: 6572 2065 7861 6d70 6c65 222c 0d0a 2020  er example",..  
-00000040: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
-00000050: 6f6e 223a 2022 4120 6d6f 6465 6c20 7769  on": "A model wi
-00000060: 7468 2061 2073 696e 676c 6520 7265 7365  th a single rese
-00000070: 7276 6f69 7220 616e 6420 6879 6472 6f2d  rvoir and hydro-
-00000080: 706f 7765 7220 7265 636f 7264 6572 222c  power recorder",
-00000090: 0d0a 2020 2020 2020 2020 226d 696e 696d  ..        "minim
-000000a0: 756d 5f76 6572 7369 6f6e 223a 2022 302e  um_version": "0.
-000000b0: 3422 0d0a 2020 2020 7d2c 0d0a 2020 2020  4"..    },..    
-000000c0: 2274 696d 6573 7465 7070 6572 223a 207b  "timestepper": {
-000000d0: 0d0a 2020 2020 2020 2020 2273 7461 7274  ..        "start
-000000e0: 223a 2022 3231 3030 2d30 312d 3031 222c  ": "2100-01-01",
-000000f0: 0d0a 2020 2020 2020 2020 2265 6e64 223a  ..        "end":
-00000100: 2022 3231 3939 2d30 312d 3031 222c 0d0a   "2199-01-01",..
-00000110: 2020 2020 2020 2020 2274 696d 6573 7465          "timeste
-00000120: 7022 3a20 370d 0a20 2020 207d 2c0d 0a20  p": 7..    },.. 
-00000130: 2020 2022 6e6f 6465 7322 3a20 5b0d 0a20     "nodes": [.. 
-00000140: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000150: 2020 2020 2020 226e 616d 6522 3a20 2263        "name": "c
-00000160: 6174 6368 6d65 6e74 3122 2c0d 0a20 2020  atchment1",..   
-00000170: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000180: 2022 6361 7463 686d 656e 7422 2c0d 0a20   "catchment",.. 
-00000190: 2020 2020 2020 2020 2020 2022 666c 6f77             "flow
-000001a0: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-000001b0: 2020 2020 2274 7970 6522 3a20 2264 6174      "type": "dat
-000001c0: 6166 7261 6d65 222c 0d0a 2020 2020 2020  aframe",..      
-000001d0: 2020 2020 2020 2020 2275 726c 223a 2022          "url": "
-000001e0: 6461 7461 2f74 6861 6d65 735f 7374 6f63  data/thames_stoc
-000001f0: 6861 7374 6963 5f66 6c6f 772e 677a 222c  hastic_flow.gz",
-00000200: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000210: 2263 6f6c 756d 6e22 3a20 2266 6c6f 7722  "column": "flow"
-00000220: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-00000230: 2022 696e 6465 785f 636f 6c22 3a20 2244   "index_col": "D
-00000240: 6174 6522 2c0d 0a20 2020 2020 2020 2020  ate",..         
-00000250: 2020 2020 2022 7061 7273 655f 6461 7465       "parse_date
-00000260: 7322 3a20 7472 7565 0d0a 2020 2020 2020  s": true..      
-00000270: 2020 2020 2020 7d0d 0a20 2020 2020 2020        }..       
-00000280: 207d 2c0d 0a20 2020 2020 2020 207b 0d0a   },..        {..
-00000290: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-000002a0: 6522 3a20 2272 6573 6572 766f 6972 3122  e": "reservoir1"
-000002b0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000002c0: 7479 7065 223a 2022 7374 6f72 6167 6522  type": "storage"
-000002d0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000002e0: 6d61 785f 766f 6c75 6d65 223a 2032 3030  max_volume": 200
-000002f0: 3030 302c 0d0a 2020 2020 2020 2020 2020  000,..          
-00000300: 2020 2269 6e69 7469 616c 5f76 6f6c 756d    "initial_volum
-00000310: 6522 3a20 3137 3030 3030 0d0a 2020 2020  e": 170000..    
-00000320: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000330: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000340: 6e61 6d65 223a 2022 7265 6c65 6173 6531  name": "release1
-00000350: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000360: 2274 7970 6522 3a20 226c 696e 6b22 2c0d  "type": "link",.
-00000370: 0a20 2020 2020 2020 2020 2020 2022 6d61  .            "ma
-00000380: 785f 666c 6f77 223a 2031 302c 0d0a 2020  x_flow": 10,..  
-00000390: 2020 2020 2020 2020 2020 2263 6f73 7422            "cost"
-000003a0: 3a20 2d35 3030 0d0a 2020 2020 2020 2020  : -500..        
-000003b0: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-000003c0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000003d0: 223a 2022 7475 7262 696e 6531 222c 0d0a  ": "turbine1",..
-000003e0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-000003f0: 6522 3a20 226c 696e 6b22 2c0d 0a20 2020  e": "link",..   
-00000400: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
-00000410: 6f77 223a 2022 7475 7262 696e 6531 5f64  ow": "turbine1_d
-00000420: 6973 6368 6172 6765 222c 0d0a 2020 2020  ischarge",..    
-00000430: 2020 2020 2020 2020 2263 6f73 7422 3a20          "cost": 
-00000440: 2d32 3030 0d0a 2020 2020 2020 2020 7d2c  -200..        },
-00000450: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-00000460: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000470: 2022 7370 696c 6c31 222c 0d0a 2020 2020   "spill1",..    
-00000480: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000490: 226c 696e 6b22 2c0d 0a20 2020 2020 2020  "link",..       
-000004a0: 2020 2020 2022 636f 7374 223a 2031 3030       "cost": 100
-000004b0: 300d 0a20 2020 2020 2020 207d 2c0d 0a20  0..        },.. 
-000004c0: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-000004d0: 2020 2020 2020 226e 616d 6522 3a20 2272        "name": "r
-000004e0: 6561 6368 3122 2c0d 0a20 2020 2020 2020  each1",..       
-000004f0: 2020 2020 2022 7479 7065 223a 2022 6c69       "type": "li
-00000500: 6e6b 220d 0a20 2020 2020 2020 207d 2c0d  nk"..        },.
-00000510: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000520: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000530: 2265 6e64 3122 2c0d 0a20 2020 2020 2020  "end1",..       
-00000540: 2020 2020 2022 7479 7065 223a 2022 6f75       "type": "ou
-00000550: 7470 7574 220d 0a20 2020 2020 2020 207d  tput"..        }
-00000560: 0d0a 2020 2020 5d2c 0d0a 2020 2020 2265  ..    ],..    "e
-00000570: 6467 6573 223a 205b 0d0a 2020 2020 2020  dges": [..      
-00000580: 2020 5b22 6361 7463 686d 656e 7431 222c    ["catchment1",
-00000590: 2022 7265 7365 7276 6f69 7231 225d 2c0d   "reservoir1"],.
-000005a0: 0a20 2020 2020 2020 205b 2272 6573 6572  .        ["reser
-000005b0: 766f 6972 3122 2c20 2272 656c 6561 7365  voir1", "release
-000005c0: 3122 5d2c 0d0a 2020 2020 2020 2020 5b22  1"],..        ["
-000005d0: 7265 7365 7276 6f69 7231 222c 2022 7475  reservoir1", "tu
-000005e0: 7262 696e 6531 225d 2c0d 0a20 2020 2020  rbine1"],..     
-000005f0: 2020 205b 2272 6573 6572 766f 6972 3122     ["reservoir1"
-00000600: 2c20 2273 7069 6c6c 3122 5d2c 0d0a 2020  , "spill1"],..  
-00000610: 2020 2020 2020 5b22 7265 6c65 6173 6531        ["release1
-00000620: 222c 2022 7265 6163 6831 225d 2c0d 0a20  ", "reach1"],.. 
-00000630: 2020 2020 2020 205b 2274 7572 6269 6e65         ["turbine
-00000640: 3122 2c20 2272 6561 6368 3122 5d2c 0d0a  1", "reach1"],..
-00000650: 2020 2020 2020 2020 5b22 7370 696c 6c31          ["spill1
-00000660: 222c 2022 7265 6163 6831 225d 2c0d 0a20  ", "reach1"],.. 
-00000670: 2020 2020 2020 205b 2272 6561 6368 3122         ["reach1"
-00000680: 2c20 2265 6e64 3122 5d0d 0a20 2020 205d  , "end1"]..    ]
-00000690: 2c0d 0a20 2020 2022 7061 7261 6d65 7465  ,..    "paramete
-000006a0: 7273 223a 207b 0d0a 2020 2020 2020 2020  rs": {..        
-000006b0: 2272 6573 6572 766f 6972 315f 6c65 7665  "reservoir1_leve
-000006c0: 6c22 3a20 7b0d 0a20 2020 2020 2020 2020  l": {..         
-000006d0: 2022 7479 7065 223a 2022 696e 7465 7270   "type": "interp
-000006e0: 6f6c 6174 6564 766f 6c75 6d65 222c 0d0a  olatedvolume",..
-000006f0: 2020 2020 2020 2020 2020 226e 6f64 6522            "node"
-00000700: 3a20 2272 6573 6572 766f 6972 3122 2c0d  : "reservoir1",.
-00000710: 0a20 2020 2020 2020 2020 2022 766f 6c75  .          "volu
-00000720: 6d65 7322 3a20 5b30 2c20 3235 3030 302c  mes": [0, 25000,
-00000730: 2035 3030 3030 2c20 3735 3030 302c 2031   50000, 75000, 1
-00000740: 3030 3030 302c 2031 3530 3030 302c 2032  00000, 150000, 2
-00000750: 3030 3030 305d 2c0d 0a20 2020 2020 2020  00000],..       
-00000760: 2020 2022 7661 6c75 6573 223a 205b 302c     "values": [0,
-00000770: 2032 392e 322c 2033 362e 382c 2034 322e   29.2, 36.8, 42.
-00000780: 322c 2034 362e 342c 2035 332e 312c 2035  2, 46.4, 53.1, 5
-00000790: 382e 355d 2c0d 0a20 2020 2020 2020 2020  8.5],..         
-000007a0: 2022 6b69 6e64 223a 2022 6375 6269 6322   "kind": "cubic"
-000007b0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-000007c0: 2020 2020 2020 2274 7572 6269 6e65 315f        "turbine1_
-000007d0: 6469 7363 6861 7267 6522 3a20 7b0d 0a20  discharge": {.. 
-000007e0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-000007f0: 223a 2022 696e 6465 7865 6461 7272 6179  ": "indexedarray
-00000800: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000810: 2269 6e64 6578 5f70 6172 616d 6574 6572  "index_parameter
-00000820: 223a 2022 7475 7262 696e 6531 5f63 6f6e  ": "turbine1_con
-00000830: 7472 6f6c 222c 0d0a 2020 2020 2020 2020  trol",..        
-00000840: 2020 2020 2270 6172 616d 7322 3a20 5b0d      "params": [.
-00000850: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000860: 2034 302e 302c 0d0a 2020 2020 2020 2020   40.0,..        
-00000870: 2020 2020 2020 2020 302e 300d 0a20 2020          0.0..   
-00000880: 2020 2020 2020 2020 205d 0d0a 2020 2020           ]..    
-00000890: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-000008a0: 2274 7572 6269 6e65 315f 636f 6e74 726f  "turbine1_contro
-000008b0: 6c22 3a20 7b0d 0a20 2020 2020 2020 2020  l": {..         
-000008c0: 2020 2022 7479 7065 223a 2022 636f 6e74     "type": "cont
-000008d0: 726f 6c63 7572 7665 696e 6465 7822 2c0d  rolcurveindex",.
-000008e0: 0a20 2020 2020 2020 2020 2020 2022 7374  .            "st
-000008f0: 6f72 6167 655f 6e6f 6465 223a 2022 7265  orage_node": "re
-00000900: 7365 7276 6f69 7231 222c 0d0a 2020 2020  servoir1",..    
-00000910: 2020 2020 2020 2020 2263 6f6e 7472 6f6c          "control
-00000920: 5f63 7572 7665 7322 3a20 5b0d 0a20 2020  _curves": [..   
-00000930: 2020 2020 2020 2020 2020 2020 2022 7475               "tu
-00000940: 7262 696e 6531 5f63 6f6e 7472 6f6c 5f63  rbine1_control_c
-00000950: 7572 7665 220d 0a20 2020 2020 2020 2020  urve"..         
-00000960: 2020 205d 0d0a 2020 2020 2020 2020 7d2c     ]..        },
-00000970: 0d0a 2020 2020 2020 2020 2274 7572 6269  ..        "turbi
-00000980: 6e65 315f 636f 6e74 726f 6c5f 6375 7276  ne1_control_curv
-00000990: 6522 3a20 7b0d 0a20 2020 2020 2020 2020  e": {..         
-000009a0: 2020 2022 7479 7065 223a 2022 6d6f 6e74     "type": "mont
-000009b0: 686c 7970 726f 6669 6c65 222c 0d0a 2020  hlyprofile",..  
-000009c0: 2020 2020 2020 2020 2020 2276 616c 7565            "value
-000009d0: 7322 3a20 5b30 2e38 2c20 302e 382c 2030  s": [0.8, 0.8, 0
-000009e0: 2e38 2c20 302e 382c 2030 2e38 2c20 302e  .8, 0.8, 0.8, 0.
-000009f0: 382c 2030 2e38 2c20 302e 382c 2030 2e38  8, 0.8, 0.8, 0.8
-00000a00: 2c20 302e 382c 2030 2e38 2c20 302e 385d  , 0.8, 0.8, 0.8]
-00000a10: 0d0a 2020 2020 2020 2020 7d0d 0a20 2020  ..        }..   
-00000a20: 207d 2c0d 0a20 2020 2022 7265 636f 7264   },..    "record
-00000a30: 6572 7322 3a20 7b0d 0a20 2020 2020 2020  ers": {..       
-00000a40: 2022 7475 7262 696e 6531 5f65 6e65 7267   "turbine1_energ
-00000a50: 7922 3a20 7b0d 0a20 2020 2020 2020 2020  y": {..         
-00000a60: 2020 2022 7479 7065 223a 2022 4879 6472     "type": "Hydr
-00000a70: 6f50 6f77 6572 5265 636f 7264 6572 222c  oPowerRecorder",
-00000a80: 0d0a 2020 2020 2020 2020 2020 2020 226e  ..            "n
-00000a90: 6f64 6522 3a20 2274 7572 6269 6e65 3122  ode": "turbine1"
-00000aa0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000ab0: 7761 7465 725f 656c 6576 6174 696f 6e5f  water_elevation_
-00000ac0: 7061 7261 6d65 7465 7222 3a20 2272 6573  parameter": "res
-00000ad0: 6572 766f 6972 315f 6c65 7665 6c22 2c0d  ervoir1_level",.
-00000ae0: 0a20 2020 2020 2020 2020 2020 2022 7475  .            "tu
-00000af0: 7262 696e 655f 656c 6576 6174 696f 6e22  rbine_elevation"
-00000b00: 3a20 3335 2e30 2c0d 0a20 2020 2020 2020  : 35.0,..       
-00000b10: 2020 2020 2022 6566 6669 6369 656e 6379       "efficiency
-00000b20: 223a 2030 2e38 352c 0d0a 2020 2020 2020  ": 0.85,..      
-00000b30: 2020 2020 2020 2266 6c6f 775f 756e 6974        "flow_unit
-00000b40: 5f63 6f6e 7665 7273 696f 6e22 3a20 3165  _conversion": 1e
-00000b50: 330d 0a20 2020 2020 2020 207d 2c0d 0a20  3..        },.. 
-00000b60: 2020 2020 2020 2022 6361 7463 686d 656e         "catchmen
-00000b70: 7431 5f66 6c6f 7722 3a20 7b0d 0a20 2020  t1_flow": {..   
-00000b80: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000b90: 2022 6e75 6d70 7961 7272 6179 6e6f 6465   "numpyarraynode
-00000ba0: 7265 636f 7264 6572 222c 0d0a 2020 2020  recorder",..    
-00000bb0: 2020 2020 2020 2020 226e 6f64 6522 3a20          "node": 
-00000bc0: 2263 6174 6368 6d65 6e74 3122 0d0a 2020  "catchment1"..  
-00000bd0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000be0: 2020 2272 6573 6572 766f 6972 315f 7374    "reservoir1_st
-00000bf0: 6f72 6167 6522 3a20 7b0d 0a20 2020 2020  orage": {..     
-00000c00: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000c10: 6e75 6d70 7961 7272 6179 7374 6f72 6167  numpyarraystorag
-00000c20: 6572 6563 6f72 6465 7222 2c0d 0a20 2020  erecorder",..   
-00000c30: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
-00000c40: 2022 7265 7365 7276 6f69 7231 220d 0a20   "reservoir1".. 
-00000c50: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000c60: 2020 2022 7475 7262 696e 6531 5f66 6c6f     "turbine1_flo
-00000c70: 7722 3a20 7b0d 0a20 2020 2020 2020 2020  w": {..         
-00000c80: 2020 2022 7479 7065 223a 2022 6e75 6d70     "type": "nump
-00000c90: 7961 7272 6179 6e6f 6465 7265 636f 7264  yarraynoderecord
-00000ca0: 6572 222c 0d0a 2020 2020 2020 2020 2020  er",..          
-00000cb0: 2020 226e 6f64 6522 3a20 2274 7572 6269    "node": "turbi
-00000cc0: 6e65 3122 0d0a 2020 2020 2020 2020 7d2c  ne1"..        },
-00000cd0: 0d0a 2020 2020 2020 2020 2272 656c 6561  ..        "relea
-00000ce0: 7365 315f 666c 6f77 223a 207b 0d0a 2020  se1_flow": {..  
-00000cf0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000d00: 3a20 226e 756d 7079 6172 7261 796e 6f64  : "numpyarraynod
-00000d10: 6572 6563 6f72 6465 7222 2c0d 0a20 2020  erecorder",..   
-00000d20: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
-00000d30: 2022 7265 6c65 6173 6531 220d 0a20 2020   "release1"..   
-00000d40: 2020 2020 207d 0d0a 2020 2020 7d0d 0a7d       }..    }..}
-00000d50: 0d0a                                     ..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4879 6472 6f70 6f77 6572  le": "Hydropower
+00000030: 2065 7861 6d70 6c65 222c 0a20 2020 2020   example",.     
+00000040: 2020 2022 6465 7363 7269 7074 696f 6e22     "description"
+00000050: 3a20 2241 206d 6f64 656c 2077 6974 6820  : "A model with 
+00000060: 6120 7369 6e67 6c65 2072 6573 6572 766f  a single reservo
+00000070: 6972 2061 6e64 2068 7964 726f 2d70 6f77  ir and hydro-pow
+00000080: 6572 2072 6563 6f72 6465 7222 2c0a 2020  er recorder",.  
+00000090: 2020 2020 2020 226d 696e 696d 756d 5f76        "minimum_v
+000000a0: 6572 7369 6f6e 223a 2022 302e 3422 0a20  ersion": "0.4". 
+000000b0: 2020 207d 2c0a 2020 2020 2274 696d 6573     },.    "times
+000000c0: 7465 7070 6572 223a 207b 0a20 2020 2020  tepper": {.     
+000000d0: 2020 2022 7374 6172 7422 3a20 2232 3130     "start": "210
+000000e0: 302d 3031 2d30 3122 2c0a 2020 2020 2020  0-01-01",.      
+000000f0: 2020 2265 6e64 223a 2022 3231 3939 2d30    "end": "2199-0
+00000100: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+00000110: 7469 6d65 7374 6570 223a 2037 0a20 2020  timestep": 7.   
+00000120: 207d 2c0a 2020 2020 226e 6f64 6573 223a   },.    "nodes":
+00000130: 205b 0a20 2020 2020 2020 207b 0a20 2020   [.        {.   
+00000140: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000150: 2022 6361 7463 686d 656e 7431 222c 0a20   "catchment1",. 
+00000160: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000170: 223a 2022 6361 7463 686d 656e 7422 2c0a  ": "catchment",.
+00000180: 2020 2020 2020 2020 2020 2020 2266 6c6f              "flo
+00000190: 7722 3a20 7b0a 2020 2020 2020 2020 2020  w": {.          
+000001a0: 2020 2020 2274 7970 6522 3a20 2264 6174      "type": "dat
+000001b0: 6166 7261 6d65 222c 0a20 2020 2020 2020  aframe",.       
+000001c0: 2020 2020 2020 2022 7572 6c22 3a20 2264         "url": "d
+000001d0: 6174 612f 7468 616d 6573 5f73 746f 6368  ata/thames_stoch
+000001e0: 6173 7469 635f 666c 6f77 2e67 7a22 2c0a  astic_flow.gz",.
+000001f0: 2020 2020 2020 2020 2020 2020 2020 2263                "c
+00000200: 6f6c 756d 6e22 3a20 2266 6c6f 7722 2c0a  olumn": "flow",.
+00000210: 2020 2020 2020 2020 2020 2020 2020 2269                "i
+00000220: 6e64 6578 5f63 6f6c 223a 2022 4461 7465  ndex_col": "Date
+00000230: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00000240: 2022 7061 7273 655f 6461 7465 7322 3a20   "parse_dates": 
+00000250: 7472 7565 0a20 2020 2020 2020 2020 2020  true.           
+00000260: 207d 0a20 2020 2020 2020 207d 2c0a 2020   }.        },.  
+00000270: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000280: 2020 2020 226e 616d 6522 3a20 2272 6573      "name": "res
+00000290: 6572 766f 6972 3122 2c0a 2020 2020 2020  ervoir1",.      
+000002a0: 2020 2020 2020 2274 7970 6522 3a20 2273        "type": "s
+000002b0: 746f 7261 6765 222c 0a20 2020 2020 2020  torage",.       
+000002c0: 2020 2020 2022 6d61 785f 766f 6c75 6d65       "max_volume
+000002d0: 223a 2032 3030 3030 302c 0a20 2020 2020  ": 200000,.     
+000002e0: 2020 2020 2020 2022 696e 6974 6961 6c5f         "initial_
+000002f0: 766f 6c75 6d65 223a 2031 3730 3030 300a  volume": 170000.
+00000300: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000310: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000320: 2022 6e61 6d65 223a 2022 7265 6c65 6173   "name": "releas
+00000330: 6531 222c 0a20 2020 2020 2020 2020 2020  e1",.           
+00000340: 2022 7479 7065 223a 2022 6c69 6e6b 222c   "type": "link",
+00000350: 0a20 2020 2020 2020 2020 2020 2022 6d61  .            "ma
+00000360: 785f 666c 6f77 223a 2031 302c 0a20 2020  x_flow": 10,.   
+00000370: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
+00000380: 202d 3530 300a 2020 2020 2020 2020 7d2c   -500.        },
+00000390: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+000003a0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
+000003b0: 7475 7262 696e 6531 222c 0a20 2020 2020  turbine1",.     
+000003c0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+000003d0: 6c69 6e6b 222c 0a20 2020 2020 2020 2020  link",.         
+000003e0: 2020 2022 6d61 785f 666c 6f77 223a 2022     "max_flow": "
+000003f0: 7475 7262 696e 6531 5f64 6973 6368 6172  turbine1_dischar
+00000400: 6765 222c 0a20 2020 2020 2020 2020 2020  ge",.           
+00000410: 2022 636f 7374 223a 202d 3230 300a 2020   "cost": -200.  
+00000420: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000430: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+00000440: 6e61 6d65 223a 2022 7370 696c 6c31 222c  name": "spill1",
+00000450: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000460: 7065 223a 2022 6c69 6e6b 222c 0a20 2020  pe": "link",.   
+00000470: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
+00000480: 2031 3030 300a 2020 2020 2020 2020 7d2c   1000.        },
+00000490: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+000004a0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
+000004b0: 7265 6163 6831 222c 0a20 2020 2020 2020  reach1",.       
+000004c0: 2020 2020 2022 7479 7065 223a 2022 6c69       "type": "li
+000004d0: 6e6b 220a 2020 2020 2020 2020 7d2c 0a20  nk".        },. 
+000004e0: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+000004f0: 2020 2020 2022 6e61 6d65 223a 2022 656e       "name": "en
+00000500: 6431 222c 0a20 2020 2020 2020 2020 2020  d1",.           
+00000510: 2022 7479 7065 223a 2022 6f75 7470 7574   "type": "output
+00000520: 220a 2020 2020 2020 2020 7d0a 2020 2020  ".        }.    
+00000530: 5d2c 0a20 2020 2022 6564 6765 7322 3a20  ],.    "edges": 
+00000540: 5b0a 2020 2020 2020 2020 5b22 6361 7463  [.        ["catc
+00000550: 686d 656e 7431 222c 2022 7265 7365 7276  hment1", "reserv
+00000560: 6f69 7231 225d 2c0a 2020 2020 2020 2020  oir1"],.        
+00000570: 5b22 7265 7365 7276 6f69 7231 222c 2022  ["reservoir1", "
+00000580: 7265 6c65 6173 6531 225d 2c0a 2020 2020  release1"],.    
+00000590: 2020 2020 5b22 7265 7365 7276 6f69 7231      ["reservoir1
+000005a0: 222c 2022 7475 7262 696e 6531 225d 2c0a  ", "turbine1"],.
+000005b0: 2020 2020 2020 2020 5b22 7265 7365 7276          ["reserv
+000005c0: 6f69 7231 222c 2022 7370 696c 6c31 225d  oir1", "spill1"]
+000005d0: 2c0a 2020 2020 2020 2020 5b22 7265 6c65  ,.        ["rele
+000005e0: 6173 6531 222c 2022 7265 6163 6831 225d  ase1", "reach1"]
+000005f0: 2c0a 2020 2020 2020 2020 5b22 7475 7262  ,.        ["turb
+00000600: 696e 6531 222c 2022 7265 6163 6831 225d  ine1", "reach1"]
+00000610: 2c0a 2020 2020 2020 2020 5b22 7370 696c  ,.        ["spil
+00000620: 6c31 222c 2022 7265 6163 6831 225d 2c0a  l1", "reach1"],.
+00000630: 2020 2020 2020 2020 5b22 7265 6163 6831          ["reach1
+00000640: 222c 2022 656e 6431 225d 0a20 2020 205d  ", "end1"].    ]
+00000650: 2c0a 2020 2020 2270 6172 616d 6574 6572  ,.    "parameter
+00000660: 7322 3a20 7b0a 2020 2020 2020 2020 2272  s": {.        "r
+00000670: 6573 6572 766f 6972 315f 6c65 7665 6c22  eservoir1_level"
+00000680: 3a20 7b0a 2020 2020 2020 2020 2020 2274  : {.          "t
+00000690: 7970 6522 3a20 2269 6e74 6572 706f 6c61  ype": "interpola
+000006a0: 7465 6476 6f6c 756d 6522 2c0a 2020 2020  tedvolume",.    
+000006b0: 2020 2020 2020 226e 6f64 6522 3a20 2272        "node": "r
+000006c0: 6573 6572 766f 6972 3122 2c0a 2020 2020  eservoir1",.    
+000006d0: 2020 2020 2020 2276 6f6c 756d 6573 223a        "volumes":
+000006e0: 205b 302c 2032 3530 3030 2c20 3530 3030   [0, 25000, 5000
+000006f0: 302c 2037 3530 3030 2c20 3130 3030 3030  0, 75000, 100000
+00000700: 2c20 3135 3030 3030 2c20 3230 3030 3030  , 150000, 200000
+00000710: 5d2c 0a20 2020 2020 2020 2020 2022 7661  ],.          "va
+00000720: 6c75 6573 223a 205b 302c 2032 392e 322c  lues": [0, 29.2,
+00000730: 2033 362e 382c 2034 322e 322c 2034 362e   36.8, 42.2, 46.
+00000740: 342c 2035 332e 312c 2035 382e 355d 2c0a  4, 53.1, 58.5],.
+00000750: 2020 2020 2020 2020 2020 226b 696e 6422            "kind"
+00000760: 3a20 2263 7562 6963 220a 2020 2020 2020  : "cubic".      
+00000770: 2020 7d2c 0a20 2020 2020 2020 2022 7475    },.        "tu
+00000780: 7262 696e 6531 5f64 6973 6368 6172 6765  rbine1_discharge
+00000790: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
+000007a0: 2022 7479 7065 223a 2022 696e 6465 7865   "type": "indexe
+000007b0: 6461 7272 6179 222c 0a20 2020 2020 2020  darray",.       
+000007c0: 2020 2020 2022 696e 6465 785f 7061 7261       "index_para
+000007d0: 6d65 7465 7222 3a20 2274 7572 6269 6e65  meter": "turbine
+000007e0: 315f 636f 6e74 726f 6c22 2c0a 2020 2020  1_control",.    
+000007f0: 2020 2020 2020 2020 2270 6172 616d 7322          "params"
+00000800: 3a20 5b0a 2020 2020 2020 2020 2020 2020  : [.            
+00000810: 2020 2020 3430 2e30 2c0a 2020 2020 2020      40.0,.      
+00000820: 2020 2020 2020 2020 2020 302e 300a 2020            0.0.  
+00000830: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+00000840: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000850: 7475 7262 696e 6531 5f63 6f6e 7472 6f6c  turbine1_control
+00000860: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
+00000870: 2022 7479 7065 223a 2022 636f 6e74 726f   "type": "contro
+00000880: 6c63 7572 7665 696e 6465 7822 2c0a 2020  lcurveindex",.  
+00000890: 2020 2020 2020 2020 2020 2273 746f 7261            "stora
+000008a0: 6765 5f6e 6f64 6522 3a20 2272 6573 6572  ge_node": "reser
+000008b0: 766f 6972 3122 2c0a 2020 2020 2020 2020  voir1",.        
+000008c0: 2020 2020 2263 6f6e 7472 6f6c 5f63 7572      "control_cur
+000008d0: 7665 7322 3a20 5b0a 2020 2020 2020 2020  ves": [.        
+000008e0: 2020 2020 2020 2020 2274 7572 6269 6e65          "turbine
+000008f0: 315f 636f 6e74 726f 6c5f 6375 7276 6522  1_control_curve"
+00000900: 0a20 2020 2020 2020 2020 2020 205d 0a20  .            ]. 
+00000910: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+00000920: 2020 2274 7572 6269 6e65 315f 636f 6e74    "turbine1_cont
+00000930: 726f 6c5f 6375 7276 6522 3a20 7b0a 2020  rol_curve": {.  
+00000940: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000950: 3a20 226d 6f6e 7468 6c79 7072 6f66 696c  : "monthlyprofil
+00000960: 6522 2c0a 2020 2020 2020 2020 2020 2020  e",.            
+00000970: 2276 616c 7565 7322 3a20 5b30 2e38 2c20  "values": [0.8, 
+00000980: 302e 382c 2030 2e38 2c20 302e 382c 2030  0.8, 0.8, 0.8, 0
+00000990: 2e38 2c20 302e 382c 2030 2e38 2c20 302e  .8, 0.8, 0.8, 0.
+000009a0: 382c 2030 2e38 2c20 302e 382c 2030 2e38  8, 0.8, 0.8, 0.8
+000009b0: 2c20 302e 385d 0a20 2020 2020 2020 207d  , 0.8].        }
+000009c0: 0a20 2020 207d 2c0a 2020 2020 2272 6563  .    },.    "rec
+000009d0: 6f72 6465 7273 223a 207b 0a20 2020 2020  orders": {.     
+000009e0: 2020 2022 7475 7262 696e 6531 5f65 6e65     "turbine1_ene
+000009f0: 7267 7922 3a20 7b0a 2020 2020 2020 2020  rgy": {.        
+00000a00: 2020 2020 2274 7970 6522 3a20 2248 7964      "type": "Hyd
+00000a10: 726f 506f 7765 7252 6563 6f72 6465 7222  roPowerRecorder"
+00000a20: 2c0a 2020 2020 2020 2020 2020 2020 226e  ,.            "n
+00000a30: 6f64 6522 3a20 2274 7572 6269 6e65 3122  ode": "turbine1"
+00000a40: 2c0a 2020 2020 2020 2020 2020 2020 2277  ,.            "w
+00000a50: 6174 6572 5f65 6c65 7661 7469 6f6e 5f70  ater_elevation_p
+00000a60: 6172 616d 6574 6572 223a 2022 7265 7365  arameter": "rese
+00000a70: 7276 6f69 7231 5f6c 6576 656c 222c 0a20  rvoir1_level",. 
+00000a80: 2020 2020 2020 2020 2020 2022 7475 7262             "turb
+00000a90: 696e 655f 656c 6576 6174 696f 6e22 3a20  ine_elevation": 
+00000aa0: 3335 2e30 2c0a 2020 2020 2020 2020 2020  35.0,.          
+00000ab0: 2020 2265 6666 6963 6965 6e63 7922 3a20    "efficiency": 
+00000ac0: 302e 3835 2c0a 2020 2020 2020 2020 2020  0.85,.          
+00000ad0: 2020 2266 6c6f 775f 756e 6974 5f63 6f6e    "flow_unit_con
+00000ae0: 7665 7273 696f 6e22 3a20 3165 330a 2020  version": 1e3.  
+00000af0: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000b00: 2022 6361 7463 686d 656e 7431 5f66 6c6f   "catchment1_flo
+00000b10: 7722 3a20 7b0a 2020 2020 2020 2020 2020  w": {.          
+00000b20: 2020 2274 7970 6522 3a20 226e 756d 7079    "type": "numpy
+00000b30: 6172 7261 796e 6f64 6572 6563 6f72 6465  arraynoderecorde
+00000b40: 7222 2c0a 2020 2020 2020 2020 2020 2020  r",.            
+00000b50: 226e 6f64 6522 3a20 2263 6174 6368 6d65  "node": "catchme
+00000b60: 6e74 3122 0a20 2020 2020 2020 207d 2c0a  nt1".        },.
+00000b70: 2020 2020 2020 2020 2272 6573 6572 766f          "reservo
+00000b80: 6972 315f 7374 6f72 6167 6522 3a20 7b0a  ir1_storage": {.
+00000b90: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000ba0: 6522 3a20 226e 756d 7079 6172 7261 7973  e": "numpyarrays
+00000bb0: 746f 7261 6765 7265 636f 7264 6572 222c  toragerecorder",
+00000bc0: 0a20 2020 2020 2020 2020 2020 2022 6e6f  .            "no
+00000bd0: 6465 223a 2022 7265 7365 7276 6f69 7231  de": "reservoir1
+00000be0: 220a 2020 2020 2020 2020 7d2c 0a20 2020  ".        },.   
+00000bf0: 2020 2020 2022 7475 7262 696e 6531 5f66       "turbine1_f
+00000c00: 6c6f 7722 3a20 7b0a 2020 2020 2020 2020  low": {.        
+00000c10: 2020 2020 2274 7970 6522 3a20 226e 756d      "type": "num
+00000c20: 7079 6172 7261 796e 6f64 6572 6563 6f72  pyarraynoderecor
+00000c30: 6465 7222 2c0a 2020 2020 2020 2020 2020  der",.          
+00000c40: 2020 226e 6f64 6522 3a20 2274 7572 6269    "node": "turbi
+00000c50: 6e65 3122 0a20 2020 2020 2020 207d 2c0a  ne1".        },.
+00000c60: 2020 2020 2020 2020 2272 656c 6561 7365          "release
+00000c70: 315f 666c 6f77 223a 207b 0a20 2020 2020  1_flow": {.     
+00000c80: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000c90: 6e75 6d70 7961 7272 6179 6e6f 6465 7265  numpyarraynodere
+00000ca0: 636f 7264 6572 222c 0a20 2020 2020 2020  corder",.       
+00000cb0: 2020 2020 2022 6e6f 6465 223a 2022 7265       "node": "re
+00000cc0: 6c65 6173 6531 220a 2020 2020 2020 2020  lease1".        
+00000cd0: 7d0a 2020 2020 7d0a 7d0a                 }.    }.}.
```

### Comparing `pywr-1.8.0/examples/thames/thames.json` & `pywr-1.9.0/examples/thames/thames.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 22% similar despite different names*

```diff
@@ -1,263 +1,254 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4120 7369 6d70 6c65  itle": "A simple
-00000030: 206d 6f64 656c 2075 7369 6e67 2074 6865   model using the
-00000040: 2072 6976 6572 2054 6861 6d65 7322 2c0d   river Thames",.
-00000050: 0a20 2020 2020 2020 2022 6465 7363 7269  .        "descri
-00000060: 7074 696f 6e22 3a20 2254 6869 7320 6d6f  ption": "This mo
-00000070: 6465 6c20 7573 6573 2072 6976 6572 2054  del uses river T
-00000080: 6861 6d65 7320 696e 666c 6f77 732e 2048  hames inflows. H
-00000090: 6f77 6576 6572 2069 7420 646f 6573 206e  owever it does n
-000000a0: 6f74 2072 6570 7265 7365 6e74 2074 6865  ot represent the
-000000b0: 2054 6861 6d65 7320 5761 7465 7220 7379   Thames Water sy
-000000c0: 7374 656d 206f 7220 7573 6520 616e 7920  stem or use any 
-000000d0: 6461 7461 2066 726f 6d20 5457 554c 2e22  data from TWUL."
-000000e0: 2c0d 0a20 2020 2020 2020 2022 6d69 6e69  ,..        "mini
-000000f0: 6d75 6d5f 7665 7273 696f 6e22 3a20 2230  mum_version": "0
-00000100: 2e34 220d 0a20 2020 207d 2c0d 0a20 2020  .4"..    },..   
-00000110: 2022 7469 6d65 7374 6570 7065 7222 3a20   "timestepper": 
-00000120: 7b0d 0a20 2020 2020 2020 2022 7374 6172  {..        "star
-00000130: 7422 3a20 2232 3130 302d 3031 2d30 3122  t": "2100-01-01"
-00000140: 2c0d 0a20 2020 2020 2020 2022 656e 6422  ,..        "end"
-00000150: 3a20 2232 3139 392d 3031 2d30 3122 2c0d  : "2199-01-01",.
-00000160: 0a20 2020 2020 2020 2022 7469 6d65 7374  .        "timest
-00000170: 6570 223a 2031 0d0a 2020 2020 7d2c 0d0a  ep": 1..    },..
-00000180: 2020 2020 2273 6365 6e61 7269 6f73 223a      "scenarios":
-00000190: 205b 0d0a 2020 2020 2020 2020 7b0d 0a20   [..        {.. 
-000001a0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000001b0: 223a 2022 6465 6d61 6e64 222c 0d0a 2020  ": "demand",..  
-000001c0: 2020 2020 2020 2020 2020 2273 697a 6522            "size"
-000001d0: 3a20 350d 0a20 2020 2020 2020 207d 0d0a  : 5..        }..
-000001e0: 2020 2020 5d2c 0d0a 2020 2020 226e 6f64      ],..    "nod
-000001f0: 6573 223a 205b 0d0a 2020 2020 2020 2020  es": [..        
-00000200: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000210: 6e61 6d65 223a 2022 6361 7463 686d 656e  name": "catchmen
-00000220: 7431 222c 0d0a 2020 2020 2020 2020 2020  t1",..          
-00000230: 2020 2274 7970 6522 3a20 2263 6174 6368    "type": "catch
-00000240: 6d65 6e74 222c 0d0a 2020 2020 2020 2020  ment",..        
-00000250: 2020 2020 2266 6c6f 7722 3a20 2266 6c6f      "flow": "flo
-00000260: 7722 0d0a 2020 2020 2020 2020 7d2c 0d0a  w"..        },..
-00000270: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-00000280: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000290: 7269 7665 7231 222c 0d0a 2020 2020 2020  river1",..      
-000002a0: 2020 2020 2020 2274 7970 6522 3a20 226c        "type": "l
-000002b0: 696e 6b22 0d0a 2020 2020 2020 2020 7d2c  ink"..        },
-000002c0: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-000002d0: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-000002e0: 2022 6d72 6631 222c 0d0a 2020 2020 2020   "mrf1",..      
-000002f0: 2020 2020 2020 2274 7970 6522 3a20 2272        "type": "r
-00000300: 6976 6572 6761 7567 6522 2c0d 0a20 2020  ivergauge",..   
-00000310: 2020 2020 2020 2020 2022 6d72 6622 3a20           "mrf": 
-00000320: 302e 362c 0d0a 2020 2020 2020 2020 2020  0.6,..          
-00000330: 2020 226d 7266 5f63 6f73 7422 3a20 2d31    "mrf_cost": -1
-00000340: 3030 300d 0a20 2020 2020 2020 207d 2c0d  000..        },.
-00000350: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000360: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000370: 2274 6572 6d31 222c 0d0a 2020 2020 2020  "term1",..      
-00000380: 2020 2020 2020 2274 7970 6522 3a20 226f        "type": "o
-00000390: 7574 7075 7422 0d0a 2020 2020 2020 2020  utput"..        
-000003a0: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-000003b0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000003c0: 223a 2022 7265 7365 7276 6f69 7231 222c  ": "reservoir1",
-000003d0: 0d0a 2020 2020 2020 2020 2020 2020 2274  ..            "t
-000003e0: 7970 6522 3a20 2273 746f 7261 6765 222c  ype": "storage",
-000003f0: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-00000400: 6178 5f76 6f6c 756d 6522 3a20 3230 302c  ax_volume": 200,
-00000410: 0d0a 2020 2020 2020 2020 2020 2020 2269  ..            "i
-00000420: 6e69 7469 616c 5f76 6f6c 756d 6522 3a20  nitial_volume": 
-00000430: 3230 302c 0d0a 2020 2020 2020 2020 2020  200,..          
-00000440: 2020 2263 6f73 7422 3a20 2d31 300d 0a20    "cost": -10.. 
-00000450: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000460: 2020 207b 0d0a 2020 2020 2020 2020 2020     {..          
-00000470: 2020 226e 616d 6522 3a20 2261 6273 3122    "name": "abs1"
-00000480: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000490: 7479 7065 223a 2022 6c69 6e6b 222c 0d0a  type": "link",..
-000004a0: 2020 2020 2020 2020 2020 2020 226d 6178              "max
-000004b0: 5f66 6c6f 7722 3a20 332e 350d 0a20 2020  _flow": 3.5..   
-000004c0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-000004d0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-000004e0: 226e 616d 6522 3a20 2264 656d 616e 6431  "name": "demand1
-000004f0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000500: 2274 7970 6522 3a20 226f 7574 7075 7422  "type": "output"
-00000510: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000520: 6d61 785f 666c 6f77 223a 2022 6465 6d61  max_flow": "dema
-00000530: 6e64 5f6d 6178 5f66 6c6f 7722 2c0d 0a20  nd_max_flow",.. 
-00000540: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-00000550: 223a 202d 3530 300d 0a20 2020 2020 2020  ": -500..       
-00000560: 207d 0d0a 2020 2020 5d2c 0d0a 2020 2020   }..    ],..    
-00000570: 2265 6467 6573 223a 205b 0d0a 2020 2020  "edges": [..    
-00000580: 2020 2020 5b22 6361 7463 686d 656e 7431      ["catchment1
-00000590: 222c 2022 7269 7665 7231 225d 2c0d 0a20  ", "river1"],.. 
-000005a0: 2020 2020 2020 205b 2272 6976 6572 3122         ["river1"
-000005b0: 2c20 226d 7266 3122 5d2c 0d0a 2020 2020  , "mrf1"],..    
-000005c0: 2020 2020 5b22 6d72 6631 222c 2022 7465      ["mrf1", "te
-000005d0: 726d 3122 5d2c 0d0a 2020 2020 2020 2020  rm1"],..        
-000005e0: 5b22 7269 7665 7231 222c 2022 6162 7331  ["river1", "abs1
-000005f0: 225d 2c0d 0a20 2020 2020 2020 205b 2261  "],..        ["a
-00000600: 6273 3122 2c20 2272 6573 6572 766f 6972  bs1", "reservoir
-00000610: 3122 5d2c 0d0a 2020 2020 2020 2020 5b22  1"],..        ["
-00000620: 7265 7365 7276 6f69 7231 222c 2022 6465  reservoir1", "de
-00000630: 6d61 6e64 3122 5d0d 0a20 2020 205d 2c0d  mand1"]..    ],.
-00000640: 0a20 2020 2022 7061 7261 6d65 7465 7273  .    "parameters
-00000650: 223a 207b 0d0a 2020 2020 2020 2020 2266  ": {..        "f
-00000660: 6c6f 775f 6661 6374 6f72 223a 207b 0d0a  low_factor": {..
-00000670: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-00000680: 6522 3a20 2266 6c6f 775f 6661 6374 6f72  e": "flow_factor
-00000690: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000006a0: 2274 7970 6522 3a20 2263 6f6e 7374 616e  "type": "constan
-000006b0: 7422 2c0d 0a20 2020 2020 2020 2020 2020  t",..           
-000006c0: 2022 7661 6c75 6522 3a20 302e 3038 3634   "value": 0.0864
-000006d0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-000006e0: 2020 2020 2020 2266 6c6f 775f 6375 6d65        "flow_cume
-000006f0: 6373 223a 207b 0d0a 2020 2020 2020 2020  cs": {..        
-00000700: 2020 2020 2020 2274 7970 6522 3a20 2264        "type": "d
-00000710: 6174 6166 7261 6d65 222c 0d0a 2020 2020  ataframe",..    
-00000720: 2020 2020 2020 2020 2020 2275 726c 223a            "url":
-00000730: 2022 2e2e 2f64 6174 612f 7468 616d 6573   "../data/thames
-00000740: 5f73 746f 6368 6173 7469 635f 666c 6f77  _stochastic_flow
-00000750: 2e67 7a22 2c0d 0a20 2020 2020 2020 2020  .gz",..         
-00000760: 2020 2020 2022 636f 6c75 6d6e 223a 2022       "column": "
-00000770: 666c 6f77 222c 0d0a 2020 2020 2020 2020  flow",..        
-00000780: 2020 2020 2020 2269 6e64 6578 5f63 6f6c        "index_col
-00000790: 223a 2022 4461 7465 222c 0d0a 2020 2020  ": "Date",..    
-000007a0: 2020 2020 2020 2020 2020 2270 6172 7365            "parse
-000007b0: 5f64 6174 6573 223a 2074 7275 650d 0a20  _dates": true.. 
-000007c0: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-000007d0: 2020 2022 666c 6f77 223a 207b 0d0a 2020     "flow": {..  
-000007e0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-000007f0: 3a20 2261 6767 7265 6761 7465 6422 2c0d  : "aggregated",.
-00000800: 0a20 2020 2020 2020 2020 2020 2022 6167  .            "ag
-00000810: 675f 6675 6e63 223a 2022 7072 6f64 7563  g_func": "produc
-00000820: 7422 2c0d 0a20 2020 2020 2020 2020 2020  t",..           
-00000830: 2022 7061 7261 6d65 7465 7273 223a 205b   "parameters": [
-00000840: 2266 6c6f 775f 6375 6d65 6373 222c 2022  "flow_cumecs", "
-00000850: 666c 6f77 5f66 6163 746f 7222 5d0d 0a20  flow_factor"].. 
-00000860: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000870: 2020 2022 6465 6d61 6e64 5f62 6173 656c     "demand_basel
-00000880: 696e 6522 3a20 7b0d 0a20 2020 2020 2020  ine": {..       
-00000890: 2020 2020 2022 7479 7065 223a 2022 636f       "type": "co
-000008a0: 6e73 7461 6e74 222c 0d0a 2020 2020 2020  nstant",..      
-000008b0: 2020 2020 2020 2276 616c 7565 7322 3a20        "values": 
-000008c0: 312e 370d 0a20 2020 2020 2020 207d 2c0d  1.7..        },.
-000008d0: 0a20 2020 2020 2020 2022 6465 6d61 6e64  .        "demand
-000008e0: 5f70 726f 6669 6c65 223a 207b 0d0a 2020  _profile": {..  
-000008f0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000900: 3a20 226d 6f6e 7468 6c79 7072 6f66 696c  : "monthlyprofil
-00000910: 6522 2c0d 0a20 2020 2020 2020 2020 2020  e",..           
-00000920: 2022 7661 6c75 6573 223a 205b 302e 392c   "values": [0.9,
-00000930: 2030 2e39 2c20 302e 392c 2030 2e39 2c20   0.9, 0.9, 0.9, 
-00000940: 312e 322c 2031 2e32 2c20 312e 322c 2031  1.2, 1.2, 1.2, 1
-00000950: 2e32 2c20 302e 392c 2030 2e39 2c20 302e  .2, 0.9, 0.9, 0.
-00000960: 392c 2030 2e39 5d0d 0a20 2020 2020 2020  9, 0.9]..       
-00000970: 207d 2c0d 0a20 2020 2020 2020 2022 6465   },..        "de
-00000980: 6d61 6e64 5f6d 6178 5f66 6c6f 7722 3a20  mand_max_flow": 
-00000990: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-000009a0: 7479 7065 223a 2022 6167 6772 6567 6174  type": "aggregat
-000009b0: 6564 222c 0d0a 2020 2020 2020 2020 2020  ed",..          
-000009c0: 2020 2261 6767 5f66 756e 6322 3a20 2270    "agg_func": "p
-000009d0: 726f 6475 6374 222c 0d0a 2020 2020 2020  roduct",..      
-000009e0: 2020 2020 2020 2270 6172 616d 6574 6572        "parameter
-000009f0: 7322 3a20 5b0d 0a20 2020 2020 2020 2020  s": [..         
-00000a00: 2020 2020 2020 2022 6465 6d61 6e64 5f62         "demand_b
-00000a10: 6173 656c 696e 6522 2c0d 0a20 2020 2020  aseline",..     
-00000a20: 2020 2020 2020 2020 2020 2022 6465 6d61             "dema
-00000a30: 6e64 5f70 726f 6669 6c65 222c 0d0a 2020  nd_profile",..  
-00000a40: 2020 2020 2020 2020 2020 2020 2020 2264                "d
-00000a50: 656d 616e 645f 7361 7669 6e67 5f66 6163  emand_saving_fac
-00000a60: 746f 7222 2c0d 0a20 2020 2020 2020 2020  tor",..         
-00000a70: 2020 2020 2020 2022 6465 6d61 6e64 5f67         "demand_g
-00000a80: 726f 7774 685f 6661 6374 6f72 220d 0a20  rowth_factor".. 
-00000a90: 2020 2020 2020 2020 2020 205d 0d0a 2020             ]..  
-00000aa0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000ab0: 2020 226c 6576 656c 3122 3a20 7b0d 0a20    "level1": {.. 
-00000ac0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000ad0: 223a 2022 6d6f 6e74 686c 7970 726f 6669  ": "monthlyprofi
-00000ae0: 6c65 222c 0d0a 2020 2020 2020 2020 2020  le",..          
-00000af0: 2020 2276 616c 7565 7322 3a20 5b30 2e38    "values": [0.8
-00000b00: 2c20 302e 3835 2c20 302e 392c 2030 2e39  , 0.85, 0.9, 0.9
-00000b10: 322c 2030 2e39 302c 2030 2e38 352c 2030  2, 0.90, 0.85, 0
-00000b20: 2e38 302c 2030 2e37 302c 2030 2e36 352c  .80, 0.70, 0.65,
-00000b30: 2030 2e37 2c20 302e 3735 2c20 302e 385d   0.7, 0.75, 0.8]
-00000b40: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-00000b50: 2020 2020 2020 226c 6576 656c 3222 3a20        "level2": 
-00000b60: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000b70: 7479 7065 223a 2022 6d6f 6e74 686c 7970  type": "monthlyp
-00000b80: 726f 6669 6c65 222c 0d0a 2020 2020 2020  rofile",..      
-00000b90: 2020 2020 2020 2276 616c 7565 7322 3a20        "values": 
-00000ba0: 5b30 2e35 2c20 302e 3535 2c20 302e 362c  [0.5, 0.55, 0.6,
-00000bb0: 2030 2e36 322c 2030 2e36 302c 2030 2e35   0.62, 0.60, 0.5
-00000bc0: 352c 2030 2e35 302c 2030 2e34 302c 2030  5, 0.50, 0.40, 0
-00000bd0: 2e33 352c 2030 2e34 2c20 302e 3435 2c20  .35, 0.4, 0.45, 
-00000be0: 302e 355d 0d0a 2020 2020 2020 2020 7d2c  0.5]..        },
-00000bf0: 0d0a 2020 2020 2020 2020 2264 656d 616e  ..        "deman
-00000c00: 645f 7361 7669 6e67 5f6c 6576 656c 2220  d_saving_level" 
-00000c10: 3a20 7b0d 0a20 2020 2020 2020 2020 2020  : {..           
-00000c20: 2022 7479 7065 223a 2022 636f 6e74 726f   "type": "contro
-00000c30: 6c63 7572 7665 696e 6465 7822 2c0d 0a20  lcurveindex",.. 
-00000c40: 2020 2020 2020 2020 2020 2022 7374 6f72             "stor
-00000c50: 6167 655f 6e6f 6465 223a 2022 7265 7365  age_node": "rese
-00000c60: 7276 6f69 7231 222c 0d0a 2020 2020 2020  rvoir1",..      
-00000c70: 2020 2020 2020 2263 6f6e 7472 6f6c 5f63        "control_c
-00000c80: 7572 7665 7322 3a20 5b0d 0a20 2020 2020  urves": [..     
-00000c90: 2020 2020 2020 2020 2020 2022 6c65 7665             "leve
-00000ca0: 6c31 222c 0d0a 2020 2020 2020 2020 2020  l1",..          
-00000cb0: 2020 2020 2020 226c 6576 656c 3222 0d0a        "level2"..
-00000cc0: 2020 2020 2020 2020 2020 2020 5d0d 0a20              ].. 
-00000cd0: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000ce0: 2020 2022 6465 6d61 6e64 5f73 6176 696e     "demand_savin
-00000cf0: 675f 6661 6374 6f72 2220 3a20 7b0d 0a20  g_factor" : {.. 
-00000d00: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000d10: 223a 2022 696e 6465 7865 6461 7272 6179  ": "indexedarray
-00000d20: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000d30: 2269 6e64 6578 5f70 6172 616d 6574 6572  "index_parameter
-00000d40: 223a 2022 6465 6d61 6e64 5f73 6176 696e  ": "demand_savin
-00000d50: 675f 6c65 7665 6c22 2c0d 0a20 2020 2020  g_level",..     
-00000d60: 2020 2020 2020 2022 7061 7261 6d73 223a         "params":
-00000d70: 205b 0d0a 2020 2020 2020 2020 2020 2020   [..            
-00000d80: 2020 2020 226c 6576 656c 305f 6661 6374      "level0_fact
-00000d90: 6f72 222c 0d0a 2020 2020 2020 2020 2020  or",..          
-00000da0: 2020 2020 2020 226c 6576 656c 315f 6661        "level1_fa
-00000db0: 6374 6f72 222c 0d0a 2020 2020 2020 2020  ctor",..        
-00000dc0: 2020 2020 2020 2020 226c 6576 656c 325f          "level2_
-00000dd0: 6661 6374 6f72 220d 0a20 2020 2020 2020  factor"..       
-00000de0: 2020 2020 205d 0d0a 2020 2020 2020 2020       ]..        
-00000df0: 7d2c 0d0a 2020 2020 2020 2020 226c 6576  },..        "lev
-00000e00: 656c 305f 6661 6374 6f72 223a 207b 0d0a  el0_factor": {..
-00000e10: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000e20: 6522 3a20 2263 6f6e 7374 616e 7422 2c0d  e": "constant",.
-00000e30: 0a20 2020 2020 2020 2020 2020 2022 7661  .            "va
-00000e40: 6c75 6573 223a 2031 2e30 0d0a 2020 2020  lues": 1.0..    
-00000e50: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000e60: 226c 6576 656c 315f 6661 6374 6f72 223a  "level1_factor":
-00000e70: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000e80: 2274 7970 6522 3a20 226d 6f6e 7468 6c79  "type": "monthly
-00000e90: 7072 6f66 696c 6522 2c0d 0a20 2020 2020  profile",..     
-00000ea0: 2020 2020 2020 2022 7661 6c75 6573 223a         "values":
-00000eb0: 205b 302e 3935 2c20 302e 3935 2c20 302e   [0.95, 0.95, 0.
-00000ec0: 3935 2c20 302e 3935 2c20 302e 3930 2c20  95, 0.95, 0.90, 
-00000ed0: 302e 3930 2c20 302e 3930 2c20 302e 3930  0.90, 0.90, 0.90
-00000ee0: 2c20 302e 3935 2c20 302e 3935 2c20 302e  , 0.95, 0.95, 0.
-00000ef0: 3935 2c20 302e 3935 5d0d 0a20 2020 2020  95, 0.95]..     
-00000f00: 2020 207d 2c0d 0a20 2020 2020 2020 2022     },..        "
-00000f10: 6c65 7665 6c32 5f66 6163 746f 7222 3a20  level2_factor": 
-00000f20: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000f30: 7479 7065 223a 2022 6d6f 6e74 686c 7970  type": "monthlyp
-00000f40: 726f 6669 6c65 222c 0d0a 2020 2020 2020  rofile",..      
-00000f50: 2020 2020 2020 2276 616c 7565 7322 3a20        "values": 
-00000f60: 5b30 2e38 2c20 302e 382c 2030 2e38 2c20  [0.8, 0.8, 0.8, 
-00000f70: 302e 382c 2030 2e37 352c 2030 2e37 352c  0.8, 0.75, 0.75,
-00000f80: 2030 2e37 352c 2030 2e37 352c 2030 2e38   0.75, 0.75, 0.8
-00000f90: 2c20 302e 382c 2030 2e38 2c20 302e 385d  , 0.8, 0.8, 0.8]
-00000fa0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-00000fb0: 2020 2020 2020 2264 656d 616e 645f 6772        "demand_gr
-00000fc0: 6f77 7468 5f66 6163 746f 7222 3a20 7b0d  owth_factor": {.
-00000fd0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
-00000fe0: 7065 223a 2022 636f 6e73 7461 6e74 7363  pe": "constantsc
-00000ff0: 656e 6172 696f 222c 0d0a 2020 2020 2020  enario",..      
-00001000: 2020 2020 2020 2273 6365 6e61 7269 6f22        "scenario"
-00001010: 3a20 2264 656d 616e 6422 2c0d 0a20 2020  : "demand",..   
-00001020: 2020 2020 2020 2020 2022 7661 6c75 6573           "values
-00001030: 2220 3a20 5b30 2e37 352c 2030 2e39 2c20  " : [0.75, 0.9, 
-00001040: 312e 302c 2031 2e31 2c20 312e 3235 5d0d  1.0, 1.1, 1.25].
-00001050: 0a0d 0a20 2020 2020 2020 207d 0d0a 2020  ...        }..  
-00001060: 2020 7d0d 0a7d 0d0a                        }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4120 7369 6d70 6c65 206d  le": "A simple m
+00000030: 6f64 656c 2075 7369 6e67 2074 6865 2072  odel using the r
+00000040: 6976 6572 2054 6861 6d65 7322 2c0a 2020  iver Thames",.  
+00000050: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
+00000060: 6f6e 223a 2022 5468 6973 206d 6f64 656c  on": "This model
+00000070: 2075 7365 7320 7269 7665 7220 5468 616d   uses river Tham
+00000080: 6573 2069 6e66 6c6f 7773 2e20 486f 7765  es inflows. Howe
+00000090: 7665 7220 6974 2064 6f65 7320 6e6f 7420  ver it does not 
+000000a0: 7265 7072 6573 656e 7420 7468 6520 5468  represent the Th
+000000b0: 616d 6573 2057 6174 6572 2073 7973 7465  ames Water syste
+000000c0: 6d20 6f72 2075 7365 2061 6e79 2064 6174  m or use any dat
+000000d0: 6120 6672 6f6d 2054 5755 4c2e 222c 0a20  a from TWUL.",. 
+000000e0: 2020 2020 2020 2022 6d69 6e69 6d75 6d5f         "minimum_
+000000f0: 7665 7273 696f 6e22 3a20 2230 2e34 220a  version": "0.4".
+00000100: 2020 2020 7d2c 0a20 2020 2022 7469 6d65      },.    "time
+00000110: 7374 6570 7065 7222 3a20 7b0a 2020 2020  stepper": {.    
+00000120: 2020 2020 2273 7461 7274 223a 2022 3231      "start": "21
+00000130: 3030 2d30 312d 3031 222c 0a20 2020 2020  00-01-01",.     
+00000140: 2020 2022 656e 6422 3a20 2232 3139 392d     "end": "2199-
+00000150: 3031 2d30 3122 2c0a 2020 2020 2020 2020  01-01",.        
+00000160: 2274 696d 6573 7465 7022 3a20 310a 2020  "timestep": 1.  
+00000170: 2020 7d2c 0a20 2020 2022 7363 656e 6172    },.    "scenar
+00000180: 696f 7322 3a20 5b0a 2020 2020 2020 2020  ios": [.        
+00000190: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000001a0: 616d 6522 3a20 2264 656d 616e 6422 2c0a  ame": "demand",.
+000001b0: 2020 2020 2020 2020 2020 2020 2273 697a              "siz
+000001c0: 6522 3a20 350a 2020 2020 2020 2020 7d0a  e": 5.        }.
+000001d0: 2020 2020 5d2c 0a20 2020 2022 6e6f 6465      ],.    "node
+000001e0: 7322 3a20 5b0a 2020 2020 2020 2020 7b0a  s": [.        {.
+000001f0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000200: 6522 3a20 2263 6174 6368 6d65 6e74 3122  e": "catchment1"
+00000210: 2c0a 2020 2020 2020 2020 2020 2020 2274  ,.            "t
+00000220: 7970 6522 3a20 2263 6174 6368 6d65 6e74  ype": "catchment
+00000230: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000240: 666c 6f77 223a 2022 666c 6f77 220a 2020  flow": "flow".  
+00000250: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000260: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+00000270: 6e61 6d65 223a 2022 7269 7665 7231 222c  name": "river1",
+00000280: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000290: 7065 223a 2022 6c69 6e6b 220a 2020 2020  pe": "link".    
+000002a0: 2020 2020 7d2c 0a20 2020 2020 2020 207b      },.        {
+000002b0: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
+000002c0: 6d65 223a 2022 6d72 6631 222c 0a20 2020  me": "mrf1",.   
+000002d0: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+000002e0: 2022 7269 7665 7267 6175 6765 222c 0a20   "rivergauge",. 
+000002f0: 2020 2020 2020 2020 2020 2022 6d72 6622             "mrf"
+00000300: 3a20 302e 362c 0a20 2020 2020 2020 2020  : 0.6,.         
+00000310: 2020 2022 6d72 665f 636f 7374 223a 202d     "mrf_cost": -
+00000320: 3130 3030 0a20 2020 2020 2020 207d 2c0a  1000.        },.
+00000330: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000340: 2020 2020 2020 226e 616d 6522 3a20 2274        "name": "t
+00000350: 6572 6d31 222c 0a20 2020 2020 2020 2020  erm1",.         
+00000360: 2020 2022 7479 7065 223a 2022 6f75 7470     "type": "outp
+00000370: 7574 220a 2020 2020 2020 2020 7d2c 0a20  ut".        },. 
+00000380: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000390: 2020 2020 2022 6e61 6d65 223a 2022 7265       "name": "re
+000003a0: 7365 7276 6f69 7231 222c 0a20 2020 2020  servoir1",.     
+000003b0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+000003c0: 7374 6f72 6167 6522 2c0a 2020 2020 2020  storage",.      
+000003d0: 2020 2020 2020 226d 6178 5f76 6f6c 756d        "max_volum
+000003e0: 6522 3a20 3230 302c 0a20 2020 2020 2020  e": 200,.       
+000003f0: 2020 2020 2022 696e 6974 6961 6c5f 766f       "initial_vo
+00000400: 6c75 6d65 223a 2032 3030 2c0a 2020 2020  lume": 200,.    
+00000410: 2020 2020 2020 2020 2263 6f73 7422 3a20          "cost": 
+00000420: 2d31 300a 2020 2020 2020 2020 7d2c 0a20  -10.        },. 
+00000430: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000440: 2020 2020 2022 6e61 6d65 223a 2022 6162       "name": "ab
+00000450: 7331 222c 0a20 2020 2020 2020 2020 2020  s1",.           
+00000460: 2022 7479 7065 223a 2022 6c69 6e6b 222c   "type": "link",
+00000470: 0a20 2020 2020 2020 2020 2020 2022 6d61  .            "ma
+00000480: 785f 666c 6f77 223a 2033 2e35 0a20 2020  x_flow": 3.5.   
+00000490: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000004a0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000004b0: 616d 6522 3a20 2264 656d 616e 6431 222c  ame": "demand1",
+000004c0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+000004d0: 7065 223a 2022 6f75 7470 7574 222c 0a20  pe": "output",. 
+000004e0: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
+000004f0: 666c 6f77 223a 2022 6465 6d61 6e64 5f6d  flow": "demand_m
+00000500: 6178 5f66 6c6f 7722 2c0a 2020 2020 2020  ax_flow",.      
+00000510: 2020 2020 2020 2263 6f73 7422 3a20 2d35        "cost": -5
+00000520: 3030 0a20 2020 2020 2020 207d 0a20 2020  00.        }.   
+00000530: 205d 2c0a 2020 2020 2265 6467 6573 223a   ],.    "edges":
+00000540: 205b 0a20 2020 2020 2020 205b 2263 6174   [.        ["cat
+00000550: 6368 6d65 6e74 3122 2c20 2272 6976 6572  chment1", "river
+00000560: 3122 5d2c 0a20 2020 2020 2020 205b 2272  1"],.        ["r
+00000570: 6976 6572 3122 2c20 226d 7266 3122 5d2c  iver1", "mrf1"],
+00000580: 0a20 2020 2020 2020 205b 226d 7266 3122  .        ["mrf1"
+00000590: 2c20 2274 6572 6d31 225d 2c0a 2020 2020  , "term1"],.    
+000005a0: 2020 2020 5b22 7269 7665 7231 222c 2022      ["river1", "
+000005b0: 6162 7331 225d 2c0a 2020 2020 2020 2020  abs1"],.        
+000005c0: 5b22 6162 7331 222c 2022 7265 7365 7276  ["abs1", "reserv
+000005d0: 6f69 7231 225d 2c0a 2020 2020 2020 2020  oir1"],.        
+000005e0: 5b22 7265 7365 7276 6f69 7231 222c 2022  ["reservoir1", "
+000005f0: 6465 6d61 6e64 3122 5d0a 2020 2020 5d2c  demand1"].    ],
+00000600: 0a20 2020 2022 7061 7261 6d65 7465 7273  .    "parameters
+00000610: 223a 207b 0a20 2020 2020 2020 2022 666c  ": {.        "fl
+00000620: 6f77 5f66 6163 746f 7222 3a20 7b0a 2020  ow_factor": {.  
+00000630: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
+00000640: 3a20 2266 6c6f 775f 6661 6374 6f72 222c  : "flow_factor",
+00000650: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000660: 7065 223a 2022 636f 6e73 7461 6e74 222c  pe": "constant",
+00000670: 0a20 2020 2020 2020 2020 2020 2022 7661  .            "va
+00000680: 6c75 6522 3a20 302e 3038 3634 0a20 2020  lue": 0.0864.   
+00000690: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000006a0: 2266 6c6f 775f 6375 6d65 6373 223a 207b  "flow_cumecs": {
+000006b0: 0a20 2020 2020 2020 2020 2020 2020 2022  .              "
+000006c0: 7479 7065 223a 2022 6461 7461 6672 616d  type": "datafram
+000006d0: 6522 2c0a 2020 2020 2020 2020 2020 2020  e",.            
+000006e0: 2020 2275 726c 223a 2022 2e2e 2f64 6174    "url": "../dat
+000006f0: 612f 7468 616d 6573 5f73 746f 6368 6173  a/thames_stochas
+00000700: 7469 635f 666c 6f77 2e67 7a22 2c0a 2020  tic_flow.gz",.  
+00000710: 2020 2020 2020 2020 2020 2020 2263 6f6c              "col
+00000720: 756d 6e22 3a20 2266 6c6f 7722 2c0a 2020  umn": "flow",.  
+00000730: 2020 2020 2020 2020 2020 2020 2269 6e64              "ind
+00000740: 6578 5f63 6f6c 223a 2022 4461 7465 222c  ex_col": "Date",
+00000750: 0a20 2020 2020 2020 2020 2020 2020 2022  .              "
+00000760: 7061 7273 655f 6461 7465 7322 3a20 7472  parse_dates": tr
+00000770: 7565 0a20 2020 2020 2020 207d 2c0a 2020  ue.        },.  
+00000780: 2020 2020 2020 2266 6c6f 7722 3a20 7b0a        "flow": {.
+00000790: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000007a0: 6522 3a20 2261 6767 7265 6761 7465 6422  e": "aggregated"
+000007b0: 2c0a 2020 2020 2020 2020 2020 2020 2261  ,.            "a
+000007c0: 6767 5f66 756e 6322 3a20 2270 726f 6475  gg_func": "produ
+000007d0: 6374 222c 0a20 2020 2020 2020 2020 2020  ct",.           
+000007e0: 2022 7061 7261 6d65 7465 7273 223a 205b   "parameters": [
+000007f0: 2266 6c6f 775f 6375 6d65 6373 222c 2022  "flow_cumecs", "
+00000800: 666c 6f77 5f66 6163 746f 7222 5d0a 2020  flow_factor"].  
+00000810: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000820: 2022 6465 6d61 6e64 5f62 6173 656c 696e   "demand_baselin
+00000830: 6522 3a20 7b0a 2020 2020 2020 2020 2020  e": {.          
+00000840: 2020 2274 7970 6522 3a20 2263 6f6e 7374    "type": "const
+00000850: 616e 7422 2c0a 2020 2020 2020 2020 2020  ant",.          
+00000860: 2020 2276 616c 7565 7322 3a20 312e 370a    "values": 1.7.
+00000870: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000880: 2020 2022 6465 6d61 6e64 5f70 726f 6669     "demand_profi
+00000890: 6c65 223a 207b 0a20 2020 2020 2020 2020  le": {.         
+000008a0: 2020 2022 7479 7065 223a 2022 6d6f 6e74     "type": "mont
+000008b0: 686c 7970 726f 6669 6c65 222c 0a20 2020  hlyprofile",.   
+000008c0: 2020 2020 2020 2020 2022 7661 6c75 6573           "values
+000008d0: 223a 205b 302e 392c 2030 2e39 2c20 302e  ": [0.9, 0.9, 0.
+000008e0: 392c 2030 2e39 2c20 312e 322c 2031 2e32  9, 0.9, 1.2, 1.2
+000008f0: 2c20 312e 322c 2031 2e32 2c20 302e 392c  , 1.2, 1.2, 0.9,
+00000900: 2030 2e39 2c20 302e 392c 2030 2e39 5d0a   0.9, 0.9, 0.9].
+00000910: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000920: 2020 2022 6465 6d61 6e64 5f6d 6178 5f66     "demand_max_f
+00000930: 6c6f 7722 3a20 7b0a 2020 2020 2020 2020  low": {.        
+00000940: 2020 2020 2274 7970 6522 3a20 2261 6767      "type": "agg
+00000950: 7265 6761 7465 6422 2c0a 2020 2020 2020  regated",.      
+00000960: 2020 2020 2020 2261 6767 5f66 756e 6322        "agg_func"
+00000970: 3a20 2270 726f 6475 6374 222c 0a20 2020  : "product",.   
+00000980: 2020 2020 2020 2020 2022 7061 7261 6d65           "parame
+00000990: 7465 7273 223a 205b 0a20 2020 2020 2020  ters": [.       
+000009a0: 2020 2020 2020 2020 2022 6465 6d61 6e64           "demand
+000009b0: 5f62 6173 656c 696e 6522 2c0a 2020 2020  _baseline",.    
+000009c0: 2020 2020 2020 2020 2020 2020 2264 656d              "dem
+000009d0: 616e 645f 7072 6f66 696c 6522 2c0a 2020  and_profile",.  
+000009e0: 2020 2020 2020 2020 2020 2020 2020 2264                "d
+000009f0: 656d 616e 645f 7361 7669 6e67 5f66 6163  emand_saving_fac
+00000a00: 746f 7222 2c0a 2020 2020 2020 2020 2020  tor",.          
+00000a10: 2020 2020 2020 2264 656d 616e 645f 6772        "demand_gr
+00000a20: 6f77 7468 5f66 6163 746f 7222 0a20 2020  owth_factor".   
+00000a30: 2020 2020 2020 2020 205d 0a20 2020 2020           ].     
+00000a40: 2020 207d 2c0a 2020 2020 2020 2020 226c     },.        "l
+00000a50: 6576 656c 3122 3a20 7b0a 2020 2020 2020  evel1": {.      
+00000a60: 2020 2020 2020 2274 7970 6522 3a20 226d        "type": "m
+00000a70: 6f6e 7468 6c79 7072 6f66 696c 6522 2c0a  onthlyprofile",.
+00000a80: 2020 2020 2020 2020 2020 2020 2276 616c              "val
+00000a90: 7565 7322 3a20 5b30 2e38 2c20 302e 3835  ues": [0.8, 0.85
+00000aa0: 2c20 302e 392c 2030 2e39 322c 2030 2e39  , 0.9, 0.92, 0.9
+00000ab0: 302c 2030 2e38 352c 2030 2e38 302c 2030  0, 0.85, 0.80, 0
+00000ac0: 2e37 302c 2030 2e36 352c 2030 2e37 2c20  .70, 0.65, 0.7, 
+00000ad0: 302e 3735 2c20 302e 385d 0a20 2020 2020  0.75, 0.8].     
+00000ae0: 2020 207d 2c0a 2020 2020 2020 2020 226c     },.        "l
+00000af0: 6576 656c 3222 3a20 7b0a 2020 2020 2020  evel2": {.      
+00000b00: 2020 2020 2020 2274 7970 6522 3a20 226d        "type": "m
+00000b10: 6f6e 7468 6c79 7072 6f66 696c 6522 2c0a  onthlyprofile",.
+00000b20: 2020 2020 2020 2020 2020 2020 2276 616c              "val
+00000b30: 7565 7322 3a20 5b30 2e35 2c20 302e 3535  ues": [0.5, 0.55
+00000b40: 2c20 302e 362c 2030 2e36 322c 2030 2e36  , 0.6, 0.62, 0.6
+00000b50: 302c 2030 2e35 352c 2030 2e35 302c 2030  0, 0.55, 0.50, 0
+00000b60: 2e34 302c 2030 2e33 352c 2030 2e34 2c20  .40, 0.35, 0.4, 
+00000b70: 302e 3435 2c20 302e 355d 0a20 2020 2020  0.45, 0.5].     
+00000b80: 2020 207d 2c0a 2020 2020 2020 2020 2264     },.        "d
+00000b90: 656d 616e 645f 7361 7669 6e67 5f6c 6576  emand_saving_lev
+00000ba0: 656c 2220 3a20 7b0a 2020 2020 2020 2020  el" : {.        
+00000bb0: 2020 2020 2274 7970 6522 3a20 2263 6f6e      "type": "con
+00000bc0: 7472 6f6c 6375 7276 6569 6e64 6578 222c  trolcurveindex",
+00000bd0: 0a20 2020 2020 2020 2020 2020 2022 7374  .            "st
+00000be0: 6f72 6167 655f 6e6f 6465 223a 2022 7265  orage_node": "re
+00000bf0: 7365 7276 6f69 7231 222c 0a20 2020 2020  servoir1",.     
+00000c00: 2020 2020 2020 2022 636f 6e74 726f 6c5f         "control_
+00000c10: 6375 7276 6573 223a 205b 0a20 2020 2020  curves": [.     
+00000c20: 2020 2020 2020 2020 2020 2022 6c65 7665             "leve
+00000c30: 6c31 222c 0a20 2020 2020 2020 2020 2020  l1",.           
+00000c40: 2020 2020 2022 6c65 7665 6c32 220a 2020       "level2".  
+00000c50: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+00000c60: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000c70: 6465 6d61 6e64 5f73 6176 696e 675f 6661  demand_saving_fa
+00000c80: 6374 6f72 2220 3a20 7b0a 2020 2020 2020  ctor" : {.      
+00000c90: 2020 2020 2020 2274 7970 6522 3a20 2269        "type": "i
+00000ca0: 6e64 6578 6564 6172 7261 7922 2c0a 2020  ndexedarray",.  
+00000cb0: 2020 2020 2020 2020 2020 2269 6e64 6578            "index
+00000cc0: 5f70 6172 616d 6574 6572 223a 2022 6465  _parameter": "de
+00000cd0: 6d61 6e64 5f73 6176 696e 675f 6c65 7665  mand_saving_leve
+00000ce0: 6c22 2c0a 2020 2020 2020 2020 2020 2020  l",.            
+00000cf0: 2270 6172 616d 7322 3a20 5b0a 2020 2020  "params": [.    
+00000d00: 2020 2020 2020 2020 2020 2020 226c 6576              "lev
+00000d10: 656c 305f 6661 6374 6f72 222c 0a20 2020  el0_factor",.   
+00000d20: 2020 2020 2020 2020 2020 2020 2022 6c65               "le
+00000d30: 7665 6c31 5f66 6163 746f 7222 2c0a 2020  vel1_factor",.  
+00000d40: 2020 2020 2020 2020 2020 2020 2020 226c                "l
+00000d50: 6576 656c 325f 6661 6374 6f72 220a 2020  evel2_factor".  
+00000d60: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+00000d70: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000d80: 6c65 7665 6c30 5f66 6163 746f 7222 3a20  level0_factor": 
+00000d90: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000da0: 7970 6522 3a20 2263 6f6e 7374 616e 7422  ype": "constant"
+00000db0: 2c0a 2020 2020 2020 2020 2020 2020 2276  ,.            "v
+00000dc0: 616c 7565 7322 3a20 312e 300a 2020 2020  alues": 1.0.    
+00000dd0: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000de0: 6c65 7665 6c31 5f66 6163 746f 7222 3a20  level1_factor": 
+00000df0: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000e00: 7970 6522 3a20 226d 6f6e 7468 6c79 7072  ype": "monthlypr
+00000e10: 6f66 696c 6522 2c0a 2020 2020 2020 2020  ofile",.        
+00000e20: 2020 2020 2276 616c 7565 7322 3a20 5b30      "values": [0
+00000e30: 2e39 352c 2030 2e39 352c 2030 2e39 352c  .95, 0.95, 0.95,
+00000e40: 2030 2e39 352c 2030 2e39 302c 2030 2e39   0.95, 0.90, 0.9
+00000e50: 302c 2030 2e39 302c 2030 2e39 302c 2030  0, 0.90, 0.90, 0
+00000e60: 2e39 352c 2030 2e39 352c 2030 2e39 352c  .95, 0.95, 0.95,
+00000e70: 2030 2e39 355d 0a20 2020 2020 2020 207d   0.95].        }
+00000e80: 2c0a 2020 2020 2020 2020 226c 6576 656c  ,.        "level
+00000e90: 325f 6661 6374 6f72 223a 207b 0a20 2020  2_factor": {.   
+00000ea0: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+00000eb0: 2022 6d6f 6e74 686c 7970 726f 6669 6c65   "monthlyprofile
+00000ec0: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000ed0: 7661 6c75 6573 223a 205b 302e 382c 2030  values": [0.8, 0
+00000ee0: 2e38 2c20 302e 382c 2030 2e38 2c20 302e  .8, 0.8, 0.8, 0.
+00000ef0: 3735 2c20 302e 3735 2c20 302e 3735 2c20  75, 0.75, 0.75, 
+00000f00: 302e 3735 2c20 302e 382c 2030 2e38 2c20  0.75, 0.8, 0.8, 
+00000f10: 302e 382c 2030 2e38 5d0a 2020 2020 2020  0.8, 0.8].      
+00000f20: 2020 7d2c 0a20 2020 2020 2020 2022 6465    },.        "de
+00000f30: 6d61 6e64 5f67 726f 7774 685f 6661 6374  mand_growth_fact
+00000f40: 6f72 223a 207b 0a20 2020 2020 2020 2020  or": {.         
+00000f50: 2020 2022 7479 7065 223a 2022 636f 6e73     "type": "cons
+00000f60: 7461 6e74 7363 656e 6172 696f 222c 0a20  tantscenario",. 
+00000f70: 2020 2020 2020 2020 2020 2022 7363 656e             "scen
+00000f80: 6172 696f 223a 2022 6465 6d61 6e64 222c  ario": "demand",
+00000f90: 0a20 2020 2020 2020 2020 2020 2022 7661  .            "va
+00000fa0: 6c75 6573 2220 3a20 5b30 2e37 352c 2030  lues" : [0.75, 0
+00000fb0: 2e39 2c20 312e 302c 2031 2e31 2c20 312e  .9, 1.0, 1.1, 1.
+00000fc0: 3235 5d0a 0a20 2020 2020 2020 207d 0a20  25]..        }. 
+00000fd0: 2020 207d 0a7d 0a                           }.}.
```

### Comparing `pywr-1.8.0/examples/thames/thames.py` & `pywr-1.9.0/examples/thames/thames.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,310 +1,310 @@
-""" Example Pywr model using a simple reservoir system.
-
-
-
-"""
-from pywr.model import Model
-from pywr.recorders import TablesRecorder
-import numpy as np
-from matplotlib import pyplot as plt
-import click
-import time
-import json
-import pandas
-MODEL_FILENAME = 'thames.json'
-
-@click.group()
-def cli():
-    pass
-
-
-@cli.command()
-def run():
-
-    # Run the model
-    model = Model.load(MODEL_FILENAME)
-
-    # Add a storage recorder
-    TablesRecorder(model, 'thames_output.h5', parameters=[p for p in model.parameters])
-
-    # Run the model
-    stats = model.run()
-    print(stats)
-    stats_df = stats.to_dataframe()
-    print(stats_df)
-
-    keys_to_plot = (
-        'time_taken_before',
-        'solver_stats.bounds_update_nonstorage',
-        'solver_stats.bounds_update_storage',
-        'solver_stats.objective_update',
-        'solver_stats.lp_solve',
-        'solver_stats.result_update',
-        'time_taken_after',
-    )
-
-    keys_to_tabulate = (
-        'timesteps',
-        'time_taken',
-        'solver',
-        'num_scenarios',
-        'speed',
-        'solver_name'
-        'solver_stats.total',
-        'solver_stats.number_of_rows',
-        'solver_stats.number_of_cols',
-        'solver_stats.number_of_nonzero',
-        'solver_stats.number_of_routes',
-        'solver_stats.number_of_nodes',
-    )
-
-    values = []
-    labels = []
-    explode = []
-    solver_sub_total = 0.0
-    for k in keys_to_plot:
-        v = stats_df.loc[k][0]
-        values.append(v)
-        label = k.split('.', 1)[-1].replace('_', ' ').capitalize()
-        explode.append(0.0)
-        if k.startswith('solver_stats'):
-            labels.append('Solver - {}'.format(label))
-            solver_sub_total += v
-        else:
-
-            labels.append(label)
-
-    values.append(stats_df.loc['solver_stats.total'][0] - solver_sub_total)
-    labels.append('Solver - Other')
-    explode.append(0.0)
-
-    values.append(stats_df.loc['time_taken'][0] - sum(values))
-    values = np.array(values) / sum(values)
-    labels.append('Other')
-    explode.append(0.0)
-
-    fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, sharey='row',
-                                   gridspec_kw={'width_ratios': [2, 1]})
-
-    print(values, labels)
-    ax1.pie(values, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)
-    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
-
-    cell_text = []
-    for index, value in stats_df.iterrows():
-        if index not in keys_to_tabulate:
-            continue
-        v = value[0]
-        if isinstance(v, (float, np.float64, np.float32)):
-            v = f'{v:.2f}'
-
-        cell_text.append([index, v])
-
-    tbl = ax2.table(cellText=cell_text, colLabels=['Statistic', 'Value'], loc='center')
-    tbl.scale(1.5, 1.5)  # may help
-    tbl.set_fontsize(14)
-    ax2.axis('off')
-
-    fig.savefig('run_statistics_w_tables.png', dpi=300)
-    fig.savefig('run_statistics_w_tables.eps')
-
-    plt.show()
-
-
-@cli.command()
-@click.option('--ext', default='png')
-@click.option('--show/--no-show', default=False)
-def figures(ext, show):
-
-    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
-        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
-
-        fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, sharey='row',
-                                       gridspec_kw={'width_ratios': [3, 1]})
-        df['2100':'2125'].plot(ax=ax1)
-        df.quantile(np.linspace(0, 1)).plot(ax=ax2)
-
-        if name.startswith('reservoir'):
-            ax1.set_ylabel('Volume [$Mm^3$]')
-        else:
-            ax1.set_ylabel('Flow [$Mm^3/day$]')
-
-        for ax in (ax1, ax2):
-            ax.set_title(name)
-            ax.grid(True)
-        plt.tight_layout()
-
-        if ext is not None:
-            fig.savefig(f'{name}.{ext}', dpi=300)
-
-    if show:
-        plt.show()
-
-
-@cli.command('plot-res')
-@click.option('--ext', default='png')
-@click.option('--show/--no-show', default=False)
-def plot_res(ext, show):
-
-    end_year = '2105'
-
-    data = {}
-    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
-        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
-        data[name] = df
-
-    fig1, ax1 = plt.subplots(figsize=(16, 5), dpi=300)
-    data['reservoir1'].loc[:end_year, 'Central'].plot(ax=ax1)
-    ax1.set_ylabel('Volume [$Mm^3$]')
-    plt.tight_layout()
-
-    fig2, ax2 = plt.subplots(figsize=(16, 5), dpi=300)
-    data['demand_saving_level'].loc[:end_year, 'Central'].plot(ax=ax2)
-    ax2.set_ylabel('Demand saving level')
-    plt.tight_layout()
-
-    fig3, ax3 = plt.subplots(figsize=(16, 5), dpi=300)
-    data['demand_max_flow'].loc[:end_year, 'Central'].plot(ax=ax3)
-    ax3.set_ylabel('Demand [$Mm^3/day$]')
-    plt.tight_layout()
-
-    for ax in (ax1, ax2, ax3):
-        ax.grid(True)
-
-    if ext is not None:
-        fig1.savefig(f'Reservoir.{ext}', dpi=300)
-        fig2.savefig(f'Demand saving level.{ext}', dpi=300)
-        fig3.savefig(f'Demand.{ext}', dpi=300)
-
-    if show:
-        plt.show()
-
-
-@cli.command('plot-res2')
-@click.option('--ext', default='png')
-@click.option('--show/--no-show', default=False)
-def plot_res2(ext, show):
-
-    end_year = '2105'
-
-    data = {}
-    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
-        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
-        data[name] = df
-
-    fig1, ax1 = plt.subplots(figsize=(16, 5), dpi=300)
-    data['reservoir1'].loc[:end_year].plot(ax=ax1)
-    ax1.set_ylabel('Volume [$Mm^3$]')
-    plt.legend()
-    plt.tight_layout()
-
-    fig2, ax2 = plt.subplots(figsize=(16, 5), dpi=300)
-    data['reservoir1'].quantile(np.linspace(0, 1)).plot(ax=ax2)
-    ax2.set_ylabel('Volume [$Mm^3$]')
-    ax2.set_xlabel('Quantile')
-    plt.tight_layout()
-
-    fig3, ax3 = plt.subplots(figsize=(16, 5), dpi=300)
-    df = data['demand_saving_level'].apply(pandas.Series.value_counts)
-    df /= df.sum(axis=0)
-    df.plot.bar(ax=ax3)
-    ax3.set_ylabel('Proportion of time.')
-    ax3.set_xlabel('Demand saving level')
-    plt.tight_layout()
-
-    for ax in (ax1, ax2, ax3):
-        ax.grid(True)
-
-    if ext is not None:
-        fig1.savefig(f'Reservoir (scenarios).{ext}', dpi=300)
-        fig2.savefig(f'Reservoir SDC (scenarios).{ext}', dpi=300)
-        fig3.savefig(f'Demand saving level count (scenarios).{ext}', dpi=300)
-
-    if show:
-        plt.show()
-
-
-@cli.command('plot-cc')
-@click.option('--ext', default='png')
-@click.option('--show/--no-show', default=False)
-def plot_control_curves(ext, show):
-
-    with open(MODEL_FILENAME) as fh:
-        data = json.load(fh)
-
-    parameters = data['parameters']
-
-    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
-
-    dates = pandas.date_range("2015-01-01", "2015-12-31")
-
-    L1_values = np.array([parameters['level1']['values'][d.month-1] for d in dates])
-    L2_values = np.array([parameters['level2']['values'][d.month-1] for d in dates])
-
-    x = np.arange(0, len(dates)) + 1
-
-    ax.fill_between(x, 1.0, L1_values, label='Level 0', alpha=0.8)
-    ax.fill_between(x, L1_values, L2_values, label='Level 1', alpha=0.8)
-    ax.fill_between(x, L2_values, 0.0, label='Level 2', alpha=0.8)
-
-    plt.xlabel("Day of year")
-    plt.ylabel("Reservoir volume [%]")
-
-    plt.grid(True)
-    plt.ylim([0.0, 1.0])
-    plt.xlim(1, 365)
-    plt.legend(["Level 0", "Level 1", "Level 2"], loc="upper right")
-    ax.plot(x, L1_values, color='k', label=None)
-    ax.plot(x, L2_values, color='k', label=None)
-    plt.tight_layout()
-
-    if ext is not None:
-        fig.savefig(f'Control curve zones.{ext}', dpi=300)
-
-    if show:
-        plt.show()
-
-
-@cli.command('plot-dsf')
-@click.option('--ext', default='png')
-@click.option('--show/--no-show', default=False)
-def plot_demand_saving_factor(ext, show):
-
-    with open(MODEL_FILENAME) as fh:
-        data = json.load(fh)
-
-    parameters = data['parameters']
-
-    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
-
-    dates = pandas.date_range("2015-01-01", "2015-12-31")
-
-    L0_values = np.array([parameters['level0_factor']['values'] for d in dates])
-    L1_values = np.array([parameters['level1_factor']['values'][d.month-1] for d in dates])
-    L2_values = np.array([parameters['level2_factor']['values'][d.month-1] for d in dates])
-
-    x = np.arange(0, len(dates)) + 1
-
-    ax.plot(x, L0_values, label='Level 0')
-    ax.plot(x, L1_values, label='Level 0')
-    ax.plot(x, L2_values, label='Level 0')
-
-    plt.xlabel("Day of year")
-    plt.ylabel("Demand restriction factor")
-
-    plt.grid(True)
-    plt.ylim(0.6, 1.2)
-    plt.xlim(1, 365)
-    plt.legend(["Level 0", "Level 1", "Level 2"], loc="upper right")
-
-    plt.tight_layout()
-
-    if ext is not None:
-        fig.savefig(f'Demand restriction factors.{ext}', dpi=300)
-
-    if show:
-        plt.show()
-
-
-if __name__ == '__main__':
-    cli()
+""" Example Pywr model using a simple reservoir system.
+
+
+
+"""
+from pywr.model import Model
+from pywr.recorders import TablesRecorder
+import numpy as np
+from matplotlib import pyplot as plt
+import click
+import time
+import json
+import pandas
+MODEL_FILENAME = 'thames.json'
+
+@click.group()
+def cli():
+    pass
+
+
+@cli.command()
+def run():
+
+    # Run the model
+    model = Model.load(MODEL_FILENAME)
+
+    # Add a storage recorder
+    TablesRecorder(model, 'thames_output.h5', parameters=[p for p in model.parameters])
+
+    # Run the model
+    stats = model.run()
+    print(stats)
+    stats_df = stats.to_dataframe()
+    print(stats_df)
+
+    keys_to_plot = (
+        'time_taken_before',
+        'solver_stats.bounds_update_nonstorage',
+        'solver_stats.bounds_update_storage',
+        'solver_stats.objective_update',
+        'solver_stats.lp_solve',
+        'solver_stats.result_update',
+        'time_taken_after',
+    )
+
+    keys_to_tabulate = (
+        'timesteps',
+        'time_taken',
+        'solver',
+        'num_scenarios',
+        'speed',
+        'solver_name'
+        'solver_stats.total',
+        'solver_stats.number_of_rows',
+        'solver_stats.number_of_cols',
+        'solver_stats.number_of_nonzero',
+        'solver_stats.number_of_routes',
+        'solver_stats.number_of_nodes',
+    )
+
+    values = []
+    labels = []
+    explode = []
+    solver_sub_total = 0.0
+    for k in keys_to_plot:
+        v = stats_df.loc[k][0]
+        values.append(v)
+        label = k.split('.', 1)[-1].replace('_', ' ').capitalize()
+        explode.append(0.0)
+        if k.startswith('solver_stats'):
+            labels.append('Solver - {}'.format(label))
+            solver_sub_total += v
+        else:
+
+            labels.append(label)
+
+    values.append(stats_df.loc['solver_stats.total'][0] - solver_sub_total)
+    labels.append('Solver - Other')
+    explode.append(0.0)
+
+    values.append(stats_df.loc['time_taken'][0] - sum(values))
+    values = np.array(values) / sum(values)
+    labels.append('Other')
+    explode.append(0.0)
+
+    fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, sharey='row',
+                                   gridspec_kw={'width_ratios': [2, 1]})
+
+    print(values, labels)
+    ax1.pie(values, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)
+    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
+
+    cell_text = []
+    for index, value in stats_df.iterrows():
+        if index not in keys_to_tabulate:
+            continue
+        v = value[0]
+        if isinstance(v, (float, np.float64, np.float32)):
+            v = f'{v:.2f}'
+
+        cell_text.append([index, v])
+
+    tbl = ax2.table(cellText=cell_text, colLabels=['Statistic', 'Value'], loc='center')
+    tbl.scale(1.5, 1.5)  # may help
+    tbl.set_fontsize(14)
+    ax2.axis('off')
+
+    fig.savefig('run_statistics_w_tables.png', dpi=300)
+    fig.savefig('run_statistics_w_tables.eps')
+
+    plt.show()
+
+
+@cli.command()
+@click.option('--ext', default='png')
+@click.option('--show/--no-show', default=False)
+def figures(ext, show):
+
+    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
+        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
+
+        fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, sharey='row',
+                                       gridspec_kw={'width_ratios': [3, 1]})
+        df['2100':'2125'].plot(ax=ax1)
+        df.quantile(np.linspace(0, 1)).plot(ax=ax2)
+
+        if name.startswith('reservoir'):
+            ax1.set_ylabel('Volume [$Mm^3$]')
+        else:
+            ax1.set_ylabel('Flow [$Mm^3/day$]')
+
+        for ax in (ax1, ax2):
+            ax.set_title(name)
+            ax.grid(True)
+        plt.tight_layout()
+
+        if ext is not None:
+            fig.savefig(f'{name}.{ext}', dpi=300)
+
+    if show:
+        plt.show()
+
+
+@cli.command('plot-res')
+@click.option('--ext', default='png')
+@click.option('--show/--no-show', default=False)
+def plot_res(ext, show):
+
+    end_year = '2105'
+
+    data = {}
+    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
+        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
+        data[name] = df
+
+    fig1, ax1 = plt.subplots(figsize=(16, 5), dpi=300)
+    data['reservoir1'].loc[:end_year, 'Central'].plot(ax=ax1)
+    ax1.set_ylabel('Volume [$Mm^3$]')
+    plt.tight_layout()
+
+    fig2, ax2 = plt.subplots(figsize=(16, 5), dpi=300)
+    data['demand_saving_level'].loc[:end_year, 'Central'].plot(ax=ax2)
+    ax2.set_ylabel('Demand saving level')
+    plt.tight_layout()
+
+    fig3, ax3 = plt.subplots(figsize=(16, 5), dpi=300)
+    data['demand_max_flow'].loc[:end_year, 'Central'].plot(ax=ax3)
+    ax3.set_ylabel('Demand [$Mm^3/day$]')
+    plt.tight_layout()
+
+    for ax in (ax1, ax2, ax3):
+        ax.grid(True)
+
+    if ext is not None:
+        fig1.savefig(f'Reservoir.{ext}', dpi=300)
+        fig2.savefig(f'Demand saving level.{ext}', dpi=300)
+        fig3.savefig(f'Demand.{ext}', dpi=300)
+
+    if show:
+        plt.show()
+
+
+@cli.command('plot-res2')
+@click.option('--ext', default='png')
+@click.option('--show/--no-show', default=False)
+def plot_res2(ext, show):
+
+    end_year = '2105'
+
+    data = {}
+    for name, df in TablesRecorder.generate_dataframes('thames_output.h5'):
+        df.columns = ['Very low', 'Low', 'Central', 'High', 'Very high']
+        data[name] = df
+
+    fig1, ax1 = plt.subplots(figsize=(16, 5), dpi=300)
+    data['reservoir1'].loc[:end_year].plot(ax=ax1)
+    ax1.set_ylabel('Volume [$Mm^3$]')
+    plt.legend()
+    plt.tight_layout()
+
+    fig2, ax2 = plt.subplots(figsize=(16, 5), dpi=300)
+    data['reservoir1'].quantile(np.linspace(0, 1)).plot(ax=ax2)
+    ax2.set_ylabel('Volume [$Mm^3$]')
+    ax2.set_xlabel('Quantile')
+    plt.tight_layout()
+
+    fig3, ax3 = plt.subplots(figsize=(16, 5), dpi=300)
+    df = data['demand_saving_level'].apply(pandas.Series.value_counts)
+    df /= df.sum(axis=0)
+    df.plot.bar(ax=ax3)
+    ax3.set_ylabel('Proportion of time.')
+    ax3.set_xlabel('Demand saving level')
+    plt.tight_layout()
+
+    for ax in (ax1, ax2, ax3):
+        ax.grid(True)
+
+    if ext is not None:
+        fig1.savefig(f'Reservoir (scenarios).{ext}', dpi=300)
+        fig2.savefig(f'Reservoir SDC (scenarios).{ext}', dpi=300)
+        fig3.savefig(f'Demand saving level count (scenarios).{ext}', dpi=300)
+
+    if show:
+        plt.show()
+
+
+@cli.command('plot-cc')
+@click.option('--ext', default='png')
+@click.option('--show/--no-show', default=False)
+def plot_control_curves(ext, show):
+
+    with open(MODEL_FILENAME) as fh:
+        data = json.load(fh)
+
+    parameters = data['parameters']
+
+    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
+
+    dates = pandas.date_range("2015-01-01", "2015-12-31")
+
+    L1_values = np.array([parameters['level1']['values'][d.month-1] for d in dates])
+    L2_values = np.array([parameters['level2']['values'][d.month-1] for d in dates])
+
+    x = np.arange(0, len(dates)) + 1
+
+    ax.fill_between(x, 1.0, L1_values, label='Level 0', alpha=0.8)
+    ax.fill_between(x, L1_values, L2_values, label='Level 1', alpha=0.8)
+    ax.fill_between(x, L2_values, 0.0, label='Level 2', alpha=0.8)
+
+    plt.xlabel("Day of year")
+    plt.ylabel("Reservoir volume [%]")
+
+    plt.grid(True)
+    plt.ylim([0.0, 1.0])
+    plt.xlim(1, 365)
+    plt.legend(["Level 0", "Level 1", "Level 2"], loc="upper right")
+    ax.plot(x, L1_values, color='k', label=None)
+    ax.plot(x, L2_values, color='k', label=None)
+    plt.tight_layout()
+
+    if ext is not None:
+        fig.savefig(f'Control curve zones.{ext}', dpi=300)
+
+    if show:
+        plt.show()
+
+
+@cli.command('plot-dsf')
+@click.option('--ext', default='png')
+@click.option('--show/--no-show', default=False)
+def plot_demand_saving_factor(ext, show):
+
+    with open(MODEL_FILENAME) as fh:
+        data = json.load(fh)
+
+    parameters = data['parameters']
+
+    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)
+
+    dates = pandas.date_range("2015-01-01", "2015-12-31")
+
+    L0_values = np.array([parameters['level0_factor']['values'] for d in dates])
+    L1_values = np.array([parameters['level1_factor']['values'][d.month-1] for d in dates])
+    L2_values = np.array([parameters['level2_factor']['values'][d.month-1] for d in dates])
+
+    x = np.arange(0, len(dates)) + 1
+
+    ax.plot(x, L0_values, label='Level 0')
+    ax.plot(x, L1_values, label='Level 0')
+    ax.plot(x, L2_values, label='Level 0')
+
+    plt.xlabel("Day of year")
+    plt.ylabel("Demand restriction factor")
+
+    plt.grid(True)
+    plt.ylim(0.6, 1.2)
+    plt.xlim(1, 365)
+    plt.legend(["Level 0", "Level 1", "Level 2"], loc="upper right")
+
+    plt.tight_layout()
+
+    if ext is not None:
+        fig.savefig(f'Demand restriction factors.{ext}', dpi=300)
+
+    if show:
+        plt.show()
+
+
+if __name__ == '__main__':
+    cli()
```

### Comparing `pywr-1.8.0/examples/two_reservoir/two_reservoir.json` & `pywr-1.9.0/tests/models/two_reservoir_constrained.json`

 * *Files 22% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8998842592592592%*

 * *Differences: {"'nodes'": "{0: {'flow': 'inflow'}, 1: {'flow': 'inflow'}, 2: {'min_volume': 3000, 'max_volume': "*

 * *            "20000, 'initial_volume': 16000}, 3: {'min_volume': 3000, 'max_volume': 20000, "*

 * *            "'initial_volume': 16000}, 5: {'max_flow': 45.0}, 6: {'max_flow': 20.0}, 9: {'mrf': "*

 * *            "5.0}, 10: {'mrf': 5.0}}",*

 * * "'parameters'": "{'transfer_controller': {'values': {insert: [(1, 10.0)], delete: [1]}}, "*

 * *                 "'inflow': OrderedDict([('type', 'dataframe'), ('url', "*

 * *                 […]*

```diff
@@ -52,73 +52,73 @@
     "metadata": {
         "description": "A simple problem with two reservoirs.",
         "minimum_version": "0.5",
         "title": "Two reservoirs"
     },
     "nodes": [
         {
-            "flow": "flow",
+            "flow": "inflow",
             "name": "catchment1",
             "type": "catchment"
         },
         {
-            "flow": "flow",
+            "flow": "inflow",
             "name": "catchment2",
             "type": "catchment"
         },
         {
             "cost": -5,
-            "initial_volume": 200,
-            "max_volume": 200,
-            "min_volume": 0,
+            "initial_volume": 16000,
+            "max_volume": 20000,
+            "min_volume": 3000,
             "name": "reservoir1",
             "type": "storage"
         },
         {
             "cost": -5,
-            "initial_volume": 200,
-            "max_volume": 200,
-            "min_volume": 0,
+            "initial_volume": 16000,
+            "max_volume": 20000,
+            "min_volume": 3000,
             "name": "reservoir2",
             "type": "storage"
         },
         {
             "cost": -500,
             "max_flow": "transfer_controller",
             "name": "transfer",
             "type": "link"
         },
         {
             "cost": -101,
-            "max_flow": 1.8,
+            "max_flow": 45.0,
             "name": "demand1",
             "type": "output"
         },
         {
             "cost": -100,
-            "max_flow": 1.0,
+            "max_flow": 20.0,
             "name": "demand2",
             "type": "output"
         },
         {
             "name": "river1",
             "type": "link"
         },
         {
             "name": "river2",
             "type": "link"
         },
         {
-            "mrf": 0.6,
+            "mrf": 5.0,
             "mrf_cost": -1000,
             "name": "compensation1",
             "type": "rivergauge"
         },
         {
-            "mrf": 0.6,
+            "mrf": 5.0,
             "mrf_cost": -1000,
             "name": "compensation2",
             "type": "rivergauge"
         },
         {
             "name": "terminator",
             "type": "output"
@@ -140,41 +140,28 @@
                 0,
                 0,
                 0,
                 0,
                 0
             ]
         },
-        "flow": {
-            "agg_func": "product",
-            "parameters": [
-                "flow_cumecs",
-                "flow_factor"
-            ],
-            "type": "aggregated"
-        },
-        "flow_cumecs": {
+        "inflow": {
             "column": "flow",
             "index_col": "Date",
             "parse_dates": true,
             "type": "dataframe",
-            "url": "../data/thames_stochastic_flow.gz"
-        },
-        "flow_factor": {
-            "name": "flow_factor",
-            "type": "constant",
-            "value": 0.0864
+            "url": "data/thames_stochastic_flow.gz"
         },
         "transfer_controller": {
             "control_curve": "control_curve",
             "storage_node": "reservoir1",
             "type": "controlcurve",
             "values": [
                 0.0,
-                1.0
+                10.0
             ]
         }
     },
     "recorders": {
         "deficit": {
             "is_objective": "minimise",
             "recorder_agg_func": "sum",
@@ -182,25 +169,31 @@
                 "deficit1",
                 "deficit2"
             ],
             "type": "aggregated"
         },
         "deficit1": {
             "node": "demand1",
-            "type": "DeficitFrequencyNode"
+            "type": "totaldeficitnode"
         },
         "deficit2": {
             "node": "demand2",
-            "type": "DeficitFrequencyNode"
+            "type": "totaldeficitnode"
+        },
+        "mean_transfer": {
+            "constraint_lower_bounds": 1.0,
+            "constraint_upper_bounds": 8.0,
+            "node": "transfer",
+            "type": "meanflownode"
         },
         "transferred": {
             "is_objective": "minimise",
             "node": "transfer",
             "type": "totalflownode"
         }
     },
     "timestepper": {
-        "end": "2200-01-01",
+        "end": "2110-01-01",
         "start": "2100-01-01",
         "timestep": 7
     }
 }
```

### Comparing `pywr-1.8.0/examples/two_reservoir/two_reservoir_moea.py` & `pywr-1.9.0/examples/two_reservoir/two_reservoir_moea.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-"""
-This example shows the trade-off (pareto frontier) of deficit against cost by altering a reservoir control curve.
-
-Two types of control curve are possible. The first is a monthly control curve containing one value for each
-month. The second is a harmonic control curve with cosine terms around a mean. Both Parameter objects
-are part of pywr.parameters.
-
-The example demonstrates the use of two different optimisation libraries: pygmo and platypus. The
-choice of library can be made with a command line argument.
-
-"""
-import json
-import os
-import pandas as pd
-import numpy as np
-import matplotlib.pyplot as plt
-
-
-def get_model_data(harmonic=True):
-
-    with open('two_reservoir.json') as fh:
-        data = json.load(fh)
-
-    if harmonic:
-        # Patch the control curve parameter
-        data['parameters']['control_curve'] = {
-            'type': 'AnnualHarmonicSeries',
-            'mean': 0.5,
-            'amplitudes': [0.5, 0.0],
-            'phases': [0.0, 0.0],
-            'mean_upper_bounds': 1.0,
-            'amplitude_upper_bounds': 1.0,
-            'is_variable': True
-        }
-
-    return data
-
-
-def platypus_main(harmonic=False):
-    import platypus
-    from pywr.optimisation.platypus import PlatypusWrapper
-
-    wrapper = PlatypusWrapper(get_model_data(harmonic=harmonic))
-
-    with platypus.ProcessPoolEvaluator() as evaluator:
-        algorithm = platypus.NSGAIII(wrapper.problem, population_size=50, evaluator=evaluator, divisions_outer=12)
-        algorithm.run(10000)
-
-    objectives = pd.DataFrame(data=np.array([s.objectives for s in algorithm.result]),
-                              columns=[o.name for o in wrapper.model_objectives])
-
-    title = 'harmonic' if harmonic else 'monthly'
-    objectives.to_hdf('two_reservoir_moea.h5', f'platypus_{title}')
-
-
-def pygmo_main(harmonic=False):
-    import pygmo as pg
-    from pywr.optimisation.pygmo import PygmoWrapper
-
-    def update_archive(pop, archive=None):
-        if archive is None:
-            combined_f = pop.get_f()
-        else:
-            combined_f = np.r_[archive, pop.get_f()]
-        indices = pg.select_best_N_mo(combined_f, 50)
-        new_archive = combined_f[indices, :]
-        new_archive = np.unique(new_archive.round(4), axis=0)
-        return new_archive
-
-    wrapper = PygmoWrapper(get_model_data(harmonic=harmonic))
-    prob = pg.problem(wrapper)
-    print(prob)
-    algo = pg.algorithm(pg.moead(gen=1))
-    #algo = pg.algorithm(pg.nsga2(gen=1))
-
-    pg.mp_island.init_pool(2)
-    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
-
-    ref_point = [216500, 4000]
-    pop = isl.get_population()
-
-    print('Evolving!')
-    archive = update_archive(pop)
-    hv = pg.hypervolume(archive)
-    vol = hv.compute(ref_point)
-    hvs = [vol, ]
-    print('Gen: {:03d}, Hypervolume: {:6.4g}, Archive size: {:d}'.format(0, vol, len(archive)))
-
-    for gen in range(20):
-        isl.evolve(1)
-        isl.wait_check()
-
-        pop = isl.get_population()
-        archive = update_archive(pop, archive)
-
-        hv = pg.hypervolume(archive)
-        vol = hv.compute(ref_point)
-        print('Gen: {:03d}, Hypervolume: {:6.4g}, Archive size: {:d}'.format(gen+1, vol, len(archive)))
-        hvs.append(vol)
-
-    hvs = pd.Series(hvs)
-
-    print('Finished!')
-
-    plt.scatter(archive[:, 0], archive[:, 1])
-
-    objectives = wrapper.model_objectives
-    plt.xlabel(objectives[0].name)
-    plt.ylabel(objectives[1].name)
-    plt.grid(True)
-    title = 'Harmonic Control Curve' if harmonic else 'Monthly Control Curve'
-    plt.savefig('{} Example ({}).pdf'.format('pygmo', title), format='pdf')
-
-    fig, ax = plt.subplots()
-    ax.plot(hvs/1e6, marker='o')
-    ax.grid(True)
-    ax.set_ylabel('Hypervolume')
-    ax.set_xlabel('Generation')
-    plt.savefig('{} Example Hypervolume ({}).pdf'.format('pygmo', title), format='pdf')
-
-    plt.show()
-
-
-if __name__ == '__main__':
-    import argparse
-
-    # Setup logging
-    import logging
-    logger = logging.getLogger('pywr')
-    logger.setLevel(logging.INFO)
-    ch = logging.StreamHandler()
-    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
-    ch.setFormatter(formatter)
-    logger.addHandler(ch)
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--harmonic', action='store_true', help='Use an harmonic control curve.')
-    parser.add_argument('--library', type=str, choices=['platypus', 'pygmo'])
-    args = parser.parse_args()
-
-    if args.library == 'platypus':
-        platypus_main(harmonic=args.harmonic)
-    elif args.library == 'pygmo':
-        pygmo_main(harmonic=args.harmonic)
-    else:
-        raise ValueError('Optimisation library "{}" not recognised.'.format(args.library))
+"""
+This example shows the trade-off (pareto frontier) of deficit against cost by altering a reservoir control curve.
+
+Two types of control curve are possible. The first is a monthly control curve containing one value for each
+month. The second is a harmonic control curve with cosine terms around a mean. Both Parameter objects
+are part of pywr.parameters.
+
+The example demonstrates the use of two different optimisation libraries: pygmo and platypus. The
+choice of library can be made with a command line argument.
+
+"""
+import json
+import os
+import pandas as pd
+import numpy as np
+import matplotlib.pyplot as plt
+
+
+def get_model_data(harmonic=True):
+
+    with open('two_reservoir.json') as fh:
+        data = json.load(fh)
+
+    if harmonic:
+        # Patch the control curve parameter
+        data['parameters']['control_curve'] = {
+            'type': 'AnnualHarmonicSeries',
+            'mean': 0.5,
+            'amplitudes': [0.5, 0.0],
+            'phases': [0.0, 0.0],
+            'mean_upper_bounds': 1.0,
+            'amplitude_upper_bounds': 1.0,
+            'is_variable': True
+        }
+
+    return data
+
+
+def platypus_main(harmonic=False):
+    import platypus
+    from pywr.optimisation.platypus import PlatypusWrapper
+
+    wrapper = PlatypusWrapper(get_model_data(harmonic=harmonic))
+
+    with platypus.ProcessPoolEvaluator() as evaluator:
+        algorithm = platypus.NSGAIII(wrapper.problem, population_size=50, evaluator=evaluator, divisions_outer=12)
+        algorithm.run(10000)
+
+    objectives = pd.DataFrame(data=np.array([s.objectives for s in algorithm.result]),
+                              columns=[o.name for o in wrapper.model_objectives])
+
+    title = 'harmonic' if harmonic else 'monthly'
+    objectives.to_hdf('two_reservoir_moea.h5', f'platypus_{title}')
+
+
+def pygmo_main(harmonic=False):
+    import pygmo as pg
+    from pywr.optimisation.pygmo import PygmoWrapper
+
+    def update_archive(pop, archive=None):
+        if archive is None:
+            combined_f = pop.get_f()
+        else:
+            combined_f = np.r_[archive, pop.get_f()]
+        indices = pg.select_best_N_mo(combined_f, 50)
+        new_archive = combined_f[indices, :]
+        new_archive = np.unique(new_archive.round(4), axis=0)
+        return new_archive
+
+    wrapper = PygmoWrapper(get_model_data(harmonic=harmonic))
+    prob = pg.problem(wrapper)
+    print(prob)
+    algo = pg.algorithm(pg.moead(gen=1))
+    #algo = pg.algorithm(pg.nsga2(gen=1))
+
+    pg.mp_island.init_pool(2)
+    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
+
+    ref_point = [216500, 4000]
+    pop = isl.get_population()
+
+    print('Evolving!')
+    archive = update_archive(pop)
+    hv = pg.hypervolume(archive)
+    vol = hv.compute(ref_point)
+    hvs = [vol, ]
+    print('Gen: {:03d}, Hypervolume: {:6.4g}, Archive size: {:d}'.format(0, vol, len(archive)))
+
+    for gen in range(20):
+        isl.evolve(1)
+        isl.wait_check()
+
+        pop = isl.get_population()
+        archive = update_archive(pop, archive)
+
+        hv = pg.hypervolume(archive)
+        vol = hv.compute(ref_point)
+        print('Gen: {:03d}, Hypervolume: {:6.4g}, Archive size: {:d}'.format(gen+1, vol, len(archive)))
+        hvs.append(vol)
+
+    hvs = pd.Series(hvs)
+
+    print('Finished!')
+
+    plt.scatter(archive[:, 0], archive[:, 1])
+
+    objectives = wrapper.model_objectives
+    plt.xlabel(objectives[0].name)
+    plt.ylabel(objectives[1].name)
+    plt.grid(True)
+    title = 'Harmonic Control Curve' if harmonic else 'Monthly Control Curve'
+    plt.savefig('{} Example ({}).pdf'.format('pygmo', title), format='pdf')
+
+    fig, ax = plt.subplots()
+    ax.plot(hvs/1e6, marker='o')
+    ax.grid(True)
+    ax.set_ylabel('Hypervolume')
+    ax.set_xlabel('Generation')
+    plt.savefig('{} Example Hypervolume ({}).pdf'.format('pygmo', title), format='pdf')
+
+    plt.show()
+
+
+if __name__ == '__main__':
+    import argparse
+
+    # Setup logging
+    import logging
+    logger = logging.getLogger('pywr')
+    logger.setLevel(logging.INFO)
+    ch = logging.StreamHandler()
+    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    ch.setFormatter(formatter)
+    logger.addHandler(ch)
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--harmonic', action='store_true', help='Use an harmonic control curve.')
+    parser.add_argument('--library', type=str, choices=['platypus', 'pygmo'])
+    args = parser.parse_args()
+
+    if args.library == 'platypus':
+        platypus_main(harmonic=args.harmonic)
+    elif args.library == 'pygmo':
+        pygmo_main(harmonic=args.harmonic)
+    else:
+        raise ValueError('Optimisation library "{}" not recognised.'.format(args.library))
```

### Comparing `pywr-1.8.0/github_deploy_key.enc` & `pywr-1.9.0/github_deploy_key.enc`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/pywr/__init__.py` & `pywr-1.9.0/pywr/__init__.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-#!/usr/bin/env python
-import os
-import sys
-from pkg_resources import get_distribution, DistributionNotFound
-try:
-    __version__ = get_distribution(__name__).version
-except DistributionNotFound:
-    # package is not installed
-    pass
-
-if sys.platform == "win32":
-    dll_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), ".libs"))
-    if sys.version_info.major >= 3 and sys.version_info.minor >= 8:
-        # https://docs.python.org/3/whatsnew/3.8.html#bpo-36085-whatsnew
-        if os.path.exists(dll_folder):
-            os.add_dll_directory(dll_folder)
-    else:
-        os.environ["PATH"] = os.environ["PATH"] + ";" + dll_folder
+#!/usr/bin/env python
+import os
+import sys
+from pkg_resources import get_distribution, DistributionNotFound
+try:
+    __version__ = get_distribution(__name__).version
+except DistributionNotFound:
+    # package is not installed
+    pass
+
+if sys.platform == "win32":
+    dll_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), ".libs"))
+    if sys.version_info.major >= 3 and sys.version_info.minor >= 8:
+        # https://docs.python.org/3/whatsnew/3.8.html#bpo-36085-whatsnew
+        if os.path.exists(dll_folder):
+            os.add_dll_directory(dll_folder)
+    else:
+        os.environ["PATH"] = os.environ["PATH"] + ";" + dll_folder
```

### Comparing `pywr-1.8.0/pywr/_component.pyx` & `pywr-1.9.0/pywr/_component.pyx`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-import logging
-
-
-logger = logging.getLogger(__name__)
-
-
-ROOT_NODE = "root"
-
-class GraphInterface:
-    def __init__(self, obj):
-        self.obj = obj
-
-    @property
-    def graph(self):
-        return self.obj.model.component_graph
-
-    def add(self, item):
-        if not isinstance(item, Component):
-            return
-        if self is self.obj.children:
-            self.graph.add_edge(item, self.obj)
-        else:
-            self.graph.add_edge(self.obj, item)
-
-    def remove(self, item):
-        if not isinstance(item, Component):
-            return
-        if self is self.obj.children:
-            self.graph.remove_edge(item, self.obj)
-        else:
-            self.graph.remove_edge(self.obj, item)
-
-    def clear(self):
-        for n in self._members:
-            if self is self.obj.children:
-                self.graph.remove_edge(n, self.obj)
-            else:
-                self.graph.remove_edge(self.obj, n)
-
-    @property
-    def _members(self):
-        if self is self.obj.children:
-            return [n for n in self.graph.predecessors(self.obj) if n != ROOT_NODE]
-        else:
-            return [x for x in self.graph.successors(self.obj)]
-
-    def __len__(self):
-        """Returns the number of nodes in the model"""
-        return len(self._members)
-
-    def __iter__(self):
-        return iter(self._members)
-
-
-
-
-cdef class Component:
-    """ Components of a Model
-
-    This is the base class for all the elements of a `pywr.Model`,
-     except the the nodes, that require updates via the `setup`, `reset`,
-     `before`, `after` and `finish` methods. This class handles
-     registering the instances on the `Model.component_graph` and
-     managing the parent/children interface.
-
-    The parent/children interface, through the `pywr.Model.component_graph`
-     is used to create a dependency tree such that the methods are
-     called in the correct order. E.g. that a `before` method in
-     one component that is a parent of another is called first.
-
-    See also
-    --------
-    pywr.Model
-
-    """
-    def __init__(self, model, name=None, comment=None):
-        self.model = model
-        self.name = name
-        self.comment = comment
-        model.component_graph.add_edge(ROOT_NODE, self)
-        self.parents = GraphInterface(self)
-        self.children = GraphInterface(self)
-
-    property name:
-        def __get__(self):
-            return self._name
-
-        def __set__(self, name):
-            # check for name collision
-            if name is  not None and name in self.model.components.keys():
-                raise ValueError('A component with the name "{}" already exists.'.format(name))
-            # apply new name
-            self._name = name
-
-    cpdef setup(self):
-        logger.debug('Setting up {}: "{}"'.format(self.__class__.__name__, self.name))
-
-    cpdef reset(self):
-        logger.debug('Resetting up {}: "{}"'.format(self.__class__.__name__, self.name))
-
-    cpdef before(self):
-        pass
-
-    cpdef after(self):
-        pass
-
-    cpdef finish(self):
-        pass
+import logging
+
+
+logger = logging.getLogger(__name__)
+
+
+ROOT_NODE = "root"
+
+class GraphInterface:
+    def __init__(self, obj):
+        self.obj = obj
+
+    @property
+    def graph(self):
+        return self.obj.model.component_graph
+
+    def add(self, item):
+        if not isinstance(item, Component):
+            return
+        if self is self.obj.children:
+            self.graph.add_edge(item, self.obj)
+        else:
+            self.graph.add_edge(self.obj, item)
+
+    def remove(self, item):
+        if not isinstance(item, Component):
+            return
+        if self is self.obj.children:
+            self.graph.remove_edge(item, self.obj)
+        else:
+            self.graph.remove_edge(self.obj, item)
+
+    def clear(self):
+        for n in self._members:
+            if self is self.obj.children:
+                self.graph.remove_edge(n, self.obj)
+            else:
+                self.graph.remove_edge(self.obj, n)
+
+    @property
+    def _members(self):
+        if self is self.obj.children:
+            return [n for n in self.graph.predecessors(self.obj) if n != ROOT_NODE]
+        else:
+            return [x for x in self.graph.successors(self.obj)]
+
+    def __len__(self):
+        """Returns the number of nodes in the model"""
+        return len(self._members)
+
+    def __iter__(self):
+        return iter(self._members)
+
+
+
+
+cdef class Component:
+    """ Components of a Model
+
+    This is the base class for all the elements of a `pywr.Model`,
+     except the the nodes, that require updates via the `setup`, `reset`,
+     `before`, `after` and `finish` methods. This class handles
+     registering the instances on the `Model.component_graph` and
+     managing the parent/children interface.
+
+    The parent/children interface, through the `pywr.Model.component_graph`
+     is used to create a dependency tree such that the methods are
+     called in the correct order. E.g. that a `before` method in
+     one component that is a parent of another is called first.
+
+    See also
+    --------
+    pywr.Model
+
+    """
+    def __init__(self, model, name=None, comment=None):
+        self.model = model
+        self.name = name
+        self.comment = comment
+        model.component_graph.add_edge(ROOT_NODE, self)
+        self.parents = GraphInterface(self)
+        self.children = GraphInterface(self)
+
+    property name:
+        def __get__(self):
+            return self._name
+
+        def __set__(self, name):
+            # check for name collision
+            if name is  not None and name in self.model.components.keys():
+                raise ValueError('A component with the name "{}" already exists.'.format(name))
+            # apply new name
+            self._name = name
+
+    cpdef setup(self):
+        logger.debug('Setting up {}: "{}"'.format(self.__class__.__name__, self.name))
+
+    cpdef reset(self):
+        logger.debug('Resetting up {}: "{}"'.format(self.__class__.__name__, self.name))
+
+    cpdef before(self):
+        pass
+
+    cpdef after(self):
+        pass
+
+    cpdef finish(self):
+        pass
```

### Comparing `pywr-1.8.0/pywr/_core.pxd` & `pywr-1.9.0/pywr/_core.pxd`

 * *Files 18% similar despite different names*

```diff
@@ -1,124 +1,136 @@
-from .parameters._parameters cimport Parameter
-
-
-cdef class Scenario:
-    cdef basestring _name
-    cdef int _size
-    cdef public slice slice
-    cdef object _ensemble_names
-
-cdef class ScenarioCollection:
-    cdef public object model
-    cdef list _scenarios
-    cdef readonly list combinations
-    cdef int[:, :] _user_combinations
-    cpdef int get_scenario_index(self, Scenario sc) except? -1
-    cpdef add_scenario(self, Scenario sc)
-    cpdef int ravel_indices(self, int[:] scenario_indices) except? -1
-
-cdef class ScenarioCombinations:
-    cdef ScenarioCollection _collection
-
-cdef class ScenarioIndex:
-    cdef readonly int global_id
-    cdef int[:] _indices
-
-cdef bint is_leap_year(int year)
-
-cdef class Timestep:
-    cdef readonly object period
-    cdef readonly int index
-    cdef readonly double days
-    cdef readonly int dayofyear
-    cdef readonly int dayofyear_index  # Day of the year for profiles
-    cdef readonly bint is_leap_year
-    cdef readonly int week_index  # Zero-based week
-    cdef readonly int day
-    cdef readonly int month
-    cdef readonly int year
-
-cdef class Domain:
-    cdef object name
-
-cdef class AbstractNode:
-    cdef double[:] _prev_flow
-    cdef double[:] _flow
-    cdef list _recorders
-    cdef Domain _domain
-    cdef AbstractNode _parent
-    cdef object _model
-    cdef object _name
-    cdef bint _allow_isolated
-    cdef public bint virtual
-    cdef public object __data
-    cdef public basestring comment
-
-    cdef Parameter _cost_param
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1
-
-    cpdef setup(self, model)
-    cpdef reset(self)
-    cpdef before(self, Timestep ts)
-    cpdef commit(self, int scenario_index, double value)
-    cpdef commit_all(self, double[:] value)
-    cpdef after(self, Timestep ts)
-    cpdef finish(self)
-    cpdef check(self,)
-
-cdef class Node(AbstractNode):
-    cdef double _cost
-    cdef double _min_flow
-    cdef double _max_flow
-    cdef double _conversion_factor
-    cdef Parameter _min_flow_param
-    cdef Parameter _max_flow_param
-
-    cdef Parameter _conversion_factor_param
-    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_conversion_factor(self) except? -1
-    cdef set_parameters(self, ScenarioIndex scenario_index)
-
-cdef class AggregatedNode(AbstractNode):
-    cdef list _nodes
-    cdef double[:] _factors
-    cdef double[:] _flow_weights
-    cdef double _max_flow
-    cdef double _min_flow
-    cdef Parameter _min_flow_param
-    cdef Parameter _max_flow_param
-
-    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1
-
-cdef class BaseInput(Node):
-    cdef object _licenses
-
-cdef class AbstractStorage(AbstractNode):
-    cdef public double[:] _volume
-    cdef public double[:] _current_pc
-
-cdef class Storage(AbstractStorage):
-    cdef double _cost
-    cdef double _initial_volume
-    cdef double _initial_volume_pc
-    cdef double _min_volume
-    cdef double _max_volume
-    cdef double _level
-    cdef double _area
-    cdef Parameter _min_volume_param
-    cdef Parameter _max_volume_param
-    cdef Parameter _level_param
-    cdef Parameter _area_param
-    cpdef _reset_storage_only(self, bint use_initial_volume = *)
-    cpdef double get_min_volume(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_max_volume(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_level(self, ScenarioIndex scenario_index) except? -1
-    cpdef double get_area(self, ScenarioIndex scenario_index) except? -1
-
-cdef class AggregatedStorage(AbstractStorage):
-    cdef list _storage_nodes
-
-cdef class VirtualStorage(Storage):
-    cdef list _nodes
-    cdef double[:] _factors
+from .parameters._parameters cimport Parameter
+
+
+cdef class Scenario:
+    cdef basestring _name
+    cdef int _size
+    cdef public slice slice
+    cdef object _ensemble_names
+
+cdef class ScenarioCollection:
+    cdef public object model
+    cdef list _scenarios
+    cdef readonly list combinations
+    cdef int[:, :] _user_combinations
+    cpdef int get_scenario_index(self, Scenario sc) except? -1
+    cpdef add_scenario(self, Scenario sc)
+    cpdef int ravel_indices(self, int[:] scenario_indices) except? -1
+
+cdef class ScenarioCombinations:
+    cdef ScenarioCollection _collection
+
+cdef class ScenarioIndex:
+    cdef readonly int global_id
+    cdef int[:] _indices
+
+cdef bint is_leap_year(int year)
+
+cdef class Timestep:
+    cdef readonly object period
+    cdef readonly int index
+    cdef readonly double days
+    cdef readonly int dayofyear
+    cdef readonly int dayofyear_index  # Day of the year for profiles
+    cdef readonly bint is_leap_year
+    cdef readonly int week_index  # Zero-based week
+    cdef readonly int day
+    cdef readonly int month
+    cdef readonly int year
+
+cdef class Domain:
+    cdef object name
+
+cdef class AbstractNode:
+    cdef double[:] _prev_flow
+    cdef double[:] _flow
+    cdef list _recorders
+    cdef Domain _domain
+    cdef AbstractNode _parent
+    cdef object _model
+    cdef object _name
+    cdef bint _allow_isolated
+    cdef public bint virtual
+    cdef public object __data
+    cdef public basestring comment
+
+    cdef Parameter _cost_param
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1
+
+    cpdef setup(self, model)
+    cpdef reset(self)
+    cpdef before(self, Timestep ts)
+    cpdef commit(self, int scenario_index, double value)
+    cpdef commit_all(self, double[:] value)
+    cpdef after(self, Timestep ts)
+    cpdef finish(self)
+    cpdef check(self,)
+
+cdef class Node(AbstractNode):
+    cdef double _cost
+    cdef double _min_flow
+    cdef double _max_flow
+    cdef double _conversion_factor
+    cdef Parameter _min_flow_param
+    cdef Parameter _max_flow_param
+
+    cdef Parameter _conversion_factor_param
+    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_conversion_factor(self) except? -1
+    cdef set_parameters(self, ScenarioIndex scenario_index)
+
+cdef class AggregatedNode(AbstractNode):
+    cdef list _nodes
+    cdef list _factors
+    cdef double[:] _flow_weights
+    cdef double _max_flow
+    cdef double _min_flow
+    cdef Parameter _min_flow_param
+    cdef Parameter _max_flow_param
+    cdef public object __agg_factor_data
+
+    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1
+    cpdef double[:] get_factors(self, ScenarioIndex scenario_index)
+    cpdef double[:] get_factors_norm(self, ScenarioIndex scenario_index)
+
+cdef class BaseInput(Node):
+    cdef object _licenses
+
+cdef class AbstractStorage(AbstractNode):
+    cdef public double[:] _volume
+    cdef public double[:] _current_pc
+
+cdef class Storage(AbstractStorage):
+    cdef double _cost
+    cdef double _initial_volume
+    cdef double _initial_volume_pc
+    cdef double _min_volume
+    cdef double _max_volume
+    cdef double _level
+    cdef double _area
+    cdef Parameter _min_volume_param
+    cdef Parameter _max_volume_param
+    cdef Parameter _level_param
+    cdef Parameter _area_param
+    cpdef double get_initial_volume(self) except? -1
+    cpdef double get_initial_pc(self) except? -1
+    cpdef _reset_storage_only(self, bint use_initial_volume = *)
+    cpdef double get_min_volume(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_max_volume(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_level(self, ScenarioIndex scenario_index) except? -1
+    cpdef double get_area(self, ScenarioIndex scenario_index) except? -1
+    cpdef after(self, Timestep ts, double[:] adjustment = *)
+
+cdef class AggregatedStorage(AbstractStorage):
+    cdef list _storage_nodes
+
+cdef class VirtualStorage(Storage):
+    cdef list _nodes
+    cdef double[:] _factors
+    cdef public bint active
+
+cdef class RollingVirtualStorage(VirtualStorage):
+    cdef public int timesteps
+    cdef double[:, :] _memory
+    cdef int _memory_pointer
```

### Comparing `pywr-1.8.0/pywr/_core.pyx` & `pywr-1.9.0/pywr/_core.pyx`

 * *Files 24% similar despite different names*

```diff
@@ -1,1157 +1,1278 @@
-from pywr._core cimport *
-from pywr._component cimport Component
-import itertools
-import numpy as np
-cimport numpy as np
-import pandas as pd
-import warnings
-
-cdef double inf = float('inf')
-
-cdef class Scenario:
-    """Represents a scenario in the model.
-
-    Typically a scenario will be used to run many similar models simultaneously. A small
-    number of `Parameter` objects in the model will return different values depending
-    on the scenario, but many will not. Multiple scenarios can be defined such that
-    some `Parameter` values vary with one scenario, but not another. Scenarios are defined
-    with a size that represents the number of ensembles in that scenario.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-        The model instance to attach the scenario to.
-    name : str
-        The name of the scenario.
-    size : int, optional
-        The number of ensembles in the scenario. The default value is 1.
-    slice : slice, optional
-        If given this defines the subset of the ensembles that are actually run
-         in the model. This is useful if a large number of ensembles is defined, but
-         certain analysis (e.g. optimisation) can only be done on a small subset.
-    ensemble_names : iterable of str, optional
-        User defined names describing each ensemble.
-
-    See Also
-    --------
-    ScenarioCollection
-    ScenarioIndex
-    """
-    def __init__(self, model, name, int size=1, slice slice=None, ensemble_names=None):
-        self._name = name
-        if size < 1:
-            raise ValueError("Size must be greater than or equal to 1.")
-        self._size = size
-        self.slice = slice
-        self.ensemble_names = ensemble_names
-        # Do this last so only set on the model if no error is raised.
-        model.scenarios.add_scenario(self)
-
-    property size:
-        def __get__(self, ):
-            return self._size
-
-    property name:
-        def __get__(self):
-            return self._name
-
-    property ensemble_names:
-        def __get__(self):
-            if self._ensemble_names is None:
-                return list(range(self._size))
-            return self._ensemble_names
-
-        def __set__(self, names):
-            if names is None:
-                self._ensemble_names = None
-                return
-            if len(names) != self._size:
-                raise ValueError("The length of ensemble_names ({}) must be equal to the size of the scenario ({})".format(len(names), self.size))  # noqa
-            self._ensemble_names = names
-
-cdef class ScenarioCollection:
-    """ Represents a collection of `Scenario` objects.
-
-    This class is used by a `Model` instance to hold the defined scenarios and control
-    which combinations of ensembles are used during model execution. By default the
-    product of all scenario ensembles (i.e. all possible combinations of ensembles) is
-    executed. However user defined slices can be set on individual `Scenario` instances
-    to restrict the number of ensembles executed from that scenario. Alternatively
-    the user may provide an array of the specific ensemble combinations (indices) that
-    should be run. The latter approach takes precedent over the former per `Scenario`
-    slices.
-
-    See Also
-    --------
-    Scenario
-    ScenarioIndex
-    """
-    def __init__(self, model):
-        self.model = model
-        self._scenarios = []
-        self.combinations = None
-        self.user_combinations = None
-
-    property scenarios:
-        def __get__(self):
-            return self._scenarios
-
-    def __getitem__(self, name):
-        cdef Scenario sc
-        for sc in self._scenarios:
-            if sc._name == name:
-                return sc
-        raise KeyError("Scenario with name '{}' not found.".format(name))
-
-    def get_combinations(self):
-        """Returns a list of ScenarioIndices for every combination of Scenarios
-        """
-        cdef Scenario scenario
-        cdef int i
-        if len(self._scenarios) == 0:
-            # model has no scenarios defined, implicitly has 1 scenario of size 1
-            combinations = [ScenarioIndex(0, np.array([0], dtype=np.int32))]
-        elif self._user_combinations is not None:
-            # use combinations given by user
-            combinations = list([ScenarioIndex(i, self._user_combinations[i, :])
-                                 for i in range(self._user_combinations.shape[0])])
-        else:
-            # product of all scenarios, taking into account Scenario.slice
-            iter = itertools.product(*[range(scenario._size)[scenario.slice]
-                                     if scenario.slice else range(scenario._size)
-                                     for scenario in self._scenarios])
-            combinations = list([ScenarioIndex(i, np.array(x, dtype=np.int32)) for i, x in enumerate(iter)])
-        if not combinations:
-            raise ValueError("No scenarios were selected to be run")
-        return combinations
-
-    def setup(self):
-        self.combinations = self.get_combinations()
-
-    property user_combinations:
-        def __get__(self, ):
-            return self._user_combinations
-
-        def __set__(self, values):
-            if values is None:
-                self._user_combinations = None
-                return
-            cdef Scenario sc
-            values = np.asarray(values, dtype=np.int32)
-            if values.ndim != 2:
-                raise ValueError('A 2-dimensional array of scenario indices must be provided.')
-            if values.shape[1] != len(self._scenarios):
-                raise ValueError('User defined combinations must have shape (N, S) where S in number of Scenarios')
-            # Check maximum values
-            for sc, v in zip(self._scenarios, values.max(axis=0)):
-                if v >= sc._size:
-                    raise ValueError('Given ensemble index for scenario "{}" out of range.'.format(sc.name))
-            if np.any(values.min(axis=0) < 0):
-                raise ValueError('Ensemble index less than zero is invalid.')
-
-            self._user_combinations = values
-
-    cpdef int get_scenario_index(self, Scenario sc) except? -1:
-        """Return the index of Scenario in this controller."""
-        return self._scenarios.index(sc)
-
-    cpdef add_scenario(self, Scenario sc):
-        if sc in self._scenarios:
-            raise ValueError("The same scenario can not be added twice.")
-        self.model.dirty = True
-        self._scenarios.append(sc)
-
-    property combination_names:
-        def __get__(self):
-            cdef ScenarioIndex si
-            cdef Scenario sc
-            cdef int i
-            cdef list names
-            for si in self.combinations:
-                names = []
-                for i, sc in enumerate(self._scenarios):
-                    names.append('{}.{:03d}'.format(sc._name, si._indices[i]))
-                yield '-'.join(names)
-
-    def __len__(self):
-        return len(self._scenarios)
-
-    property shape:
-        def __get__(self):
-            if self._user_combinations is not None:
-                # raise ValueError("ScenarioCollection.shape is undefined if user_combinations is defined.")
-                return (len(self._user_combinations), )
-            if len(self._scenarios) == 0:
-                return (1, )
-            return tuple(len(range(sc.size)[sc.slice]) if sc.slice is not None else sc.size for sc in self._scenarios)
-
-    property multiindex:
-        def __get__(self):
-            cdef Scenario sc
-            if len(self._scenarios) == 0:
-                return pd.MultiIndex.from_product([range(1)], names=[''])
-            else:
-                ensemble_names = [scenario.ensemble_names for scenario in self._scenarios]
-                indices = [[ensemble_names[n][i] for n, i in enumerate(scenario_index.indices)]
-                           for scenario_index in self.model.scenarios.get_combinations()]
-                names = [sc._name for sc in self._scenarios]
-                return pd.MultiIndex.from_tuples(indices, names=names)
-
-    cpdef int ravel_indices(self, int[:] scenario_indices) except? -1:
-        if scenario_indices is None:
-            return 0
-        # Case where scenario_indices is empty for no scenarios defined
-        if scenario_indices.size == 0:
-            return 0
-        return np.ravel_multi_index(scenario_indices, np.array(self.shape))
-
-cdef class ScenarioIndex:
-    """A ScenarioIndex is an indexer for a given combination of scenarios.
-
-    The ScenarioIndex is used to represent a given combination of scenarios
-    during a simulation. It is given to Parameter instances to allow them
-    to alter their calculations based on each scenario. Instances of this
-    class are generated by Pywr, and users are unlikely to need to
-    instantiate this class.
-
-    Attributes
-    ----------
-    global_id : int
-        Read-only global scenario index that this ScenarioIndex refers to.
-    indices : np.array
-        The indices for each of the Scenarios in this model.
-
-    See Also
-    --------
-    Scenario
-    ScenarioCollection
-    """
-    def __init__(self, int global_id, int[:] indices):
-        self.global_id = global_id
-        self._indices = indices
-
-    property indices:
-        def __get__(self):
-            return np.array(self._indices)
-
-    def __repr__(self):
-        return "<ScenarioIndex gid={:d} indices={}>".format(self.global_id, tuple(np.asarray(self._indices)))
-
-
-cdef bint is_leap_year(int year):
-    # http://stackoverflow.com/a/11595914/1300519
-    return ((year & 3) == 0 and ((year % 25) != 0 or (year & 15) == 0))
-
-
-cdef class Timestep:
-    """A representation of a simulation time-step.
-
-    Attributes
-    ----------
-    period : pandas.Period
-        Read-only global scenario index that this ScenarioIndex refers to.
-    index : int
-        The time-step index of the simulation.
-    days : int
-        The time-step length in days.
-    dayofyear : int
-        The day of the year (from `pandas.Period.dayofyear`).
-    dayofyear_index : int
-        A zero-based index for the day of the year. This begins at 0 and ends at 366. The 29th of February
-        is skipped if it is not a leap year.
-    day : int
-        The day of the month.
-    week_index : int
-        A zero-based index for the week of the year. The last week of the year (index 51) is always slightly
-        longer than 7 days.
-    month : int
-        The month number (1 - 12).
-    year : int
-        The year.
-    is_leap_year : bool
-        True if the current year is a leap year; false otherwise.
-    """
-    def __init__(self, period, int index, double days):
-        self.period = period
-        self.index = index
-        if days <= 0:
-            raise ValueError("The days argument must be > 0.")
-        self.days = days
-        self.dayofyear = period.dayofyear
-        self.day = period.day
-        self.month = period.month
-        self.year = period.year
-        self.is_leap_year = is_leap_year(self.year)
-
-        # Calculate day of year index (zero based)
-        cdef int i = self.dayofyear - 1
-        if not self.is_leap_year:
-            if i > 58: # 28th Feb
-                i += 1
-        self.dayofyear_index = i
-
-        # Calculate week of year
-        if self.dayofyear_index >= 364:
-            # last week of year is slightly longer than 7 days
-            self.week_index = 51
-        else:
-            self.week_index = self.dayofyear_index // 7
-
-    property datetime:
-        """Timestep representation as a `datetime.datetime` object"""
-        def __get__(self, ):
-            return self.period.to_timestamp()
-
-    def __repr__(self):
-        return "<Timestep date=\"{}\">".format(self.period.strftime("%Y-%m-%d"))
-
-cdef class Domain:
-    """ Domain class which all Node objects must have. """
-    def __init__(self, name):
-        self.name = name
-
-cdef class AbstractNode:
-    """ Base class for all nodes in Pywr.
-
-    This class is not intended to be used directly.
-    """
-    def __cinit__(self):
-        self._allow_isolated = False
-        self.virtual = False
-
-    def __init__(self, model, name, comment=None, **kwargs):
-        self._model = model
-        self.name = name
-        self.comment = comment
-
-        self._parent = kwargs.pop('parent', None)
-        self._domain = kwargs.pop('domain', None)
-        self._recorders = []
-
-        self._flow = np.empty([0], np.float64)
-
-        # there shouldn't be any unhandled keyword arguments by this point
-        if kwargs:
-            raise TypeError("__init__() got an unexpected keyword argument '{}'".format(list(kwargs.items())[0]))
-
-    component_attrs = []  # redefined by subclasses
-    property components:
-        """Generator that returns all of the Components attached to the Node
-
-        This is used by Model.find_orphaned_parameters and isn't performance
-        critical.
-        """
-        def __get__(self):
-            for attr in self.component_attrs:
-                try:
-                    component = getattr(self, attr)
-                except AttributeError:
-                    pass
-                else:
-                    if isinstance(component, Component):
-                        yield component
-
-    property allow_isolated:
-        """ A property to flag whether this Node can be unconnected in a network. """
-        def __get__(self):
-            return self._allow_isolated
-
-        def __set__(self, value):
-            self._allow_isolated = value
-
-    property name:
-        """ Name of the node. """
-        def __get__(self):
-            return self._name
-
-        def __set__(self, name):
-            # check for name collision
-            if name in self.model.nodes.keys():
-                raise ValueError('A node with the name "{}" already exists.'.format(name))
-            # apply new name
-            self._name = name
-
-    property fully_qualified_name:
-        def __get__(self):
-            if self._parent is not None:
-                return '{}.{}'.format(self._parent.fully_qualified_name, self.name)
-            return self.name
-
-    property recorders:
-        """ Returns a list of `pywr.recorders.Recorder` objects attached to this node.
-
-         See also
-         --------
-         pywr.recorders.Recorder
-         """
-        def __get__(self):
-            return self._recorders
-
-    property model:
-        """The recorder for the node, e.g. a NumpyArrayRecorder
-        """
-        def __get__(self):
-            return self._model
-
-        def __set__(self, value):
-            self._model = value
-
-    property domain:
-        def __get__(self):
-            if self._domain is None and self._parent is not None:
-                return self._parent._domain
-            return self._domain
-
-        def __set__(self, value):
-            if self._parent is not None:
-                import warnings
-                warnings.warn("Setting domain property of node with a parent.")
-            self._domain = value
-
-    property parent:
-        """The parent Node/Storage of this object.
-        """
-        def __get__(self):
-            return self._parent
-
-        def __set__(self, value):
-            self._parent = value
-
-    property prev_flow:
-        """Total flow via this node in the previous timestep
-        """
-        def __get__(self):
-            return np.array(self._prev_flow)
-
-    property flow:
-        """Total flow via this node in the current timestep
-        """
-        def __get__(self):
-            return np.array(self._flow)
-
-    def __repr__(self):
-        if self.name:
-            # e.g. <Node "oxford">
-            return '<{} "{}">'.format(self.__class__.__name__, self.name)
-        else:
-            return '<{} "{}">'.format(self.__class__.__name__, hex(id(self)))
-
-    cpdef setup(self, model):
-        """Called before the first run of the model"""
-        cdef int ncomb = len(model.scenarios.combinations)
-        self._flow = np.empty(ncomb, dtype=np.float64)
-        self._prev_flow = np.zeros(ncomb, dtype=np.float64)
-
-    cpdef reset(self):
-        """Called at the beginning of a run"""
-        cdef int i
-        for i in range(self._flow.shape[0]):
-            self._flow[i] = 0.0
-            self._prev_flow[i] = 0.0
-
-    cpdef before(self, Timestep ts):
-        """Called at the beginning of the timestep"""
-        cdef int i
-        for i in range(self._flow.shape[0]):
-            self._flow[i] = 0.0
-
-    cpdef commit(self, int scenario_index, double value):
-        """Called once for each route the node is a member of"""
-        self._flow[scenario_index] += value
-
-    cpdef commit_all(self, double[:] value):
-        """Called once for each route the node is a member of"""
-        cdef int i
-        for i in range(self._flow.shape[0]):
-            self._flow[i] += value[i]
-
-    cpdef after(self, Timestep ts):
-        self._prev_flow[:] = self._flow[:]
-
-    cpdef finish(self):
-        pass
-
-    cpdef check(self,):
-        pass
-
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
-        return 0.0
-
-cdef class Node(AbstractNode):
-    """ Node class from which all others inherit
-    """
-    def __cinit__(self):
-        """Initialise the node attributes
-        """
-        # Initialised attributes to zero
-        self._min_flow = 0.0
-        self._max_flow = inf
-        self._cost = 0.0
-        # Conversion is default to unity so that there is no loss
-        self._conversion_factor = 1.0
-        # Parameters are initialised to None which corresponds to
-        # a static value
-        self._min_flow_param = None
-        self._max_flow_param = None
-        self._cost_param = None
-        self._conversion_factor_param = None
-        self._domain = None
-
-    component_attrs = ["min_flow", "max_flow", "cost", "conversion_factor"]
-
-    property cost:
-        """The cost per unit flow via the node
-
-        The cost may be set to either a constant (i.e. a float) or a Parameter.
-
-        The value returned can be positive (i.e. a cost), negative (i.e. a
-        benefit) or netural. Typically supply nodes will have an associated
-        cost and demands will provide a benefit.
-        """
-        def __get__(self):
-            if self._cost_param is None:
-                return self._cost
-            return self._cost_param
-
-        def __set__(self, value):
-            if isinstance(value, Parameter):
-                self._cost_param = value
-            else:
-                self._cost_param = None
-                self._cost = value
-
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
-        """Get the cost per unit flow at a given timestep
-        """
-        if self._cost_param is None:
-            return self._cost
-        return self._cost_param.get_value(scenario_index)
-
-    property min_flow:
-        """The minimum flow constraint on the node
-
-        The minimum flow may be set to either a constant (i.e. a float) or a
-        Parameter.
-        """
-        def __get__(self):
-            if self._min_flow_param is None:
-                return self._min_flow
-            return self._min_flow_param
-
-        def __set__(self, value):
-            if value is None:
-                self._min_flow = 0
-                self._min_flow_param = None
-            elif isinstance(value, Parameter):
-                self._min_flow_param = value
-            else:
-                self._min_flow_param = None
-                self._min_flow = value
-
-    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1:
-        """Get the minimum flow at a given timestep
-        """
-        if self._min_flow_param is None:
-            return self._min_flow
-        return self._min_flow_param.get_value(scenario_index)
-
-    property max_flow:
-        """The maximum flow constraint on the node
-
-        The maximum flow may be set to either a constant (i.e. a float) or a
-        Parameter.
-        """
-        def __get__(self):
-            if self._max_flow_param is None:
-                return self._max_flow
-            return self._max_flow_param
-
-        def __set__(self, value):
-            if value is None:
-                self._max_flow = inf
-                self._max_flow_param = None
-            elif isinstance(value, Parameter):
-                self._max_flow_param = value
-            else:
-                self._max_flow_param = None
-                self._max_flow = value
-
-    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1:
-        """Get the maximum flow at a given timestep
-        """
-        if self._max_flow_param is None:
-            return self._max_flow
-        return self._max_flow_param.get_value(scenario_index)
-
-    property conversion_factor:
-        """The conversion between inflow and outflow for the node
-
-        The conversion factor may be set to either a constant (i.e. a float) or
-        a Parameter.
-        """
-        def __set__(self, value):
-            self._conversion_factor_param = None
-            if isinstance(value, Parameter):
-                raise ValueError("Conversion factor can not be a Parameter.")
-            else:
-                self._conversion_factor = value
-
-    cpdef double get_conversion_factor(self) except? -1:
-        """Get the conversion factor
-
-        Note: the conversion factor must be a constant.
-        """
-        return self._conversion_factor
-
-    cdef set_parameters(self, ScenarioIndex scenario_index):
-        """Update the constant attributes by evaluating any Parameter objects
-
-        This is useful when the `get_` functions need to be accessed multiple
-        times and there is a benefit to caching the values.
-        """
-        if self._min_flow_param is not None:
-            self._min_flow = self._min_flow_param.get_value(scenario_index)
-        if self._max_flow_param is not None:
-            self._max_flow = self._max_flow_param.get_value(scenario_index)
-        if self._cost_param is not None:
-            self._cost = self._cost_param.get_value(scenario_index)
-
-
-cdef class BaseLink(Node):
-    pass
-
-
-cdef class BaseInput(Node):
-    pass
-
-
-cdef class BaseOutput(Node):
-    pass
-
-
-cdef class AggregatedNode(AbstractNode):
-    """ Base class for a special type of node that is the aggregated sum of `Node` objects.
-
-    This class is intended to be used isolated from the network.
-    """
-    def __cinit__(self, ):
-        self._allow_isolated = True
-        self.virtual = True
-        self._factors = None
-        self._flow_weights = None
-        self._min_flow = 0.0
-        self._max_flow = inf
-        self._min_flow_param = None
-        self._max_flow_param = None
-
-    component_attrs = ["min_flow", "max_flow"]
-
-    property nodes:
-        def __get__(self):
-            return self._nodes
-
-        def __set__(self, value):
-            self._nodes = list(value)
-            self.model.dirty = True
-
-    cpdef after(self, Timestep ts):
-        AbstractNode.after(self, ts)
-        cdef int i, j
-        cdef Node n
-
-        cdef double[:] weights
-        if self._flow_weights is not None:
-            weights = self._flow_weights
-        else:
-            weights = np.ones(len(self._nodes))
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-            self._flow[i] = 0.0
-            for j, n in enumerate(self._nodes):
-                self._flow[i] += n._flow[i]*weights[j]
-
-    property factors:
-        def __get__(self):
-            if self._factors is None:
-                return None
-            else:
-                return np.asarray(self._factors, np.float64)
-
-        def __set__(self, values):
-            values = np.array(values, np.float64)
-            if np.any(values < 1e-6):
-                warnings.warn("Very small factors in AggregateNode result in ill-conditioned matrix")
-            self._factors = values
-            self.model.dirty = True
-
-    property flow_weights:
-        def __get__(self):
-            if self._flow_weights is None:
-                return None
-            else:
-                return np.asarray(self._flow_weights, np.float64)
-
-        def __set__(self, values):
-            values = np.array(values, np.float64)
-            if np.any(values < 1e-6):
-                warnings.warn("Very small flow_weights in AggregateNode result in ill-conditioned matrix")
-            self._flow_weights = values
-            self.model.dirty = True
-
-    property min_flow:
-        """The minimum flow constraint on the node
-
-        The minimum flow may be set to either a constant (i.e. a float) or a
-        Parameter.
-        """
-        def __get__(self):
-            if self._min_flow_param is None:
-                return self._min_flow
-            return self._min_flow_param
-
-        def __set__(self, value):
-            if value is None:
-                self._min_flow = -inf
-                self._min_flow_param = None
-            elif isinstance(value, Parameter):
-                self._min_flow_param = value
-            else:
-                self._min_flow_param = None
-                self._min_flow = value
-
-    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1:
-        """Get the minimum flow at a given timestep
-        """
-        if self._min_flow_param is None:
-            return self._min_flow
-        return self._min_flow_param.get_value(scenario_index)
-
-    property max_flow:
-        """The maximum flow constraint on the node
-
-        The maximum flow may be set to either a constant (i.e. a float) or a
-        Parameter.
-        """
-        def __get__(self):
-            if self._max_flow_param is None:
-                return self._max_flow
-            return self._max_flow_param
-
-        def __set__(self, value):
-            if value is None:
-                self._max_flow = inf
-                self._max_flow_param = None
-            elif isinstance(value, Parameter):
-                self._max_flow_param = value
-            else:
-                self._max_flow_param = None
-                self._max_flow = value
-
-    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1:
-        """Get the maximum flow at a given timestep
-        """
-        if self._max_flow_param is None:
-            return self._max_flow
-        return self._max_flow_param.get_value(scenario_index)
-
-    @classmethod
-    def load(cls, data, model):
-        from pywr.parameters import load_parameter
-        name = data["name"]
-        nodes = [model._get_node_from_ref(model, node_name) for node_name in data["nodes"]]
-        agg = cls(model, name, nodes)
-        try:
-            agg.factors = data["factors"]
-        except KeyError:
-            pass
-        try:
-            agg.flow_weights = data["flow_weights"]
-        except KeyError:
-            pass
-        try:
-            agg.min_flow = load_parameter(model, data["min_flow"])
-        except KeyError:
-            pass
-        try:
-            agg.max_flow = load_parameter(model, data["max_flow"])
-        except KeyError:
-            pass
-        return agg
-
-cdef class StorageInput(BaseInput):
-    cpdef commit(self, int scenario_index, double volume):
-        BaseInput.commit(self, scenario_index, volume)
-        self._parent.commit(scenario_index, -volume)
-
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
-        # Return negative of parent cost
-        return -self.parent.get_cost(scenario_index)
-
-cdef class StorageOutput(BaseOutput):
-    cpdef commit(self, int scenario_index, double volume):
-        BaseOutput.commit(self, scenario_index, volume)
-        self._parent.commit(scenario_index, volume)
-
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
-        # Return parent cost
-        return self.parent.get_cost(scenario_index)
-
-
-cdef class AbstractStorage(AbstractNode):
-    """ Base class for all Storage objects.
-
-    Notes
-    -----
-    Do not initialise this class directly. Use `pywr.core.Storage`.
-    """
-    property volume:
-        def __get__(self, ):
-            return np.asarray(self._volume)
-
-    property current_pc:
-        """ Current percentage full """
-        def __get__(self, ):
-            return np.asarray(self._current_pc)
-
-    cpdef setup(self, model):
-        """ Called before the first run of the model"""
-        AbstractNode.setup(self, model)
-        cdef int ncomb = len(model.scenarios.combinations)
-        self._volume = np.zeros(ncomb, dtype=np.float64)
-        self._current_pc = np.zeros(ncomb, dtype=np.float64)
-
-
-cdef class Storage(AbstractStorage):
-    def __cinit__(self, ):
-        self.initial_volume = None
-        self.initial_volume_pc = None
-        self._min_volume = 0.0
-        self._max_volume = 0.0
-        self._cost = 0.0
-
-        self._min_volume_param = None
-        self._max_volume_param = None
-        self._level_param = None
-        self._area_param = None
-        self._cost_param = None
-        self._domain = None
-        self._allow_isolated = True
-
-    component_attrs = ["cost", "min_volume", "max_volume", "level", "area"]
-
-    property cost:
-        """The cost per unit increased in volume stored
-
-        The cost may be set to either a constant (i.e. a float) or a Parameter.
-
-        The value returned can be positive (i.e. a cost), negative (i.e. a
-        benefit) or netural. Typically supply nodes will have an associated
-        cost and demands will provide a benefit.
-        """
-        def __get__(self):
-            if self._cost_param is None:
-                return self._cost
-            return self._cost_param
-
-        def __set__(self, value):
-            if isinstance(value, Parameter):
-                self._cost_param = value
-            else:
-                self._cost_param = None
-                self._cost = value
-
-    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
-        """Get the cost per unit flow at a given timestep
-        """
-        if self._cost_param is None:
-            return self._cost
-        return self._cost_param.get_value(scenario_index)
-
-    property initial_volume:
-        def __get__(self, ):
-            return self._initial_volume
-
-        def __set__(self, value):
-            if value is None:
-                self._initial_volume = np.nan
-            else:
-                self._initial_volume = value
-
-    property initial_volume_pc:
-        def __get__(self, ):
-            return self._initial_volume_pc
-
-        def __set__(self, value):
-            if value is None:
-                self._initial_volume_pc = np.nan
-            else:
-                self._initial_volume_pc = value
-
-    property min_volume:
-        def __get__(self):
-            if self._min_volume_param is None:
-                return self._min_volume
-            return self._min_volume_param
-
-        def __set__(self, value):
-            self._min_volume_param = None
-            if isinstance(value, Parameter):
-                self._min_volume_param = value
-            else:
-                self._min_volume = value
-
-    cpdef double get_min_volume(self, ScenarioIndex scenario_index) except? -1:
-        if self._min_volume_param is None:
-            return self._min_volume
-        return self._min_volume_param.get_value(scenario_index)
-
-    property max_volume:
-        def __get__(self):
-            if self._max_volume_param is None:
-                return self._max_volume
-            return self._max_volume_param
-
-        def __set__(self, value):
-            self._max_volume_param = None
-            if isinstance(value, Parameter):
-                self._max_volume_param = value
-            else:
-                self._max_volume = value
-
-    cpdef double get_max_volume(self, ScenarioIndex scenario_index) except? -1:
-        if self._max_volume_param is None:
-            return self._max_volume
-        return self._max_volume_param.get_value(scenario_index)
-
-    property level:
-        def __get__(self):
-            if self._level_param is None:
-                return self._level
-            return self._level_param
-
-        def __set__(self, value):
-            self._level_param = None
-            if value is None:
-                self._level_param = None
-                self._level = 0.0
-            elif isinstance(value, Parameter):
-                self._level_param = value
-            else:
-                self._level = value
-
-    cpdef double get_level(self, ScenarioIndex scenario_index) except? -1:
-        if self._level_param is None:
-            return self._level
-        return self._level_param.get_value(scenario_index)
-
-    property area:
-        def __get__(self):
-            if self._area_param is None:
-                return self._area
-            return self._area_param
-
-        def __set__(self, value):
-            self._area_param = None
-            if value is None:
-                self._area_param = None
-                self._area = 0.0
-            elif isinstance(value, Parameter):
-                self._area_param = value
-            else:
-                self._area = value
-
-    cpdef double get_area(self, ScenarioIndex scenario_index) except? -1:
-        if self._area_param is None:
-            return self._area
-        return self._area_param.get_value(scenario_index)
-
-    property domain:
-        def __get__(self):
-            return self._domain
-
-        def __set__(self, value):
-            self._domain = value
-
-    cpdef reset(self):
-        """Called at the beginning of a run"""
-        AbstractStorage.reset(self)
-        self._reset_storage_only()
-
-    cpdef _reset_storage_only(self, bint use_initial_volume=True):
-        """Reset the current volume of the storage node.
-
-        Parameters
-        ==========
-        use_initial_volume : bool (default: True)
-            Reset the volume to the initial volume of the storage node. If false the volume is reset to max_volume.
-        """
-        cdef int i
-        cdef double mxv = self._max_volume
-        cdef double reset_volume, reset_pc
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-
-            if self._max_volume_param is not None:
-                # Ensure variable maximum volume is taken in to account
-                if use_initial_volume:
-                    # Max volume is a parameter; require both initial_volume and initial_volume_pc be given.
-                    # The parameter will not be evaluated at the beginning of the model run.
-                    if not np.isfinite(self._initial_volume_pc) or not np.isfinite(self._initial_volume):
-                        raise RuntimeError('Both `initial_volume` and `initial_volume_pc` must be supplied if'
-                                           '`max_volume` is defined as a parameter.')
-
-                    reset_volume = self._initial_volume
-                    reset_pc = self._initial_volume_pc
-                elif self.model.timestep.index > 0:
-                    # If it's not the first time-step, and we want to reset to max capacity then the
-                    # parameter should already have been evaluated.
-                    mxv = self._max_volume_param.get_value(si)
-                    # Reset to max capacity, accounting for variable volume
-                    reset_volume = mxv
-                    # We do this division (rather than setting reset_pc = 1.0) to be consistent with the case below
-                    # when mxv is zero.
-                    try:
-                        reset_pc = reset_volume / mxv
-                    except ZeroDivisionError:
-                        reset_pc = np.nan
-            else:
-                # Max volume is a double.
-                if use_initial_volume:
-                    # User only has to supply absolute or relative initial volume
-                    if np.isfinite(self._initial_volume_pc):
-                        reset_volume = self._initial_volume_pc * mxv
-                    elif np.isfinite(self._initial_volume):
-                        reset_volume = self._initial_volume
-                    else:
-                        raise RuntimeError('Initial volume must be set as either a percentage or absolute volume.')
-                else:
-                    reset_volume = mxv
-
-                # Compute the proportional volume.
-                try:
-                    reset_pc = reset_volume / mxv
-                except ZeroDivisionError:
-                    reset_pc = np.nan
-
-            self._volume[i] = reset_volume
-            self._current_pc[i] = reset_pc
-
-    cpdef after(self, Timestep ts):
-        AbstractStorage.after(self, ts)
-        cdef int i
-        cdef double mxv, mnv
-        cdef ScenarioIndex si
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-            self._volume[i] += self._flow[i]*ts.days
-            # Ensure variable maximum volume is taken in to account
-
-            mxv = self.get_max_volume(si)
-            mnv = self.get_min_volume(si)
-
-            if abs(self._volume[i] - mxv) < 1e-6:
-                self._volume[i] = mxv
-            if abs(self._volume[i] - mnv) < 1e-6:
-                self._volume[i] = mnv
-
-            try:
-                self._current_pc[i] = self._volume[i] / mxv
-            except ZeroDivisionError:
-                self._current_pc[i] = np.nan
-
-cdef class AggregatedStorage(AbstractStorage):
-    """ Base class for a special type of storage node that is the aggregated sum of `Storage` objects.
-
-    This class is intended to be used isolated from the network.
-    """
-    def __cinit__(self, ):
-        self._allow_isolated = True
-        self.virtual = True
-
-    property storage_nodes:
-        def __get__(self):
-            return self._storage_nodes
-
-        def __set__(self, value):
-            self._storage_nodes = list(value)
-
-    property initial_volume:
-        def __get__(self, ):
-            cdef Storage s
-            return np.sum([s._initial_volume for s in self._storage_nodes])
-
-    cpdef reset(self):
-        cdef int i
-        cdef double mxv = 0.0
-        cdef ScenarioIndex si
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-            mxv = 0.0
-            for s in self._storage_nodes:
-                mxv += s.get_max_volume(si)
-
-            self._volume[i] = self.initial_volume
-            # Ensure variable maximum volume is taken in to account
-            try:
-                self._current_pc[i] = self._volume[i] / mxv
-            except ZeroDivisionError:
-                self._current_pc[i] = np.nan
-
-    cpdef after(self, Timestep ts):
-        AbstractStorage.after(self, ts)
-        cdef int i
-        cdef Storage s
-        cdef double mxv
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-            self._flow[i] = 0.0
-            mxv = 0.0
-            for s in self._storage_nodes:
-                self._flow[i] += s._flow[i]
-                mxv += s.get_max_volume(si)
-            self._volume[i] += self._flow[i]*ts.days
-
-            # Ensure variable maximum volume is taken in to account
-            try:
-                self._current_pc[i] = self._volume[i] / mxv
-            except ZeroDivisionError:
-                self._current_pc[i] = np.nan
-
-    @classmethod
-    def load(cls, data, model):
-        name = data["name"]
-        nodes = [model._get_node_from_ref(model, node_name) for node_name in data["storage_nodes"]]
-        agg = cls(model, name, nodes)
-        return agg
-
-
-cdef class VirtualStorage(Storage):
-    def __cinit__(self, ):
-        self._allow_isolated = True
-        self.virtual = True
-
-    property nodes:
-        def __get__(self):
-            return self._nodes
-
-        def __set__(self, value):
-            self._nodes = list(value)
-            self.model.dirty = True
-
-    property factors:
-        def __get__(self):
-            return np.array(self._factors)
-
-        def __set__(self, value):
-            self._factors = np.array(value, dtype=np.float64)
-
-    cpdef after(self, Timestep ts):
-        cdef int i
-        cdef ScenarioIndex si
-        cdef AbstractNode n
-
-        for i, si in enumerate(self.model.scenarios.combinations):
-            self._flow[i] = 0.0
-            for n, f in zip(self._nodes, self._factors):
-                self._flow[i] -= f*n._flow[i]
-        Storage.after(self, ts)
+from pywr._core cimport *
+from pywr._component cimport Component
+import itertools
+import numpy as np
+cimport numpy as np
+import pandas as pd
+import warnings
+
+cdef double inf = float('inf')
+
+cdef class Scenario:
+    """Represents a scenario in the model.
+
+    Typically a scenario will be used to run many similar models simultaneously. A small
+    number of `Parameter` objects in the model will return different values depending
+    on the scenario, but many will not. Multiple scenarios can be defined such that
+    some `Parameter` values vary with one scenario, but not another. Scenarios are defined
+    with a size that represents the number of ensembles in that scenario.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+        The model instance to attach the scenario to.
+    name : str
+        The name of the scenario.
+    size : int, optional
+        The number of ensembles in the scenario. The default value is 1.
+    slice : slice, optional
+        If given this defines the subset of the ensembles that are actually run
+         in the model. This is useful if a large number of ensembles is defined, but
+         certain analysis (e.g. optimisation) can only be done on a small subset.
+    ensemble_names : iterable of str, optional
+        User defined names describing each ensemble.
+
+    See Also
+    --------
+    ScenarioCollection
+    ScenarioIndex
+    """
+    def __init__(self, model, name, int size=1, slice slice=None, ensemble_names=None):
+        self._name = name
+        if size < 1:
+            raise ValueError("Size must be greater than or equal to 1.")
+        self._size = size
+        self.slice = slice
+        self.ensemble_names = ensemble_names
+        # Do this last so only set on the model if no error is raised.
+        model.scenarios.add_scenario(self)
+
+    property size:
+        def __get__(self, ):
+            return self._size
+
+    property name:
+        def __get__(self):
+            return self._name
+
+    property ensemble_names:
+        def __get__(self):
+            if self._ensemble_names is None:
+                return list(range(self._size))
+            return self._ensemble_names
+
+        def __set__(self, names):
+            if names is None:
+                self._ensemble_names = None
+                return
+            if len(names) != self._size:
+                raise ValueError("The length of ensemble_names ({}) must be equal to the size of the scenario ({})".format(len(names), self.size))  # noqa
+            self._ensemble_names = names
+
+cdef class ScenarioCollection:
+    """ Represents a collection of `Scenario` objects.
+
+    This class is used by a `Model` instance to hold the defined scenarios and control
+    which combinations of ensembles are used during model execution. By default the
+    product of all scenario ensembles (i.e. all possible combinations of ensembles) is
+    executed. However user defined slices can be set on individual `Scenario` instances
+    to restrict the number of ensembles executed from that scenario. Alternatively
+    the user may provide an array of the specific ensemble combinations (indices) that
+    should be run. The latter approach takes precedent over the former per `Scenario`
+    slices.
+
+    See Also
+    --------
+    Scenario
+    ScenarioIndex
+    """
+    def __init__(self, model):
+        self.model = model
+        self._scenarios = []
+        self.combinations = None
+        self.user_combinations = None
+
+    property scenarios:
+        def __get__(self):
+            return self._scenarios
+
+    def __getitem__(self, name):
+        cdef Scenario sc
+        for sc in self._scenarios:
+            if sc._name == name:
+                return sc
+        raise KeyError("Scenario with name '{}' not found.".format(name))
+
+    def get_combinations(self):
+        """Returns a list of ScenarioIndices for every combination of Scenarios
+        """
+        cdef Scenario scenario
+        cdef int i
+        if len(self._scenarios) == 0:
+            # model has no scenarios defined, implicitly has 1 scenario of size 1
+            combinations = [ScenarioIndex(0, np.array([0], dtype=np.int32))]
+        elif self._user_combinations is not None:
+            # use combinations given by user
+            combinations = list([ScenarioIndex(i, self._user_combinations[i, :])
+                                 for i in range(self._user_combinations.shape[0])])
+        else:
+            # product of all scenarios, taking into account Scenario.slice
+            iter = itertools.product(*[range(scenario._size)[scenario.slice]
+                                     if scenario.slice else range(scenario._size)
+                                     for scenario in self._scenarios])
+            combinations = list([ScenarioIndex(i, np.array(x, dtype=np.int32)) for i, x in enumerate(iter)])
+        if not combinations:
+            raise ValueError("No scenarios were selected to be run")
+        return combinations
+
+    def setup(self):
+        self.combinations = self.get_combinations()
+
+    property user_combinations:
+        def __get__(self, ):
+            return self._user_combinations
+
+        def __set__(self, values):
+            if values is None:
+                self._user_combinations = None
+                return
+            cdef Scenario sc
+            values = np.asarray(values, dtype=np.int32)
+            if values.ndim != 2:
+                raise ValueError('A 2-dimensional array of scenario indices must be provided.')
+            if values.shape[1] != len(self._scenarios):
+                raise ValueError('User defined combinations must have shape (N, S) where S in number of Scenarios')
+            # Check maximum values
+            for sc, v in zip(self._scenarios, values.max(axis=0)):
+                if v >= sc._size:
+                    raise ValueError('Given ensemble index for scenario "{}" out of range.'.format(sc.name))
+            if np.any(values.min(axis=0) < 0):
+                raise ValueError('Ensemble index less than zero is invalid.')
+
+            self._user_combinations = values
+
+    cpdef int get_scenario_index(self, Scenario sc) except? -1:
+        """Return the index of Scenario in this controller."""
+        return self._scenarios.index(sc)
+
+    cpdef add_scenario(self, Scenario sc):
+        if sc in self._scenarios:
+            raise ValueError("The same scenario can not be added twice.")
+        self.model.dirty = True
+        self._scenarios.append(sc)
+
+    property combination_names:
+        def __get__(self):
+            cdef ScenarioIndex si
+            cdef Scenario sc
+            cdef int i
+            cdef list names
+            for si in self.combinations:
+                names = []
+                for i, sc in enumerate(self._scenarios):
+                    names.append('{}.{:03d}'.format(sc._name, si._indices[i]))
+                yield '-'.join(names)
+
+    def __len__(self):
+        return len(self._scenarios)
+
+    property shape:
+        def __get__(self):
+            if self._user_combinations is not None:
+                # raise ValueError("ScenarioCollection.shape is undefined if user_combinations is defined.")
+                return (len(self._user_combinations), )
+            if len(self._scenarios) == 0:
+                return (1, )
+            return tuple(len(range(sc.size)[sc.slice]) if sc.slice is not None else sc.size for sc in self._scenarios)
+
+    property multiindex:
+        def __get__(self):
+            cdef Scenario sc
+            if len(self._scenarios) == 0:
+                return pd.MultiIndex.from_product([range(1)], names=[''])
+            else:
+                ensemble_names = [scenario.ensemble_names for scenario in self._scenarios]
+                indices = [[ensemble_names[n][i] for n, i in enumerate(scenario_index.indices)]
+                           for scenario_index in self.model.scenarios.get_combinations()]
+                names = [sc._name for sc in self._scenarios]
+                return pd.MultiIndex.from_tuples(indices, names=names)
+
+    cpdef int ravel_indices(self, int[:] scenario_indices) except? -1:
+        if scenario_indices is None:
+            return 0
+        # Case where scenario_indices is empty for no scenarios defined
+        if scenario_indices.size == 0:
+            return 0
+        return np.ravel_multi_index(scenario_indices, np.array(self.shape))
+
+cdef class ScenarioIndex:
+    """A ScenarioIndex is an indexer for a given combination of scenarios.
+
+    The ScenarioIndex is used to represent a given combination of scenarios
+    during a simulation. It is given to Parameter instances to allow them
+    to alter their calculations based on each scenario. Instances of this
+    class are generated by Pywr, and users are unlikely to need to
+    instantiate this class.
+
+    Attributes
+    ----------
+    global_id : int
+        Read-only global scenario index that this ScenarioIndex refers to.
+    indices : np.array
+        The indices for each of the Scenarios in this model.
+
+    See Also
+    --------
+    Scenario
+    ScenarioCollection
+    """
+    def __init__(self, int global_id, int[:] indices):
+        self.global_id = global_id
+        self._indices = indices
+
+    property indices:
+        def __get__(self):
+            return np.array(self._indices)
+
+    def __repr__(self):
+        return "<ScenarioIndex gid={:d} indices={}>".format(self.global_id, tuple(np.asarray(self._indices)))
+
+
+cdef bint is_leap_year(int year):
+    # http://stackoverflow.com/a/11595914/1300519
+    return ((year & 3) == 0 and ((year % 25) != 0 or (year & 15) == 0))
+
+
+cdef class Timestep:
+    """A representation of a simulation time-step.
+
+    Attributes
+    ----------
+    period : pandas.Period
+        Read-only global scenario index that this ScenarioIndex refers to.
+    index : int
+        The time-step index of the simulation.
+    days : int
+        The time-step length in days.
+    dayofyear : int
+        The day of the year (from `pandas.Period.dayofyear`).
+    dayofyear_index : int
+        A zero-based index for the day of the year. This begins at 0 and ends at 366. The 29th of February
+        is skipped if it is not a leap year.
+    day : int
+        The day of the month.
+    week_index : int
+        A zero-based index for the week of the year. The last week of the year (index 51) is always slightly
+        longer than 7 days.
+    month : int
+        The month number (1 - 12).
+    year : int
+        The year.
+    is_leap_year : bool
+        True if the current year is a leap year; false otherwise.
+    """
+    def __init__(self, period, int index, double days):
+        self.period = period
+        self.index = index
+        if days <= 0:
+            raise ValueError("The days argument must be > 0.")
+        self.days = days
+        self.dayofyear = period.dayofyear
+        self.day = period.day
+        self.month = period.month
+        self.year = period.year
+        self.is_leap_year = is_leap_year(self.year)
+
+        # Calculate day of year index (zero based)
+        cdef int i = self.dayofyear - 1
+        if not self.is_leap_year:
+            if i > 58: # 28th Feb
+                i += 1
+        self.dayofyear_index = i
+
+        # Calculate week of year
+        if self.dayofyear_index >= 364:
+            # last week of year is slightly longer than 7 days
+            self.week_index = 51
+        else:
+            self.week_index = self.dayofyear_index // 7
+
+    property datetime:
+        """Timestep representation as a `datetime.datetime` object"""
+        def __get__(self, ):
+            return self.period.to_timestamp()
+
+    def __repr__(self):
+        return "<Timestep date=\"{}\">".format(self.period.strftime("%Y-%m-%d"))
+
+cdef class Domain:
+    """ Domain class which all Node objects must have. """
+    def __init__(self, name):
+        self.name = name
+
+cdef class AbstractNode:
+    """ Base class for all nodes in Pywr.
+
+    This class is not intended to be used directly.
+    """
+    def __cinit__(self):
+        self._allow_isolated = False
+        self.virtual = False
+
+    def __init__(self, model, name, comment=None, **kwargs):
+        self._model = model
+        self.name = name
+        self.comment = comment
+
+        self._parent = kwargs.pop('parent', None)
+        self._domain = kwargs.pop('domain', None)
+        self._recorders = []
+
+        self._flow = np.empty([0], np.float64)
+
+        # there shouldn't be any unhandled keyword arguments by this point
+        if kwargs:
+            raise TypeError("__init__() got an unexpected keyword argument '{}'".format(list(kwargs.items())[0]))
+
+    component_attrs = []  # redefined by subclasses
+    property components:
+        """Generator that returns all of the Components attached to the Node
+
+        This is used by Model.find_orphaned_parameters and isn't performance
+        critical.
+        """
+        def __get__(self):
+            for attr in self.component_attrs:
+                try:
+                    component = getattr(self, attr)
+                except AttributeError:
+                    pass
+                else:
+                    if isinstance(component, Component):
+                        yield component
+
+    property allow_isolated:
+        """ A property to flag whether this Node can be unconnected in a network. """
+        def __get__(self):
+            return self._allow_isolated
+
+        def __set__(self, value):
+            self._allow_isolated = value
+
+    property name:
+        """ Name of the node. """
+        def __get__(self):
+            return self._name
+
+        def __set__(self, name):
+            # check for name collision
+            if name in self.model.nodes.keys():
+                raise ValueError('A node with the name "{}" already exists.'.format(name))
+            # apply new name
+            self._name = name
+
+    property fully_qualified_name:
+        def __get__(self):
+            if self._parent is not None:
+                return '{}.{}'.format(self._parent.fully_qualified_name, self.name)
+            return self.name
+
+    property recorders:
+        """ Returns a list of `pywr.recorders.Recorder` objects attached to this node.
+
+         See also
+         --------
+         pywr.recorders.Recorder
+         """
+        def __get__(self):
+            return self._recorders
+
+    property model:
+        """The recorder for the node, e.g. a NumpyArrayRecorder
+        """
+        def __get__(self):
+            return self._model
+
+        def __set__(self, value):
+            self._model = value
+
+    property domain:
+        def __get__(self):
+            if self._domain is None and self._parent is not None:
+                return self._parent._domain
+            return self._domain
+
+        def __set__(self, value):
+            if self._parent is not None:
+                import warnings
+                warnings.warn("Setting domain property of node with a parent.")
+            self._domain = value
+
+    property parent:
+        """The parent Node/Storage of this object.
+        """
+        def __get__(self):
+            return self._parent
+
+        def __set__(self, value):
+            self._parent = value
+
+    property prev_flow:
+        """Total flow via this node in the previous timestep
+        """
+        def __get__(self):
+            return np.array(self._prev_flow)
+
+    property flow:
+        """Total flow via this node in the current timestep
+        """
+        def __get__(self):
+            return np.array(self._flow)
+
+    def __repr__(self):
+        if self.name:
+            # e.g. <Node "oxford">
+            return '<{} "{}">'.format(self.__class__.__name__, self.name)
+        else:
+            return '<{} "{}">'.format(self.__class__.__name__, hex(id(self)))
+
+    cpdef setup(self, model):
+        """Called before the first run of the model"""
+        cdef int ncomb = len(model.scenarios.combinations)
+        self._flow = np.empty(ncomb, dtype=np.float64)
+        self._prev_flow = np.zeros(ncomb, dtype=np.float64)
+
+    cpdef reset(self):
+        """Called at the beginning of a run"""
+        cdef int i
+        for i in range(self._flow.shape[0]):
+            self._flow[i] = 0.0
+            self._prev_flow[i] = 0.0
+
+    cpdef before(self, Timestep ts):
+        """Called at the beginning of the timestep"""
+        cdef int i
+        for i in range(self._flow.shape[0]):
+            self._flow[i] = 0.0
+
+    cpdef commit(self, int scenario_index, double value):
+        """Called once for each route the node is a member of"""
+        self._flow[scenario_index] += value
+
+    cpdef commit_all(self, double[:] value):
+        """Called once for each route the node is a member of"""
+        cdef int i
+        for i in range(self._flow.shape[0]):
+            self._flow[i] += value[i]
+
+    cpdef after(self, Timestep ts):
+        self._prev_flow[:] = self._flow[:]
+
+    cpdef finish(self):
+        pass
+
+    cpdef check(self,):
+        pass
+
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
+        return 0.0
+
+cdef class Node(AbstractNode):
+    """ Node class from which all others inherit
+    """
+    def __cinit__(self):
+        """Initialise the node attributes
+        """
+        # Initialised attributes to zero
+        self._min_flow = 0.0
+        self._max_flow = inf
+        self._cost = 0.0
+        # Conversion is default to unity so that there is no loss
+        self._conversion_factor = 1.0
+        # Parameters are initialised to None which corresponds to
+        # a static value
+        self._min_flow_param = None
+        self._max_flow_param = None
+        self._cost_param = None
+        self._conversion_factor_param = None
+        self._domain = None
+
+    component_attrs = ["min_flow", "max_flow", "cost", "conversion_factor"]
+
+    property cost:
+        """The cost per unit flow via the node
+
+        The cost may be set to either a constant (i.e. a float) or a Parameter.
+
+        The value returned can be positive (i.e. a cost), negative (i.e. a
+        benefit) or netural. Typically supply nodes will have an associated
+        cost and demands will provide a benefit.
+        """
+        def __get__(self):
+            if self._cost_param is None:
+                return self._cost
+            return self._cost_param
+
+        def __set__(self, value):
+            if isinstance(value, Parameter):
+                self._cost_param = value
+            else:
+                self._cost_param = None
+                self._cost = value
+
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
+        """Get the cost per unit flow at a given timestep
+        """
+        if self._cost_param is None:
+            return self._cost
+        return self._cost_param.get_value(scenario_index)
+
+    property min_flow:
+        """The minimum flow constraint on the node
+
+        The minimum flow may be set to either a constant (i.e. a float) or a
+        Parameter.
+        """
+        def __get__(self):
+            if self._min_flow_param is None:
+                return self._min_flow
+            return self._min_flow_param
+
+        def __set__(self, value):
+            if value is None:
+                self._min_flow = 0
+                self._min_flow_param = None
+            elif isinstance(value, Parameter):
+                self._min_flow_param = value
+            else:
+                self._min_flow_param = None
+                self._min_flow = value
+
+    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1:
+        """Get the minimum flow at a given timestep
+        """
+        if self._min_flow_param is None:
+            return self._min_flow
+        return self._min_flow_param.get_value(scenario_index)
+
+    property max_flow:
+        """The maximum flow constraint on the node
+
+        The maximum flow may be set to either a constant (i.e. a float) or a
+        Parameter.
+        """
+        def __get__(self):
+            if self._max_flow_param is None:
+                return self._max_flow
+            return self._max_flow_param
+
+        def __set__(self, value):
+            if value is None:
+                self._max_flow = inf
+                self._max_flow_param = None
+            elif isinstance(value, Parameter):
+                self._max_flow_param = value
+            else:
+                self._max_flow_param = None
+                self._max_flow = value
+
+    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1:
+        """Get the maximum flow at a given timestep
+        """
+        if self._max_flow_param is None:
+            return self._max_flow
+        return self._max_flow_param.get_value(scenario_index)
+
+    property conversion_factor:
+        """The conversion between inflow and outflow for the node
+
+        The conversion factor may be set to either a constant (i.e. a float) or
+        a Parameter.
+        """
+        def __set__(self, value):
+            self._conversion_factor_param = None
+            if isinstance(value, Parameter):
+                raise ValueError("Conversion factor can not be a Parameter.")
+            else:
+                self._conversion_factor = value
+
+    cpdef double get_conversion_factor(self) except? -1:
+        """Get the conversion factor
+
+        Note: the conversion factor must be a constant.
+        """
+        return self._conversion_factor
+
+    cdef set_parameters(self, ScenarioIndex scenario_index):
+        """Update the constant attributes by evaluating any Parameter objects
+
+        This is useful when the `get_` functions need to be accessed multiple
+        times and there is a benefit to caching the values.
+        """
+        if self._min_flow_param is not None:
+            self._min_flow = self._min_flow_param.get_value(scenario_index)
+        if self._max_flow_param is not None:
+            self._max_flow = self._max_flow_param.get_value(scenario_index)
+        if self._cost_param is not None:
+            self._cost = self._cost_param.get_value(scenario_index)
+
+
+cdef class BaseLink(Node):
+    pass
+
+
+cdef class BaseInput(Node):
+    pass
+
+
+cdef class BaseOutput(Node):
+    pass
+
+
+cdef class AggregatedNode(AbstractNode):
+    """ Base class for a special type of node that is the aggregated sum of `Node` objects.
+
+    This class is intended to be used isolated from the network.
+    """
+    def __cinit__(self, ):
+        self._allow_isolated = True
+        self.virtual = True
+        self._factors = None
+        self._flow_weights = None
+        self._min_flow = 0.0
+        self._max_flow = inf
+        self._min_flow_param = None
+        self._max_flow_param = None
+
+    component_attrs = ["min_flow", "max_flow"]
+
+    property nodes:
+        def __get__(self):
+            return self._nodes
+
+        def __set__(self, value):
+            self._nodes = list(value)
+            self.model.dirty = True
+
+    cpdef after(self, Timestep ts):
+        AbstractNode.after(self, ts)
+        cdef int i, j
+        cdef Node n
+
+        cdef double[:] weights
+        if self._flow_weights is not None:
+            weights = self._flow_weights
+        else:
+            weights = np.ones(len(self._nodes))
+
+        for i, si in enumerate(self.model.scenarios.combinations):
+            self._flow[i] = 0.0
+            for j, n in enumerate(self._nodes):
+                self._flow[i] += n._flow[i]*weights[j]
+
+    property factors:
+        def __get__(self):
+            return self._factors
+
+        def __set__(self, values):
+            from pywr.parameters import ConstantParameter
+
+            # remove existing factors (if any)
+            if self._factors is not None:
+                for factor in self._factors:
+                    factor.parents.remove(self)
+
+            if values is None:
+                factors = None
+            else:
+                factors = []
+                for val in values:
+                    if isinstance(val, (int, float)):
+                        factors.append(ConstantParameter(self.model, val))
+                    else:
+                        factors.append(val)
+
+            self._factors = factors
+            self.model.dirty = True
+
+    property has_fixed_factors:
+        """Returns true if all factors are of type `ConstantParameter`"""
+        def __get__(self):
+            from pywr.parameters import ConstantParameter
+            return all([isinstance(p, ConstantParameter) for p in self.factors])
+
+    property flow_weights:
+        def __get__(self):
+            if self._flow_weights is None:
+                return None
+            else:
+                return np.asarray(self._flow_weights, np.float64)
+
+        def __set__(self, values):
+            values = np.array(values, np.float64)
+            if np.any(np.abs(values) < 1e-6):
+                warnings.warn("Very small flow_weights in AggregateNode result in ill-conditioned matrix")
+            self._flow_weights = values
+            self.model.dirty = True
+
+    property min_flow:
+        """The minimum flow constraint on the node
+
+        The minimum flow may be set to either a constant (i.e. a float) or a
+        Parameter.
+        """
+        def __get__(self):
+            if self._min_flow_param is None:
+                return self._min_flow
+            return self._min_flow_param
+
+        def __set__(self, value):
+            if value is None:
+                self._min_flow = -inf
+                self._min_flow_param = None
+            elif isinstance(value, Parameter):
+                self._min_flow_param = value
+            else:
+                self._min_flow_param = None
+                self._min_flow = value
+
+    cpdef double get_min_flow(self, ScenarioIndex scenario_index) except? -1:
+        """Get the minimum flow at a given timestep
+        """
+        if self._min_flow_param is None:
+            return self._min_flow
+        return self._min_flow_param.get_value(scenario_index)
+
+    property max_flow:
+        """The maximum flow constraint on the node
+
+        The maximum flow may be set to either a constant (i.e. a float) or a
+        Parameter.
+        """
+        def __get__(self):
+            if self._max_flow_param is None:
+                return self._max_flow
+            return self._max_flow_param
+
+        def __set__(self, value):
+            if value is None:
+                self._max_flow = inf
+                self._max_flow_param = None
+            elif isinstance(value, Parameter):
+                self._max_flow_param = value
+            else:
+                self._max_flow_param = None
+                self._max_flow = value
+
+    cpdef double get_max_flow(self, ScenarioIndex scenario_index) except? -1:
+        """Get the maximum flow at a given timestep
+        """
+        if self._max_flow_param is None:
+            return self._max_flow
+        return self._max_flow_param.get_value(scenario_index)
+
+    cpdef double[:] get_factors(self, ScenarioIndex scenario_index):
+        """Get node factors for the current timestep and given scenario index.
+        """
+        cdef Parameter p
+        return np.array([p.get_value(scenario_index) for p in self.factors], np.float64)
+
+    cpdef double[:] get_factors_norm(self, ScenarioIndex scenario_index):
+        """Get node factors normalised by the factor of the first node
+        """
+        cdef double f0, f
+        cdef int i
+        cdef double[:] factors_norm, factors
+
+        factors = self.get_factors(scenario_index)
+        f0 = factors[0]
+        factors_norm = np.empty(len(factors), np.float64)
+
+        for i in range(len(factors)):
+            factors_norm[i] = f0/factors[i]
+        return factors_norm
+
+    @classmethod
+    def load_factors(cls, model, data):
+        """ Class method to load factors data from dict. """
+        from pywr.parameters import load_parameter
+
+        factors = None
+        if 'factors' in data:
+            factors = []
+            for pdata in data.pop('factors'):
+                factors.append(load_parameter(model, pdata))
+        return factors
+
+    @classmethod
+    def load(cls, data, model):
+        from pywr.parameters import load_parameter
+        name = data["name"]
+        nodes = [model._get_node_from_ref(model, node_name) for node_name in data["nodes"]]
+        agg = cls(model, name, nodes)
+        agg.factors = cls.load_factors(model, data)
+
+        try:
+            agg.flow_weights = data["flow_weights"]
+        except KeyError:
+            pass
+        try:
+            agg.min_flow = load_parameter(model, data["min_flow"])
+        except KeyError:
+            pass
+        try:
+            agg.max_flow = load_parameter(model, data["max_flow"])
+        except KeyError:
+            pass
+        return agg
+
+cdef class StorageInput(BaseInput):
+    cpdef commit(self, int scenario_index, double volume):
+        BaseInput.commit(self, scenario_index, volume)
+        self._parent.commit(scenario_index, -volume)
+
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
+        # Return negative of parent cost
+        return -self.parent.get_cost(scenario_index)
+
+cdef class StorageOutput(BaseOutput):
+    cpdef commit(self, int scenario_index, double volume):
+        BaseOutput.commit(self, scenario_index, volume)
+        self._parent.commit(scenario_index, volume)
+
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
+        # Return parent cost
+        return self.parent.get_cost(scenario_index)
+
+
+cdef class AbstractStorage(AbstractNode):
+    """ Base class for all Storage objects.
+
+    Notes
+    -----
+    Do not initialise this class directly. Use `pywr.core.Storage`.
+    """
+    property volume:
+        def __get__(self, ):
+            return np.asarray(self._volume)
+
+    property current_pc:
+        """ Current percentage full """
+        def __get__(self, ):
+            return np.asarray(self._current_pc)
+
+    cpdef setup(self, model):
+        """ Called before the first run of the model"""
+        AbstractNode.setup(self, model)
+        cdef int ncomb = len(model.scenarios.combinations)
+        self._volume = np.zeros(ncomb, dtype=np.float64)
+        self._current_pc = np.zeros(ncomb, dtype=np.float64)
+
+
+cdef class Storage(AbstractStorage):
+    def __cinit__(self, ):
+        self.initial_volume = None
+        self.initial_volume_pc = None
+        self._min_volume = 0.0
+        self._max_volume = 0.0
+        self._cost = 0.0
+
+        self._min_volume_param = None
+        self._max_volume_param = None
+        self._level_param = None
+        self._area_param = None
+        self._cost_param = None
+        self._domain = None
+        self._allow_isolated = True
+
+    component_attrs = ["cost", "min_volume", "max_volume", "level", "area"]
+
+    property cost:
+        """The cost per unit increased in volume stored
+
+        The cost may be set to either a constant (i.e. a float) or a Parameter.
+
+        The value returned can be positive (i.e. a cost), negative (i.e. a
+        benefit) or netural. Typically supply nodes will have an associated
+        cost and demands will provide a benefit.
+        """
+        def __get__(self):
+            if self._cost_param is None:
+                return self._cost
+            return self._cost_param
+
+        def __set__(self, value):
+            if isinstance(value, Parameter):
+                self._cost_param = value
+            else:
+                self._cost_param = None
+                self._cost = value
+
+    cpdef double get_cost(self, ScenarioIndex scenario_index) except? -1:
+        """Get the cost per unit flow at a given timestep
+        """
+        if self._cost_param is None:
+            return self._cost
+        return self._cost_param.get_value(scenario_index)
+
+    property initial_volume:
+        def __get__(self, ):
+            return self._initial_volume
+
+        def __set__(self, value):
+            if value is None:
+                self._initial_volume = np.nan
+            else:
+                self._initial_volume = value
+
+    property initial_volume_pc:
+        def __get__(self, ):
+            return self._initial_volume_pc
+
+        def __set__(self, value):
+            if value is None:
+                self._initial_volume_pc = np.nan
+            else:
+                self._initial_volume_pc = value
+
+    cpdef double get_initial_volume(self) except? -1:
+        """Returns the absolute initial volume. """
+        cdef double mxv = self._max_volume
+
+        if self._max_volume_param is not None:
+            # Max volume is a parameter; require both initial_volume and initial_volume_pc be given.
+            # The parameter will not be evaluated at the beginning of the model run.
+            if not np.isfinite(self._initial_volume_pc) or not np.isfinite(self._initial_volume):
+                raise RuntimeError('Both `initial_volume` and `initial_volume_pc` must be supplied if'
+                                   '`max_volume` is defined as a parameter.')
+
+            initial_volume = self._initial_volume
+        else:
+            # User only has to supply absolute or relative initial volume
+            if np.isfinite(self._initial_volume_pc):
+                initial_volume = self._initial_volume_pc * mxv
+            elif np.isfinite(self._initial_volume):
+                initial_volume = self._initial_volume
+            else:
+                raise RuntimeError('Initial volume must be set as either a percentage or absolute volume.')
+        return initial_volume
+
+    cpdef double get_initial_pc(self) except? -1:
+        """Returns the initial volume as a proportion. """
+        cdef double mxv = self._max_volume
+
+        if self._max_volume_param is not None:
+            # Max volume is a parameter; require both initial_volume and initial_volume_pc be given.
+            # The parameter will not be evaluated at the beginning of the model run.
+            if not np.isfinite(self._initial_volume_pc) or not np.isfinite(self._initial_volume):
+                raise RuntimeError('Both `initial_volume` and `initial_volume_pc` must be supplied if'
+                                   '`max_volume` is defined as a parameter.')
+            initial_pc = self._initial_volume_pc
+        else:
+            # User only has to supply absolute or relative initial volume
+            if np.isfinite(self._initial_volume_pc):
+                initial_pc = self._initial_volume_pc
+            elif np.isfinite(self._initial_volume):
+                try:
+                    initial_pc = self._initial_volume / mxv
+                except ZeroDivisionError:
+                    initial_pc = np.nan
+            else:
+                raise RuntimeError('Initial volume must be set as either a percentage or absolute volume.')
+        return initial_pc
+
+    property min_volume:
+        def __get__(self):
+            if self._min_volume_param is None:
+                return self._min_volume
+            return self._min_volume_param
+
+        def __set__(self, value):
+            self._min_volume_param = None
+            if isinstance(value, Parameter):
+                self._min_volume_param = value
+            else:
+                self._min_volume = value
+
+    cpdef double get_min_volume(self, ScenarioIndex scenario_index) except? -1:
+        if self._min_volume_param is None:
+            return self._min_volume
+        return self._min_volume_param.get_value(scenario_index)
+
+    property max_volume:
+        def __get__(self):
+            if self._max_volume_param is None:
+                return self._max_volume
+            return self._max_volume_param
+
+        def __set__(self, value):
+            self._max_volume_param = None
+            if isinstance(value, Parameter):
+                self._max_volume_param = value
+            else:
+                self._max_volume = value
+
+    cpdef double get_max_volume(self, ScenarioIndex scenario_index) except? -1:
+        if self._max_volume_param is None:
+            return self._max_volume
+        return self._max_volume_param.get_value(scenario_index)
+
+    property level:
+        def __get__(self):
+            if self._level_param is None:
+                return self._level
+            return self._level_param
+
+        def __set__(self, value):
+            self._level_param = None
+            if value is None:
+                self._level_param = None
+                self._level = 0.0
+            elif isinstance(value, Parameter):
+                self._level_param = value
+            else:
+                self._level = value
+
+    cpdef double get_level(self, ScenarioIndex scenario_index) except? -1:
+        if self._level_param is None:
+            return self._level
+        return self._level_param.get_value(scenario_index)
+
+    property area:
+        def __get__(self):
+            if self._area_param is None:
+                return self._area
+            return self._area_param
+
+        def __set__(self, value):
+            self._area_param = None
+            if value is None:
+                self._area_param = None
+                self._area = 0.0
+            elif isinstance(value, Parameter):
+                self._area_param = value
+            else:
+                self._area = value
+
+    cpdef double get_area(self, ScenarioIndex scenario_index) except? -1:
+        if self._area_param is None:
+            return self._area
+        return self._area_param.get_value(scenario_index)
+
+    property domain:
+        def __get__(self):
+            return self._domain
+
+        def __set__(self, value):
+            self._domain = value
+
+    cpdef reset(self):
+        """Called at the beginning of a run"""
+        AbstractStorage.reset(self)
+        self._reset_storage_only()
+
+    cpdef _reset_storage_only(self, bint use_initial_volume=True):
+        """Reset the current volume of the storage node.
+
+        Parameters
+        ==========
+        use_initial_volume : bool (default: True)
+            Reset the volume to the initial volume of the storage node. If false the volume is reset to max_volume.
+        """
+        cdef int i
+        cdef double mxv = self._max_volume
+        cdef double reset_volume, reset_pc
+
+        for i, si in enumerate(self.model.scenarios.combinations):
+
+            if use_initial_volume:
+                reset_volume = self.get_initial_volume()
+                reset_pc = self.get_initial_pc()
+            else:
+                if self._max_volume_param is not None:
+                    # If it's not the first time-step, and we want to reset to max capacity then the
+                    # parameter should already have been evaluated.
+                    mxv = self._max_volume_param.get_value(si)
+                    # Reset to max capacity, accounting for variable volume
+                    reset_volume = mxv
+                    # We do this division (rather than setting reset_pc = 1.0) to be consistent with the case below
+                    # when mxv is zero.
+                    try:
+                        reset_pc = reset_volume / mxv
+                    except ZeroDivisionError:
+                        reset_pc = np.nan
+                else:
+                    reset_volume = mxv
+                    # Compute the proportional volume.
+                    try:
+                        reset_pc = reset_volume / mxv
+                    except ZeroDivisionError:
+                        reset_pc = np.nan
+
+            self._volume[i] = reset_volume
+            self._current_pc[i] = reset_pc
+
+    cpdef after(self, Timestep ts, double[:] adjustment = None):
+        AbstractStorage.after(self, ts)
+        cdef int i
+        cdef double mxv, mnv
+        cdef ScenarioIndex si
+
+        for i, si in enumerate(self.model.scenarios.combinations):
+            self._volume[i] += self._flow[i]*ts.days
+            # Ensure variable maximum volume is taken in to account
+            mxv = self.get_max_volume(si)
+            mnv = self.get_min_volume(si)
+
+            # Apply any storage adjustment if given
+            if adjustment is not None:
+                self._volume[i] += adjustment[i]
+                # Ensure volume stays within bounds.
+                self._volume[i] = min(mxv, max(mnv, self._volume[i]))
+
+            if abs(self._volume[i] - mxv) < 1e-6:
+                self._volume[i] = mxv
+            if abs(self._volume[i] - mnv) < 1e-6:
+                self._volume[i] = mnv
+
+            try:
+                self._current_pc[i] = self._volume[i] / mxv
+            except ZeroDivisionError:
+                self._current_pc[i] = np.nan
+
+cdef class AggregatedStorage(AbstractStorage):
+    """ Base class for a special type of storage node that is the aggregated sum of `Storage` objects.
+
+    This class is intended to be used isolated from the network.
+    """
+    def __cinit__(self, ):
+        self._allow_isolated = True
+        self.virtual = True
+
+    property storage_nodes:
+        def __get__(self):
+            return self._storage_nodes
+
+        def __set__(self, value):
+            self._storage_nodes = list(value)
+
+    property initial_volume:
+        def __get__(self, ):
+            cdef Storage s
+            return np.sum([s.get_initial_volume() for s in self._storage_nodes])
+
+    cpdef reset(self):
+        cdef int i
+        cdef double mxv = 0.0
+        cdef ScenarioIndex si
+
+        for i, si in enumerate(self.model.scenarios.combinations):
+            mxv = 0.0
+            for s in self._storage_nodes:
+                mxv += s.get_max_volume(si)
+
+            self._volume[i] = self.initial_volume
+            # Ensure variable maximum volume is taken in to account
+            try:
+                self._current_pc[i] = self._volume[i] / mxv
+            except ZeroDivisionError:
+                self._current_pc[i] = np.nan
+
+    cpdef after(self, Timestep ts):
+        AbstractStorage.after(self, ts)
+        cdef int i
+        cdef Storage s
+        cdef double mxv
+
+        for i, si in enumerate(self.model.scenarios.combinations):
+            self._flow[i] = 0.0
+            mxv = 0.0
+            for s in self._storage_nodes:
+                self._flow[i] += s._flow[i]
+                mxv += s.get_max_volume(si)
+            self._volume[i] += self._flow[i]*ts.days
+
+            # Ensure variable maximum volume is taken in to account
+            try:
+                self._current_pc[i] = self._volume[i] / mxv
+            except ZeroDivisionError:
+                self._current_pc[i] = np.nan
+
+    @classmethod
+    def load(cls, data, model):
+        name = data["name"]
+        nodes = [model._get_node_from_ref(model, node_name) for node_name in data["storage_nodes"]]
+        agg = cls(model, name, nodes)
+        return agg
+
+
+cdef class VirtualStorage(Storage):
+    def __cinit__(self, ):
+        self._allow_isolated = True
+        self.virtual = True
+        self.active = True
+
+    cpdef reset(self):
+        self.active = True
+        Storage.reset(self)
+
+    property nodes:
+        def __get__(self):
+            return self._nodes
+
+        def __set__(self, value):
+            self._nodes = list(value)
+            self.model.dirty = True
+
+    property factors:
+        def __get__(self):
+            return np.array(self._factors)
+
+        def __set__(self, value):
+            self._factors = np.array(value, dtype=np.float64)
+
+    cpdef after(self, Timestep ts, double[:] adjustment = None):
+        cdef int i
+        cdef ScenarioIndex si
+        cdef AbstractNode n
+
+        if self.active:
+            for i, si in enumerate(self.model.scenarios.combinations):
+                self._flow[i] = 0.0
+                for n, f in zip(self._nodes, self._factors):
+                    self._flow[i] -= f*n._flow[i]
+            Storage.after(self, ts, adjustment = adjustment)
+
+
+cdef class RollingVirtualStorage(VirtualStorage):
+    def __cinit__(self):
+        self._memory_pointer = 0
+
+    cpdef setup(self, model):
+        super(RollingVirtualStorage, self).setup(model)
+        cdef int ncomb = len(model.scenarios.combinations)
+        if self.timesteps < 2:
+            raise ValueError('The number of time-steps for a RollingVirtualStorage node must be greater than one.')
+        self._memory = np.zeros((self.timesteps-1, ncomb), dtype=np.float64)
+        self._memory_pointer = 0
+
+    cpdef reset(self):
+        VirtualStorage.reset(self)
+        self._memory[:] = 0.0
+        self._memory_pointer = 0
+
+    cpdef after(self, Timestep ts, double[:] adjustment = None):
+        cdef int i
+        cdef ScenarioIndex si
+
+        assert adjustment is None
+
+        # Update the storage volumes by applying an adjustment
+        VirtualStorage.after(self, ts, adjustment=self._memory[self._memory_pointer, :])
+
+        # Store today's flow in the memory and increment the memory pointer
+        for i, si in enumerate(self.model.scenarios.combinations):
+            # Flow is negative in VirtualStorage to remove from the store, save the +ve number here for
+            # returning to the store later
+            self._memory[self._memory_pointer, i] = -self._flow[i] * ts.days
+        self._memory_pointer = (self._memory_pointer + 1) % (self.timesteps - 1)
```

### Comparing `pywr-1.8.0/pywr/_model.pyx` & `pywr-1.9.0/pywr/_model.pyx`

 * *Files 16% similar despite different names*

```diff
@@ -1,993 +1,998 @@
-import os
-import pandas
-import json
-import networkx as nx
-import copy
-from packaging.version import parse as parse_version
-import warnings
-import inspect
-import time
-from functools import wraps
-import logging
-logger = logging.getLogger(__name__)
-
-
-import pywr
-from pywr.timestepper import Timestepper
-
-from pywr.nodes import NodeMeta
-from pywr.parameters import load_parameter
-from pywr.recorders import load_recorder
-
-from pywr._core import (BaseInput, BaseLink, BaseOutput, StorageInput, StorageOutput, Timestep, ScenarioIndex)
-from pywr._component import ROOT_NODE
-from pywr._component cimport Component
-from pywr.nodes import Storage, AggregatedStorage, AggregatedNode, VirtualStorage
-from pywr._core import ScenarioCollection, Scenario
-from pywr._core cimport AbstractNode
-from .dataframe_tools import load_dataframe
-from pywr.parameters._parameters import Parameter as BaseParameter
-from pywr.parameters._parameters cimport Parameter as BaseParameter
-from pywr.recorders import ParameterRecorder, IndexParameterRecorder, Recorder
-
-
-class OrphanedParameterWarning(Warning):
-    pass
-
-
-class ModelDocumentWarning(Warning):
-    pass
-
-
-class ModelStructureError(Exception):
-    pass
-
-
-class Model(object):
-    """Model of a water supply network"""
-    def __init__(self, **kwargs):
-        """Initialise a new Model instance
-
-        Parameters
-        ----------
-        solver : string
-            The name of the underlying solver to use. See the `pywr.solvers`
-            package. If no value is given, the default GLPK solver is used.
-        start : pandas.Timestamp
-            The date of the first timestep in the model
-        end : pandas.Timestamp
-            The date of the last timestep in the model
-        timestep : int or datetime.timedelta
-            Number of days in each timestep
-        """
-        self.graph = nx.DiGraph()
-        self.metadata = {}
-
-        solver_name = kwargs.pop("solver", None)
-        solver_args = kwargs.pop("solver_args", {})
-
-        # time arguments
-        start = kwargs.pop("start", "2015-01-01")
-        end = kwargs.pop("end", "2015-12-31")
-        timestep = kwargs.pop("timestep", 1)
-        self.timestepper = Timestepper(start, end, timestep)
-
-        self.data = {}
-        self.dirty = True
-
-        self.path = kwargs.pop('path', None)
-        if self.path is not None:
-            if os.path.exists(self.path) and not os.path.isdir(self.path):
-                self.path = os.path.dirname(self.path)
-
-        # Import this here once everything else is defined.
-        # This avoids circular references in the solver classes
-        from pywr.solvers import solver_registry
-
-        if solver_name is None:
-            # See if there is a environment variable defining the solver
-            solver_name = os.environ.get('PYWR_SOLVER', None)
-
-        if solver_name is not None:
-            # use specific solver
-            solver = None
-            name1 = solver_name.lower()
-            for cls in solver_registry:
-                if name1 == cls.name.lower():
-                    solver = cls
-            if solver is None:
-                raise KeyError('Unrecognised solver: {}'.format(solver_name))
-        else:
-            # use default solver
-            solver = solver_registry[0]
-        self.solver = solver(**solver_args)
-        self.component_graph = nx.DiGraph()
-        self.component_graph.add_node(ROOT_NODE)
-        self.component_tree_flat = None
-
-        self.tables = {}
-        self.scenarios = ScenarioCollection(self)
-
-        if kwargs:
-            key = list(kwargs.keys())[0]
-            raise TypeError("'{}' is an invalid keyword argument for this function".format(key))
-
-        self._time_before = None
-        self._time_after = None
-        self.reset()
-
-    @property
-    def components(self):
-        return NamedIterator(n for n in self.component_graph.nodes() if n != ROOT_NODE)
-
-    @property
-    def recorders(self):
-        return NamedIterator(n for n in self.components if isinstance(n, Recorder))
-
-    @property
-    def parameters(self):
-        return NamedIterator(n for n in self.components if isinstance(n, BaseParameter))
-
-    @property
-    def variables(self):
-        return NamedIterator(n for n in self.parameters if n.is_variable)
-
-    @property
-    def constraints(self):
-        return NamedIterator(n for n in self.recorders if n.is_constraint)
-
-    @property
-    def objectives(self):
-        return NamedIterator(n for n in self.recorders if n.is_objective)
-
-    def is_feasible(self):
-        """Returns True if none of the constraints are violated.
-
-        This function checks `is_constraint_violated()` for all defined constraints. If any constraints
-        are violated this function returns False. The checking of constraint violation requires that a
-        simulation has been completed before this function is called.
-        """
-        for c in self.constraints:
-            if c.is_constraint_violated():
-                return False
-        return True
-
-    def check(self):
-        """Check the validity of the model
-
-        Raises an Exception if the model is invalid.
-        """
-        logger.info("Checking model ...")
-        for node in self.nodes:
-            node.check()
-        self.check_graph()
-        orphans = self.find_orphaned_parameters()
-        if orphans:
-            warnings.warn("Model has {} orphaned parameters".format(len(orphans)), OrphanedParameterWarning)
-
-    def check_graph(self):
-        """Check the connectivity of the graph is valid"""
-        all_nodes = set(self.graph.nodes())
-        routes = self.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
-        # identify nodes that aren't in at least one route
-        seen = set()
-        for route in routes:
-            for node in route:
-                seen.add(node)
-        isolated_nodes = all_nodes ^ seen
-        for node in isolated_nodes:
-            if node.allow_isolated is False:
-                raise ModelStructureError("Node is not part of a valid route: {}".format(node.name))
-
-    @property
-    def nodes(self):
-        """Returns a model node iterator"""
-        return NodeIterator(self)
-
-    def edges(self):
-        """Returns a list of Edges in the model
-
-        An edge is described as a 2-tuple of the source and dest Nodes.
-        """
-        return self.graph.edges()
-
-    @classmethod
-    def loads(cls, data, model=None, path=None, solver=None, **kwargs):
-        """Read JSON data from a string and parse it as a model document"""
-        try:
-            data = json.loads(data)
-        except ValueError as e:
-            message = e.args[0]
-            if path:
-                e.args = ("{} [{}]".format(e.args[0], os.path.basename(path)),)
-            raise(e)
-        cls._load_includes(data, path)
-        return cls.load(data, model, path, solver, **kwargs)
-
-    @classmethod
-    def _load_includes(cls, data, path=None):
-        """Load included JSON references
-
-        Parameters
-        ----------
-        data : dict
-            The model dictionary.
-        path : str
-            Path to the model document (None if in-memory).
-
-        This method is private and shouldn't need to be called by the user.
-        Note that the data dictionary is modified in-place.
-        """
-        if "includes" in data:
-            for filename in data["includes"]:
-                _, ext = os.path.splitext(filename)
-                if path is not None:
-                    filename = os.path.join(os.path.dirname(path), filename)
-
-                ext = ext.lower()
-                if ext == '.json':
-                    cls._load_json_include(data, filename)
-                elif ext == '.py':
-                    cls._load_py_include(filename)
-                else:
-                    raise NotImplementedError(f'Include file type "{ext}" not supported.')
-
-    @classmethod
-    def _load_py_include(cls, filename):
-        import runpy
-        runpy.run_path(filename)
-
-    @classmethod
-    def _load_json_include(cls, data, filename):
-        with open(filename, "r") as f:
-            try:
-                include_data = json.loads(f.read())
-            except ValueError as e:
-                message = e.args[0]
-                e.args = ("{} [{}]".format(e.args[0], os.path.basename(filename)),)
-                raise(e)
-        for key, value in include_data.items():
-            if isinstance(value, list):
-                try:
-                    data[key].extend(value)
-                except KeyError:
-                    data[key] = value
-            elif isinstance(value, dict):
-                try:
-                    data[key].update(value)
-                except KeyError:
-                    data[key] = value
-            else:
-                raise TypeError("Invalid type for key \"{}\" in include \"{}\".".format(key, filename))
-        return None  # data modified in-place
-
-    @classmethod
-    def load(cls, data, model=None, path=None, solver=None, **kwargs):
-        """Load an existing model
-
-        Parameters
-        ----------
-        data : file-like, string, or dict
-            A file-like object to read JSON data from, a filename to read,
-            or a parsed dict
-        model : Model (optional)
-            An existing model to append to
-        path : str (optional)
-            Path to the model document for relative pathnames
-        solver : str (optional)
-            Name of the solver to use for the model. This overrides the solver
-            section of the model document.
-        """
-        if isinstance(data, str):
-            # argument is a filename
-            logger.info('Loading model from file: "{}"'.format(path))
-            path = data
-            with open(path, "r") as f:
-                data = f.read()
-            return cls.loads(data, model, path, solver)
-
-        if hasattr(data, 'read'):
-            logger.info('Loading model from file-like object.')
-            # argument is a file-like object
-            data = data.read()
-            return cls.loads(data, model, path, solver)
-
-        # data is a dictionary, make a copy to avoid modify the input
-        data = copy.deepcopy(data)
-
-        # check minimum version
-        try:
-            minimum_version = data["metadata"]["minimum_version"]
-        except KeyError:
-            warnings.warn("Missing \"minimum_version\" item in metadata.", ModelDocumentWarning)
-        else:
-            minimum_version = parse_version(minimum_version)
-            pywr_version = parse_version(pywr.__version__)
-            if pywr_version < minimum_version:
-                warnings.warn("Document requires version {} or newer, but only have {}.".format(
-                    minimum_version, pywr_version), RuntimeWarning)
-
-        cls._load_includes(data, path)
-
-        try:
-            solver_data = data['solver']
-        except KeyError:
-            solver_name = solver
-            solver_args = kwargs.pop('solver_args', {})
-        else:
-            solver_name = data["solver"].pop("name")
-            solver_args = data["solver"]
-
-        try:
-            timestepper_data = data['timestepper']
-        except KeyError:
-            start = end = None
-            timestep = 1
-        else:
-            start = pandas.to_datetime(timestepper_data['start'])
-            end = pandas.to_datetime(timestepper_data['end'])
-            timestep = timestepper_data['timestep']
-
-        if model is None:
-            model = cls(
-                solver=solver_name,
-                solver_args=solver_args,
-                start=start,
-                end=end,
-                timestep=timestep,
-                path=path,
-            )
-        model.metadata = data["metadata"]
-
-        # load scenarios
-        try:
-            scenarios_data = data["scenarios"]
-        except KeyError:
-            # Leave to default of no scenarios
-            pass
-        else:
-            for scen_data in scenarios_data:
-                scen_name = scen_data["name"]
-                size = scen_data["size"]
-                ensemble_names = scen_data.pop("ensemble_names", None)
-                s_slice = scen_data.pop("slice", None)
-                if s_slice:
-                    s_slice = slice(*s_slice)
-                Scenario(model, scen_name, size=size, slice=s_slice, ensemble_names=ensemble_names)
-
-        try:
-            scenario_combinations = data["scenario_combinations"]
-        except KeyError:
-            pass
-        else:
-            model.scenarios.user_combinations = scenario_combinations
-
-        # load table references
-        try:
-            tables_data = data["tables"]
-        except KeyError:
-            # Default to no table entries
-            pass
-        else:
-            for table_name, table_data in tables_data.items():
-                model.tables[table_name] = load_dataframe(model, table_data)
-
-        # collect nodes to load
-        nodes_to_load = {}
-        for node_data in data["nodes"]:
-            node_name = node_data["name"]
-            nodes_to_load[node_name] = node_data
-        model._nodes_to_load = nodes_to_load
-
-        def collect_components(data, key):
-            components_data = data.get(key, {})
-            for name, component_data in components_data.items():
-                component_data["name"] = name
-            return components_data
-
-        model._parameters_to_load = collect_components(data, "parameters")
-        model._recorders_to_load = collect_components(data, "recorders")
-
-        @listify
-        def load_components(components_to_load, load_component):
-            while True:
-                try:
-                    name, component_data = components_to_load.popitem()
-                except KeyError:
-                    break
-
-                #If unable to load a node, then reraise the exception with some
-                #useful information like node name and parameter name.
-                try:
-                      component = load_component(model, component_data, name)
-                except Exception as err:
-                    logger.critical("Error loading component %s", name)
-                    #Reraise the exception
-                    raise
-
-                yield component
-
-        load_components(model._recorders_to_load, load_recorder)
-        for parameter in load_components(model._parameters_to_load, load_parameter):
-            if not isinstance(parameter, BaseParameter):
-                raise TypeError("Named parameters cannot be literal values. Use type `constant` instead.")
-
-        # load the remaining nodes
-        for node_name in list(nodes_to_load.keys()):
-            node = cls._get_node_from_ref(model, node_name)
-
-        del(model._recorders_to_load)
-        del(model._parameters_to_load)
-        del(model._nodes_to_load)
-
-        # load edges
-        for edge_data in data['edges']:
-            node_from_name = edge_data[0]
-            node_to_name = edge_data[1]
-            if len(edge_data) > 2:
-                slot_from, slot_to = edge_data[2:]
-            else:
-                slot_from = slot_to = None
-            node_from = model.nodes[node_from_name]
-            node_to = model.nodes[node_to_name]
-            node_from.connect(node_to, from_slot=slot_from, to_slot=slot_to)
-
-        logger.info('Model load complete!')
-        return model
-
-    @classmethod
-    def _get_node_from_ref(cls, model, node_name):
-        try:
-            # first check if node has already been loaded
-            node = model.nodes[node_name]
-        except KeyError:
-            # if not, load it now
-            node_data = model._nodes_to_load[node_name]
-            node_type = node_data['type'].lower()
-            cls = NodeMeta.node_registry[node_type]
-            node = cls.load(node_data, model)
-            del(model._nodes_to_load[node_name])
-        return node
-
-    def find_all_routes(self, type1, type2, valid=None, max_length=None, domain_match='strict'):
-        """Find all routes between two nodes or types of node
-
-        Parameters
-        ----------
-        type1 : Node class or instance
-            The source node instance (or class)
-        type2 : Node class or instance
-            The destination  node instance (or class)
-        valid : tuple of Node classes
-            A tuple of Node classes that the route can traverse. For example,
-            a route between a Catchment and Terminator can generally only
-            traverse River nodes.
-        max_length : integer
-            Maximum length of the route including start and end nodes.
-        domain_match : string
-            A string to control the behaviour of different domains on the route.
-                'strict' : all nodes must have the same domain as the first node.
-                'any' : any domain is permitted on any node (i.e. nodes can have different domains)
-                'different' : at least two different domains must be present on the route
-
-        Returns a list of all the routes between the two nodes. A route is
-        specified as a list of all the nodes between the source and
-        destination with the same domain has the source.
-        """
-
-        nodes = sorted(self.graph.nodes(), key=lambda n: n.name)
-
-        if inspect.isclass(type1):
-            # find all nodes of type1
-            type1_nodes = []
-            for node in nodes:
-                if isinstance(node, type1):
-                    type1_nodes.append(node)
-        else:
-            type1_nodes = [type1]
-
-        if inspect.isclass(type2):
-            # find all nodes of type2
-            type2_nodes = []
-            for node in nodes:
-                if isinstance(node, type2):
-                    type2_nodes.append(node)
-        else:
-            type2_nodes = [type2]
-
-        # find all routes between type1_nodes and type2_nodes
-        all_routes = []
-        for node1 in type1_nodes:
-            for node2 in type2_nodes:
-                for route in nx.all_simple_paths(self.graph, node1, node2):
-                    is_valid = True
-                    # Check valid intermediate nodes
-                    if valid is not None and len(route) > 2:
-                        for node in route[1:-1]:
-                            if not isinstance(node, valid):
-                                is_valid = False
-                    # Check domains
-                    if domain_match == 'strict':
-                        # Domains must match the first node
-                        for node in route[1:]:
-                            if node.domain != route[0].domain:
-                                is_valid = False
-                    elif domain_match == 'different':
-                        # Ensure at least two different domains are present
-                        domains_found = set()
-                        for node in route:
-                            domains_found.add(node.domain)
-                        if len(domains_found) < 2:
-                            is_valid = False
-                    elif domain_match == 'any':
-                        # No filtering required
-                        pass
-                    else:
-                        raise ValueError("domain_match '{}' not understood.".format(domain_match))
-
-                    # Check length
-                    if max_length is not None:
-                        if len(route) > max_length:
-                            is_valid = False
-
-                    if is_valid:
-                        all_routes.append(route)
-
-        # Now sort the routes to ensure determinism
-        all_routes = sorted(all_routes, key=lambda r: tuple(n.fully_qualified_name for n in r))
-        return all_routes
-
-    def step(self):
-        """ Step the model forward by one day
-
-        This method progresses the model by one time-step. The anatomy
-        of a time-step is as follows:
-          1. Call `Model.setup` if the `Model.dirty` is True.
-          2. Progress the `Model.timestepper` by one step.
-          3. Call `Model.before` to ensure all nodes and components are ready for solve.
-            a. Call `Node.before` on all nodes
-            b. Refresh the component dependency tree
-            c. Call `Component.before` on all components, respecting dependency order
-            d. Call `Parameter.calc_values` on all Parameters, respecting dependency order
-          4. Call `Model.solve` to solve the linear programme
-          5. Call `Model.after` to ensure all nodes and components
-            complete any work in the timestep.
-
-        It is important to note that the current timestep object is the
-        same during phases (3), (4) and (5) above. However the internal state of
-        nodes changes during phase (4) and (5). During stages (3) and (5) the
-        nodes are updated before the components. Therefore during the component
-        update in phase (5) the internal state of nodes is already updated (e.g.
-        current storage volumes). This has consequences for any component
-        algorithms in phase (5) that rely on the state being as it was before
-        this update. In general component `after` methods should not recompute
-        any component state or rely on the internal node state.
-
-        A dependency tree is used during the `before` and `after` updates of
-        components. This ensures that components that rely on the state of
-        another component are updated first.
-
-
-        See also
-        --------
-        `Component`
-
-
-        """
-        if self.dirty or self.timestepper.dirty:
-            self.setup()
-        self.timestep = next(self.timestepper)
-        return self._step()
-
-    def _step(self):
-        self.before()
-        # solve the current timestep
-        ret = self.solve()
-        self.after()
-        return ret
-
-    def solve(self):
-        """Call solver to solve the current timestep"""
-        return self.solver.solve(self)
-
-    def run(self):
-        """Run the model
-        """
-        logger.info('Start model run ...')
-        t0 = time.time()
-        timestep = None
-        try:
-            if self.dirty or self.timestepper.dirty:
-                self.setup()
-            else:
-                self.reset()
-            t1 = time.time()
-            for timestep in self.timestepper:
-                self.timestep = timestep
-                ret = self._step()
-            t2 = time.time()
-        finally:
-            self.finish()
-        t3 = time.time()
-
-        if timestep is None:
-            raise RuntimeError("Nothing to run! Timestepper length is {}".format(len(self.timestepper)))
-
-        # return ModelResult instance
-        time_taken = t2 - t1
-        time_taken_with_overhead = t3 - t0
-        num_scenarios = len(self.scenarios.combinations)
-        try:
-            speed = (timestep.index * num_scenarios) / time_taken
-        except ZeroDivisionError:
-            speed = float('nan')
-        result = ModelResult(
-            num_scenarios=num_scenarios,
-            timestep=timestep,
-            time_taken=time_taken,
-            time_taken_before=self._time_before,
-            time_taken_after=self._time_after,
-            time_taken_with_overhead=time_taken_with_overhead,
-            speed=speed,
-            solver_name=self.solver.name,
-            solver_stats=self.solver.stats,
-            version=pywr.__version__,
-        )
-        logger.info('Model run complete!')
-        return result
-
-    def setup(self, ):
-        """Setup the model for the first time or if it has changed since
-        last run."""
-        logger.info('Setting up model ...')
-        self.timestepper.setup()
-        self.scenarios.setup()
-        length_changed = self.timestepper.reset()
-        for node in self.graph.nodes():
-            try:
-                node.setup(self)
-            except Exception as err:
-              #reraise the exception after logging some info about source of error
-              logger.critical("An error occurred setting up node during setup %s",
-                              node.name)
-              raise
-
-        components = self.flatten_component_tree(rebuild=True)
-        for component in components:
-            try:
-                component.setup()
-            except Exception as err:
-                #reraise the exception after logging some info about source of error
-                logger.critical("An error occurred setting up component during setup %s",
-                                component.name)
-                raise
-
-        self.solver.setup(self)
-        self.reset()
-        self.dirty = False
-        logger.info('Setting up complete!')
-
-    def reset(self, start=None):
-        """Reset model to it's initial conditions"""
-        logger.info('Resetting model ...')
-        length_changed = self.timestepper.reset(start=start)
-        for node in self.nodes:
-            if length_changed:
-                try:
-                    node.setup(self)
-                except Exception as err:
-                    #reraise the exception after logging some info about source of error
-                    logger.critical("An error occurred calling setup while resetting node %s",node.name)
-                    raise
-            try:
-                node.reset()
-            except Exception as err:
-                #reraise the exception after logging some info about source of error
-                logger.critical("An error occurred calling reset on node %s",node.name)
-                raise
-
-        components = self.flatten_component_tree(rebuild=False)
-        for component in components:
-            if length_changed:
-                try:
-                    component.setup()
-                except Exception as err:
-                    #reraise the exception after logging some info about source of error
-                    logger.critical("An error occurred calling setup while resetting component %s",
-                                    component.name)
-                    raise
-
-            try:
-                component.reset()
-            except Exception as err:
-                #reraise the exception after logging some info about source of error
-                logger.critical("An error occurred calling reset on component %s",
-                                component.name)
-                raise
-
-        self.solver.reset()
-        # reset the timers
-        self._time_before = 0.0
-        self._time_after = 0.0
-        logger.info('Reset complete!')
-
-    def before(self):
-        """ Perform initialisation work before solve on each timestep.
-
-        This method calls the `before()` method on all nodes and components
-        in the model. Nodes are updated first, components second.
-
-        See also
-        --------
-        `Model.step`
-        """
-        cdef AbstractNode node
-        cdef Component component
-        cdef BaseParameter param
-        cdef double t0 = time.time()
-        for node in self.graph.nodes():
-            node.before(self.timestep)
-        cdef list components = self.flatten_component_tree(rebuild=False)
-        for component in components:
-            component.before()
-        for component in components:
-            if isinstance(component, BaseParameter):
-                param = component
-                param.calc_values(self.timestep)
-        self._time_before += time.time() - t0
-
-    def after(self):
-        cdef AbstractNode node
-        cdef Component component
-        cdef double t0 = time.time()
-        for node in self.graph.nodes():
-            node.after(self.timestep)
-        cdef list components = self.flatten_component_tree(rebuild=False)
-        for component in components:
-            component.after()
-        self._time_after += time.time() - t0
-
-    def finish(self):
-        for node in self.graph.nodes():
-            node.finish()
-        components = self.flatten_component_tree(rebuild=False)
-        for component in components:
-            try:
-                component.finish()
-            except Exception as err:
-                #reraise the exception after logging some info about source of error
-                logger.critical("An error occurred finishing component %s", component.name)
-                raise
-
-    def to_dataframe(self):
-        """ Return a DataFrame from any Recorders with a `to_dataframe` attribute
-
-        """
-        dfs = {r.name: r.to_dataframe() for r in self.recorders if hasattr(r, 'to_dataframe')}
-        df = pandas.concat(dfs, axis=1)
-        df.columns.set_names('Recorder', level=0, inplace=True)
-        return df
-
-    def flatten_component_tree(self, rebuild=False):
-        if self.component_tree_flat is None or rebuild is True:
-            self.component_tree_flat = []
-            G = self.component_graph
-
-            # Test some properties of the dependency tree
-            # Do not permit cycles in the dependencies
-            ncycles = len(list(nx.simple_cycles(G)))
-            if ncycles != 0:
-                raise ModelStructureError("Cyclical ({}) dependencies found in the model's components.".format(ncycles))
-            # Do not permit self-loops
-            for n in nx.nodes_with_selfloops(G):
-                raise ModelStructureError('Component "{}" contains a self-loop.'.format(n))
-
-            for node in nx.dfs_postorder_nodes(G, ROOT_NODE):
-                if node == ROOT_NODE:
-                    continue
-                self.component_tree_flat.append(node)
-            # order components so that they can be iterated over easily in an
-            # sequence which respects dependencies
-            self.component_tree_flat = self.component_tree_flat[::-1]
-        return self.component_tree_flat
-
-    def find_orphaned_parameters(self):
-        """Helper function to find orphaned parameters
-
-        Returns a set of parameters which:
-            1) Have no parent components
-            2) Are not referenced directly by a node
-        """
-        all_parameters = set(self.parameters)
-        visited = set()
-        # add all parameters referenced by another component
-        for parameter in self.components:
-            if parameter.parents:
-                visited.add(parameter)
-        # find all parameters referenced by a node
-        for node in self.graph.nodes():
-            for component in node.components:
-                visited.add(component)
-        # identify unseen parameters
-        orphans = all_parameters - visited
-        return orphans
-
-
-class NodeIterator(object):
-    """Iterator for Nodes in a Model which also supports indexing
-
-    Notes
-    -----
-    Although it's not very efficient to have to read through all of the nodes
-    in a model when accessing one by name (e.g. model.nodes['reservoir']), it's
-    easier than having to keep a dictionary up to date. The solvers should
-    avoid using this class, and use Model.graph.nodes() directly.
-    """
-    def __init__(self, model):
-        self.model = model
-        self.position = 0
-        self.length = None
-
-    def _nodes(self, hide_children=True):
-        for node in self.model.graph.nodes():
-            if hide_children is False or node.parent is None:  # don't return child nodes (e.g. StorageInput)
-                yield node
-        return
-
-    def __getitem__(self, key):
-        """Get a node from the graph by it's name"""
-        for node in self._nodes(hide_children=False):
-            if node.name == key:
-                return node
-        raise KeyError("'{}'".format(key))
-
-    def __delitem__(self, key):
-        """Remove a node from the graph"""
-        if isinstance(key, str):
-            node = self[key]
-        else:
-            node = key
-        # recursive delete to remove all sub-nodes
-        nodes_to_delete = []
-        for node2 in self.model.graph.nodes():
-            if node2.parent == node:
-                nodes_to_delete.append(node2)
-        # Avoid dictionary modification
-        for node2 in nodes_to_delete:
-            del(self[node2])
-        self.model.graph.remove_node(node)
-
-    def keys(self):
-        for node in self._nodes():
-            yield node.name
-
-    def values(self):
-        for node in self._nodes():
-            yield node
-
-    def items(self):
-        for node in self._nodes():
-            yield (node.name, node)
-
-    def __len__(self):
-        """Returns the number of nodes in the model"""
-        return len(list(self._nodes()))
-
-    def __contains__(self, value):
-        for node in self._nodes():
-            if node.name == value or node == value:
-                return True
-        return False
-
-    def __iter__(self):
-        return self
-
-    def __next__(self):
-        return self.next()
-
-    def next(self):
-        if self.position == 0:
-            self.nodes = list(self._nodes())
-            self.length = len(self.nodes)
-        if self.position < self.length:
-            node = self.nodes[self.position]
-            self.position += 1
-            return node
-        raise StopIteration()
-
-
-class NamedIterator(object):
-    def __init__(self, objects=None):
-        if objects:
-            self._objects = list(objects)
-        else:
-            self._objects = []
-
-    def __getitem__(self, key):
-        """Get a node from the graph by it's name"""
-        for obj in self._objects:
-            if obj.name == key:
-                return obj
-        raise KeyError("'{}'".format(key))
-
-    def __delitem__(self, key):
-        """Remove a node from the graph by it's name"""
-        obj = self[key]
-        self._objects.remove(obj)
-
-    def __setitem__(self, key, obj):
-        # TODO: check for name collisions / duplication
-        self._objects.append(obj)
-
-    def keys(self):
-        for obj in self._objects:
-            yield obj.name
-
-    def values(self):
-        for obj in self._objects:
-            yield obj
-
-    def items(self):
-        for obj in self._objects:
-            yield (obj.name, obj)
-
-    def __len__(self):
-        """Returns the number of nodes in the model"""
-        return len(self._objects)
-
-    def __iter__(self):
-        return iter(self._objects)
-
-    def __contains__(self, value):
-        for obj in self._objects:
-            if obj.name == value or obj == value:
-                return True
-        return False
-
-    def append(self, obj):
-        # TODO: check for name collisions / duplication
-        self._objects.append(obj)
-
-
-class ModelResult(object):
-    def __init__(self, num_scenarios, timestep, time_taken, time_taken_before, time_taken_after, time_taken_with_overhead,
-                 speed, solver_name, solver_stats, version):
-        self.timestep = timestep
-        self.timesteps = timestep.index + 1
-        self.time_taken = time_taken
-        self.time_taken_before = time_taken_before
-        self.time_taken_after = time_taken_after
-        self.time_taken_with_overhead = time_taken_with_overhead
-        self.speed = speed
-        self.num_scenarios = num_scenarios
-        self.solver_name = solver_name
-        self.solver_stats = solver_stats
-        self.version = version
-
-    def to_dict(self):
-        return {attr: value for attr, value in self.__dict__.items()}
-
-    def to_dataframe(self):
-        d = self.to_dict()
-        # Update timestep to use the underlying pandas Timestamp
-        d['timestep'] = d['timestep'].datetime
-        # Must flatten the solver stats dict before passing to pandas
-        solver_stats = d.pop('solver_stats')
-        for k, v in solver_stats.items():
-            d['solver_stats.{}'.format(k)] = v
-        df = pandas.DataFrame(pandas.Series(d), columns=["Value"])
-        return df
-
-    def __repr__(self):
-        return "Model executed {:d} scenarios in {:.1f} seconds, running at {:.1f} timesteps per second.".format(
-            self.num_scenarios, self.time_taken_with_overhead, self.speed)
-
-    def _repr_html_(self):
-        return self.to_dataframe()._repr_html_()
-
-
-def listify(f):
-    @wraps(f)
-    def wrapper(*args, **kwargs):
-        return list(f(*args, **kwargs))
-    return wrapper
+import os
+import pandas
+import json
+import networkx as nx
+import copy
+from packaging.version import parse as parse_version
+import warnings
+import inspect
+import time
+from functools import wraps
+import logging
+logger = logging.getLogger(__name__)
+
+
+import pywr
+from pywr.timestepper import Timestepper
+
+from pywr.nodes import NodeMeta
+from pywr.parameters import load_parameter
+from pywr.recorders import load_recorder
+
+from pywr._core import (BaseInput, BaseLink, BaseOutput, StorageInput, StorageOutput, Timestep, ScenarioIndex)
+from pywr._component import ROOT_NODE
+from pywr._component cimport Component
+from pywr.nodes import Storage, AggregatedStorage, AggregatedNode, VirtualStorage
+from pywr._core import ScenarioCollection, Scenario
+from pywr._core cimport AbstractNode
+from .dataframe_tools import load_dataframe
+from pywr.parameters._parameters import Parameter as BaseParameter
+from pywr.parameters._parameters cimport Parameter as BaseParameter
+from pywr.recorders import ParameterRecorder, IndexParameterRecorder, Recorder
+
+
+class OrphanedParameterWarning(Warning):
+    pass
+
+
+class ModelDocumentWarning(Warning):
+    pass
+
+
+class ModelStructureError(Exception):
+    pass
+
+
+class Model(object):
+    """Model of a water supply network"""
+    def __init__(self, **kwargs):
+        """Initialise a new Model instance
+
+        Parameters
+        ----------
+        solver : string
+            The name of the underlying solver to use. See the `pywr.solvers`
+            package. If no value is given, the default GLPK solver is used.
+        start : pandas.Timestamp
+            The date of the first timestep in the model
+        end : pandas.Timestamp
+            The date of the last timestep in the model
+        timestep : int or datetime.timedelta
+            Number of days in each timestep
+        """
+        self.graph = nx.DiGraph()
+        self.metadata = {}
+
+        solver_name = kwargs.pop("solver", None)
+        solver_args = kwargs.pop("solver_args", {})
+
+        # time arguments
+        start = kwargs.pop("start", "2015-01-01")
+        end = kwargs.pop("end", "2015-12-31")
+        timestep = kwargs.pop("timestep", 1)
+        self.timestepper = Timestepper(start, end, timestep)
+
+        self.data = {}
+        self.dirty = True
+
+        self.path = kwargs.pop('path', None)
+        if self.path is not None:
+            if os.path.exists(self.path) and not os.path.isdir(self.path):
+                self.path = os.path.dirname(self.path)
+
+        # Import this here once everything else is defined.
+        # This avoids circular references in the solver classes
+        from pywr.solvers import solver_registry
+
+        if solver_name is None:
+            # See if there is a environment variable defining the solver
+            solver_name = os.environ.get('PYWR_SOLVER', None)
+
+        if solver_name is not None:
+            # use specific solver
+            solver = None
+            name1 = solver_name.lower()
+            for cls in solver_registry:
+                if name1 == cls.name.lower():
+                    solver = cls
+            if solver is None:
+                raise KeyError('Unrecognised solver: {}'.format(solver_name))
+        else:
+            # use default solver
+            solver = solver_registry[0]
+        self.solver = solver(**solver_args)
+        self.component_graph = nx.DiGraph()
+        self.component_graph.add_node(ROOT_NODE)
+        self.component_tree_flat = None
+
+        self.tables = {}
+        self.scenarios = ScenarioCollection(self)
+
+        if kwargs:
+            key = list(kwargs.keys())[0]
+            raise TypeError("'{}' is an invalid keyword argument for this function".format(key))
+
+        self._time_before = None
+        self._time_after = None
+        self.reset()
+
+    @property
+    def components(self):
+        return NamedIterator(n for n in self.component_graph.nodes() if n != ROOT_NODE)
+
+    @property
+    def recorders(self):
+        return NamedIterator(n for n in self.components if isinstance(n, Recorder))
+
+    @property
+    def parameters(self):
+        return NamedIterator(n for n in self.components if isinstance(n, BaseParameter))
+
+    @property
+    def variables(self):
+        return NamedIterator(n for n in self.parameters if n.is_variable)
+
+    @property
+    def constraints(self):
+        return NamedIterator(n for n in self.recorders if n.is_constraint)
+
+    @property
+    def objectives(self):
+        return NamedIterator(n for n in self.recorders if n.is_objective)
+
+    def is_feasible(self):
+        """Returns True if none of the constraints are violated.
+
+        This function checks `is_constraint_violated()` for all defined constraints. If any constraints
+        are violated this function returns False. The checking of constraint violation requires that a
+        simulation has been completed before this function is called.
+        """
+        for c in self.constraints:
+            if c.is_constraint_violated():
+                return False
+        return True
+
+    def check(self):
+        """Check the validity of the model
+
+        Raises an Exception if the model is invalid.
+        """
+        logger.info("Checking model ...")
+        for node in self.nodes:
+            node.check()
+        self.check_graph()
+        orphans = self.find_orphaned_parameters()
+        if orphans:
+            warnings.warn("Model has {} orphaned parameters".format(len(orphans)), OrphanedParameterWarning)
+
+    def check_graph(self):
+        """Check the connectivity of the graph is valid"""
+        all_nodes = set(self.graph.nodes())
+        routes = self.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
+        # identify nodes that aren't in at least one route
+        seen = set()
+        for route in routes:
+            for node in route:
+                seen.add(node)
+        isolated_nodes = all_nodes ^ seen
+        for node in isolated_nodes:
+            if node.allow_isolated is False:
+                raise ModelStructureError("Node is not part of a valid route: {}".format(node.name))
+
+    @property
+    def nodes(self):
+        """Returns a model node iterator"""
+        return NodeIterator(self)
+
+    def edges(self):
+        """Returns a list of Edges in the model
+
+        An edge is described as a 2-tuple of the source and dest Nodes.
+        """
+        return self.graph.edges()
+
+    @classmethod
+    def loads(cls, data, model=None, path=None, solver=None, **kwargs):
+        """Read JSON data from a string and parse it as a model document"""
+        try:
+            data = json.loads(data)
+        except ValueError as e:
+            message = e.args[0]
+            if path:
+                e.args = ("{} [{}]".format(e.args[0], os.path.basename(path)),)
+            raise(e)
+        cls._load_includes(data, path)
+        return cls.load(data, model, path, solver, **kwargs)
+
+    @classmethod
+    def _load_includes(cls, data, path=None):
+        """Load included JSON references
+
+        Parameters
+        ----------
+        data : dict
+            The model dictionary.
+        path : str
+            Path to the model document (None if in-memory).
+
+        This method is private and shouldn't need to be called by the user.
+        Note that the data dictionary is modified in-place.
+        """
+        if "includes" in data:
+            for filename in data["includes"]:
+                _, ext = os.path.splitext(filename)
+                if path is not None:
+                    filename = os.path.join(os.path.dirname(path), filename)
+
+                ext = ext.lower()
+                if ext == '.json':
+                    cls._load_json_include(data, filename)
+                elif ext == '.py':
+                    cls._load_py_include(filename)
+                else:
+                    raise NotImplementedError(f'Include file type "{ext}" not supported.')
+
+    @classmethod
+    def _load_py_include(cls, filename):
+        import runpy
+        runpy.run_path(filename)
+
+    @classmethod
+    def _load_json_include(cls, data, filename):
+        with open(filename, "r") as f:
+            try:
+                include_data = json.loads(f.read())
+            except ValueError as e:
+                message = e.args[0]
+                e.args = ("{} [{}]".format(e.args[0], os.path.basename(filename)),)
+                raise(e)
+        for key, value in include_data.items():
+            if isinstance(value, list):
+                try:
+                    data[key].extend(value)
+                except KeyError:
+                    data[key] = value
+            elif isinstance(value, dict):
+                try:
+                    data[key].update(value)
+                except KeyError:
+                    data[key] = value
+            else:
+                raise TypeError("Invalid type for key \"{}\" in include \"{}\".".format(key, filename))
+        return None  # data modified in-place
+
+    @classmethod
+    def load(cls, data, model=None, path=None, solver=None, **kwargs):
+        """Load an existing model
+
+        Parameters
+        ----------
+        data : file-like, string, or dict
+            A file-like object to read JSON data from, a filename to read,
+            or a parsed dict
+        model : Model (optional)
+            An existing model to append to
+        path : str (optional)
+            Path to the model document for relative pathnames
+        solver : str (optional)
+            Name of the solver to use for the model. This overrides the solver
+            section of the model document.
+        """
+        if isinstance(data, str):
+            # argument is a filename
+            logger.info('Loading model from file: "{}"'.format(path))
+            path = data
+            with open(path, "r") as f:
+                data = f.read()
+            return cls.loads(data, model, path, solver)
+
+        if hasattr(data, 'read'):
+            logger.info('Loading model from file-like object.')
+            # argument is a file-like object
+            data = data.read()
+            return cls.loads(data, model, path, solver)
+
+        return cls._load_from_dict(data, model=model, path=path, solver=None, **kwargs)
+
+    @classmethod
+    def _load_from_dict(cls, data, model=None, path=None, solver=None, **kwargs):
+        """Load data from a dictionary."""
+        # data is a dictionary, make a copy to avoid modify the input
+        data = copy.deepcopy(data)
+
+        # check minimum version
+        try:
+            minimum_version = data["metadata"]["minimum_version"]
+        except KeyError:
+            warnings.warn("Missing \"minimum_version\" item in metadata.", ModelDocumentWarning)
+        else:
+            minimum_version = parse_version(minimum_version)
+            pywr_version = parse_version(pywr.__version__)
+            if pywr_version < minimum_version:
+                warnings.warn("Document requires version {} or newer, but only have {}.".format(
+                    minimum_version, pywr_version), RuntimeWarning)
+
+        cls._load_includes(data, path)
+
+        try:
+            solver_data = data['solver']
+        except KeyError:
+            solver_name = solver
+            solver_args = kwargs.pop('solver_args', {})
+        else:
+            solver_name = data["solver"].pop("name")
+            solver_args = data["solver"]
+
+        try:
+            timestepper_data = data['timestepper']
+        except KeyError:
+            start = end = None
+            timestep = 1
+        else:
+            start = pandas.to_datetime(timestepper_data['start'])
+            end = pandas.to_datetime(timestepper_data['end'])
+            timestep = timestepper_data['timestep']
+
+        if model is None:
+            model = cls(
+                solver=solver_name,
+                solver_args=solver_args,
+                start=start,
+                end=end,
+                timestep=timestep,
+                path=path,
+            )
+        model.metadata = data["metadata"]
+
+        # load scenarios
+        try:
+            scenarios_data = data["scenarios"]
+        except KeyError:
+            # Leave to default of no scenarios
+            pass
+        else:
+            for scen_data in scenarios_data:
+                scen_name = scen_data["name"]
+                size = scen_data["size"]
+                ensemble_names = scen_data.pop("ensemble_names", None)
+                s_slice = scen_data.pop("slice", None)
+                if s_slice:
+                    s_slice = slice(*s_slice)
+                Scenario(model, scen_name, size=size, slice=s_slice, ensemble_names=ensemble_names)
+
+        try:
+            scenario_combinations = data["scenario_combinations"]
+        except KeyError:
+            pass
+        else:
+            model.scenarios.user_combinations = scenario_combinations
+
+        # load table references
+        try:
+            tables_data = data["tables"]
+        except KeyError:
+            # Default to no table entries
+            pass
+        else:
+            for table_name, table_data in tables_data.items():
+                model.tables[table_name] = load_dataframe(model, table_data)
+
+        # collect nodes to load
+        nodes_to_load = {}
+        for node_data in data["nodes"]:
+            node_name = node_data["name"]
+            nodes_to_load[node_name] = node_data
+        model._nodes_to_load = nodes_to_load
+
+        def collect_components(data, key):
+            components_data = data.get(key, {})
+            for name, component_data in components_data.items():
+                component_data["name"] = name
+            return components_data
+
+        model._parameters_to_load = collect_components(data, "parameters")
+        model._recorders_to_load = collect_components(data, "recorders")
+
+        @listify
+        def load_components(components_to_load, load_component):
+            while True:
+                try:
+                    name, component_data = components_to_load.popitem()
+                except KeyError:
+                    break
+
+                #If unable to load a node, then reraise the exception with some
+                #useful information like node name and parameter name.
+                try:
+                      component = load_component(model, component_data, name)
+                except Exception as err:
+                    logger.critical("Error loading component %s", name)
+                    #Reraise the exception
+                    raise
+
+                yield component
+
+        load_components(model._recorders_to_load, load_recorder)
+        for parameter in load_components(model._parameters_to_load, load_parameter):
+            if not isinstance(parameter, BaseParameter):
+                raise TypeError("Named parameters cannot be literal values. Use type `constant` instead.")
+
+        # load the remaining nodes
+        for node_name in list(nodes_to_load.keys()):
+            node = cls._get_node_from_ref(model, node_name)
+
+        del(model._recorders_to_load)
+        del(model._parameters_to_load)
+        del(model._nodes_to_load)
+
+        # load edges
+        for edge_data in data['edges']:
+            node_from_name = edge_data[0]
+            node_to_name = edge_data[1]
+            if len(edge_data) > 2:
+                slot_from, slot_to = edge_data[2:]
+            else:
+                slot_from = slot_to = None
+            node_from = model.nodes[node_from_name]
+            node_to = model.nodes[node_to_name]
+            node_from.connect(node_to, from_slot=slot_from, to_slot=slot_to)
+
+        logger.info('Model load complete!')
+        return model
+
+    @classmethod
+    def _get_node_from_ref(cls, model, node_name):
+        try:
+            # first check if node has already been loaded
+            node = model.nodes[node_name]
+        except KeyError:
+            # if not, load it now
+            node_data = model._nodes_to_load[node_name]
+            node_type = node_data['type'].lower()
+            cls = NodeMeta.node_registry[node_type]
+            node = cls.load(node_data, model)
+            del(model._nodes_to_load[node_name])
+        return node
+
+    def find_all_routes(self, type1, type2, valid=None, max_length=None, domain_match='strict'):
+        """Find all routes between two nodes or types of node
+
+        Parameters
+        ----------
+        type1 : Node class or instance
+            The source node instance (or class)
+        type2 : Node class or instance
+            The destination  node instance (or class)
+        valid : tuple of Node classes
+            A tuple of Node classes that the route can traverse. For example,
+            a route between a Catchment and Terminator can generally only
+            traverse River nodes.
+        max_length : integer
+            Maximum length of the route including start and end nodes.
+        domain_match : string
+            A string to control the behaviour of different domains on the route.
+                'strict' : all nodes must have the same domain as the first node.
+                'any' : any domain is permitted on any node (i.e. nodes can have different domains)
+                'different' : at least two different domains must be present on the route
+
+        Returns a list of all the routes between the two nodes. A route is
+        specified as a list of all the nodes between the source and
+        destination with the same domain has the source.
+        """
+
+        nodes = sorted(self.graph.nodes(), key=lambda n: n.name)
+
+        if inspect.isclass(type1):
+            # find all nodes of type1
+            type1_nodes = []
+            for node in nodes:
+                if isinstance(node, type1):
+                    type1_nodes.append(node)
+        else:
+            type1_nodes = [type1]
+
+        if inspect.isclass(type2):
+            # find all nodes of type2
+            type2_nodes = []
+            for node in nodes:
+                if isinstance(node, type2):
+                    type2_nodes.append(node)
+        else:
+            type2_nodes = [type2]
+
+        # find all routes between type1_nodes and type2_nodes
+        all_routes = []
+        for node1 in type1_nodes:
+            for node2 in type2_nodes:
+                for route in nx.all_simple_paths(self.graph, node1, node2):
+                    is_valid = True
+                    # Check valid intermediate nodes
+                    if valid is not None and len(route) > 2:
+                        for node in route[1:-1]:
+                            if not isinstance(node, valid):
+                                is_valid = False
+                    # Check domains
+                    if domain_match == 'strict':
+                        # Domains must match the first node
+                        for node in route[1:]:
+                            if node.domain != route[0].domain:
+                                is_valid = False
+                    elif domain_match == 'different':
+                        # Ensure at least two different domains are present
+                        domains_found = set()
+                        for node in route:
+                            domains_found.add(node.domain)
+                        if len(domains_found) < 2:
+                            is_valid = False
+                    elif domain_match == 'any':
+                        # No filtering required
+                        pass
+                    else:
+                        raise ValueError("domain_match '{}' not understood.".format(domain_match))
+
+                    # Check length
+                    if max_length is not None:
+                        if len(route) > max_length:
+                            is_valid = False
+
+                    if is_valid:
+                        all_routes.append(route)
+
+        # Now sort the routes to ensure determinism
+        all_routes = sorted(all_routes, key=lambda r: tuple(n.fully_qualified_name for n in r))
+        return all_routes
+
+    def step(self):
+        """ Step the model forward by one day
+
+        This method progresses the model by one time-step. The anatomy
+        of a time-step is as follows:
+          1. Call `Model.setup` if the `Model.dirty` is True.
+          2. Progress the `Model.timestepper` by one step.
+          3. Call `Model.before` to ensure all nodes and components are ready for solve.
+            a. Call `Node.before` on all nodes
+            b. Refresh the component dependency tree
+            c. Call `Component.before` on all components, respecting dependency order
+            d. Call `Parameter.calc_values` on all Parameters, respecting dependency order
+          4. Call `Model.solve` to solve the linear programme
+          5. Call `Model.after` to ensure all nodes and components
+            complete any work in the timestep.
+
+        It is important to note that the current timestep object is the
+        same during phases (3), (4) and (5) above. However the internal state of
+        nodes changes during phase (4) and (5). During stages (3) and (5) the
+        nodes are updated before the components. Therefore during the component
+        update in phase (5) the internal state of nodes is already updated (e.g.
+        current storage volumes). This has consequences for any component
+        algorithms in phase (5) that rely on the state being as it was before
+        this update. In general component `after` methods should not recompute
+        any component state or rely on the internal node state.
+
+        A dependency tree is used during the `before` and `after` updates of
+        components. This ensures that components that rely on the state of
+        another component are updated first.
+
+
+        See also
+        --------
+        `Component`
+
+
+        """
+        if self.dirty or self.timestepper.dirty:
+            self.setup()
+        self.timestep = next(self.timestepper)
+        return self._step()
+
+    def _step(self):
+        self.before()
+        # solve the current timestep
+        ret = self.solve()
+        self.after()
+        return ret
+
+    def solve(self):
+        """Call solver to solve the current timestep"""
+        return self.solver.solve(self)
+
+    def run(self):
+        """Run the model
+        """
+        logger.info('Start model run ...')
+        t0 = time.time()
+        timestep = None
+        try:
+            if self.dirty or self.timestepper.dirty:
+                self.setup()
+            else:
+                self.reset()
+            t1 = time.time()
+            for timestep in self.timestepper:
+                self.timestep = timestep
+                ret = self._step()
+            t2 = time.time()
+        finally:
+            self.finish()
+        t3 = time.time()
+
+        if timestep is None:
+            raise RuntimeError("Nothing to run! Timestepper length is {}".format(len(self.timestepper)))
+
+        # return ModelResult instance
+        time_taken = t2 - t1
+        time_taken_with_overhead = t3 - t0
+        num_scenarios = len(self.scenarios.combinations)
+        try:
+            speed = (timestep.index * num_scenarios) / time_taken
+        except ZeroDivisionError:
+            speed = float('nan')
+        result = ModelResult(
+            num_scenarios=num_scenarios,
+            timestep=timestep,
+            time_taken=time_taken,
+            time_taken_before=self._time_before,
+            time_taken_after=self._time_after,
+            time_taken_with_overhead=time_taken_with_overhead,
+            speed=speed,
+            solver_name=self.solver.name,
+            solver_stats=self.solver.stats,
+            version=pywr.__version__,
+        )
+        logger.info('Model run complete!')
+        return result
+
+    def setup(self, ):
+        """Setup the model for the first time or if it has changed since
+        last run."""
+        logger.info('Setting up model ...')
+        self.timestepper.setup()
+        self.scenarios.setup()
+        length_changed = self.timestepper.reset()
+        for node in self.graph.nodes():
+            try:
+                node.setup(self)
+            except Exception as err:
+              #reraise the exception after logging some info about source of error
+              logger.critical("An error occurred setting up node during setup %s",
+                              node.name)
+              raise
+
+        components = self.flatten_component_tree(rebuild=True)
+        for component in components:
+            try:
+                component.setup()
+            except Exception as err:
+                #reraise the exception after logging some info about source of error
+                logger.critical("An error occurred setting up component during setup %s",
+                                component.name)
+                raise
+
+        self.solver.setup(self)
+        self.reset()
+        self.dirty = False
+        logger.info('Setting up complete!')
+
+    def reset(self, start=None):
+        """Reset model to it's initial conditions"""
+        logger.info('Resetting model ...')
+        length_changed = self.timestepper.reset(start=start)
+        for node in self.nodes:
+            if length_changed:
+                try:
+                    node.setup(self)
+                except Exception as err:
+                    #reraise the exception after logging some info about source of error
+                    logger.critical("An error occurred calling setup while resetting node %s",node.name)
+                    raise
+            try:
+                node.reset()
+            except Exception as err:
+                #reraise the exception after logging some info about source of error
+                logger.critical("An error occurred calling reset on node %s",node.name)
+                raise
+
+        components = self.flatten_component_tree(rebuild=False)
+        for component in components:
+            if length_changed:
+                try:
+                    component.setup()
+                except Exception as err:
+                    #reraise the exception after logging some info about source of error
+                    logger.critical("An error occurred calling setup while resetting component %s",
+                                    component.name)
+                    raise
+
+            try:
+                component.reset()
+            except Exception as err:
+                #reraise the exception after logging some info about source of error
+                logger.critical("An error occurred calling reset on component %s",
+                                component.name)
+                raise
+
+        self.solver.reset()
+        # reset the timers
+        self._time_before = 0.0
+        self._time_after = 0.0
+        logger.info('Reset complete!')
+
+    def before(self):
+        """ Perform initialisation work before solve on each timestep.
+
+        This method calls the `before()` method on all nodes and components
+        in the model. Nodes are updated first, components second.
+
+        See also
+        --------
+        `Model.step`
+        """
+        cdef AbstractNode node
+        cdef Component component
+        cdef BaseParameter param
+        cdef double t0 = time.time()
+        for node in self.graph.nodes():
+            node.before(self.timestep)
+        cdef list components = self.flatten_component_tree(rebuild=False)
+        for component in components:
+            component.before()
+        for component in components:
+            if isinstance(component, BaseParameter):
+                param = component
+                param.calc_values(self.timestep)
+        self._time_before += time.time() - t0
+
+    def after(self):
+        cdef AbstractNode node
+        cdef Component component
+        cdef double t0 = time.time()
+        for node in self.graph.nodes():
+            node.after(self.timestep)
+        cdef list components = self.flatten_component_tree(rebuild=False)
+        for component in components:
+            component.after()
+        self._time_after += time.time() - t0
+
+    def finish(self):
+        for node in self.graph.nodes():
+            node.finish()
+        components = self.flatten_component_tree(rebuild=False)
+        for component in components:
+            try:
+                component.finish()
+            except Exception as err:
+                #reraise the exception after logging some info about source of error
+                logger.critical("An error occurred finishing component %s", component.name)
+                raise
+
+    def to_dataframe(self):
+        """ Return a DataFrame from any Recorders with a `to_dataframe` attribute
+
+        """
+        dfs = {r.name: r.to_dataframe() for r in self.recorders if hasattr(r, 'to_dataframe')}
+        df = pandas.concat(dfs, axis=1)
+        df.columns.set_names('Recorder', level=0, inplace=True)
+        return df
+
+    def flatten_component_tree(self, rebuild=False):
+        if self.component_tree_flat is None or rebuild is True:
+            self.component_tree_flat = []
+            G = self.component_graph
+
+            # Test some properties of the dependency tree
+            # Do not permit cycles in the dependencies
+            ncycles = len(list(nx.simple_cycles(G)))
+            if ncycles != 0:
+                raise ModelStructureError("Cyclical ({}) dependencies found in the model's components.".format(ncycles))
+            # Do not permit self-loops
+            for n in nx.nodes_with_selfloops(G):
+                raise ModelStructureError('Component "{}" contains a self-loop.'.format(n))
+
+            for node in nx.dfs_postorder_nodes(G, ROOT_NODE):
+                if node == ROOT_NODE:
+                    continue
+                self.component_tree_flat.append(node)
+            # order components so that they can be iterated over easily in an
+            # sequence which respects dependencies
+            self.component_tree_flat = self.component_tree_flat[::-1]
+        return self.component_tree_flat
+
+    def find_orphaned_parameters(self):
+        """Helper function to find orphaned parameters
+
+        Returns a set of parameters which:
+            1) Have no parent components
+            2) Are not referenced directly by a node
+        """
+        all_parameters = set(self.parameters)
+        visited = set()
+        # add all parameters referenced by another component
+        for parameter in self.components:
+            if parameter.parents:
+                visited.add(parameter)
+        # find all parameters referenced by a node
+        for node in self.graph.nodes():
+            for component in node.components:
+                visited.add(component)
+        # identify unseen parameters
+        orphans = all_parameters - visited
+        return orphans
+
+
+class NodeIterator(object):
+    """Iterator for Nodes in a Model which also supports indexing
+
+    Notes
+    -----
+    Although it's not very efficient to have to read through all of the nodes
+    in a model when accessing one by name (e.g. model.nodes['reservoir']), it's
+    easier than having to keep a dictionary up to date. The solvers should
+    avoid using this class, and use Model.graph.nodes() directly.
+    """
+    def __init__(self, model):
+        self.model = model
+        self.position = 0
+        self.length = None
+
+    def _nodes(self, hide_children=True):
+        for node in self.model.graph.nodes():
+            if hide_children is False or node.parent is None:  # don't return child nodes (e.g. StorageInput)
+                yield node
+        return
+
+    def __getitem__(self, key):
+        """Get a node from the graph by it's name"""
+        for node in self._nodes(hide_children=False):
+            if node.name == key:
+                return node
+        raise KeyError("'{}'".format(key))
+
+    def __delitem__(self, key):
+        """Remove a node from the graph"""
+        if isinstance(key, str):
+            node = self[key]
+        else:
+            node = key
+        # recursive delete to remove all sub-nodes
+        nodes_to_delete = []
+        for node2 in self.model.graph.nodes():
+            if node2.parent == node:
+                nodes_to_delete.append(node2)
+        # Avoid dictionary modification
+        for node2 in nodes_to_delete:
+            del(self[node2])
+        self.model.graph.remove_node(node)
+
+    def keys(self):
+        for node in self._nodes():
+            yield node.name
+
+    def values(self):
+        for node in self._nodes():
+            yield node
+
+    def items(self):
+        for node in self._nodes():
+            yield (node.name, node)
+
+    def __len__(self):
+        """Returns the number of nodes in the model"""
+        return len(list(self._nodes()))
+
+    def __contains__(self, value):
+        for node in self._nodes():
+            if node.name == value or node == value:
+                return True
+        return False
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        return self.next()
+
+    def next(self):
+        if self.position == 0:
+            self.nodes = list(self._nodes())
+            self.length = len(self.nodes)
+        if self.position < self.length:
+            node = self.nodes[self.position]
+            self.position += 1
+            return node
+        raise StopIteration()
+
+
+class NamedIterator(object):
+    def __init__(self, objects=None):
+        if objects:
+            self._objects = list(objects)
+        else:
+            self._objects = []
+
+    def __getitem__(self, key):
+        """Get a node from the graph by it's name"""
+        for obj in self._objects:
+            if obj.name == key:
+                return obj
+        raise KeyError("'{}'".format(key))
+
+    def __delitem__(self, key):
+        """Remove a node from the graph by it's name"""
+        obj = self[key]
+        self._objects.remove(obj)
+
+    def __setitem__(self, key, obj):
+        # TODO: check for name collisions / duplication
+        self._objects.append(obj)
+
+    def keys(self):
+        for obj in self._objects:
+            yield obj.name
+
+    def values(self):
+        for obj in self._objects:
+            yield obj
+
+    def items(self):
+        for obj in self._objects:
+            yield (obj.name, obj)
+
+    def __len__(self):
+        """Returns the number of nodes in the model"""
+        return len(self._objects)
+
+    def __iter__(self):
+        return iter(self._objects)
+
+    def __contains__(self, value):
+        for obj in self._objects:
+            if obj.name == value or obj == value:
+                return True
+        return False
+
+    def append(self, obj):
+        # TODO: check for name collisions / duplication
+        self._objects.append(obj)
+
+
+class ModelResult(object):
+    def __init__(self, num_scenarios, timestep, time_taken, time_taken_before, time_taken_after, time_taken_with_overhead,
+                 speed, solver_name, solver_stats, version):
+        self.timestep = timestep
+        self.timesteps = timestep.index + 1
+        self.time_taken = time_taken
+        self.time_taken_before = time_taken_before
+        self.time_taken_after = time_taken_after
+        self.time_taken_with_overhead = time_taken_with_overhead
+        self.speed = speed
+        self.num_scenarios = num_scenarios
+        self.solver_name = solver_name
+        self.solver_stats = solver_stats
+        self.version = version
+
+    def to_dict(self):
+        return {attr: value for attr, value in self.__dict__.items()}
+
+    def to_dataframe(self):
+        d = self.to_dict()
+        # Update timestep to use the underlying pandas Timestamp
+        d['timestep'] = d['timestep'].datetime
+        # Must flatten the solver stats dict before passing to pandas
+        solver_stats = d.pop('solver_stats')
+        for k, v in solver_stats.items():
+            d['solver_stats.{}'.format(k)] = v
+        df = pandas.DataFrame(pandas.Series(d), columns=["Value"])
+        return df
+
+    def __repr__(self):
+        return "Model executed {:d} scenarios in {:.1f} seconds, running at {:.1f} timesteps per second.".format(
+            self.num_scenarios, self.time_taken_with_overhead, self.speed)
+
+    def _repr_html_(self):
+        return self.to_dataframe()._repr_html_()
+
+
+def listify(f):
+    @wraps(f)
+    def wrapper(*args, **kwargs):
+        return list(f(*args, **kwargs))
+    return wrapper
```

### Comparing `pywr-1.8.0/pywr/domains/groundwater.py` & `pywr-1.9.0/pywr/domains/groundwater.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,111 +1,111 @@
-from ..core import Storage
-from ..parameters import InterpolatedVolumeParameter
-from ..parameters.groundwater import KeatingStreamFlowParameter
-
-import numbers
-from scipy.interpolate import interp1d
-
-
-class KeatingAquifer(Storage):
-    def __init__(self, model, name,
-                 num_streams, num_additional_inputs,
-                 stream_flow_levels, transmissivity, coefficient,
-                 levels, volumes=None, area=None, storativity=None,
-                 **kwargs):
-        """Storage node with one or more Keating outflows
-
-        Parameters
-        ----------
-        model : pywr.core.Model
-            The Pywr Model.
-        name : string
-            A unique name for the node in the model.
-        num_streams : integer
-            Number of keating outflows.
-        num_additional_inputs : integer
-            Number of additional outflows (e.g. for direct abstraction or
-            discharge from the aquifer).
-        stream_flow_levels : list of list of floats
-            For each stream a list of levels to pass to the keating streamflow
-            parameter.
-        transmissivity : list of floats
-            The transmissivity for each stream flow level.
-        coefficient : list of floats
-            The coefficient for each stream flow level.
-        levels : list of floats
-            A list of levels for the level-volume relationship. The length
-            should be greater than 1.
-        volumes : list of floats (optional)
-            A list of volumes for the level-volume relationship. The length
-            should be the same as `levels`.
-        area : float (optional)
-            Area of the aquifer in m2.
-        storativity : list of floats (optional)
-            Storativity of the aquifer as a factor (e.g. 0.05). This defines
-            part of the volume-level relationship. The length should be one
-            less than `levels`.
-
-        Either supply the `volumes` argument or both the `area` and
-        `storativity` arguments.
-
-        See also documentation for the `KeatingStreamFlowParameter`.
-        """
-        super(KeatingAquifer, self).__init__(model, name,
-            num_inputs=(num_streams + num_additional_inputs), **kwargs)
-
-        if not (num_streams > 0):
-            raise ValueError("Keating aquifer must have at least one stream outflow")
-        if len(stream_flow_levels) != num_streams:
-            raise ValueError("Stream flow levels must have `num_streams` items")
-        for i in stream_flow_levels:
-            if len(i) != len(transmissivity):
-                raise ValueError("Items in stream flow levels should have the same length as transmissivity")
-        if not isinstance(coefficient, numbers.Number):
-            raise ValueError("Coefficient must be a scalar")
-
-        if volumes is None:
-            if not isinstance(area, numbers.Number):
-                raise ValueError("Area must be a scalar")
-            if len(storativity) != (len(levels) - 1):
-                raise ValueError("Storativity must have one less item than levels")
-            heights = [levels[n+1] - levels[n] for n in range(0, len(levels)-1)]
-            volumes = [0.0]
-            for n, (s, h) in enumerate(zip(storativity, heights)):
-                volumes.append(volumes[-1] + area * s * h * 0.001)
-        else:
-            # check volumes
-            if len(volumes) != len(levels):
-                raise ValueError("Volumes must have the same length as levels")
-
-        self.area = area
-
-        if len(levels) != len(volumes):
-            raise ValueError("Levels and volumes must have the same length")
-
-        self._volumes = volumes
-        self._levels = levels
-        self._level_to_volume = interp1d(levels, volumes)
-
-        self.max_volume = max(volumes)
-        self.min_volume = min(volumes)
-
-        self.level = InterpolatedVolumeParameter(model, self, volumes, levels)
-
-        # initialise streamflow parameters
-        for n, node in enumerate(self.inputs[0:num_streams]):
-            parameter = KeatingStreamFlowParameter(model, self, stream_flow_levels[n],
-                                                   transmissivity,
-                                                   coefficient)
-            node.max_flow = parameter
-            node.min_flow = parameter
-
-    def initial_level():
-        def fget(self):
-            # get the initial level from the volume
-            return self.level.interp(self.initial_volume)
-        def fset(self, value):
-            # actually sets the initial volume
-            volume = self._level_to_volume(value)
-            self.initial_volume = volume
-        return locals()
-    initial_level = property(**initial_level())
+from ..core import Storage
+from ..parameters import InterpolatedVolumeParameter
+from ..parameters.groundwater import KeatingStreamFlowParameter
+
+import numbers
+from scipy.interpolate import interp1d
+
+
+class KeatingAquifer(Storage):
+    def __init__(self, model, name,
+                 num_streams, num_additional_inputs,
+                 stream_flow_levels, transmissivity, coefficient,
+                 levels, volumes=None, area=None, storativity=None,
+                 **kwargs):
+        """Storage node with one or more Keating outflows
+
+        Parameters
+        ----------
+        model : pywr.core.Model
+            The Pywr Model.
+        name : string
+            A unique name for the node in the model.
+        num_streams : integer
+            Number of keating outflows.
+        num_additional_inputs : integer
+            Number of additional outflows (e.g. for direct abstraction or
+            discharge from the aquifer).
+        stream_flow_levels : list of list of floats
+            For each stream a list of levels to pass to the keating streamflow
+            parameter.
+        transmissivity : list of floats
+            The transmissivity for each stream flow level.
+        coefficient : list of floats
+            The coefficient for each stream flow level.
+        levels : list of floats
+            A list of levels for the level-volume relationship. The length
+            should be greater than 1.
+        volumes : list of floats (optional)
+            A list of volumes for the level-volume relationship. The length
+            should be the same as `levels`.
+        area : float (optional)
+            Area of the aquifer in m2.
+        storativity : list of floats (optional)
+            Storativity of the aquifer as a factor (e.g. 0.05). This defines
+            part of the volume-level relationship. The length should be one
+            less than `levels`.
+
+        Either supply the `volumes` argument or both the `area` and
+        `storativity` arguments.
+
+        See also documentation for the `KeatingStreamFlowParameter`.
+        """
+        super(KeatingAquifer, self).__init__(model, name,
+            num_inputs=(num_streams + num_additional_inputs), **kwargs)
+
+        if not (num_streams > 0):
+            raise ValueError("Keating aquifer must have at least one stream outflow")
+        if len(stream_flow_levels) != num_streams:
+            raise ValueError("Stream flow levels must have `num_streams` items")
+        for i in stream_flow_levels:
+            if len(i) != len(transmissivity):
+                raise ValueError("Items in stream flow levels should have the same length as transmissivity")
+        if not isinstance(coefficient, numbers.Number):
+            raise ValueError("Coefficient must be a scalar")
+
+        if volumes is None:
+            if not isinstance(area, numbers.Number):
+                raise ValueError("Area must be a scalar")
+            if len(storativity) != (len(levels) - 1):
+                raise ValueError("Storativity must have one less item than levels")
+            heights = [levels[n+1] - levels[n] for n in range(0, len(levels)-1)]
+            volumes = [0.0]
+            for n, (s, h) in enumerate(zip(storativity, heights)):
+                volumes.append(volumes[-1] + area * s * h * 0.001)
+        else:
+            # check volumes
+            if len(volumes) != len(levels):
+                raise ValueError("Volumes must have the same length as levels")
+
+        self.area = area
+
+        if len(levels) != len(volumes):
+            raise ValueError("Levels and volumes must have the same length")
+
+        self._volumes = volumes
+        self._levels = levels
+        self._level_to_volume = interp1d(levels, volumes)
+
+        self.max_volume = max(volumes)
+        self.min_volume = min(volumes)
+
+        self.level = InterpolatedVolumeParameter(model, self, volumes, levels)
+
+        # initialise streamflow parameters
+        for n, node in enumerate(self.inputs[0:num_streams]):
+            parameter = KeatingStreamFlowParameter(model, self, stream_flow_levels[n],
+                                                   transmissivity,
+                                                   coefficient)
+            node.max_flow = parameter
+            node.min_flow = parameter
+
+    def initial_level():
+        def fget(self):
+            # get the initial level from the volume
+            return self.level.interp(self.initial_volume)
+        def fset(self, value):
+            # actually sets the initial volume
+            volume = self._level_to_volume(value)
+            self.initial_volume = volume
+        return locals()
+    initial_level = property(**initial_level())
```

### Comparing `pywr-1.8.0/pywr/domains/river.py` & `pywr-1.9.0/pywr/domains/river.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,248 +1,248 @@
-
-from pywr.nodes import Node, Domain, Input, Output, Link, Storage, PiecewiseLink, MultiSplitLink
-from pywr.parameters import pop_kwarg_parameter, ConstantParameter, Parameter, load_parameter
-from pywr.parameters.control_curves import ControlCurveParameter
-
-DEFAULT_RIVER_DOMAIN = Domain(name='river', color='#33CCFF')
-
-class RiverDomainMixin(object):
-    def __init__(self, *args, **kwargs):
-        # if 'domain' not in kwargs:
-        #     kwargs['domain'] = DEFAULT_RIVER_DOMAIN
-        if 'color' not in kwargs:
-            self.color = '#6ECFF6' # blue
-        super(RiverDomainMixin, self).__init__(*args, **kwargs)
-
-
-class Catchment(RiverDomainMixin, Input):
-    """A hydrological catchment, supplying water to the river network"""
-    def __init__(self, *args, **kwargs):
-        """Initialise a new Catchment node.
-
-        A Catchment is an input node with a fixed inflow. I.e. min_flow and
-        max_flow are the same. The value is specified as a flow keyword, and
-        overrides any min_flow or max_flow keyword arguments.
-
-        Parameters
-        ----------
-        flow : float or function
-            The amount of water supplied by the catchment each timestep
-        """
-        self.color = '#82CA9D' # green
-        # Grab flow from kwargs
-        flow = kwargs.pop('flow', 0.0)
-        # Min/max flow set in super inits
-        super(Catchment, self).__init__(*args, **kwargs)
-        self.flow = flow
-
-    def get_flow(self, timestep):
-        """ flow is ensured that both min_flow and max_flow are the same. """
-        return self.get_min_flow(timestep)
-
-    def __setattr__(self, name, value):
-        if name == 'flow':
-            self.min_flow = value
-            self.max_flow = value
-            return
-        super(Catchment, self).__setattr__(name, value)
-
-    @classmethod
-    def load(cls, data, model):
-        flow = data.pop('flow', 0.0)
-        if flow is not None:
-            flow = load_parameter(model, flow)
-        node = super(Catchment, cls).load(data, model)
-        node.flow = flow
-        return node
-
-
-class Reservoir(RiverDomainMixin, Storage):
-    """A reservoir node with control curve.
-
-    The Reservoir is a subclass of Storage with additional functionality to provide a
-    simple control curve. The Reservoir has above_curve_cost when it is above its curve
-    and the user defined cost when it is below. Typically the costs are negative
-    to represent a benefit of filling the reservoir when it is below its curve.
-    """
-    def __init__(self, model, *args, **kwargs):
-        """
-
-        Keywords:
-            control_curve - A Parameter object that can return the control curve position,
-                as a percentage of fill, for the given timestep.
-        """
-        control_curve = pop_kwarg_parameter(kwargs, 'control_curve', None)
-        above_curve_cost = kwargs.pop('above_curve_cost', None)
-        cost = kwargs.pop('cost', 0.0)
-        if above_curve_cost is not None:
-            if control_curve is None:
-                # Make a default control curve at 100% capacity
-                control_curve = ConstantParameter(model, 1.0)
-            elif not isinstance(control_curve, Parameter):
-                # Assume parameter is some kind of constant and coerce to ConstantParameter
-                control_curve = ConstantParameter(model, control_curve)
-
-            if not isinstance(cost, Parameter):
-                # In the case where an above_curve_cost is given and cost is not a Parameter
-                # a default cost Parameter is created.
-                kwargs['cost'] = ControlCurveParameter(model, self, control_curve, [above_curve_cost, cost])
-            else:
-                raise ValueError('If an above_curve_cost is given cost must not be a Parameter.')
-        else:
-            # reinstate the given cost parameter to pass to the parent constructors
-            kwargs['cost'] = cost
-        super(Reservoir, self).__init__(model, *args, **kwargs)
-
-
-class River(RiverDomainMixin, Link):
-    """A node in the river network
-
-    This node may have multiple upstream nodes (i.e. a confluence) but only
-    one downstream node.
-    """
-    def __init__(self, *args, **kwargs):
-        super(River, self).__init__(*args, **kwargs)
-
-
-class RiverSplit(MultiSplitLink):
-    """A split in the river network
-
-    RiverSplit is a specialised version of `pywr.nodes.MultiSplitLink` with a more convenient init method.
-     It is intended for a simple case of where fixed ratio of flow is required to be distributed
-     to multiple downstream routes.
-
-    Parameters
-    ----------
-    factors : iterable of floats
-        The factors to force on the additional splits. Number of extra_slot is assumed to be one less
-        than the length of factors (as per `pywr.nodes.MultiSplitLink` documentation).
-    slot_names : iterable
-        The identifiers to refer to the slots when connect from this Node. Length must be one more than
-         the number of extra slots required.
-
-    See also
-    --------
-    pywr.nodes.MultiSplitLink
-
-    """
-    def __init__(self, *args, **kwargs):
-        def _make_iterable(val):
-            try:
-                len(val)
-            except TypeError:
-                return [val]
-            return val
-
-        factors = _make_iterable(kwargs.pop('factors'))
-        extra_slots = len(factors) - 1
-
-        # These are the defaults to pass to the parent class that makes this
-        # class a convenience.
-        # create keyword arguments for PiecewiseLink
-        kwargs['cost'] = _make_iterable(kwargs.pop('cost', 0.0))
-        kwargs['max_flow'] = _make_iterable(kwargs.pop('max_flow', None))
-        kwargs['extra_slots'] = extra_slots
-        kwargs['factors'] = factors
-
-        super(RiverSplit, self).__init__(*args, **kwargs)
-
-    @classmethod
-    def load(cls, data, model):
-        max_flow = load_parameter(model, data.pop('max_flow', None))
-        cost = load_parameter(model, data.pop('cost', 0.0))
-
-        del(data["type"])
-        return cls(model, max_flow=max_flow, cost=cost, **data)
-
-
-class RiverSplitWithGauge(RiverSplit):
-    """A split in the river network with a minimum residual flow
-
-    As per `RiverSplit` but by default creates another route in the underlying object
-     to model a MRF. This route is such that the MRF is not part of forced ratios. The
-     intent of this object is to model the case where a proportion of flow can be
-     abstracted above the MRF (e.g. 90% of flow above MRF).
-
-    Parameters
-    ----------
-    mrf : float
-        The minimum residual flow (MRF) at the gauge
-    mrf_cost : float
-        The cost of the route via the MRF
-    cost : float
-        The cost of the other (unconstrained) route
-    factors : iterable of floats
-        The factors to force on the additional splits. Number of extra_slot is assumed to be one less
-        than the length of factors (as per `MultiSplitLink` documentation).
-    slot_names : iterable
-        The identifiers to refer to the slots when connect from this Node. Length must be one more than
-         the number of extra slots required.
-    """
-    def __init__(self, model, name, mrf=0.0, cost=0.0, mrf_cost=0.0, **kwargs):
-        kwargs['cost'] = [mrf_cost, cost]
-        kwargs['max_flow'] = [mrf, None]
-        super(RiverSplitWithGauge, self).__init__(model, name, **kwargs)
-
-    @classmethod
-    def load(cls, data, model):
-        cost = load_parameter(model, data.pop('cost', 0.0))
-        mrf_cost = load_parameter(model, data.pop('mrf_cost', 0.0))
-        mrf = load_parameter(model, data.pop('mrf', 0.0))
-        name = data.pop("name")
-        data.pop("type", None)
-        parameter = cls(model, name, mrf=mrf, cost=cost, mrf_cost=mrf_cost, **data)
-        return parameter
-
-class Discharge(Catchment):
-    """An inline discharge to the river network
-
-    This node is similar to a catchment, but sits inline to the river network,
-    rather than at the head of the river.
-    """
-    pass
-
-class RiverGauge(RiverDomainMixin, PiecewiseLink):
-    """A river gauging station, with a minimum residual flow (MRF)
-    """
-    def __init__(self, *args, **kwargs):
-        """Initialise a new RiverGauge instance
-
-        Parameters
-        ----------
-        mrf : float
-            The minimum residual flow (MRF) at the gauge
-        mrf_cost : float
-            The cost of the route via the MRF
-        cost : float
-            The cost of the other (unconstrained) route
-        """
-        # create keyword arguments for PiecewiseLink
-        cost = kwargs.pop('cost', 0.0)
-        kwargs['cost'] = [kwargs.pop('mrf_cost', 0.0), cost]
-        kwargs['max_flow'] = [kwargs.pop('mrf', 0.0), None]
-        super(RiverGauge, self).__init__(*args, **kwargs)
-
-    def mrf():
-        def fget(self):
-            return self.sublinks[0].max_flow
-        def fset(self, value):
-            self.sublinks[0].max_flow = value
-        return locals()
-    mrf = property(**mrf())
-
-    def mrf_cost():
-        def fget(self):
-            return self.sublinks[0].cost
-        def fset(self, value):
-            self.sublinks[0].cost = value
-        return locals()
-    mrf_cost = property(**mrf_cost())
-
-    @classmethod
-    def load(cls, data, model):
-        mrf = load_parameter(model, data.pop("mrf"))
-        mrf_cost = load_parameter(model, data.pop("mrf_cost"))
-        cost = load_parameter(model, data.pop("cost", 0.0))
-        del(data["type"])
-        node = cls(model, mrf=mrf, mrf_cost=mrf_cost, cost=cost, **data)
-        return node
+
+from pywr.nodes import Node, Domain, Input, Output, Link, Storage, PiecewiseLink, MultiSplitLink
+from pywr.parameters import pop_kwarg_parameter, ConstantParameter, Parameter, load_parameter
+from pywr.parameters.control_curves import ControlCurveParameter
+
+DEFAULT_RIVER_DOMAIN = Domain(name='river', color='#33CCFF')
+
+class RiverDomainMixin(object):
+    def __init__(self, *args, **kwargs):
+        # if 'domain' not in kwargs:
+        #     kwargs['domain'] = DEFAULT_RIVER_DOMAIN
+        if 'color' not in kwargs:
+            self.color = '#6ECFF6' # blue
+        super(RiverDomainMixin, self).__init__(*args, **kwargs)
+
+
+class Catchment(RiverDomainMixin, Input):
+    """A hydrological catchment, supplying water to the river network"""
+    def __init__(self, *args, **kwargs):
+        """Initialise a new Catchment node.
+
+        A Catchment is an input node with a fixed inflow. I.e. min_flow and
+        max_flow are the same. The value is specified as a flow keyword, and
+        overrides any min_flow or max_flow keyword arguments.
+
+        Parameters
+        ----------
+        flow : float or function
+            The amount of water supplied by the catchment each timestep
+        """
+        self.color = '#82CA9D' # green
+        # Grab flow from kwargs
+        flow = kwargs.pop('flow', 0.0)
+        # Min/max flow set in super inits
+        super(Catchment, self).__init__(*args, **kwargs)
+        self.flow = flow
+
+    def get_flow(self, timestep):
+        """ flow is ensured that both min_flow and max_flow are the same. """
+        return self.get_min_flow(timestep)
+
+    def __setattr__(self, name, value):
+        if name == 'flow':
+            self.min_flow = value
+            self.max_flow = value
+            return
+        super(Catchment, self).__setattr__(name, value)
+
+    @classmethod
+    def load(cls, data, model):
+        flow = data.pop('flow', 0.0)
+        if flow is not None:
+            flow = load_parameter(model, flow)
+        node = super(Catchment, cls).load(data, model)
+        node.flow = flow
+        return node
+
+
+class Reservoir(RiverDomainMixin, Storage):
+    """A reservoir node with control curve.
+
+    The Reservoir is a subclass of Storage with additional functionality to provide a
+    simple control curve. The Reservoir has above_curve_cost when it is above its curve
+    and the user defined cost when it is below. Typically the costs are negative
+    to represent a benefit of filling the reservoir when it is below its curve.
+    """
+    def __init__(self, model, *args, **kwargs):
+        """
+
+        Keywords:
+            control_curve - A Parameter object that can return the control curve position,
+                as a percentage of fill, for the given timestep.
+        """
+        control_curve = pop_kwarg_parameter(kwargs, 'control_curve', None)
+        above_curve_cost = kwargs.pop('above_curve_cost', None)
+        cost = kwargs.pop('cost', 0.0)
+        if above_curve_cost is not None:
+            if control_curve is None:
+                # Make a default control curve at 100% capacity
+                control_curve = ConstantParameter(model, 1.0)
+            elif not isinstance(control_curve, Parameter):
+                # Assume parameter is some kind of constant and coerce to ConstantParameter
+                control_curve = ConstantParameter(model, control_curve)
+
+            if not isinstance(cost, Parameter):
+                # In the case where an above_curve_cost is given and cost is not a Parameter
+                # a default cost Parameter is created.
+                kwargs['cost'] = ControlCurveParameter(model, self, control_curve, [above_curve_cost, cost])
+            else:
+                raise ValueError('If an above_curve_cost is given cost must not be a Parameter.')
+        else:
+            # reinstate the given cost parameter to pass to the parent constructors
+            kwargs['cost'] = cost
+        super(Reservoir, self).__init__(model, *args, **kwargs)
+
+
+class River(RiverDomainMixin, Link):
+    """A node in the river network
+
+    This node may have multiple upstream nodes (i.e. a confluence) but only
+    one downstream node.
+    """
+    def __init__(self, *args, **kwargs):
+        super(River, self).__init__(*args, **kwargs)
+
+
+class RiverSplit(MultiSplitLink):
+    """A split in the river network
+
+    RiverSplit is a specialised version of `pywr.nodes.MultiSplitLink` with a more convenient init method.
+     It is intended for a simple case of where fixed ratio of flow is required to be distributed
+     to multiple downstream routes.
+
+    Parameters
+    ----------
+    factors : iterable of floats
+        The factors to force on the additional splits. Number of extra_slot is assumed to be one less
+        than the length of factors (as per `pywr.nodes.MultiSplitLink` documentation).
+    slot_names : iterable
+        The identifiers to refer to the slots when connect from this Node. Length must be one more than
+         the number of extra slots required.
+
+    See also
+    --------
+    pywr.nodes.MultiSplitLink
+
+    """
+    def __init__(self, *args, **kwargs):
+        def _make_iterable(val):
+            try:
+                len(val)
+            except TypeError:
+                return [val]
+            return val
+
+        factors = _make_iterable(kwargs.pop('factors'))
+        extra_slots = len(factors) - 1
+
+        # These are the defaults to pass to the parent class that makes this
+        # class a convenience.
+        # create keyword arguments for PiecewiseLink
+        kwargs['cost'] = _make_iterable(kwargs.pop('cost', 0.0))
+        kwargs['max_flow'] = _make_iterable(kwargs.pop('max_flow', None))
+        kwargs['extra_slots'] = extra_slots
+        kwargs['factors'] = factors
+
+        super(RiverSplit, self).__init__(*args, **kwargs)
+
+    @classmethod
+    def load(cls, data, model):
+        max_flow = load_parameter(model, data.pop('max_flow', None))
+        cost = load_parameter(model, data.pop('cost', 0.0))
+
+        del(data["type"])
+        return cls(model, max_flow=max_flow, cost=cost, **data)
+
+
+class RiverSplitWithGauge(RiverSplit):
+    """A split in the river network with a minimum residual flow
+
+    As per `RiverSplit` but by default creates another route in the underlying object
+     to model a MRF. This route is such that the MRF is not part of forced ratios. The
+     intent of this object is to model the case where a proportion of flow can be
+     abstracted above the MRF (e.g. 90% of flow above MRF).
+
+    Parameters
+    ----------
+    mrf : float
+        The minimum residual flow (MRF) at the gauge
+    mrf_cost : float
+        The cost of the route via the MRF
+    cost : float
+        The cost of the other (unconstrained) route
+    factors : iterable of floats
+        The factors to force on the additional splits. Number of extra_slot is assumed to be one less
+        than the length of factors (as per `MultiSplitLink` documentation).
+    slot_names : iterable
+        The identifiers to refer to the slots when connect from this Node. Length must be one more than
+         the number of extra slots required.
+    """
+    def __init__(self, model, name, mrf=0.0, cost=0.0, mrf_cost=0.0, **kwargs):
+        kwargs['cost'] = [mrf_cost, cost]
+        kwargs['max_flow'] = [mrf, None]
+        super(RiverSplitWithGauge, self).__init__(model, name, **kwargs)
+
+    @classmethod
+    def load(cls, data, model):
+        cost = load_parameter(model, data.pop('cost', 0.0))
+        mrf_cost = load_parameter(model, data.pop('mrf_cost', 0.0))
+        mrf = load_parameter(model, data.pop('mrf', 0.0))
+        name = data.pop("name")
+        data.pop("type", None)
+        parameter = cls(model, name, mrf=mrf, cost=cost, mrf_cost=mrf_cost, **data)
+        return parameter
+
+class Discharge(Catchment):
+    """An inline discharge to the river network
+
+    This node is similar to a catchment, but sits inline to the river network,
+    rather than at the head of the river.
+    """
+    pass
+
+class RiverGauge(RiverDomainMixin, PiecewiseLink):
+    """A river gauging station, with a minimum residual flow (MRF)
+    """
+    def __init__(self, *args, **kwargs):
+        """Initialise a new RiverGauge instance
+
+        Parameters
+        ----------
+        mrf : float
+            The minimum residual flow (MRF) at the gauge
+        mrf_cost : float
+            The cost of the route via the MRF
+        cost : float
+            The cost of the other (unconstrained) route
+        """
+        # create keyword arguments for PiecewiseLink
+        cost = kwargs.pop('cost', 0.0)
+        kwargs['cost'] = [kwargs.pop('mrf_cost', 0.0), cost]
+        kwargs['max_flow'] = [kwargs.pop('mrf', 0.0), None]
+        super(RiverGauge, self).__init__(*args, **kwargs)
+
+    def mrf():
+        def fget(self):
+            return self.sublinks[0].max_flow
+        def fset(self, value):
+            self.sublinks[0].max_flow = value
+        return locals()
+    mrf = property(**mrf())
+
+    def mrf_cost():
+        def fget(self):
+            return self.sublinks[0].cost
+        def fset(self, value):
+            self.sublinks[0].cost = value
+        return locals()
+    mrf_cost = property(**mrf_cost())
+
+    @classmethod
+    def load(cls, data, model):
+        mrf = load_parameter(model, data.pop("mrf"))
+        mrf_cost = load_parameter(model, data.pop("mrf_cost"))
+        cost = load_parameter(model, data.pop("cost", 0.0))
+        del(data["type"])
+        node = cls(model, mrf=mrf, mrf_cost=mrf_cost, cost=cost, **data)
+        return node
```

### Comparing `pywr-1.8.0/pywr/h5tools.py` & `pywr-1.9.0/pywr/h5tools.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-import tables
-import os
-
-
-class H5Store(object):
-    def __init__(self, filename, filter_kwds=None, mode="r", title='', metadata=None, create_directories=False):
-        self._opened = False
-        if isinstance(filename, (str, os.PathLike)):
-            # filename is a path to open
-            self.filename = filename
-            # Note sure how else to deal with str / unicode requirements in pytables
-            # See this issue: https://github.com/PyTables/PyTables/issues/522
-            import sys
-            if filter_kwds:
-                if sys.version_info[0] == 2 and 'complib' in filter_kwds:
-                    filter_kwds['complib'] = filter_kwds['complib'].encode()
-                filters = tables.Filters(**filter_kwds)
-            else:
-                filters = None
-
-            # Create directories for the filename if required
-            if create_directories:
-                try:
-                    os.makedirs(os.path.dirname(filename))
-                except OSError as exception:
-                    import errno
-                    if exception.errno != errno.EEXIST:
-                        raise
-
-            self.file = tables.open_file(filename, mode=mode, filters=filters, title=title)
-            self._opened = True
-        elif isinstance(filename, tables.File):
-            # filename is a pytables file
-            self.file = filename
-            assert(self.file.isopen)
-            self.filename = self.file.filename
-            self._opened = False
-        else:
-            raise TypeError("{} must be initalised with a filename to open or an open tables.File".format(self.__class__.__name__))
-
-        # now update metadata if given
-        if metadata is not None and self.file.mode != 'r':
-            for k, v in metadata.items():
-                setattr(self.file.root._v_attrs, k, v)
-
-    def __del__(self):
-        if self._opened and self.file.isopen:
-            self.file.close()
+import tables
+import os
+
+
+class H5Store(object):
+    def __init__(self, filename, filter_kwds=None, mode="r", title='', metadata=None, create_directories=False):
+        self._opened = False
+        if isinstance(filename, (str, os.PathLike)):
+            # filename is a path to open
+            self.filename = filename
+            # Note sure how else to deal with str / unicode requirements in pytables
+            # See this issue: https://github.com/PyTables/PyTables/issues/522
+            import sys
+            if filter_kwds:
+                if sys.version_info[0] == 2 and 'complib' in filter_kwds:
+                    filter_kwds['complib'] = filter_kwds['complib'].encode()
+                filters = tables.Filters(**filter_kwds)
+            else:
+                filters = None
+
+            # Create directories for the filename if required
+            if create_directories:
+                try:
+                    os.makedirs(os.path.dirname(filename))
+                except OSError as exception:
+                    import errno
+                    if exception.errno != errno.EEXIST:
+                        raise
+
+            self.file = tables.open_file(filename, mode=mode, filters=filters, title=title)
+            self._opened = True
+        elif isinstance(filename, tables.File):
+            # filename is a pytables file
+            self.file = filename
+            assert(self.file.isopen)
+            self.filename = self.file.filename
+            self._opened = False
+        else:
+            raise TypeError("{} must be initalised with a filename to open or an open tables.File".format(self.__class__.__name__))
+
+        # now update metadata if given
+        if metadata is not None and self.file.mode != 'r':
+            for k, v in metadata.items():
+                setattr(self.file.root._v_attrs, k, v)
+
+    def __del__(self):
+        if self._opened and self.file.isopen:
+            self.file.close()
```

### Comparing `pywr-1.8.0/pywr/nodes.py` & `pywr-1.9.0/pywr/nodes.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,942 +1,1097 @@
-import numpy as np
-
-from pywr import _core
-from pywr._core import Node as BaseNode
-from pywr._core import (BaseInput, BaseLink, BaseOutput, StorageInput,
-    StorageOutput, Timestep, ScenarioIndex)
-
-from pywr.parameters import pop_kwarg_parameter, load_parameter, load_parameter_values, FlowDelayParameter
-
-from pywr.domains import Domain
-
-
-class Drawable(object):
-    """Mixin class for objects that are drawable on a diagram of the network.
-    """
-    def __init__(self, *args, **kwargs):
-        self.position = kwargs.pop('position', {})
-        self.color = kwargs.pop('color', 'black')
-        self.visible = kwargs.pop('visible', True)
-        super(Drawable, self).__init__(*args, **kwargs)
-
-
-class Connectable(object):
-    """A mixin class providing methods for connecting nodes in the model graph"""
-    def iter_slots(self, slot_name=None, is_connector=True):
-        """ Returns the object(s) wich should be connected to given slot_name
-
-        Overload this method when implementing compound nodes which have
-        multiple slots and may return something other than self.
-
-        is_connector is True when self's connect method has been used. I.e. self
-        is connecting to another object. This is useful for providing an
-        appropriate response object in circumstances where a subnode should make
-        the actual connection rather than self.
-        """
-        if slot_name is not None:
-            raise ValueError('{} does not have slot: {}'.format(self, slot_name))
-        yield self
-
-    def connect(self, node, from_slot=None, to_slot=None):
-        """Create an edge from this Node to another Node
-
-        Parameters
-        ----------
-        node : Node
-            The node to connect to
-        from_slot : object (optional)
-            The outgoing slot on this node to connect to
-        to_slot : object (optional)
-            The incoming slot on the target node to connect to
-        """
-        if self.model is not node.model:
-            raise RuntimeError("Can't connect Nodes in different Models")
-        if not isinstance(node, Connectable):
-            raise TypeError("Other node ({}) is not connectable.".format(node))
-
-        # Get slot from this node
-        for node1 in self.iter_slots(slot_name=from_slot, is_connector=True):
-            # And slot to connect from other node
-            for node2 in node.iter_slots(slot_name=to_slot, is_connector=False):
-                self.model.graph.add_edge(node1, node2)
-        self.model.dirty = True
-
-    def disconnect(self, node=None, slot_name=None, all_slots=True):
-        """Remove a connection from this Node to another Node
-
-        Parameters
-        ----------
-        node : Node (optional)
-            The node to remove the connection to. If another node is not
-            specified, all connections from this node will be removed.
-        slot_name : integer (optional)
-            If specified, only remove the connection to a specific slot name.
-            Otherwise connections from all slots are removed.
-        """
-        if node is not None:
-            self._disconnect(node, slot_name=slot_name, all_slots=all_slots)
-        else:
-            neighbors = self.model.graph.neighbors(self)
-            for neighbor in [neighbor for neighbor in neighbors]:
-                self._disconnect(neighbor, slot_name=slot_name, all_slots=all_slots)
-
-    def _disconnect(self, node, slot_name=None, all_slots=True):
-        """As disconnect, except node argument is required"""
-        disconnected = False
-        try:
-            self.model.graph.remove_edge(self, node)
-        except:
-            for node_slot in node.iter_slots(slot_name=slot_name, is_connector=False, all_slots=all_slots):
-                try:
-                    self.model.graph.remove_edge(self, node_slot)
-                except nx.exception.NetworkXError:
-                    pass
-                else:
-                    disconnected = True
-        else:
-            disconnected = True
-        if not disconnected:
-            raise nx.exception.NetworkXError('{} is not connected to {}'.format(self, node))
-        self.model.dirty = True
-
-
-class NodeMeta(type):
-    """Node metaclass used to keep a registry of Node classes"""
-    # node subclasses are stored in a dict for convenience
-    node_registry = {}
-    def __new__(meta, name, bases, dct):
-        return super(NodeMeta, meta).__new__(meta, name, bases, dct)
-    def __init__(cls, name, bases, dct):
-        super(NodeMeta, cls).__init__(name, bases, dct)
-        cls.node_registry[name.lower()] = cls
-    def __call__(cls, *args, **kwargs):
-        # Create new instance of Node (or subclass thereof)
-        node = type.__call__(cls, *args, **kwargs)
-        # Add node to Model graph. This needs to be done here, so that if the
-        # __init__ method of Node raises an exception it is not added.
-        node.model.graph.add_node(node)
-        node.model.dirty = True
-        return node
-
-
-class Node(Drawable, Connectable, BaseNode, metaclass=NodeMeta):
-    """Base object from which all other nodes inherit
-
-    This BaseNode is not connectable by default, and the Node class should
-    be used for actual Nodes in the model. The BaseNode provides an abstract
-    class for other Node types (e.g. StorageInput) that are not directly
-    Connectable.
-    """
-    def __init__(self, model, name, **kwargs):
-        """Initialise a new Node object
-
-        Parameters
-        ----------
-        model : Model
-            The model the node belongs to
-        name : string
-            A unique name for the node
-        """
-        color = kwargs.pop('color', 'black')
-        min_flow = pop_kwarg_parameter(kwargs, 'min_flow', 0.0)
-        if min_flow is None:
-            min_flow = 0.0
-        max_flow = pop_kwarg_parameter(kwargs, 'max_flow', float('inf'))
-        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
-        conversion_factor = pop_kwarg_parameter(kwargs, 'conversion_factor', 1.0)
-
-        super(Node, self).__init__(model, name, **kwargs)
-
-        self.slots = {}
-        self.color = color
-        self.min_flow = min_flow
-        self.max_flow = max_flow
-        self.cost = cost
-        self.conversion_factor = conversion_factor
-
-    def check(self):
-        """Check the node is valid
-
-        Raises an exception if the node is invalid
-        """
-        pass
-
-    @classmethod
-    def load(cls, data, model):
-        name = data.pop('name')
-
-        cost = data.pop('cost', 0.0)
-        min_flow = data.pop('min_flow', None)
-        max_flow = data.pop('max_flow', None)
-
-        data.pop('type')
-        node = cls(model=model, name=name,
-                   **data)
-
-        cost = load_parameter(model, cost)
-        min_flow = load_parameter(model, min_flow)
-        max_flow = load_parameter(model, max_flow)
-        if cost is None:
-            cost = 0.0
-        if min_flow is None:
-            min_flow = 0.0
-        if max_flow is None:
-            max_flow = np.inf
-        node.cost = cost
-        node.min_flow = min_flow
-        node.max_flow = max_flow
-
-        return node
-
-
-class Input(Node, BaseInput):
-    """A general input at any point in the network
-
-    """
-    def __init__(self, *args, **kwargs):
-        """Initialise a new Input node
-
-        Parameters
-        ----------
-        min_flow : float (optional)
-            A simple minimum flow constraint for the input. Defaults to None
-        max_flow : float (optional)
-            A simple maximum flow constraint for the input. Defaults to 0.0
-        """
-        super(Input, self).__init__(*args, **kwargs)
-        self.color = '#F26C4F' # light red
-
-
-class Output(Node, BaseOutput):
-    """A general output at any point from the network
-
-    """
-    def __init__(self, *args, **kwargs):
-        """Initialise a new Output node
-
-        Parameters
-        ----------
-        min_flow : float (optional)
-            A simple minimum flow constraint for the output. Defaults to 0.0
-        max_flow : float (optional)
-            A simple maximum flow constraint for the output. Defaults to None
-        """
-        kwargs['color'] = kwargs.pop('color', '#FFF467')  # light yellow
-        super(Output, self).__init__(*args, **kwargs)
-
-
-class Link(Node, BaseLink):
-    """A link in the supply network, such as a pipe
-
-    Connections between Nodes in the network are created using edges (see the
-    Node.connect and Node.disconnect methods). However, these edges cannot
-    hold constraints (e.g. a maximum flow constraint). In this instance a Link
-    node should be used.
-    """
-    def __init__(self, *args, **kwargs):
-        """Initialise a new Link node
-
-        Parameters
-        ----------
-        max_flow : float or function (optional)
-            A maximum flow constraint on the link, e.g. 5.0
-        """
-        kwargs['color'] = kwargs.pop('color', '#A0A0A0')  # 45% grey
-        super(Link, self).__init__(*args, **kwargs)
-
-
-class Storage(Drawable, Connectable, _core.Storage, metaclass=NodeMeta):
-    """A generic storage Node
-
-    In terms of connections in the network the Storage node behaves like any
-    other node, provided there is only 1 input and 1 output. If there are
-    multiple sub-nodes the connections need to be explicit about which they
-    are connecting to. For example:
-
-    >>> storage(model, 'reservoir', num_outputs=1, num_inputs=2)
-    >>> supply.connect(storage)
-    >>> storage.connect(demand1, from_slot=0)
-    >>> storage.connect(demand2, from_slot=1)
-
-    The attribtues of the sub-nodes can be modified directly (and
-    independently). For example:
-
-    >>> storage.outputs[0].max_flow = 15.0
-
-    If a recorder is set on the storage node, instead of recording flow it
-    records changes in storage. Any recorders set on the output or input
-    sub-nodes record flow as normal.
-
-    Parameters
-    ----------
-    model : Model
-        Model instance to which this storage node is attached.
-    name : str
-        The name of the storage node.
-    num_inputs, num_outputs : integer (optional)
-        The number of input and output nodes to create internally. Defaults to 1.
-    min_volume : float (optional)
-        The minimum volume of the storage. Defaults to 0.0.
-    max_volume : float, Parameter (optional)
-        The maximum volume of the storage. Defaults to 0.0.
-    initial_volume, initial_volume_pc : float (optional)
-        Specify initial volume in either absolute or proportional terms. Both are required if `max_volume`
-        is a parameter because the parameter will not be evaluated at the first time-step. If both are given
-        and `max_volume` is not a Parameter, then the absolute value is ignored.
-    cost : float, Parameter (optional)
-        The cost of net flow in to the storage node. I.e. a positive cost penalises increasing volume by
-        giving a benefit to negative net flow (release), and a negative cost penalises decreasing volume
-        by giving a benefit to positive net flow (inflow).
-    area, level : float, Parameter (optional)
-        Optional float or Parameter defining the area and level of the storage node. These values are
-        accessible through the `get_area` and `get_level` methods respectively.
-    """
-    def __init__(self, model, name, num_outputs=1, num_inputs=1, *args, **kwargs):
-        # cast number of inputs/outputs to integer
-        # this is needed if values come in as strings sometimes
-        num_outputs = int(num_outputs)
-        num_inputs = int(num_inputs)
-
-        min_volume = pop_kwarg_parameter(kwargs, 'min_volume', 0.0)
-        if min_volume is None:
-            min_volume = 0.0
-        max_volume = pop_kwarg_parameter(kwargs, 'max_volume', 0.0)
-        initial_volume = kwargs.pop('initial_volume', None)
-        initial_volume_pc = kwargs.pop('initial_volume_pc', None)
-        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
-        level = pop_kwarg_parameter(kwargs, 'level', None)
-        area = pop_kwarg_parameter(kwargs, 'area', None)
-
-        super(Storage, self).__init__(model, name, **kwargs)
-
-        self.outputs = []
-        for n in range(0, num_outputs):
-            self.outputs.append(StorageOutput(model, name="[output{}]".format(n), parent=self))
-
-        self.inputs = []
-        for n in range(0, num_inputs):
-            self.inputs.append(StorageInput(model, name="[input{}]".format(n), parent=self))
-
-        self.min_volume = min_volume
-        self.max_volume = max_volume
-        self.initial_volume = initial_volume
-        self.initial_volume_pc = initial_volume_pc
-        self.cost = cost
-        self.level = level
-        self.area = area
-
-        # TODO FIXME!
-        # StorageOutput and StorageInput are Cython classes, which do not have
-        # NodeMeta as their metaclass, therefore they don't get added to the
-        # model graph automatically.
-        for node in self.outputs:
-            self.model.graph.add_node(node)
-        for node in self.inputs:
-            self.model.graph.add_node(node)
-
-        # TODO: keyword arguments for input and output nodes specified with prefix
-        '''
-        input_kwargs, output_kwargs = {}, {}
-        keys = list(kwargs.keys())
-        for key in keys:
-            if key.startswith('input_'):
-                input_kwargs[key.replace('input_', '')] = kwargs.pop(key)
-            elif key.startswith('output_'):
-                output_kwargs[key.replace('output_', '')] = kwargs.pop(key)
-        '''
-
-    def iter_slots(self, slot_name=None, is_connector=True, all_slots=False):
-        if is_connector:
-            if not self.inputs:
-                raise StopIteration
-            if slot_name is None:
-                if all_slots or len(self.inputs) == 1:
-                    for node in self.inputs:
-                        yield node
-                else:
-                    raise ValueError("Must specify slot identifier.")
-            else:
-                try:
-                    yield self.inputs[slot_name]
-                except IndexError:
-                    raise IndexError('{} does not have slot: {}'.format(self, slot_name))
-        else:
-            if not self.outputs:
-                raise StopIteration
-            if slot_name is None:
-                if all_slots or len(self.outputs) == 1:
-                    for node in self.outputs:
-                        yield node
-                else:
-                    raise ValueError("Must specify slot identifier.")
-            else:
-                yield self.outputs[slot_name]
-
-    def check(self):
-        pass  # TODO
-
-    @classmethod
-    def load(cls, data, model):
-        name = data.pop('name')
-        num_inputs = int(data.pop('inputs', 1))
-        num_outputs = int(data.pop('outputs', 1))
-        initial_volume = data.pop('initial_volume', None)
-        initial_volume_pc = data.pop('initial_volume_pc', None)
-        max_volume = data.pop('max_volume')
-        min_volume = data.pop('min_volume', 0.0)
-        level = data.pop('level', None)
-        area = data.pop('area', None)
-        cost = data.pop('cost', 0.0)
-
-        data.pop('type', None)
-        # Create the instance
-        node = cls(model=model, name=name, num_inputs=num_inputs, num_outputs=num_outputs, **data)
-
-        # Load the parameters after the instance has been created to prevent circular
-        # loading errors
-
-        # Try to coerce initial volume to float.
-        if initial_volume is not None:
-            try:
-                initial_volume = float(initial_volume)
-            except TypeError:
-                initial_volume = load_parameter_values(model, initial_volume)
-        node.initial_volume = initial_volume
-        node.initial_volume_pc = initial_volume_pc
-
-        max_volume = load_parameter(model, max_volume)
-        if max_volume is not None:
-            node.max_volume = max_volume
-
-        min_volume = load_parameter(model, min_volume)
-        if min_volume is not None:
-            node.min_volume = min_volume
-
-        cost = load_parameter(model, cost)
-        if cost is None:
-            cost = 0.0
-        node.cost = cost
-
-        if level is not None:
-            level = load_parameter(model, level)
-        node.level = level
-
-        if area is not None:
-            area = load_parameter(model, area)
-        node.area = area
-
-        return node
-
-    def __repr__(self):
-        return '<{} "{}">'.format(self.__class__.__name__, self.name)
-
-
-class VirtualStorage(Drawable, _core.VirtualStorage, metaclass=NodeMeta):
-    """A virtual storage unit
-
-    Parameters
-    ----------
-    model: pywr.core.Model
-    name: str
-        The name of the virtual node
-    nodes: list of nodes
-        List of inflow/outflow nodes that affect the storage volume
-    factors: list of floats
-        List of factors to multiply node flow by. Positive factors remove
-        water from the storage, negative factors remove it.
-    min_volume: float or parameter
-        The minimum volume the storage is allowed to reach.
-    max_volume: float or parameter
-        The maximum volume of the storage.
-    initial_volume: float
-        The initial storage volume.
-    cost: float or parameter
-        The cost of flow into/outfrom the storage.
-
-    Notes
-    -----
-    TODO: The cost property is not currently respected. See issue #242.
-    """
-    def __init__(self, model, name, nodes, **kwargs):
-        min_volume = pop_kwarg_parameter(kwargs, 'min_volume', 0.0)
-        if min_volume is None:
-            min_volume = 0.0
-        max_volume = pop_kwarg_parameter(kwargs, 'max_volume', 0.0)
-        if 'volume' in kwargs:
-            # support older API where volume kwarg was the initial volume
-            initial_volume = kwargs.pop('volume')
-        else:
-            initial_volume = kwargs.pop('initial_volume', 0.0)
-        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
-
-        factors = kwargs.pop('factors', None)
-
-        super(VirtualStorage, self).__init__(model, name, **kwargs)
-
-        self.min_volume = min_volume
-        self.max_volume = max_volume
-        self.initial_volume = initial_volume
-        self.cost = cost
-        self.nodes = nodes
-
-        if factors is None:
-            self.factors = [1.0 for i in range(len(nodes))]
-        else:
-            self.factors = factors
-
-    def check(self):
-        super(VirtualStorage, self).check()
-        if self.cost not in (0.0, None):
-            raise NotImplementedError("VirtualStorage does not currently support a non-zero cost.")
-
-    @classmethod
-    def load(cls, data, model):
-        del(data["type"])
-        nodes = []
-        for node_name in data.pop("nodes"):
-            nodes.append(model._get_node_from_ref(model, node_name))
-        node = cls(model, nodes=nodes, **data)
-        return node
-
-class AnnualVirtualStorage(VirtualStorage):
-    """A virtual storage which resets annually, useful for licences
-
-    See documentation for `pywr.core.VirtualStorage`.
-
-    Parameters
-    ----------
-    reset_day: int
-        The day of the month (0-31) to reset the volume to the initial value.
-    reset_month: int
-        The month of the year (0-12) to reset the volume to the initial value.
-    reset_to_initial_volume: bool
-        Reset the volume to the initial volume instead of maximum volume each year (default is False).
-
-    """
-    def __init__(self, *args, **kwargs):
-        self.reset_day = kwargs.pop('reset_day', 1)
-        self.reset_month = kwargs.pop('reset_month', 1)
-        self.reset_to_initial_volume = kwargs.pop('reset_to_initial_volume', False)
-        self._last_reset_year = None
-
-        super(AnnualVirtualStorage, self).__init__(*args, **kwargs)
-
-    def reset(self):
-        super(AnnualVirtualStorage, self).reset()
-        self._last_reset_year = None
-
-    def before(self, ts):
-        super(AnnualVirtualStorage, self).before(ts)
-
-        # Reset the storage volume if necessary
-        if ts.year != self._last_reset_year:
-            # I.e. we're in a new year and ...
-            # ... we're at or past the reset month/day
-            if ts.month > self.reset_month or \
-                    (ts.month == self.reset_month and ts.day >= self.reset_day):
-                # Reset to maximum volume (i.e. full capacity. )
-                self._reset_storage_only(use_initial_volume=self.reset_to_initial_volume)
-                self._last_reset_year = ts.year
-
-
-class PiecewiseLink(Node):
-    """ An extension of Node that represents a non-linear Link with a piece wise cost function.
-
-    This object is intended to model situations where there is a benefit of supplying certain flow rates
-    but beyond a fixed limit there is a change in (or zero) cost.
-
-    Parameters
-    ----------
-    max_flow : iterable
-        A monotonic increasing list of maximum flows for the piece wise function
-    cost : iterable
-        A list of costs corresponding to the max_flow steps
-
-    Notes
-    -----
-
-    This Node is implemented using a compound node structure like so:
-
-    ::
-
-                | Separate Domain         |
-        Output -> Sublink 0 -> Sub Output -> Input
-               -> Sublink 1 ---^
-               ...             |
-               -> Sublink n ---|
-
-    This means routes do not directly traverse this node due to the separate
-    domain in the middle. Instead several new routes are made for each of
-    the sublinks and connections to the Output/Input node. The reason for this
-    breaking of the route is to avoid an geometric increase in the number
-    of routes when multiple PiecewiseLinks are present in the same route.
-    """
-    def __init__(self, *args, **kwargs):
-        self.allow_isolated = True
-        costs = kwargs.pop('cost')
-        max_flows = kwargs.pop('max_flow')
-        super(PiecewiseLink, self).__init__(*args, **kwargs)
-
-        if len(costs) != len(max_flows):
-            raise ValueError("Piecewise max_flow and cost keywords must be the same length.")
-
-        # TODO look at the application of Domains here. Having to use
-        # Input/Output instead of BaseInput/BaseOutput because of a different
-        # domain is required on the sub-nodes and they need to be connected
-        self.sub_domain = Domain()
-        self.input = Input(self.model, name='{} Input'.format(self.name), parent=self)
-        self.output = Output(self.model, name='{} Output'.format(self.name), parent=self)
-
-        self.sub_output = Output(self.model, name='{} Sub Output'.format(self.name), parent=self,
-                             domain=self.sub_domain)
-        self.sub_output.connect(self.input)
-        self.sublinks = []
-        for max_flow, cost in zip(max_flows, costs):
-            self.sublinks.append(Input(self.model, name='{} Sublink {}'.format(self.name, len(self.sublinks)),
-                                      cost=cost, max_flow=max_flow, parent=self, domain=self.sub_domain))
-            self.sublinks[-1].connect(self.sub_output)
-            self.output.connect(self.sublinks[-1])
-
-    def iter_slots(self, slot_name=None, is_connector=True):
-        if is_connector:
-            yield self.input
-        else:
-            yield self.output
-            # All sublinks are connected upstream and downstream
-            #for link in self.sublinks:
-            #    yield link
-
-    def after(self, timestep):
-        """
-        Set total flow on this link as sum of sublinks
-        """
-        for lnk in self.sublinks:
-            self.commit_all(lnk.flow)
-        # Make sure save is done after setting aggregated flow
-        super(PiecewiseLink, self).after(timestep)
-
-    @classmethod
-    def load(cls, data, model):
-        # max_flow and cost should be lists of parameter definitions
-        max_flow = [load_parameter(model, p) for p in data.pop('max_flow')]
-        cost = [load_parameter(model, p) for p in data.pop('cost')]
-
-        del(data["type"])
-        return cls(model, max_flow=max_flow, cost=cost, **data)
-
-
-class MultiSplitLink(PiecewiseLink):
-    """ An extension of PiecewiseLink that includes additional slots to connect from.
-
-    Conceptually this node looks like the following internally,
-
-    ::
-
-                 / -->-- X0 -->-- \\
-        A -->-- Xo -->-- X1 -->-- Xi -->-- C
-                 \\ -->-- X2 -->-- /
-                         |
-                         Bo -->-- Bi --> D
-
-    An additional sublink in the PiecewiseLink (i.e. X2 above) and nodes
-    (i.e. Bo and Bi) in this class are added for each extra slot.
-
-    Finally a mechanism is provided to (optionally) fix the ratio between the
-    last non-split sublink (i.e. X1) and each of the extra sublinks (i.e. X2).
-    This mechanism uses `AggregatedNode` internally.
-
-    Parameters
-    ----------
-    max_flow : iterable
-        A monotonic increasing list of maximum flows for the piece wise function
-    cost : iterable
-        A list of costs corresponding to the max_flow steps
-    extra_slots : int, optional (default 1)
-        Number of additional slots (and sublinks) to provide. Must be greater
-        than zero.
-    slot_names : iterable, optional (default range of ints)
-        The names by which to refer to the slots during connection to other
-        nodes. Length must be one more than the number of extra_slots. The first
-        item refers to the PiecewiseLink connection with the following items for
-        each extra slot.
-    factors : iterable, optional (default None)
-        If given, the length must be equal to one more than the number of
-        extra_slots. Each item is the proportion of total flow to pass through
-        the additional sublinks. If no factor is required for a particular
-        sublink then use `None` for its items. Factors are normalised prior to
-        use in the solver.
-
-    Notes
-    -----
-    Users must be careful when using the factor mechanism. Factors use the last
-    non-split sublink (i.e. X1 but not X0). If this link is constrained with a
-    maximum or minimum flow, or if it there is another unconstrained link
-    (i.e. if X0 is unconstrained) then ratios across this whole node may not be
-    enforced as expected.
-
-    """
-    def __init__(self, *args, **kwargs):
-        self.allow_isolated = True
-        costs = list(kwargs.pop('cost'))
-        max_flows = list(kwargs.pop('max_flow'))
-
-        extra_slots = kwargs.pop('extra_slots', 1)
-        if extra_slots < 1:
-            raise ValueError("extra_slots must be at least 1.")
-
-        # No cost or maximum flow on the additional links
-        # The max_flows could be problematic with the aggregated node.
-        costs.extend([0.0]*extra_slots)
-        max_flows.extend([None]*extra_slots)
-        # Edit the kwargs to get the PiecewiseLink to setup as we want.
-        kwargs['cost'] = costs
-        kwargs['max_flow'] = max_flows
-
-        # Default to integer names
-        self.slot_names = list(kwargs.pop('slot_names', range(extra_slots+1)))
-        if extra_slots+1 != len(self.slot_names):
-            raise ValueError("slot_names must be one more than the number of extra_slots.")
-
-        factors = kwargs.pop('factors', None)
-        # Finally initialise the parent.
-        super(MultiSplitLink, self).__init__(*args, **kwargs)
-
-        self._extra_inputs = []
-        self._extra_outputs = []
-        n = len(self.sublinks) - extra_slots
-        for i in range(extra_slots):
-            # create a new input inside the piecewise link which only has access
-            # to flow travelling via the last sublink (X2)
-            otpt = Output(self.model, '{} Extra Output {}'.format(self.name, i),
-                          domain=self.sub_domain, parent=self)
-            inpt = Input(self.model, '{} Extra Input {}'.format(self.name, i), parent=self)
-
-            otpt.connect(inpt)
-            self.sublinks[n+i].connect(otpt)
-
-            self._extra_inputs.append(inpt)
-            self._extra_outputs.append(otpt)
-
-        # Now create an aggregated node for addition constaints if required.
-        if factors is not None:
-            if extra_slots+1 != len(factors):
-                raise ValueError("factors must have a length equal to extra_slots.")
-
-            nodes = []
-            valid_factors = []
-            for r, nd in zip(factors, self.sublinks[n-1:]):
-                if r is not None:
-                    nodes.append(nd)
-                    valid_factors.append(r)
-
-            agg = AggregatedNode(self.model, "{} Agg".format(self.name), nodes)
-            agg.factors = valid_factors
-
-
-    def iter_slots(self, slot_name=None, is_connector=True):
-        if is_connector:
-            i = self.slot_names.index(slot_name)
-            if i == 0:
-                yield self.input
-            else:
-                yield self._extra_inputs[i-1]
-        else:
-            yield self.output
-
-
-class AggregatedStorage( Drawable, _core.AggregatedStorage, metaclass=NodeMeta):
-    """ An aggregated sum of other `Storage` nodes
-
-    This object should behave like `Storage` by returning current `flow`, `volume` and `current_pc`.
-    However this object can not be connected to others within the network.
-
-    Parameters
-    ----------
-    model - `Model` instance
-    name - str
-    storage_nodes - list or iterable of `Storage` objects
-        The `Storage` objects which to return the sum total of
-
-    Notes
-    -----
-    This node can not be connected to other nodes in the network.
-
-    """
-    def __init__(self, model, name, storage_nodes, **kwargs):
-        super(AggregatedStorage, self).__init__(model, name, **kwargs)
-        self.storage_nodes = storage_nodes
-
-
-class AggregatedNode(Drawable, _core.AggregatedNode, metaclass=NodeMeta):
-    """ An aggregated sum of other `Node` nodes
-
-    This object should behave like `Node` by returning current `flow`.
-    However this object can not be connected to others within the network.
-
-    Parameters
-    ----------
-    model - `Model` instance
-    name - str
-    nodes - list or iterable of `Node` objects
-        The `Node` objects which to return the sum total of
-
-    Notes
-    -----
-    This node can not be connected to other nodes in the network.
-
-    """
-    def __init__(self, model, name, nodes, **kwargs):
-        super(AggregatedNode, self).__init__(model, name, **kwargs)
-        self.nodes = nodes
-
-
-class BreakLink(Node):
-    """Compound node used to reduce the number of routes in a model
-
-    Parameters
-    ----------
-    model : `pywr.model.Model`
-    name : string
-    min_flow : float or `pywr.parameters.Parameter`
-    max_flow : float or `pywr.parameters.Parameter`
-    cost : float or `pywr.parameters.Parameter`
-
-    Notes
-    -----
-
-    In a model with form (3, 1, 3), i.e. 3 (A,B,C) inputs connected to 3
-    outputs (D,E,F) via a bottleneck (X), there are 3*3 routes = 9 routes.
-
-    ::
-
-        A -->\\ /--> D
-        B --> X --> E
-        C -->/ \\--> F
-
-    If X is a storage, there are only 6 routes: A->X_o, B->X_o, C->X_o and
-    X_i->D_o, X_i->E_o, X_i->F_o.
-
-    The `BreakLink` node is a compound node composed of a `Storage` with zero
-    volume and a `Link`. It can be used in place of a normal `Link`, but
-    with the benefit that it reduces the number of routes in the model (in
-    the situation described above). The resulting LP is easier to solve.
-    """
-    allow_isolated = True
-
-    def __init__(self, model, name, **kwargs):
-        storage_name = "{} (storage)".format(name)
-        link_name = "{} (link)".format(name)
-        assert(storage_name not in model.nodes)
-        assert(link_name not in model.nodes)
-        self.storage = Storage(
-            model,
-            name=storage_name,
-            min_volume=0,
-            max_volume=0,
-            initial_volume=0,
-            cost=0,
-        )
-        self.link = Link(
-            model,
-            name=link_name
-        )
-
-        self.storage.connect(self.link)
-
-        super(BreakLink, self).__init__(model, name, **kwargs)
-
-    def min_flow():
-        def fget(self):
-            return self.link.min_flow
-        def fset(self, value):
-            self.link.min_flow = value
-        return locals()
-    min_flow = property(**min_flow())
-
-    def max_flow():
-        def fget(self):
-            return self.link.max_flow
-        def fset(self, value):
-            self.link.max_flow = value
-        return locals()
-    max_flow = property(**max_flow())
-
-    def cost():
-        def fget(self):
-            return self.link.cost
-        def fset(self, value):
-            self.link.cost = value
-        return locals()
-    cost = property(**cost())
-
-    def iter_slots(self, slot_name=None, is_connector=True):
-        if is_connector:
-            # connecting FROM the transfer TO something else
-            yield self.link
-        else:
-            # connecting FROM something else TO the transfer
-            yield self.storage.outputs[0]
-
-    def after(self, timestep):
-        super(BreakLink, self).after(timestep)
-        # update flow on transfer node to flow via link node
-        self.commit_all(self.link.flow)
-
-
-class DelayNode(Node):
-    """ A node that delays flow for a given number of timesteps or days.
-
-    This is a composite node consisting internally of an Input and an Output node. A
-    `FlowDelayParameter` is used to delay the flow of the output node for a given period prior
-    to this delayed flow being set as the flow of the input node. Connections to the node are connected
-    to the internal output node and connection from the node are connected to the internal input node
-    node.
-
-    Parameters
-    ----------
-    model : `pywr.model.Model`
-    name : string
-        Name of the node.
-    timesteps: int
-        Number of timesteps to delay the flow.
-    days: int
-        Number of days to delay the flow. Specifying a number of days (instead of a number
-        of timesteps) is only valid if the number of days is exactly divisible by the model
-        timestep delta.
-    initial_flow: float
-        Flow provided by node for initial timesteps prior to any delayed flow being available.
-        This is constant across all delayed timesteps and any model scenarios. Default is 0.0.
-    """
-
-    def __init__(self, model, name, **kwargs):
-        self.allow_isolated = True
-        output_name = "{} Output".format(name)
-        input_name = "{} Input".format(name)
-        param_name = "{} - delay parameter".format(name)
-        assert(output_name not in model.nodes)
-        assert(input_name not in model.nodes)
-        assert(param_name not in model.parameters)
-
-        days = kwargs.pop('days', 0)
-        timesteps = kwargs.pop('timesteps', 0)
-        initial_flow = kwargs.pop('initial_flow', 0.0)
-
-        self.output = Output(model, name=output_name)
-        self.delay_param = FlowDelayParameter(model, self.output, timesteps=timesteps, days=days,
-                                              initial_flow=initial_flow, name=param_name)
-        self.input = Input(model, name=input_name, min_flow=self.delay_param, max_flow=self.delay_param)
-        super().__init__(model, name, **kwargs)
-
-    def iter_slots(self, slot_name=None, is_connector=True):
-        if is_connector:
-            yield self.input
-        else:
-            yield self.output
-
-    def after(self, timestep):
-        super().after(timestep)
-        # delayed flow is saved to the DelayNode
-        self.commit_all(self.input.flow)
-
-
-from pywr.domains.river import *
+import numpy as np
+
+from pywr import _core
+from pywr._core import Node as BaseNode
+from pywr._core import (BaseInput, BaseLink, BaseOutput, StorageInput,
+    StorageOutput, Timestep, ScenarioIndex)
+
+from pywr.parameters import pop_kwarg_parameter, load_parameter, load_parameter_values, FlowDelayParameter
+
+from pywr.domains import Domain
+
+
+class Drawable(object):
+    """Mixin class for objects that are drawable on a diagram of the network.
+    """
+    def __init__(self, *args, **kwargs):
+        self.position = kwargs.pop('position', {})
+        self.color = kwargs.pop('color', 'black')
+        self.visible = kwargs.pop('visible', True)
+        super(Drawable, self).__init__(*args, **kwargs)
+
+
+class Connectable(object):
+    """A mixin class providing methods for connecting nodes in the model graph"""
+    def iter_slots(self, slot_name=None, is_connector=True):
+        """ Returns the object(s) wich should be connected to given slot_name
+
+        Overload this method when implementing compound nodes which have
+        multiple slots and may return something other than self.
+
+        is_connector is True when self's connect method has been used. I.e. self
+        is connecting to another object. This is useful for providing an
+        appropriate response object in circumstances where a subnode should make
+        the actual connection rather than self.
+        """
+        if slot_name is not None:
+            raise ValueError('{} does not have slot: {}'.format(self, slot_name))
+        yield self
+
+    def connect(self, node, from_slot=None, to_slot=None):
+        """Create an edge from this Node to another Node
+
+        Parameters
+        ----------
+        node : Node
+            The node to connect to
+        from_slot : object (optional)
+            The outgoing slot on this node to connect to
+        to_slot : object (optional)
+            The incoming slot on the target node to connect to
+        """
+        if self.model is not node.model:
+            raise RuntimeError("Can't connect Nodes in different Models")
+        if not isinstance(node, Connectable):
+            raise TypeError("Other node ({}) is not connectable.".format(node))
+
+        # Get slot from this node
+        for node1 in self.iter_slots(slot_name=from_slot, is_connector=True):
+            # And slot to connect from other node
+            for node2 in node.iter_slots(slot_name=to_slot, is_connector=False):
+                self.model.graph.add_edge(node1, node2)
+        self.model.dirty = True
+
+    def disconnect(self, node=None, slot_name=None, all_slots=True):
+        """Remove a connection from this Node to another Node
+
+        Parameters
+        ----------
+        node : Node (optional)
+            The node to remove the connection to. If another node is not
+            specified, all connections from this node will be removed.
+        slot_name : integer (optional)
+            If specified, only remove the connection to a specific slot name.
+            Otherwise connections from all slots are removed.
+        """
+        if node is not None:
+            self._disconnect(node, slot_name=slot_name, all_slots=all_slots)
+        else:
+            neighbors = self.model.graph.neighbors(self)
+            for neighbor in [neighbor for neighbor in neighbors]:
+                self._disconnect(neighbor, slot_name=slot_name, all_slots=all_slots)
+
+    def _disconnect(self, node, slot_name=None, all_slots=True):
+        """As disconnect, except node argument is required"""
+        disconnected = False
+        try:
+            self.model.graph.remove_edge(self, node)
+        except:
+            for node_slot in node.iter_slots(slot_name=slot_name, is_connector=False, all_slots=all_slots):
+                try:
+                    self.model.graph.remove_edge(self, node_slot)
+                except nx.exception.NetworkXError:
+                    pass
+                else:
+                    disconnected = True
+        else:
+            disconnected = True
+        if not disconnected:
+            raise nx.exception.NetworkXError('{} is not connected to {}'.format(self, node))
+        self.model.dirty = True
+
+
+class NodeMeta(type):
+    """Node metaclass used to keep a registry of Node classes"""
+    # node subclasses are stored in a dict for convenience
+    node_registry = {}
+    def __new__(meta, name, bases, dct):
+        return super(NodeMeta, meta).__new__(meta, name, bases, dct)
+    def __init__(cls, name, bases, dct):
+        super(NodeMeta, cls).__init__(name, bases, dct)
+        cls.node_registry[name.lower()] = cls
+    def __call__(cls, *args, **kwargs):
+        # Create new instance of Node (or subclass thereof)
+        node = type.__call__(cls, *args, **kwargs)
+        # Add node to Model graph. This needs to be done here, so that if the
+        # __init__ method of Node raises an exception it is not added.
+        node.model.graph.add_node(node)
+        node.model.dirty = True
+        return node
+
+
+class Node(Drawable, Connectable, BaseNode, metaclass=NodeMeta):
+    """Base object from which all other nodes inherit
+
+    This BaseNode is not connectable by default, and the Node class should
+    be used for actual Nodes in the model. The BaseNode provides an abstract
+    class for other Node types (e.g. StorageInput) that are not directly
+    Connectable.
+    """
+    def __init__(self, model, name, **kwargs):
+        """Initialise a new Node object
+
+        Parameters
+        ----------
+        model : Model
+            The model the node belongs to
+        name : string
+            A unique name for the node
+        """
+        color = kwargs.pop('color', 'black')
+        min_flow = pop_kwarg_parameter(kwargs, 'min_flow', 0.0)
+        if min_flow is None:
+            min_flow = 0.0
+        max_flow = pop_kwarg_parameter(kwargs, 'max_flow', float('inf'))
+        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
+        conversion_factor = pop_kwarg_parameter(kwargs, 'conversion_factor', 1.0)
+
+        super(Node, self).__init__(model, name, **kwargs)
+
+        self.slots = {}
+        self.color = color
+        self.min_flow = min_flow
+        self.max_flow = max_flow
+        self.cost = cost
+        self.conversion_factor = conversion_factor
+
+    def check(self):
+        """Check the node is valid
+
+        Raises an exception if the node is invalid
+        """
+        pass
+
+    @classmethod
+    def load(cls, data, model):
+        name = data.pop('name')
+
+        cost = data.pop('cost', 0.0)
+        min_flow = data.pop('min_flow', None)
+        max_flow = data.pop('max_flow', None)
+
+        data.pop('type')
+        node = cls(model=model, name=name,
+                   **data)
+
+        cost = load_parameter(model, cost)
+        min_flow = load_parameter(model, min_flow)
+        max_flow = load_parameter(model, max_flow)
+        if cost is None:
+            cost = 0.0
+        if min_flow is None:
+            min_flow = 0.0
+        if max_flow is None:
+            max_flow = np.inf
+        node.cost = cost
+        node.min_flow = min_flow
+        node.max_flow = max_flow
+
+        return node
+
+
+class Input(Node, BaseInput):
+    """A general input at any point in the network
+
+    """
+    def __init__(self, *args, **kwargs):
+        """Initialise a new Input node
+
+        Parameters
+        ----------
+        min_flow : float (optional)
+            A simple minimum flow constraint for the input. Defaults to None
+        max_flow : float (optional)
+            A simple maximum flow constraint for the input. Defaults to 0.0
+        """
+        super(Input, self).__init__(*args, **kwargs)
+        self.color = '#F26C4F' # light red
+
+
+class Output(Node, BaseOutput):
+    """A general output at any point from the network
+
+    """
+    def __init__(self, *args, **kwargs):
+        """Initialise a new Output node
+
+        Parameters
+        ----------
+        min_flow : float (optional)
+            A simple minimum flow constraint for the output. Defaults to 0.0
+        max_flow : float (optional)
+            A simple maximum flow constraint for the output. Defaults to None
+        """
+        kwargs['color'] = kwargs.pop('color', '#FFF467')  # light yellow
+        super(Output, self).__init__(*args, **kwargs)
+
+
+class Link(Node, BaseLink):
+    """A link in the supply network, such as a pipe
+
+    Connections between Nodes in the network are created using edges (see the
+    Node.connect and Node.disconnect methods). However, these edges cannot
+    hold constraints (e.g. a maximum flow constraint). In this instance a Link
+    node should be used.
+    """
+    def __init__(self, *args, **kwargs):
+        """Initialise a new Link node
+
+        Parameters
+        ----------
+        max_flow : float or function (optional)
+            A maximum flow constraint on the link, e.g. 5.0
+        """
+        kwargs['color'] = kwargs.pop('color', '#A0A0A0')  # 45% grey
+        super(Link, self).__init__(*args, **kwargs)
+
+
+class Storage(Drawable, Connectable, _core.Storage, metaclass=NodeMeta):
+    """A generic storage Node
+
+    In terms of connections in the network the Storage node behaves like any
+    other node, provided there is only 1 input and 1 output. If there are
+    multiple sub-nodes the connections need to be explicit about which they
+    are connecting to. For example:
+
+    >>> storage(model, 'reservoir', num_outputs=1, num_inputs=2)
+    >>> supply.connect(storage)
+    >>> storage.connect(demand1, from_slot=0)
+    >>> storage.connect(demand2, from_slot=1)
+
+    The attribtues of the sub-nodes can be modified directly (and
+    independently). For example:
+
+    >>> storage.outputs[0].max_flow = 15.0
+
+    If a recorder is set on the storage node, instead of recording flow it
+    records changes in storage. Any recorders set on the output or input
+    sub-nodes record flow as normal.
+
+    Parameters
+    ----------
+    model : Model
+        Model instance to which this storage node is attached.
+    name : str
+        The name of the storage node.
+    num_inputs, num_outputs : integer (optional)
+        The number of input and output nodes to create internally. Defaults to 1.
+    min_volume : float (optional)
+        The minimum volume of the storage. Defaults to 0.0.
+    max_volume : float, Parameter (optional)
+        The maximum volume of the storage. Defaults to 0.0.
+    initial_volume, initial_volume_pc : float (optional)
+        Specify initial volume in either absolute or proportional terms. Both are required if `max_volume`
+        is a parameter because the parameter will not be evaluated at the first time-step. If both are given
+        and `max_volume` is not a Parameter, then the absolute value is ignored.
+    cost : float, Parameter (optional)
+        The cost of net flow in to the storage node. I.e. a positive cost penalises increasing volume by
+        giving a benefit to negative net flow (release), and a negative cost penalises decreasing volume
+        by giving a benefit to positive net flow (inflow).
+    area, level : float, Parameter (optional)
+        Optional float or Parameter defining the area and level of the storage node. These values are
+        accessible through the `get_area` and `get_level` methods respectively.
+    """
+    def __init__(self, model, name, num_outputs=1, num_inputs=1, *args, **kwargs):
+        # cast number of inputs/outputs to integer
+        # this is needed if values come in as strings sometimes
+        num_outputs = int(num_outputs)
+        num_inputs = int(num_inputs)
+
+        min_volume = pop_kwarg_parameter(kwargs, 'min_volume', 0.0)
+        if min_volume is None:
+            min_volume = 0.0
+        max_volume = pop_kwarg_parameter(kwargs, 'max_volume', 0.0)
+        initial_volume = kwargs.pop('initial_volume', None)
+        initial_volume_pc = kwargs.pop('initial_volume_pc', None)
+        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
+        level = pop_kwarg_parameter(kwargs, 'level', None)
+        area = pop_kwarg_parameter(kwargs, 'area', None)
+
+        super(Storage, self).__init__(model, name, **kwargs)
+
+        self.outputs = []
+        for n in range(0, num_outputs):
+            self.outputs.append(StorageOutput(model, name="[output{}]".format(n), parent=self))
+
+        self.inputs = []
+        for n in range(0, num_inputs):
+            self.inputs.append(StorageInput(model, name="[input{}]".format(n), parent=self))
+
+        self.min_volume = min_volume
+        self.max_volume = max_volume
+        self.initial_volume = initial_volume
+        self.initial_volume_pc = initial_volume_pc
+        self.cost = cost
+        self.level = level
+        self.area = area
+
+        # TODO FIXME!
+        # StorageOutput and StorageInput are Cython classes, which do not have
+        # NodeMeta as their metaclass, therefore they don't get added to the
+        # model graph automatically.
+        for node in self.outputs:
+            self.model.graph.add_node(node)
+        for node in self.inputs:
+            self.model.graph.add_node(node)
+
+        # TODO: keyword arguments for input and output nodes specified with prefix
+        '''
+        input_kwargs, output_kwargs = {}, {}
+        keys = list(kwargs.keys())
+        for key in keys:
+            if key.startswith('input_'):
+                input_kwargs[key.replace('input_', '')] = kwargs.pop(key)
+            elif key.startswith('output_'):
+                output_kwargs[key.replace('output_', '')] = kwargs.pop(key)
+        '''
+
+    def iter_slots(self, slot_name=None, is_connector=True, all_slots=False):
+        if is_connector:
+            if not self.inputs:
+                raise StopIteration
+            if slot_name is None:
+                if all_slots or len(self.inputs) == 1:
+                    for node in self.inputs:
+                        yield node
+                else:
+                    raise ValueError("Must specify slot identifier.")
+            else:
+                try:
+                    yield self.inputs[slot_name]
+                except IndexError:
+                    raise IndexError('{} does not have slot: {}'.format(self, slot_name))
+        else:
+            if not self.outputs:
+                raise StopIteration
+            if slot_name is None:
+                if all_slots or len(self.outputs) == 1:
+                    for node in self.outputs:
+                        yield node
+                else:
+                    raise ValueError("Must specify slot identifier.")
+            else:
+                yield self.outputs[slot_name]
+
+    def check(self):
+        pass  # TODO
+
+    @classmethod
+    def load(cls, data, model):
+        name = data.pop('name')
+        num_inputs = int(data.pop('inputs', 1))
+        num_outputs = int(data.pop('outputs', 1))
+        initial_volume = data.pop('initial_volume', None)
+        initial_volume_pc = data.pop('initial_volume_pc', None)
+        max_volume = data.pop('max_volume')
+        min_volume = data.pop('min_volume', 0.0)
+        level = data.pop('level', None)
+        area = data.pop('area', None)
+        cost = data.pop('cost', 0.0)
+
+        data.pop('type', None)
+        # Create the instance
+        node = cls(model=model, name=name, num_inputs=num_inputs, num_outputs=num_outputs, **data)
+
+        # Load the parameters after the instance has been created to prevent circular
+        # loading errors
+
+        # Try to coerce initial volume to float.
+        if initial_volume is not None:
+            try:
+                initial_volume = float(initial_volume)
+            except TypeError:
+                initial_volume = load_parameter_values(model, initial_volume)
+        node.initial_volume = initial_volume
+        node.initial_volume_pc = initial_volume_pc
+
+        max_volume = load_parameter(model, max_volume)
+        if max_volume is not None:
+            node.max_volume = max_volume
+
+        min_volume = load_parameter(model, min_volume)
+        if min_volume is not None:
+            node.min_volume = min_volume
+
+        cost = load_parameter(model, cost)
+        if cost is None:
+            cost = 0.0
+        node.cost = cost
+
+        if level is not None:
+            level = load_parameter(model, level)
+        node.level = level
+
+        if area is not None:
+            area = load_parameter(model, area)
+        node.area = area
+
+        return node
+
+    def __repr__(self):
+        return '<{} "{}">'.format(self.__class__.__name__, self.name)
+
+
+class VirtualStorage(Drawable, _core.VirtualStorage, metaclass=NodeMeta):
+    """A virtual storage unit
+
+    Parameters
+    ----------
+    model: pywr.core.Model
+    name: str
+        The name of the virtual node
+    nodes: list of nodes
+        List of inflow/outflow nodes that affect the storage volume
+    factors: list of floats
+        List of factors to multiply node flow by. Positive factors remove
+        water from the storage, negative factors remove it.
+    min_volume: float or parameter
+        The minimum volume the storage is allowed to reach.
+    max_volume: float or parameter
+        The maximum volume of the storage.
+    initial_volume: float
+        The initial storage volume.
+    cost: float or parameter
+        The cost of flow into/outfrom the storage.
+
+    Notes
+    -----
+    TODO: The cost property is not currently respected. See issue #242.
+    """
+    def __init__(self, model, name, nodes, **kwargs):
+        min_volume = pop_kwarg_parameter(kwargs, 'min_volume', 0.0)
+        if min_volume is None:
+            min_volume = 0.0
+        max_volume = pop_kwarg_parameter(kwargs, 'max_volume', 0.0)
+        if 'volume' in kwargs:
+            # support older API where volume kwarg was the initial volume
+            initial_volume = kwargs.pop('volume')
+        else:
+            initial_volume = kwargs.pop('initial_volume', 0.0)
+        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
+
+        factors = kwargs.pop('factors', None)
+
+        super(VirtualStorage, self).__init__(model, name, **kwargs)
+
+        self.min_volume = min_volume
+        self.max_volume = max_volume
+        self.initial_volume = initial_volume
+        self.cost = cost
+        self.nodes = nodes
+
+        if factors is None:
+            self.factors = [1.0 for i in range(len(nodes))]
+        else:
+            self.factors = factors
+
+    def check(self):
+        super(VirtualStorage, self).check()
+        if self.cost not in (0.0, None):
+            raise NotImplementedError("VirtualStorage does not currently support a non-zero cost.")
+
+    @classmethod
+    def load(cls, data, model):
+        del(data["type"])
+        nodes = []
+        for node_name in data.pop("nodes"):
+            nodes.append(model._get_node_from_ref(model, node_name))
+        node = cls(model, nodes=nodes, **data)
+        return node
+
+
+class RollingVirtualStorage(Drawable, _core.RollingVirtualStorage, metaclass=NodeMeta):
+    """A rolling virtual storage node useful for implementing rolling licences.
+
+    Parameters
+    ----------
+    model: pywr.core.Model
+    name: str
+        The name of the virtual node
+    nodes: list of nodes
+        List of inflow/out flow nodes that affect the storage volume
+    factors: list of floats
+        List of factors to multiply node flow by. Positive factors remove
+        water from the storage, negative factors remove it.
+    min_volume: float or parameter
+        The minimum volume the storage is allowed to reach.
+    max_volume: float or parameter
+        The maximum volume of the storage.
+    initial_volume: float
+        The initial storage volume.
+    timesteps : int
+        The number of timesteps to apply to the rolling storage over.
+    days : int
+        The number of days to apply the rolling storage over. Specifying a number of days (instead of a number
+        of timesteps) is only valid with models running a timestep of daily frequency.
+    cost: float or parameter
+        The cost of flow into/outfrom the storage.
+
+    Notes
+    -----
+    TODO: The cost property is not currently respected. See issue #242.
+    """
+    def __init__(self, model, name, nodes, **kwargs):
+        min_volume = pop_kwarg_parameter(kwargs, 'min_volume', 0.0)
+        if min_volume is None:
+            min_volume = 0.0
+        max_volume = pop_kwarg_parameter(kwargs, 'max_volume', 0.0)
+        initial_volume = kwargs.pop('initial_volume', 0.0)
+        cost = pop_kwarg_parameter(kwargs, 'cost', 0.0)
+        factors = kwargs.pop('factors', None)
+        days = kwargs.pop('days', None)
+        timesteps = kwargs.pop('timesteps', 0)
+
+        if not timesteps and not days:
+            raise ValueError("Either `timesteps` or `days` must be specified.")
+
+        super().__init__(model, name, **kwargs)
+
+        self.min_volume = min_volume
+        self.max_volume = max_volume
+        self.initial_volume = initial_volume
+        self.cost = cost
+        self.nodes = nodes
+        self.days = days
+        self.timesteps = timesteps
+
+        if factors is None:
+            self.factors = [1.0 for i in range(len(nodes))]
+        else:
+            self.factors = factors
+
+    def check(self):
+        super().check()
+        if self.cost not in (0.0, None):
+            raise NotImplementedError("RollingVirtualStorage does not currently support a non-zero cost.")
+
+    def setup(self, model):
+        if self.days is not None and self.days > 0:
+            try:
+                self.timesteps = self.days // self.model.timestepper.delta
+            except TypeError:
+                raise TypeError('A rolling period defined as a number of days is only valid with daily time-steps.')
+        if self.timesteps < 1:
+            raise ValueError('The number of time-steps for a RollingVirtualStorage node must be greater than one.')
+        super().setup(model)
+
+    @classmethod
+    def load(cls, data, model):
+        del(data["type"])
+        nodes = []
+        for node_name in data.pop("nodes"):
+            nodes.append(model._get_node_from_ref(model, node_name))
+        node = cls(model, nodes=nodes, **data)
+        return node
+
+
+class AnnualVirtualStorage(VirtualStorage):
+    """A virtual storage which resets annually, useful for licences
+
+    See documentation for `pywr.core.VirtualStorage`.
+
+    Parameters
+    ----------
+    reset_day: int
+        The day of the month (0-31) to reset the volume to the initial value.
+    reset_month: int
+        The month of the year (0-12) to reset the volume to the initial value.
+    reset_to_initial_volume: bool
+        Reset the volume to the initial volume instead of maximum volume each year (default is False).
+
+    """
+    def __init__(self, *args, **kwargs):
+        self.reset_day = kwargs.pop('reset_day', 1)
+        self.reset_month = kwargs.pop('reset_month', 1)
+        self.reset_to_initial_volume = kwargs.pop('reset_to_initial_volume', False)
+        self._last_reset_year = None
+
+        super(AnnualVirtualStorage, self).__init__(*args, **kwargs)
+
+    def reset(self):
+        super(AnnualVirtualStorage, self).reset()
+        self._last_reset_year = None
+
+    def before(self, ts):
+        super(AnnualVirtualStorage, self).before(ts)
+
+        # Reset the storage volume if necessary
+        if ts.year != self._last_reset_year:
+            # I.e. we're in a new year and ...
+            # ... we're at or past the reset month/day
+            if ts.month > self.reset_month or \
+                    (ts.month == self.reset_month and ts.day >= self.reset_day):
+                # Reset to maximum volume (i.e. full capacity. )
+                self._reset_storage_only(use_initial_volume=self.reset_to_initial_volume)
+                self._last_reset_year = ts.year
+                self.active = True
+
+
+class SeasonalVirtualStorage(AnnualVirtualStorage):
+    """A virtual storage node that operates only for a specified period within a year.
+
+    This node is most useful for representing licences that are only enforced during specified periods. The
+    `reset_day` and `reset_month` parameters indicate when the node starts operating and the `end_day` and `end_month`
+    when it stops operating. For the period when the node is not operating, the volume of the node remains unchanged
+    and the node does not apply any constraints to the model.
+
+    The end_day and end_month can represent a date earlier in the year that the reset_day and and reset_month. This
+    situation represents a licence that operates across a year boundary. For example, one that is active between
+    October and March and not active between April and September.
+
+    Parameters
+    ----------
+    reset_day : int
+        The day of the month (0-31) when the node starts operating and its volume is reset to the initial value or
+        maximum volume.
+    reset_month : int
+        The month of the year (0-12) when the node starts operating and its volume is reset to the initial value or
+        maximum volume.
+    reset_to_initial_volume : bool
+        Reset the volume to the initial volume instead of maximum volume each year (default is False).
+    end_day : int
+        The day of the month (0-31) when the node stops operating.
+    end_month : int
+        The month of the year (0-12) when the node stops operating.
+    """
+
+    def __init__(self, *args, **kwargs):
+        self.end_day = kwargs.pop('end_day', 31)
+        self.end_month = kwargs.pop('end_month', 12)
+        self._last_active_year = None
+
+        super().__init__(*args, **kwargs)
+
+    def before(self, ts):
+        super().before(ts)
+
+        if ts.year != self._last_active_year:
+            if ts.index == 0:
+                if self._last_reset_year == ts.year:
+                    # First timestep is later in year than reset date
+                    if self.end_month < self.reset_month or \
+                            (self.end_month == self.reset_month and self.end_day <= self.reset_day):
+                        # end date is earlier in year than reset date so do not deactivate node in first year
+                        self._last_active_year = ts.year
+                else:
+                    # First timestep is earlier in year than reset date
+                    if self.end_month > self.reset_month or \
+                            (self.end_month == self.reset_month and self.end_day >= self.reset_day):
+                        # end date is later in year than reset date so node needs to be deactivated
+                        self.active = False
+            elif ts.month > self.end_month or \
+                    (ts.month == self.end_month and ts.day >= self.end_day):
+                self._last_active_year = ts.year
+                self.active = False
+
+
+class PiecewiseLink(Node):
+    """ An extension of Node that represents a non-linear Link with a piece wise cost function.
+
+    This object is intended to model situations where there is a benefit of supplying certain flow rates
+    but beyond a fixed limit there is a change in (or zero) cost.
+
+    Parameters
+    ----------
+    max_flow : iterable
+        A monotonic increasing list of maximum flows for the piece wise function
+    cost : iterable
+        A list of costs corresponding to the max_flow steps
+
+    Notes
+    -----
+
+    This Node is implemented using a compound node structure like so:
+
+    ::
+
+                | Separate Domain         |
+        Output -> Sublink 0 -> Sub Output -> Input
+               -> Sublink 1 ---^
+               ...             |
+               -> Sublink n ---|
+
+    This means routes do not directly traverse this node due to the separate
+    domain in the middle. Instead several new routes are made for each of
+    the sublinks and connections to the Output/Input node. The reason for this
+    breaking of the route is to avoid an geometric increase in the number
+    of routes when multiple PiecewiseLinks are present in the same route.
+    """
+    def __init__(self, *args, **kwargs):
+        self.allow_isolated = True
+        costs = kwargs.pop('cost')
+        max_flows = kwargs.pop('max_flow')
+        super(PiecewiseLink, self).__init__(*args, **kwargs)
+
+        if len(costs) != len(max_flows):
+            raise ValueError("Piecewise max_flow and cost keywords must be the same length.")
+
+        # TODO look at the application of Domains here. Having to use
+        # Input/Output instead of BaseInput/BaseOutput because of a different
+        # domain is required on the sub-nodes and they need to be connected
+        self.sub_domain = Domain()
+        self.input = Input(self.model, name='{} Input'.format(self.name), parent=self)
+        self.output = Output(self.model, name='{} Output'.format(self.name), parent=self)
+
+        self.sub_output = Output(self.model, name='{} Sub Output'.format(self.name), parent=self,
+                             domain=self.sub_domain)
+        self.sub_output.connect(self.input)
+        self.sublinks = []
+        for max_flow, cost in zip(max_flows, costs):
+            self.sublinks.append(Input(self.model, name='{} Sublink {}'.format(self.name, len(self.sublinks)),
+                                      cost=cost, max_flow=max_flow, parent=self, domain=self.sub_domain))
+            self.sublinks[-1].connect(self.sub_output)
+            self.output.connect(self.sublinks[-1])
+
+    def iter_slots(self, slot_name=None, is_connector=True):
+        if is_connector:
+            yield self.input
+        else:
+            yield self.output
+            # All sublinks are connected upstream and downstream
+            #for link in self.sublinks:
+            #    yield link
+
+    def after(self, timestep):
+        """
+        Set total flow on this link as sum of sublinks
+        """
+        for lnk in self.sublinks:
+            self.commit_all(lnk.flow)
+        # Make sure save is done after setting aggregated flow
+        super(PiecewiseLink, self).after(timestep)
+
+    @classmethod
+    def load(cls, data, model):
+        # max_flow and cost should be lists of parameter definitions
+        max_flow = [load_parameter(model, p) for p in data.pop('max_flow')]
+        cost = [load_parameter(model, p) for p in data.pop('cost')]
+
+        del(data["type"])
+        return cls(model, max_flow=max_flow, cost=cost, **data)
+
+
+class MultiSplitLink(PiecewiseLink):
+    """ An extension of PiecewiseLink that includes additional slots to connect from.
+
+    Conceptually this node looks like the following internally,
+
+    ::
+
+                 / -->-- X0 -->-- \\
+        A -->-- Xo -->-- X1 -->-- Xi -->-- C
+                 \\ -->-- X2 -->-- /
+                         |
+                         Bo -->-- Bi --> D
+
+    An additional sublink in the PiecewiseLink (i.e. X2 above) and nodes
+    (i.e. Bo and Bi) in this class are added for each extra slot.
+
+    Finally a mechanism is provided to (optionally) fix the ratio between the
+    last non-split sublink (i.e. X1) and each of the extra sublinks (i.e. X2).
+    This mechanism uses `AggregatedNode` internally.
+
+    Parameters
+    ----------
+    max_flow : iterable
+        A monotonic increasing list of maximum flows for the piece wise function
+    cost : iterable
+        A list of costs corresponding to the max_flow steps
+    extra_slots : int, optional (default 1)
+        Number of additional slots (and sublinks) to provide. Must be greater
+        than zero.
+    slot_names : iterable, optional (default range of ints)
+        The names by which to refer to the slots during connection to other
+        nodes. Length must be one more than the number of extra_slots. The first
+        item refers to the PiecewiseLink connection with the following items for
+        each extra slot.
+    factors : iterable, optional (default None)
+        If given, the length must be equal to one more than the number of
+        extra_slots. Each item is the proportion of total flow to pass through
+        the additional sublinks. If no factor is required for a particular
+        sublink then use `None` for its items. Factors are normalised prior to
+        use in the solver.
+
+    Notes
+    -----
+    Users must be careful when using the factor mechanism. Factors use the last
+    non-split sublink (i.e. X1 but not X0). If this link is constrained with a
+    maximum or minimum flow, or if it there is another unconstrained link
+    (i.e. if X0 is unconstrained) then ratios across this whole node may not be
+    enforced as expected.
+
+    """
+    def __init__(self, *args, **kwargs):
+        self.allow_isolated = True
+        costs = list(kwargs.pop('cost'))
+        max_flows = list(kwargs.pop('max_flow'))
+
+        extra_slots = kwargs.pop('extra_slots', 1)
+        if extra_slots < 1:
+            raise ValueError("extra_slots must be at least 1.")
+
+        # No cost or maximum flow on the additional links
+        # The max_flows could be problematic with the aggregated node.
+        costs.extend([0.0]*extra_slots)
+        max_flows.extend([None]*extra_slots)
+        # Edit the kwargs to get the PiecewiseLink to setup as we want.
+        kwargs['cost'] = costs
+        kwargs['max_flow'] = max_flows
+
+        # Default to integer names
+        self.slot_names = list(kwargs.pop('slot_names', range(extra_slots+1)))
+        if extra_slots+1 != len(self.slot_names):
+            raise ValueError("slot_names must be one more than the number of extra_slots.")
+
+        factors = kwargs.pop('factors', None)
+        # Finally initialise the parent.
+        super(MultiSplitLink, self).__init__(*args, **kwargs)
+
+        self._extra_inputs = []
+        self._extra_outputs = []
+        n = len(self.sublinks) - extra_slots
+        for i in range(extra_slots):
+            # create a new input inside the piecewise link which only has access
+            # to flow travelling via the last sublink (X2)
+            otpt = Output(self.model, '{} Extra Output {}'.format(self.name, i),
+                          domain=self.sub_domain, parent=self)
+            inpt = Input(self.model, '{} Extra Input {}'.format(self.name, i), parent=self)
+
+            otpt.connect(inpt)
+            self.sublinks[n+i].connect(otpt)
+
+            self._extra_inputs.append(inpt)
+            self._extra_outputs.append(otpt)
+
+        # Now create an aggregated node for addition constraints if required.
+        if factors is not None:
+            if extra_slots+1 != len(factors):
+                raise ValueError("factors must have a length equal to extra_slots.")
+
+            nodes = []
+            valid_factors = []
+            for r, nd in zip(factors, self.sublinks[n-1:]):
+                if r is not None:
+                    nodes.append(nd)
+                    valid_factors.append(r)
+
+            agg = AggregatedNode(self.model, "{} Agg".format(self.name), nodes)
+            agg.factors = valid_factors
+
+
+    def iter_slots(self, slot_name=None, is_connector=True):
+        if is_connector:
+            i = self.slot_names.index(slot_name)
+            if i == 0:
+                yield self.input
+            else:
+                yield self._extra_inputs[i-1]
+        else:
+            yield self.output
+
+    @classmethod
+    def load(cls, data, model):
+        # max_flow and cost should be lists of parameter definitions
+        max_flow = [load_parameter(model, p) for p in data.pop('max_flow')]
+        cost = [load_parameter(model, p) for p in data.pop('cost')]
+        factors = AggregatedNode.load_factors(model, data)
+
+        del(data["type"])
+        return cls(model, max_flow=max_flow, cost=cost, factors=factors, **data)
+
+
+class AggregatedStorage(Drawable, _core.AggregatedStorage, metaclass=NodeMeta):
+    """ An aggregated sum of other `Storage` nodes
+
+    This object should behave like `Storage` by returning current `flow`, `volume` and `current_pc`.
+    However this object can not be connected to others within the network.
+
+    Parameters
+    ----------
+    model - `Model` instance
+    name - str
+    storage_nodes - list or iterable of `Storage` objects
+        The `Storage` objects which to return the sum total of
+
+    Notes
+    -----
+    This node can not be connected to other nodes in the network.
+
+    """
+    def __init__(self, model, name, storage_nodes, **kwargs):
+        super(AggregatedStorage, self).__init__(model, name, **kwargs)
+        self.storage_nodes = storage_nodes
+
+
+class AggregatedNode(Drawable, _core.AggregatedNode, metaclass=NodeMeta):
+    """ An aggregated sum of other `Node` nodes
+
+    This object should behave like `Node` by returning current `flow`.
+    However this object can not be connected to others within the network.
+
+    Parameters
+    ----------
+    model - `Model` instance
+    name - str
+    nodes - list or iterable of `Node` objects
+        The `Node` objects which to return the sum total of
+
+    Notes
+    -----
+    This node can not be connected to other nodes in the network.
+
+    """
+    def __init__(self, model, name, nodes, **kwargs):
+        super(AggregatedNode, self).__init__(model, name, **kwargs)
+        self.nodes = nodes
+
+
+class BreakLink(Node):
+    """Compound node used to reduce the number of routes in a model
+
+    Parameters
+    ----------
+    model : `pywr.model.Model`
+    name : string
+    min_flow : float or `pywr.parameters.Parameter`
+    max_flow : float or `pywr.parameters.Parameter`
+    cost : float or `pywr.parameters.Parameter`
+
+    Notes
+    -----
+
+    In a model with form (3, 1, 3), i.e. 3 (A,B,C) inputs connected to 3
+    outputs (D,E,F) via a bottleneck (X), there are 3*3 routes = 9 routes.
+
+    ::
+
+        A -->\\ /--> D
+        B --> X --> E
+        C -->/ \\--> F
+
+    If X is a storage, there are only 6 routes: A->X_o, B->X_o, C->X_o and
+    X_i->D_o, X_i->E_o, X_i->F_o.
+
+    The `BreakLink` node is a compound node composed of a `Storage` with zero
+    volume and a `Link`. It can be used in place of a normal `Link`, but
+    with the benefit that it reduces the number of routes in the model (in
+    the situation described above). The resulting LP is easier to solve.
+    """
+    allow_isolated = True
+
+    def __init__(self, model, name, **kwargs):
+        storage_name = "{} (storage)".format(name)
+        link_name = "{} (link)".format(name)
+        assert(storage_name not in model.nodes)
+        assert(link_name not in model.nodes)
+        self.storage = Storage(
+            model,
+            name=storage_name,
+            min_volume=0,
+            max_volume=0,
+            initial_volume=0,
+            cost=0,
+        )
+        self.link = Link(
+            model,
+            name=link_name
+        )
+
+        self.storage.connect(self.link)
+
+        super(BreakLink, self).__init__(model, name, **kwargs)
+
+    def min_flow():
+        def fget(self):
+            return self.link.min_flow
+        def fset(self, value):
+            self.link.min_flow = value
+        return locals()
+    min_flow = property(**min_flow())
+
+    def max_flow():
+        def fget(self):
+            return self.link.max_flow
+        def fset(self, value):
+            self.link.max_flow = value
+        return locals()
+    max_flow = property(**max_flow())
+
+    def cost():
+        def fget(self):
+            return self.link.cost
+        def fset(self, value):
+            self.link.cost = value
+        return locals()
+    cost = property(**cost())
+
+    def iter_slots(self, slot_name=None, is_connector=True):
+        if is_connector:
+            # connecting FROM the transfer TO something else
+            yield self.link
+        else:
+            # connecting FROM something else TO the transfer
+            yield self.storage.outputs[0]
+
+    def after(self, timestep):
+        super(BreakLink, self).after(timestep)
+        # update flow on transfer node to flow via link node
+        self.commit_all(self.link.flow)
+
+
+class DelayNode(Node):
+    """ A node that delays flow for a given number of timesteps or days.
+
+    This is a composite node consisting internally of an Input and an Output node. A
+    `FlowDelayParameter` is used to delay the flow of the output node for a given period prior
+    to this delayed flow being set as the flow of the input node. Connections to the node are connected
+    to the internal output node and connection from the node are connected to the internal input node
+    node.
+
+    Parameters
+    ----------
+    model : `pywr.model.Model`
+    name : string
+        Name of the node.
+    timesteps: int
+        Number of timesteps to delay the flow.
+    days: int
+        Number of days to delay the flow. Specifying a number of days (instead of a number
+        of timesteps) is only valid if the number of days is exactly divisible by the model
+        timestep delta.
+    initial_flow: float
+        Flow provided by node for initial timesteps prior to any delayed flow being available.
+        This is constant across all delayed timesteps and any model scenarios. Default is 0.0.
+    """
+
+    def __init__(self, model, name, **kwargs):
+        self.allow_isolated = True
+        output_name = "{} Output".format(name)
+        input_name = "{} Input".format(name)
+        param_name = "{} - delay parameter".format(name)
+        assert(output_name not in model.nodes)
+        assert(input_name not in model.nodes)
+        assert(param_name not in model.parameters)
+
+        days = kwargs.pop('days', 0)
+        timesteps = kwargs.pop('timesteps', 0)
+        initial_flow = kwargs.pop('initial_flow', 0.0)
+
+        self.output = Output(model, name=output_name)
+        self.delay_param = FlowDelayParameter(model, self.output, timesteps=timesteps, days=days,
+                                              initial_flow=initial_flow, name=param_name)
+        self.input = Input(model, name=input_name, min_flow=self.delay_param, max_flow=self.delay_param)
+        super().__init__(model, name, **kwargs)
+
+    def iter_slots(self, slot_name=None, is_connector=True):
+        if is_connector:
+            yield self.input
+        else:
+            yield self.output
+
+    def after(self, timestep):
+        super().after(timestep)
+        # delayed flow is saved to the DelayNode
+        self.commit_all(self.input.flow)
+
+
+from pywr.domains.river import *
```

### Comparing `pywr-1.8.0/pywr/notebook/__init__.py` & `pywr-1.9.0/pywr/notebook/__init__.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,360 +1,360 @@
-import os
-import json
-import inspect
-import warnings
-from IPython.core.display import Javascript, display
-from jinja2 import Template
-from pywr.core import Node
-from pywr.core import Model
-from pywr.nodes import NodeMeta
-from pywr._component import Component
-from pywr.parameters._parameters import get_parameter_from_registry
-
-from .figures import *
-
-# load javascript template for d3 graph
-folder = os.path.dirname(__file__)
-with open(os.path.join(folder, "draw_graph.js"), "r") as f:
-    draw_graph_template = Template(f.read())
-with open(os.path.join(folder, "save_graph.js"), "r") as f:
-    save_graph_template = Template(f.read())
-with open(os.path.join(folder, "graph.css"), "r") as f:
-    draw_graph_css = f.read()
-with open(os.path.join(folder, "template.html"), "r") as f:
-    html_template = Template(f.read())
-
-
-class PywrSchematic:
-
-    def __init__(self, model, width=500, height=400, labels=False, attributes=False, css=None):
-        """This object contains methods that allow the graph of a pywr model network to be displayed in a jupyter
-        notebook or saved to an html file.
-
-        It also contains a method to save the node positions of a notebook graph back to a pywr model json file. Note
-        that this method currently does not work if the object has be instantiated using a model object.
-
-        Parameters
-        ----------
-        model : pywr.core.Model or json-dict that describes a model
-            The model to display
-        width : int
-            The width of the svg canvas to draw the graph on
-        height : int
-            The height of the svg canvas to draw the graph on
-        labels : bool
-            If True, each graph node is labelled with its name. If false, the node names are displayed
-            during mouseover events
-        attributes : bool
-            If True, a table of node attributes is displayed during mouseover events
-        css : string
-            Stylesheet data to use instead of default
-        """
-        if isinstance(model, Model):
-            self.graph = pywr_model_to_d3_json(model, attributes)
-            # TODO update when schema branch is merged
-            self.json = None
-        else:
-            self.graph = pywr_json_to_d3_json(model, attributes)
-            if isinstance(model, str):
-                with open(model) as d:
-                    self.json = json.load(d)
-            else:
-                self.json = model
-
-        self.height = height
-        self.width = width
-        self.labels = labels
-        self.attributes = attributes
-
-        if css is None:
-            self.css = draw_graph_css
-        else:
-            self.css = css
-
-    def draw_graph(self):
-        """Draw pywr schematic graph in a jupyter notebook"""
-        js = draw_graph_template.render(
-            graph=self.graph,
-            width=self.width,
-            height=self.height,
-            element="element",
-            labels=self.labels,
-            attributes=self.attributes,
-            css=self.css.replace("\n", "")
-        )
-        display(Javascript(data=js))
-
-    def save_graph(self, filename, save_unfixed=False, filetype="json"):
-        """Save a copy of the model JSON with update schematic positions.
-
-        When run in a jupyter notebook this will trigger a download.
-
-        Parameters
-        ----------
-        filename: str
-            The name of the file to save the output data to.
-        save_unfixed: bool
-            If True, then all node position are saved to output file. If False, only nodes who have had their position
-            fixed in the d3 graph have their positions saved.
-        filetype: str
-            Should be either 'json' to save the model data with updated node positions to a JSON file or 'csv' to save
-            node positions to a csv file.
-        """
-
-        if filetype not in ["json", "csv"]:
-            warnings.warn(f"Output filetype '{filetype}' not recognised. Please use either 'json' or 'csv'</p>",
-                          stacklevel=2)
-
-        if self.json is None and filetype == "json":
-            warnings.warn("Node positions cannot be saved to JSON if PywrSchematic object has been instantiated using "
-                          "a pywr model object. Please use a JSON file path or model dict instead.", stacklevel=2)
-        else:
-            display(Javascript(save_graph_template.render(
-                model_data=json.dumps(self.json),
-                height=self.height,
-                width=self.width,
-                save_unfixed=json.dumps(save_unfixed),
-                filename=json.dumps(filename),
-                filetype=json.dumps(filetype)
-            )))
-
-    def to_html(self, filename="model.html", title="Model Schematic"):
-        """Save an HTML file of schematic
-
-        Parameters
-        ----------
-        filename: str
-            The name of the html file
-        title: str
-            The schematic title
-        """
-
-        # TODO add option to get node position from graph that has already been drawn in a notebook
-
-        js = draw_graph_template.render(
-            graph=self.graph,
-            width=self.width,
-            height=self.height,
-            element=json.dumps(".schematic"),
-            labels=self.labels,
-            attributes=self.attributes,
-            css=""
-        )
-
-        html = html_template.render(
-            title=title,
-            css=self.css,
-            d3_script=js
-        )
-
-        with open(filename, "w") as f:
-            f.write(html)
-
-
-def draw_graph(model, width=500, height=400, labels=False, attributes=False, css=None):
-    """Display a Pywr model using D3 in Jupyter
-
-    Functionality for creating the d3 graph is now in the PywrSchematic object
-    """
-    schematic = PywrSchematic(model, width=width, height=height, labels=labels, attributes=attributes, css=css)
-    schematic.draw_graph()
-
-
-def pywr_model_to_d3_json(model, attributes=False):
-    """
-    Convert a Pywr graph to a structure d3 can display
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    attributes: bool (default=False)
-        If True, attribute data for each node is extract
-    """
-    nodes = []
-    node_names = []
-    for node in model.graph.nodes():
-        if node.parent is None and node.virtual is False:
-            nodes.append(node)
-            node_names.append(node.name)
-
-    edges = []
-    for edge in model.graph.edges():
-        node_source, node_target = edge
-
-        # where a link is to/from a subnode, display a link to the parent instead
-        if node_source.parent is not None:
-            node_source = node_source.parent
-        if node_target.parent is not None:
-            node_target = node_target.parent
-
-        if node_source is node_target:
-            # link is between two subnodes
-            continue
-
-        index_source = node_names.index(node_source.name)
-        index_target = node_names.index(node_target.name)
-        edges.append({'source': index_source, 'target': index_target})
-
-    json_nodes = []
-    for n, node in enumerate(nodes):
-        node_dict = {"name": node.name}
-        classes = []
-        cls = node.__class__
-        classes.append(cls)
-        while True:
-            for base in cls.__bases__:
-                if issubclass(base, Node) and base is not Node:
-                    classes.append(base)
-            if classes[-1] is cls:
-                break
-            else:
-                cls = classes[-1]
-        classes = classes[::-1]
-        node_dict["clss"] = [cls.__name__.lower() for cls in classes]
-        try:
-            node_dict["position"] = node.position["schematic"]
-        except KeyError:
-            pass
-
-        if attributes:
-            node_dict["attributes"] = get_node_attr(node)
-
-        json_nodes.append(node_dict)
-
-    graph = {
-        "nodes": json_nodes,
-        "links": edges}
-
-    return graph
-
-
-def get_node_attr(node):
-    """
-    Returns a dictionary that contains node attributes as strings
-
-    Parameters
-    ----------
-    node : a pywr node object
-    """
-    attrs = inspect.getmembers(node, lambda a:not(inspect.isroutine(a)))
-    attribute_data = []
-    for att in attrs:
-
-        attr_name, attr_val = att
-        if attr_name.startswith("_"):
-            continue
-        attr_type = type(attr_val).__name__
-
-        attrs_to_skip = ["component_attrs", "components", "color", "model", "input", "output",
-                         "inputs", "outputs", "sub_domain", "sub_output", "sublinks", "visible",
-                         "fully_qualified_name", "allow_isolated"]
-        if not attr_val or attr_name.lower() in attrs_to_skip:
-            continue
-
-        if isinstance(attr_val, Component):
-            attr_val = attr_val.name
-            if not attr_val:
-                attr_val = attr_type
-            else:
-                attr_val = attr_val + " - " + attr_type
-
-        if isinstance(attr_val, list):
-            new_vals = []
-            for val in attr_val:
-                val_name = str(val)
-                val_name = val_name.replace("[", "").replace("]", "")
-                new_vals.append(val_name)
-            attr_val = "".join(new_vals)
-        else:
-            attr_val = str(attr_val)
-
-        attribute_data.append({"attribute": attr_name, "value": attr_val})
-
-    return attribute_data
-
-
-def pywr_json_to_d3_json(model, attributes=False):
-    """
-    Converts a JSON file or a JSON-derived dict into structure that d3js can use.
-
-    Parameters
-    ----------
-    model : dict or str
-        str inputs should be a path to a json file containing the model.
-    """
-
-    if isinstance(model, str):
-        with open(model) as d:
-            model = json.load(d)
-
-    nodes = []
-    node_classes = create_node_class_trees()
-    for node in model["nodes"]:
-
-        if node["type"].lower() in ["annualvirtualstorage", "virtualstorage", "aggregatednode", "aggregatedstorage"]:
-            # Do not add virtual nodes to the graph
-            continue
-
-        json_node = {'name': node["name"], 'clss': node_classes[node["type"].lower()]}
-        try:
-            json_node['position'] = node['position']['schematic']
-        except KeyError:
-            pass
-
-        if attributes:
-            json_node["attributes"] = []
-            for name, val in node.items():
-
-                if name == "type":
-                    continue
-
-                attr_val = val
-
-                if isinstance(val, dict):
-                    try:
-                        attr_val = get_parameter_from_registry(val["type"]).__name__
-                    except KeyError:
-                        pass
-                elif val in model["parameters"].keys():
-                    param = model["parameters"][val]
-                    attr_type = get_parameter_from_registry(param["type"]).__name__
-                    attr_val = attr_val + " - " + attr_type
-                else:
-                    attr_val = str(attr_val)
-
-                attr_dict = {"attribute": name, "value": attr_val}
-                json_node["attributes"].append(attr_dict)
-
-        nodes.append(json_node)
-
-    nodes_names = [node["name"] for node in nodes]
-
-    edges = []
-    for edge in model["edges"]:
-        sourceindex = nodes_names.index(edge[0])
-        targetindex = nodes_names.index(edge[1])
-        edges.append({'source': sourceindex, 'target': targetindex})
-
-    graph = {
-        "nodes": nodes,
-        "links": edges}
-
-    return graph
-
-
-def create_node_class_trees():
-    # create class tree for each node type
-    node_class_trees = {}
-    for name, cls in NodeMeta.node_registry.items():
-        classes = [cls]
-        while True:
-            for base in cls.__bases__:
-                if issubclass(base, Node) and base is not Node:
-                    classes.append(base)
-            if classes[-1] is cls:
-                break
-            else:
-                cls = classes[-1]
-        clss = [cls.__name__.lower() for cls in classes[::-1]]
-        node_class_trees[name] = clss
-    return node_class_trees
+import os
+import json
+import inspect
+import warnings
+from IPython.core.display import Javascript, display
+from jinja2 import Template
+from pywr.core import Node
+from pywr.core import Model
+from pywr.nodes import NodeMeta
+from pywr._component import Component
+from pywr.parameters._parameters import get_parameter_from_registry
+
+from .figures import *
+
+# load javascript template for d3 graph
+folder = os.path.dirname(__file__)
+with open(os.path.join(folder, "draw_graph.js"), "r") as f:
+    draw_graph_template = Template(f.read())
+with open(os.path.join(folder, "save_graph.js"), "r") as f:
+    save_graph_template = Template(f.read())
+with open(os.path.join(folder, "graph.css"), "r") as f:
+    draw_graph_css = f.read()
+with open(os.path.join(folder, "template.html"), "r") as f:
+    html_template = Template(f.read())
+
+
+class PywrSchematic:
+
+    def __init__(self, model, width=500, height=400, labels=False, attributes=False, css=None):
+        """This object contains methods that allow the graph of a pywr model network to be displayed in a jupyter
+        notebook or saved to an html file.
+
+        It also contains a method to save the node positions of a notebook graph back to a pywr model json file. Note
+        that this method currently does not work if the object has be instantiated using a model object.
+
+        Parameters
+        ----------
+        model : pywr.core.Model or json-dict that describes a model
+            The model to display
+        width : int
+            The width of the svg canvas to draw the graph on
+        height : int
+            The height of the svg canvas to draw the graph on
+        labels : bool
+            If True, each graph node is labelled with its name. If false, the node names are displayed
+            during mouseover events
+        attributes : bool
+            If True, a table of node attributes is displayed during mouseover events
+        css : string
+            Stylesheet data to use instead of default
+        """
+        if isinstance(model, Model):
+            self.graph = pywr_model_to_d3_json(model, attributes)
+            # TODO update when schema branch is merged
+            self.json = None
+        else:
+            self.graph = pywr_json_to_d3_json(model, attributes)
+            if isinstance(model, str):
+                with open(model) as d:
+                    self.json = json.load(d)
+            else:
+                self.json = model
+
+        self.height = height
+        self.width = width
+        self.labels = labels
+        self.attributes = attributes
+
+        if css is None:
+            self.css = draw_graph_css
+        else:
+            self.css = css
+
+    def draw_graph(self):
+        """Draw pywr schematic graph in a jupyter notebook"""
+        js = draw_graph_template.render(
+            graph=self.graph,
+            width=self.width,
+            height=self.height,
+            element="element",
+            labels=self.labels,
+            attributes=self.attributes,
+            css=self.css.replace("\n", "")
+        )
+        display(Javascript(data=js))
+
+    def save_graph(self, filename, save_unfixed=False, filetype="json"):
+        """Save a copy of the model JSON with update schematic positions.
+
+        When run in a jupyter notebook this will trigger a download.
+
+        Parameters
+        ----------
+        filename: str
+            The name of the file to save the output data to.
+        save_unfixed: bool
+            If True, then all node position are saved to output file. If False, only nodes who have had their position
+            fixed in the d3 graph have their positions saved.
+        filetype: str
+            Should be either 'json' to save the model data with updated node positions to a JSON file or 'csv' to save
+            node positions to a csv file.
+        """
+
+        if filetype not in ["json", "csv"]:
+            warnings.warn(f"Output filetype '{filetype}' not recognised. Please use either 'json' or 'csv'</p>",
+                          stacklevel=2)
+
+        if self.json is None and filetype == "json":
+            warnings.warn("Node positions cannot be saved to JSON if PywrSchematic object has been instantiated using "
+                          "a pywr model object. Please use a JSON file path or model dict instead.", stacklevel=2)
+        else:
+            display(Javascript(save_graph_template.render(
+                model_data=json.dumps(self.json),
+                height=self.height,
+                width=self.width,
+                save_unfixed=json.dumps(save_unfixed),
+                filename=json.dumps(filename),
+                filetype=json.dumps(filetype)
+            )))
+
+    def to_html(self, filename="model.html", title="Model Schematic"):
+        """Save an HTML file of schematic
+
+        Parameters
+        ----------
+        filename: str
+            The name of the html file
+        title: str
+            The schematic title
+        """
+
+        # TODO add option to get node position from graph that has already been drawn in a notebook
+
+        js = draw_graph_template.render(
+            graph=self.graph,
+            width=self.width,
+            height=self.height,
+            element=json.dumps(".schematic"),
+            labels=self.labels,
+            attributes=self.attributes,
+            css=""
+        )
+
+        html = html_template.render(
+            title=title,
+            css=self.css,
+            d3_script=js
+        )
+
+        with open(filename, "w") as f:
+            f.write(html)
+
+
+def draw_graph(model, width=500, height=400, labels=False, attributes=False, css=None):
+    """Display a Pywr model using D3 in Jupyter
+
+    Functionality for creating the d3 graph is now in the PywrSchematic object
+    """
+    schematic = PywrSchematic(model, width=width, height=height, labels=labels, attributes=attributes, css=css)
+    schematic.draw_graph()
+
+
+def pywr_model_to_d3_json(model, attributes=False):
+    """
+    Convert a Pywr graph to a structure d3 can display
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    attributes: bool (default=False)
+        If True, attribute data for each node is extract
+    """
+    nodes = []
+    node_names = []
+    for node in model.graph.nodes():
+        if node.parent is None and node.virtual is False:
+            nodes.append(node)
+            node_names.append(node.name)
+
+    edges = []
+    for edge in model.graph.edges():
+        node_source, node_target = edge
+
+        # where a link is to/from a subnode, display a link to the parent instead
+        if node_source.parent is not None:
+            node_source = node_source.parent
+        if node_target.parent is not None:
+            node_target = node_target.parent
+
+        if node_source is node_target:
+            # link is between two subnodes
+            continue
+
+        index_source = node_names.index(node_source.name)
+        index_target = node_names.index(node_target.name)
+        edges.append({'source': index_source, 'target': index_target})
+
+    json_nodes = []
+    for n, node in enumerate(nodes):
+        node_dict = {"name": node.name}
+        classes = []
+        cls = node.__class__
+        classes.append(cls)
+        while True:
+            for base in cls.__bases__:
+                if issubclass(base, Node) and base is not Node:
+                    classes.append(base)
+            if classes[-1] is cls:
+                break
+            else:
+                cls = classes[-1]
+        classes = classes[::-1]
+        node_dict["clss"] = [cls.__name__.lower() for cls in classes]
+        try:
+            node_dict["position"] = node.position["schematic"]
+        except KeyError:
+            pass
+
+        if attributes:
+            node_dict["attributes"] = get_node_attr(node)
+
+        json_nodes.append(node_dict)
+
+    graph = {
+        "nodes": json_nodes,
+        "links": edges}
+
+    return graph
+
+
+def get_node_attr(node):
+    """
+    Returns a dictionary that contains node attributes as strings
+
+    Parameters
+    ----------
+    node : a pywr node object
+    """
+    attrs = inspect.getmembers(node, lambda a:not(inspect.isroutine(a)))
+    attribute_data = []
+    for att in attrs:
+
+        attr_name, attr_val = att
+        if attr_name.startswith("_"):
+            continue
+        attr_type = type(attr_val).__name__
+
+        attrs_to_skip = ["component_attrs", "components", "color", "model", "input", "output",
+                         "inputs", "outputs", "sub_domain", "sub_output", "sublinks", "visible",
+                         "fully_qualified_name", "allow_isolated"]
+        if not attr_val or attr_name.lower() in attrs_to_skip:
+            continue
+
+        if isinstance(attr_val, Component):
+            attr_val = attr_val.name
+            if not attr_val:
+                attr_val = attr_type
+            else:
+                attr_val = attr_val + " - " + attr_type
+
+        if isinstance(attr_val, list):
+            new_vals = []
+            for val in attr_val:
+                val_name = str(val)
+                val_name = val_name.replace("[", "").replace("]", "")
+                new_vals.append(val_name)
+            attr_val = "".join(new_vals)
+        else:
+            attr_val = str(attr_val)
+
+        attribute_data.append({"attribute": attr_name, "value": attr_val})
+
+    return attribute_data
+
+
+def pywr_json_to_d3_json(model, attributes=False):
+    """
+    Converts a JSON file or a JSON-derived dict into structure that d3js can use.
+
+    Parameters
+    ----------
+    model : dict or str
+        str inputs should be a path to a json file containing the model.
+    """
+
+    if isinstance(model, str):
+        with open(model) as d:
+            model = json.load(d)
+
+    nodes = []
+    node_classes = create_node_class_trees()
+    for node in model["nodes"]:
+
+        if node["type"].lower() in ["annualvirtualstorage", "virtualstorage", "aggregatednode", "aggregatedstorage"]:
+            # Do not add virtual nodes to the graph
+            continue
+
+        json_node = {'name': node["name"], 'clss': node_classes[node["type"].lower()]}
+        try:
+            json_node['position'] = node['position']['schematic']
+        except KeyError:
+            pass
+
+        if attributes:
+            json_node["attributes"] = []
+            for name, val in node.items():
+
+                if name == "type":
+                    continue
+
+                attr_val = val
+
+                if isinstance(val, dict):
+                    try:
+                        attr_val = get_parameter_from_registry(val["type"]).__name__
+                    except KeyError:
+                        pass
+                elif val in model["parameters"].keys():
+                    param = model["parameters"][val]
+                    attr_type = get_parameter_from_registry(param["type"]).__name__
+                    attr_val = attr_val + " - " + attr_type
+                else:
+                    attr_val = str(attr_val)
+
+                attr_dict = {"attribute": name, "value": attr_val}
+                json_node["attributes"].append(attr_dict)
+
+        nodes.append(json_node)
+
+    nodes_names = [node["name"] for node in nodes]
+
+    edges = []
+    for edge in model["edges"]:
+        sourceindex = nodes_names.index(edge[0])
+        targetindex = nodes_names.index(edge[1])
+        edges.append({'source': sourceindex, 'target': targetindex})
+
+    graph = {
+        "nodes": nodes,
+        "links": edges}
+
+    return graph
+
+
+def create_node_class_trees():
+    # create class tree for each node type
+    node_class_trees = {}
+    for name, cls in NodeMeta.node_registry.items():
+        classes = [cls]
+        while True:
+            for base in cls.__bases__:
+                if issubclass(base, Node) and base is not Node:
+                    classes.append(base)
+            if classes[-1] is cls:
+                break
+            else:
+                cls = classes[-1]
+        clss = [cls.__name__.lower() for cls in classes[::-1]]
+        node_class_trees[name] = clss
+    return node_class_trees
```

### Comparing `pywr-1.8.0/pywr/notebook/draw_graph.js` & `pywr-1.9.0/pywr/notebook/draw_graph.js`

 * *Files 12% similar despite different names*

#### js-beautify {}

```diff
@@ -1,264 +1,264 @@
-// javascript jinja2 template for drawing a directional graph
-
-require.config({
-    paths: {
-        d3: 'https://d3js.org/d3.v5.min'
-    }
-});
-
-require(["d3"], function(d3) {
-    const graph = {
-        {
-            graph
-        }
-    };
-
-    const links = graph.links.map(d => Object.create(d));
-    const nodes = graph.nodes.map(d => Object.create(d));
-
-    const style = d3.selectAll({
-        {
-            element
-        }
-    }).append("style");
-    style.html("{{ css }}");
-
-    const div = d3.selectAll({
-        {
-            element
-        }
-    }).append("div").classed("pywr_schematic", true);
-
-    const width = {
-            {
-                width
-            }
-        },
-        height = {
-            {
-                height
-            }
-        };
-
-    const simulation = d3.forceSimulation(nodes)
-        .force("link", d3.forceLink(graph.links))
-        .force("charge", d3.forceManyBody().strength(-120))
-        .force('center', d3.forceCenter(width / 2, height / 2))
-        .force("x", d3.forceX())
-        .force("y", d3.forceY());
-
-    div.style("height", height + "px")
-        .style("width", width + "px");
-
-    const svg = div.append("svg")
-        .attr("width", width)
-        .attr("height", height);
-
-    const posX = d3.scaleLinear()
-        .range([0, width])
-        .domain([-100, 100]);
-
-    const posY = d3.scaleLinear()
-        .range([0, height])
-        .domain([100, -100]); // map-style, +ve is up
-
-    // set initial node positions
-    for (let i = 0; i < nodes.length; i++) {
-        let n = nodes[i];
-        if (n.position != undefined) {
-            n.fx = posX(n.position[0]);
-            n.fy = posY(n.position[1]);
-            n.fixed = true;
-        } else {
-            n.fixed = false;
-        }
-    }
-
-    // define end-arrow svg marker
-    svg.append("svg:defs").append("svg:marker")
-        .attr("id", "end-arrow")
-        .attr("viewBox", "0 -5 10 10")
-        .attr("refX", 6)
-        .attr("markerWidth", 3.5)
-        .attr("markerHeight", 3.5)
-        .attr("orient", "auto")
-        .append("svg:path")
-        .attr("d", "M0,-5L10,0L0,5")
-        .attr("fill", "#333");
-
-    const link = svg.selectAll(".link")
-        .data(graph.links)
-        .enter().append("svg:path")
-        .attr("class", "link")
-        .style("fill", "none")
-        .style("stroke", "#333")
-        .style("stroke-width", 2)
-        .style("marker-end", function(d) {
-            return "url(#end-arrow)";
-        });
-
-
-    function dblclick(d) {
-        d.fx = d.x;
-        d.fy = d.y;
-    }
-
-    function drag() {
-
-        function dragstarted(d) {
-            if (!d3.event.active) simulation.alphaTarget(0.3).restart();
-            d.fx = d.x;
-            d.fy = d.y;
-        }
-
-        function dragged(d) {
-            d.fx = d3.event.x;
-            d.fy = d3.event.y;
-        }
-
-        function dragended(d) {
-            if (!d3.event.active) simulation.alphaTarget(0);
-            d.fixed = true;
-        }
-
-        return d3.drag()
-            .on("start", dragstarted)
-            .on("drag", dragged)
-            .on("end", dragended);
-    }
-
-    const node = svg.selectAll(".node")
-        .data(nodes)
-        .enter()
-        .append("g")
-        .on("dblclick", dblclick)
-        .call(drag(simulation));
-
-    let node_size = 5;
-
-    node.append("circle")
-        .attr("class", "node")
-        .attr("r", node_size)
-        .attr("class", function(d) {
-            let clss = "node";
-            for (let i = 0; i < d.clss.length; i++) {
-                clss += " node-" + d.clss[i];
-            };
-            return clss;
-        });
-
-    {
-        %
-        if labels %
-    }
-    node.append("text")
-        .attr("dx", 10)
-        .attr("dy", 5)
-        .style("font-weight", 100)
-        .classed("node-text", true)
-        .text(function(d) {
-            return d.name
-        }); {
-        %
-        else %
-    }
-    node.append("title")
-        .text(function(d) {
-            return d.name;
-        }); {
-        % endif %
-    }
-
-    function tick() {
-
-        node.attr("transform", function(d) {
-            // ensure nodes do not go beyond svg bounds
-            d.x = Math.max(node_size, Math.min(width - node_size, d.x))
-            d.y = Math.max(node_size, Math.min(height - node_size, d.y));
-            return "translate(" + d.x + "," + d.y + ")";
-        });
-
-        link.attr("d", function(d) {
-            let deltaX = d.target.x - d.source.x,
-                deltaY = d.target.y - d.source.y,
-                dist = Math.sqrt(deltaX * deltaX + deltaY * deltaY),
-                normX = deltaX / dist,
-                normY = deltaY / dist,
-                sourcePadding = node_size,
-                targetPadding = node_size + 3,
-                sourceX = d.source.x + (sourcePadding * normX),
-                sourceY = d.source.y + (sourcePadding * normY),
-                targetX = d.target.x - (targetPadding * normX),
-                targetY = d.target.y - (targetPadding * normY);
-            return "M" + sourceX + "," + sourceY + "L" + targetX + "," + targetY;
-        });
-    }
-    simulation.on("tick", tick);
-
-    {
-        %
-        if attributes %
-    }
-    node.on("mouseover", function(d) {
-
-        d3.select(".table-tooltip").remove();
-
-        const table = d3.selectAll({
-                {
-                    element
-                }
-            })
-            .append("table")
-            .classed("table-tooltip", true);
-
-        const thead = table.append('thead')
-        const tbody = table.append('tbody');
-
-        const columns = ["attribute", "value"]
-        thead.append('tr')
-            .selectAll('th')
-            .data(columns).enter()
-            .append('th')
-            .text(function(column) {
-                return column;
-            });
-
-        const table_data = Object.assign([], d["attributes"])
-        table_data.push({
-            "attribute": "x coordinate",
-            "value": d.x.toFixed(2)
-        })
-        table_data.push({
-            "attribute": "y coordinate",
-            "value": d.y.toFixed(2)
-        })
-
-        const rows = tbody.selectAll('tr')
-            .data(table_data)
-            .enter()
-            .append('tr');
-
-        rows.selectAll('td')
-            .data(function(d) {
-                return columns.map(function(column) {
-                    return {
-                        column: column,
-                        value: d[column]
-                    };
-                });
-            })
-            .enter()
-            .append('td')
-            .text(function(d) {
-                return d.value;
-            });
-
-    }).on("mouseout", function() {
-        d3.select(".table-tooltip").transition().delay(2000).remove();
-    }); {
-        % endif %
-    }
-
-}, function(err) {
-    element.append("<p style='color:red'>d3 failed to load:" + err + "</p>");
+// javascript jinja2 template for drawing a directional graph
+
+require.config({
+    paths: {
+        d3: 'https://d3js.org/d3.v5.min'
+    }
+});
+
+require(["d3"], function(d3) {
+    const graph = {
+        {
+            graph
+        }
+    };
+
+    const links = graph.links.map(d => Object.create(d));
+    const nodes = graph.nodes.map(d => Object.create(d));
+
+    const style = d3.selectAll({
+        {
+            element
+        }
+    }).append("style");
+    style.html("{{ css }}");
+
+    const div = d3.selectAll({
+        {
+            element
+        }
+    }).append("div").classed("pywr_schematic", true);
+
+    const width = {
+            {
+                width
+            }
+        },
+        height = {
+            {
+                height
+            }
+        };
+
+    const simulation = d3.forceSimulation(nodes)
+        .force("link", d3.forceLink(graph.links))
+        .force("charge", d3.forceManyBody().strength(-120))
+        .force('center', d3.forceCenter(width / 2, height / 2))
+        .force("x", d3.forceX())
+        .force("y", d3.forceY());
+
+    div.style("height", height + "px")
+        .style("width", width + "px");
+
+    const svg = div.append("svg")
+        .attr("width", width)
+        .attr("height", height);
+
+    const posX = d3.scaleLinear()
+        .range([0, width])
+        .domain([-100, 100]);
+
+    const posY = d3.scaleLinear()
+        .range([0, height])
+        .domain([100, -100]); // map-style, +ve is up
+
+    // set initial node positions
+    for (let i = 0; i < nodes.length; i++) {
+        let n = nodes[i];
+        if (n.position != undefined) {
+            n.fx = posX(n.position[0]);
+            n.fy = posY(n.position[1]);
+            n.fixed = true;
+        } else {
+            n.fixed = false;
+        }
+    }
+
+    // define end-arrow svg marker
+    svg.append("svg:defs").append("svg:marker")
+        .attr("id", "end-arrow")
+        .attr("viewBox", "0 -5 10 10")
+        .attr("refX", 6)
+        .attr("markerWidth", 3.5)
+        .attr("markerHeight", 3.5)
+        .attr("orient", "auto")
+        .append("svg:path")
+        .attr("d", "M0,-5L10,0L0,5")
+        .attr("fill", "#333");
+
+    const link = svg.selectAll(".link")
+        .data(graph.links)
+        .enter().append("svg:path")
+        .attr("class", "link")
+        .style("fill", "none")
+        .style("stroke", "#333")
+        .style("stroke-width", 2)
+        .style("marker-end", function(d) {
+            return "url(#end-arrow)";
+        });
+
+
+    function dblclick(d) {
+        d.fx = d.x;
+        d.fy = d.y;
+    }
+
+    function drag() {
+
+        function dragstarted(d) {
+            if (!d3.event.active) simulation.alphaTarget(0.3).restart();
+            d.fx = d.x;
+            d.fy = d.y;
+        }
+
+        function dragged(d) {
+            d.fx = d3.event.x;
+            d.fy = d3.event.y;
+        }
+
+        function dragended(d) {
+            if (!d3.event.active) simulation.alphaTarget(0);
+            d.fixed = true;
+        }
+
+        return d3.drag()
+            .on("start", dragstarted)
+            .on("drag", dragged)
+            .on("end", dragended);
+    }
+
+    const node = svg.selectAll(".node")
+        .data(nodes)
+        .enter()
+        .append("g")
+        .on("dblclick", dblclick)
+        .call(drag(simulation));
+
+    let node_size = 5;
+
+    node.append("circle")
+        .attr("class", "node")
+        .attr("r", node_size)
+        .attr("class", function(d) {
+            let clss = "node";
+            for (let i = 0; i < d.clss.length; i++) {
+                clss += " node-" + d.clss[i];
+            };
+            return clss;
+        });
+
+    {
+        %
+        if labels %
+    }
+    node.append("text")
+        .attr("dx", 10)
+        .attr("dy", 5)
+        .style("font-weight", 100)
+        .classed("node-text", true)
+        .text(function(d) {
+            return d.name
+        }); {
+        %
+        else %
+    }
+    node.append("title")
+        .text(function(d) {
+            return d.name;
+        }); {
+        % endif %
+    }
+
+    function tick() {
+
+        node.attr("transform", function(d) {
+            // ensure nodes do not go beyond svg bounds
+            d.x = Math.max(node_size, Math.min(width - node_size, d.x))
+            d.y = Math.max(node_size, Math.min(height - node_size, d.y));
+            return "translate(" + d.x + "," + d.y + ")";
+        });
+
+        link.attr("d", function(d) {
+            let deltaX = d.target.x - d.source.x,
+                deltaY = d.target.y - d.source.y,
+                dist = Math.sqrt(deltaX * deltaX + deltaY * deltaY),
+                normX = deltaX / dist,
+                normY = deltaY / dist,
+                sourcePadding = node_size,
+                targetPadding = node_size + 3,
+                sourceX = d.source.x + (sourcePadding * normX),
+                sourceY = d.source.y + (sourcePadding * normY),
+                targetX = d.target.x - (targetPadding * normX),
+                targetY = d.target.y - (targetPadding * normY);
+            return "M" + sourceX + "," + sourceY + "L" + targetX + "," + targetY;
+        });
+    }
+    simulation.on("tick", tick);
+
+    {
+        %
+        if attributes %
+    }
+    node.on("mouseover", function(d) {
+
+        d3.select(".table-tooltip").remove();
+
+        const table = d3.selectAll({
+                {
+                    element
+                }
+            })
+            .append("table")
+            .classed("table-tooltip", true);
+
+        const thead = table.append('thead')
+        const tbody = table.append('tbody');
+
+        const columns = ["attribute", "value"]
+        thead.append('tr')
+            .selectAll('th')
+            .data(columns).enter()
+            .append('th')
+            .text(function(column) {
+                return column;
+            });
+
+        const table_data = Object.assign([], d["attributes"])
+        table_data.push({
+            "attribute": "x coordinate",
+            "value": d.x.toFixed(2)
+        })
+        table_data.push({
+            "attribute": "y coordinate",
+            "value": d.y.toFixed(2)
+        })
+
+        const rows = tbody.selectAll('tr')
+            .data(table_data)
+            .enter()
+            .append('tr');
+
+        rows.selectAll('td')
+            .data(function(d) {
+                return columns.map(function(column) {
+                    return {
+                        column: column,
+                        value: d[column]
+                    };
+                });
+            })
+            .enter()
+            .append('td')
+            .text(function(d) {
+                return d.value;
+            });
+
+    }).on("mouseout", function() {
+        d3.select(".table-tooltip").transition().delay(2000).remove();
+    }); {
+        % endif %
+    }
+
+}, function(err) {
+    element.append("<p style='color:red'>d3 failed to load:" + err + "</p>");
 });
```

### Comparing `pywr-1.8.0/pywr/notebook/save_graph.js` & `pywr-1.9.0/pywr/notebook/save_graph.js`

 * *Files 15% similar despite different names*

#### js-beautify {}

```diff
@@ -1,104 +1,104 @@
-require.config({
-    paths: {
-        d3: 'https://d3js.org/d3.v5.min'
-    }
-});
-
-require(["d3"], function(d3) {
-
-    const model_data = {
-            {
-                model_data
-            }
-        },
-        filetype = {
-            {
-                filetype
-            }
-        },
-        filename = {
-            {
-                filename
-            }
-        },
-        save_unfixed = {
-            {
-                save_unfixed
-            }
-        };
-
-    const width = {
-            {
-                width
-            }
-        },
-        height = {
-            {
-                height
-            }
-        };
-
-    const nodes = d3.select(".pywr_schematic").selectAll(".node").data();
-
-    // scales to convert back to values between -100 and 100
-    const posX = d3.scaleLinear()
-        .range([-100, 100])
-        .domain([0, width]);
-    const posY = d3.scaleLinear()
-        .range([100, -100])
-        .domain([0, height]);
-
-    const output_data = ["Node name,Fixed,x,y"];
-    // loop through model nodes and get postions
-    for (let i = 0; i < nodes.length; i++) {
-
-        let node_data = nodes[i];
-
-        if (!(node_data.fixed) && !(save_unfixed)) {
-            // If node is unfixed and unfixed node positions are not being saved move to next node
-            continue
-        }
-
-        if (filetype == "json") {
-            if ("position" in model_data["nodes"][i]) {
-                // ensure that any geographic position are not overwritten
-                model_data["nodes"][i]["position"]["schematic"] = [posX(node_data.x), posY(node_data.y)]
-            } else {
-                model_data["nodes"][i]["position"] = {
-                    "schematic": [posX(node_data.x), posY(node_data.y)]
-                }
-            }
-        } else if (filetype == "csv") {
-            let position_data = [
-                node_data.name,
-                node_data.fixed,
-                posX(node_data.x),
-                posY(node_data.y),
-            ];
-            output_data.push(position_data.join(","));
-        }
-    }
-
-    if (filetype == "json") {
-        download(filename, JSON.stringify(model_data));
-    } else if (filetype == "csv") {
-        download(filename, output_data.join("\n"));
-    }
-
-}, function(err) {
-    element.append("<p style='color:red'>d3.js failed to load:" + err + "</p>");
-});
-
-function download(filename, text) {
-    let pom = document.createElement('a');
-    pom.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));
-    pom.setAttribute('download', filename);
-
-    if (document.createEvent) {
-        let event = document.createEvent('MouseEvents');
-        event.initEvent('click', true, true);
-        pom.dispatchEvent(event);
-    } else {
-        pom.click();
-    }
+require.config({
+    paths: {
+        d3: 'https://d3js.org/d3.v5.min'
+    }
+});
+
+require(["d3"], function(d3) {
+
+    const model_data = {
+            {
+                model_data
+            }
+        },
+        filetype = {
+            {
+                filetype
+            }
+        },
+        filename = {
+            {
+                filename
+            }
+        },
+        save_unfixed = {
+            {
+                save_unfixed
+            }
+        };
+
+    const width = {
+            {
+                width
+            }
+        },
+        height = {
+            {
+                height
+            }
+        };
+
+    const nodes = d3.select(".pywr_schematic").selectAll(".node").data();
+
+    // scales to convert back to values between -100 and 100
+    const posX = d3.scaleLinear()
+        .range([-100, 100])
+        .domain([0, width]);
+    const posY = d3.scaleLinear()
+        .range([100, -100])
+        .domain([0, height]);
+
+    const output_data = ["Node name,Fixed,x,y"];
+    // loop through model nodes and get postions
+    for (let i = 0; i < nodes.length; i++) {
+
+        let node_data = nodes[i];
+
+        if (!(node_data.fixed) && !(save_unfixed)) {
+            // If node is unfixed and unfixed node positions are not being saved move to next node
+            continue
+        }
+
+        if (filetype == "json") {
+            if ("position" in model_data["nodes"][i]) {
+                // ensure that any geographic position are not overwritten
+                model_data["nodes"][i]["position"]["schematic"] = [posX(node_data.x), posY(node_data.y)]
+            } else {
+                model_data["nodes"][i]["position"] = {
+                    "schematic": [posX(node_data.x), posY(node_data.y)]
+                }
+            }
+        } else if (filetype == "csv") {
+            let position_data = [
+                node_data.name,
+                node_data.fixed,
+                posX(node_data.x),
+                posY(node_data.y),
+            ];
+            output_data.push(position_data.join(","));
+        }
+    }
+
+    if (filetype == "json") {
+        download(filename, JSON.stringify(model_data));
+    } else if (filetype == "csv") {
+        download(filename, output_data.join("\n"));
+    }
+
+}, function(err) {
+    element.append("<p style='color:red'>d3.js failed to load:" + err + "</p>");
+});
+
+function download(filename, text) {
+    let pom = document.createElement('a');
+    pom.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));
+    pom.setAttribute('download', filename);
+
+    if (document.createEvent) {
+        let event = document.createEvent('MouseEvents');
+        event.initEvent('click', true, true);
+        pom.dispatchEvent(event);
+    } else {
+        pom.click();
+    }
 }
```

### Comparing `pywr-1.8.0/pywr/optimisation/__init__.py` & `pywr-1.9.0/pywr/optimisation/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,114 +1,115 @@
-from ..core import Model
-import uuid
-import logging
-logger = logging.getLogger(__name__)
-
-
-def cache_variable_parameters(model):
-    variables = []
-    variable_map = [0, ]
-    for var in model.variables:
-        size = var.double_size + var.integer_size
-
-        if size <= 0:
-            raise ValueError('Variable parameter "{}" does not have a size > 0.'.format(var.name))
-
-        variable_map.append(variable_map[-1] + size)
-        variables.append(var)
-
-    return variables, variable_map
-
-
-def cache_constraints(model):
-    constraints = []
-    for r in model.constraints:
-        constraints.append(r)
-
-    return constraints
-
-
-def cache_objectives(model):
-    # This is done to make sure the order is fixed during optimisation.
-    objectives = []
-    for r in model.objectives:
-        objectives.append(r)
-    return objectives
-
-
-# Global variables for individual processes to cache the model and some of its data.
-# The cache is keyed by a UID for each `BaseOptimisationWrapper`
-class ModelCache:
-    def __init__(self):
-        self.model = None
-        self.variables = None
-        self.variable_map = None
-        self.objectives = None
-        self.constraints = None
-MODEL_CACHE = {}
-
-
-class BaseOptimisationWrapper(object):
-    """ A helper class for running pywr optimisations with platypus.
-    """
-    def __init__(self, pywr_model_json, *args, **kwargs):
-        uid = kwargs.pop('uid', None)
-        super(BaseOptimisationWrapper, self).__init__(*args, **kwargs)
-        self.pywr_model_json = pywr_model_json
-
-        if uid is None:
-            uid = uuid.uuid4().hex  # Create a unique ID for caching.
-        self.uid = uid
-        self.run_stats = None
-
-    # The following properties enable attribute caching when repeat execution of the same model is undertaken.
-    @property
-    def _cached(self):
-        global MODEL_CACHE
-        try:
-            cache = MODEL_CACHE[self.uid]
-        except KeyError:
-            model = self.make_model()
-            model.setup()
-
-            cache = ModelCache()
-            cache.model = model
-            cache.variables, cache.variable_map = cache_variable_parameters(model)
-            cache.objectives = cache_objectives(model)
-            cache.constraints = cache_constraints(model)
-            MODEL_CACHE[self.uid] = cache
-        return cache
-
-    @property
-    def model(self):
-        return self._cached.model
-
-    @property
-    def model_variables(self):
-        return self._cached.variables
-
-    @property
-    def model_variable_map(self):
-        return self._cached.variable_map
-
-    @property
-    def model_objectives(self):
-        return self._cached.objectives
-
-    @property
-    def model_constraints(self):
-        return self._cached.constraints
-
-    def make_model(self):
-        m = Model.load(self.pywr_model_json)
-        # Apply any user defined changes to the model
-        self.customise_model(m)
-        return m
-
-    def customise_model(self, model):
-        pass  # By default there is no customisation.
-
-
-def clear_global_model_cache():
-    """ Clear the module level model cache. """
-    global MODEL_CACHE
-    MODEL_CACHE = {}
+from ..core import Model
+import uuid
+import logging
+logger = logging.getLogger(__name__)
+
+
+def cache_variable_parameters(model):
+    variables = []
+    variable_map = [0, ]
+    for var in model.variables:
+        size = var.double_size + var.integer_size
+
+        if size <= 0:
+            raise ValueError('Variable parameter "{}" does not have a size > 0.'.format(var.name))
+
+        variable_map.append(variable_map[-1] + size)
+        variables.append(var)
+
+    return variables, variable_map
+
+
+def cache_constraints(model):
+    constraints = []
+    for r in model.constraints:
+        constraints.append(r)
+
+    return constraints
+
+
+def cache_objectives(model):
+    # This is done to make sure the order is fixed during optimisation.
+    objectives = []
+    for r in model.objectives:
+        objectives.append(r)
+    return objectives
+
+
+# Global variables for individual processes to cache the model and some of its data.
+# The cache is keyed by a UID for each `BaseOptimisationWrapper`
+class ModelCache:
+    def __init__(self):
+        self.model = None
+        self.variables = None
+        self.variable_map = None
+        self.objectives = None
+        self.constraints = None
+MODEL_CACHE = {}
+
+
+class BaseOptimisationWrapper(object):
+    """ A helper class for running pywr optimisations with platypus.
+    """
+    def __init__(self, pywr_model_json, *args, **kwargs):
+        uid = kwargs.pop('uid', None)
+        self.pywr_model_klass = kwargs.pop("model_klass", Model)
+        super(BaseOptimisationWrapper, self).__init__(*args, **kwargs)
+        self.pywr_model_json = pywr_model_json
+
+        if uid is None:
+            uid = uuid.uuid4().hex  # Create a unique ID for caching.
+        self.uid = uid
+        self.run_stats = None
+
+    # The following properties enable attribute caching when repeat execution of the same model is undertaken.
+    @property
+    def _cached(self):
+        global MODEL_CACHE
+        try:
+            cache = MODEL_CACHE[self.uid]
+        except KeyError:
+            model = self.make_model()
+            model.setup()
+
+            cache = ModelCache()
+            cache.model = model
+            cache.variables, cache.variable_map = cache_variable_parameters(model)
+            cache.objectives = cache_objectives(model)
+            cache.constraints = cache_constraints(model)
+            MODEL_CACHE[self.uid] = cache
+        return cache
+
+    @property
+    def model(self):
+        return self._cached.model
+
+    @property
+    def model_variables(self):
+        return self._cached.variables
+
+    @property
+    def model_variable_map(self):
+        return self._cached.variable_map
+
+    @property
+    def model_objectives(self):
+        return self._cached.objectives
+
+    @property
+    def model_constraints(self):
+        return self._cached.constraints
+
+    def make_model(self):
+        m = self.pywr_model_klass.load(self.pywr_model_json)
+        # Apply any user defined changes to the model
+        self.customise_model(m)
+        return m
+
+    def customise_model(self, model):
+        pass  # By default there is no customisation.
+
+
+def clear_global_model_cache():
+    """ Clear the module level model cache. """
+    global MODEL_CACHE
+    MODEL_CACHE = {}
```

### Comparing `pywr-1.8.0/pywr/optimisation/platypus.py` & `pywr-1.9.0/pywr/optimisation/platypus.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-import numpy as np
-import platypus
-from . import cache_constraints, cache_objectives, cache_variable_parameters, BaseOptimisationWrapper
-
-import logging
-logger = logging.getLogger(__name__)
-
-
-def count_constraints(constraints):
-    """Count the number of constraints.
-
-     Recorders that are doubled bounded will create two constraints in the platypus problem.
-     """
-    count = 0
-    for c in constraints:
-        if c.is_double_bounded_constraint:
-            count += 2
-        elif c.is_constraint:
-            count += 1
-        else:
-            raise ValueError(f'Constraint "{c.name}" has no bounds defined.')
-    return count
-
-
-class PlatypusWrapper(BaseOptimisationWrapper):
-    """ A helper class for running pywr optimisations with platypus.
-    """
-    def __init__(self, *args, **kwargs):
-        super(PlatypusWrapper, self).__init__(*args, **kwargs)
-
-        # To determine the number of variables, etc
-        m = self.model
-
-        # Cache the variables, objectives and constraints
-        variables, variable_map = cache_variable_parameters(m)
-        objectives = cache_objectives(m)
-        constraints = cache_constraints(m)
-
-        if len(variables) < 1:
-            raise ValueError('At least one variable must be defined.')
-
-        if len(objectives) < 1:
-            raise ValueError('At least one objective must be defined.')
-
-        self.problem = platypus.Problem(variable_map[-1], len(objectives), count_constraints(constraints))
-        self.problem.function = self.evaluate
-        self.problem.wrapper = self
-
-        # Setup the problem; subclasses can change this behaviour
-        self._make_variables(variables)
-        self._make_constraints(constraints)
-
-    def _make_variables(self, variables):
-        """Setup the variable types. """
-
-        ix = 0
-        for var in variables:
-            if var.double_size > 0:
-                lower = var.get_double_lower_bounds()
-                upper = var.get_double_upper_bounds()
-                for i in range(var.double_size):
-                    self.problem.types[ix] = platypus.Real(lower[i], upper[i])
-                    ix += 1
-
-            if var.integer_size > 0:
-                lower = var.get_integer_lower_bounds()
-                upper = var.get_integer_upper_bounds()
-                for i in range(var.integer_size):
-                    # Integers are cast to real
-                    self.problem.types[ix] = platypus.Real(lower[i], upper[i])
-                    ix += 1
-
-    def _make_constraints(self, constraints):
-        """ Setup the constraints. """
-
-        ic = 0  # platypus constraint index
-        for c in constraints:
-            if c.is_double_bounded_constraint:
-                # Need to create two constraints
-                self.problem.constraints[ic] = platypus.Constraint('>=', value=c.constraint_lower_bounds)
-                self.problem.constraints[ic + 1] = platypus.Constraint('<=', value=c.constraint_upper_bounds)
-                ic += 2
-            elif c.is_equality_constraint:
-                self.problem.constraints[ic] = platypus.Constraint('==', value=c.constraint_lower_bounds)
-                ic += 1
-            elif c.is_lower_bounded_constraint:
-                self.problem.constraints[ic] = platypus.Constraint('>=', value=c.constraint_lower_bounds)
-                ic += 1
-            elif c.is_upper_bounded_constraint:
-                self.problem.constraints[ic] = platypus.Constraint('<=', value=c.constraint_upper_bounds)
-                ic += 1
-            else:
-                raise RuntimeError(f'The bounds of constraint "{c.name}" could not be identified correctly.')
-
-    def evaluate(self, solution):
-        logger.info('Evaluating solution ...')
-
-        for ivar, var in enumerate(self.model_variables):
-            j = slice(self.model_variable_map[ivar], self.model_variable_map[ivar+1])
-            x = np.array(solution[j])
-            assert len(x) == var.double_size + var.integer_size
-            if var.double_size > 0:
-                var.set_double_variables(np.array(x[:var.double_size]))
-
-            if var.integer_size > 0:
-                ints = np.round(np.array(x[-var.integer_size:])).astype(np.int32)
-                var.set_integer_variables(ints)
-
-        self.run_stats = self.model.run()
-
-        objectives = []
-        for r in self.model_objectives:
-            sign = 1.0 if r.is_objective == 'minimise' else -1.0
-            value = r.aggregated_value()
-            objectives.append(sign*value)
-
-        constraints = []
-        for c in self.model_constraints:
-            x = c.aggregated_value()
-            if c.is_double_bounded_constraint:
-                # Double bounded recorder is translated to two platypus constraints.
-                constraints.extend([x, x])
-            else:
-                constraints.append(x)
-
-        # Return values to the solution
-        logger.info(f'Evaluation completed in {self.run_stats.time_taken:.2f} seconds '
-                    f'({self.run_stats.speed:.2f} ts/s).')
-        if len(constraints) > 0:
-            return objectives, constraints
-        else:
-            return objectives
-
-
-class PywrRandomGenerator(platypus.RandomGenerator):
-    """A Platypus Generator that injects current and/or alternative setups of the Pywr model into the population.
-
-    When use_current is true the first Solution returned from the generate method is taken from the wrapper
-    (i.e. the Pywr model being wrapped) as the current values of the variable Parameters. This allows the population
-    to be seeded with the current model configuration, which is often an initial solution. Additional solutions
-    can be provided in as an iterable of solutions. These can come from an alternative source such as previous
-    optimisation.
-
-    Parameters
-    ==========
-    wrapper : PlatypusWrapper
-        Wrapper from which to grab the current model and decision variables.
-    use_current: Bool
-        Whether to generate an initial solution using the model's current configuration. Default is true.
-        Set this to False and pass some solutions to use pre-generated
-    solutions : List of dicts
-        An iterable of initial solutions to use (default is None). If given these alternative solutions
-        are provided to Platypus in order. Each item in the list should be a dictionary containing keys
-        for each of the variable Parameters in the optimisation. The value of each key should be another
-        dictionary container keys "doubles" and/or "integers" to provide the appropriate values as
-        dictated by the Parameter's type.
-    """
-    def __init__(self, *args, **kwargs):
-        self.wrapper = kwargs.pop('wrapper', None)
-        self.use_current = kwargs.pop('use_current', True)
-        self.solutions = kwargs.pop('solutions', None)
-        super().__init__(*args, **kwargs)
-        self._wrapped_generated = False
-        self._solution_pointer = 0
-
-    def generate(self, problem):
-        solution = None
-        if self.wrapper is not None:
-            if self.use_current and not self._wrapped_generated:
-                solution = platypus.Solution(problem)
-                # Gather the variable values from the wrapper.
-                variables = []
-                for ivar, var in enumerate(self.wrapper.model_variables):
-                    if var.double_size > 0:
-                        variables.extend(np.array(var.get_double_variables(), dtype=np.float64))
-                    if var.integer_size > 0:
-                        variables.extend(np.array(var.get_integer_variables(), dtype=np.int32))
-                solution.variables = variables
-                self._wrapped_generated = True  # Only include one solution with the current config.
-            elif self.solutions is not None and self._solution_pointer < len(self.solutions):
-                # Use one of the given solutions
-                solution = platypus.Solution(problem)
-                given_solution = self.solutions[self._solution_pointer]
-                variables = []
-                for ivar, var in enumerate(self.wrapper.model_variables):
-                    if var.double_size > 0:
-                        variables.extend(np.array(given_solution[var.name]['doubles'], dtype=np.float64))
-                    if var.integer_size > 0:
-                        variables.extend(np.array(given_solution[var.name]['integers'], dtype=np.int32))
-                solution.variables = variables
-                self._solution_pointer += 1  # Increment the internal pointer to return the next solution
-
-        if solution is None:
-            # Default to behaviour of RandomGenerator
-            solution = super().generate(problem)
-        return solution
+import numpy as np
+import platypus
+from . import cache_constraints, cache_objectives, cache_variable_parameters, BaseOptimisationWrapper
+
+import logging
+logger = logging.getLogger(__name__)
+
+
+def count_constraints(constraints):
+    """Count the number of constraints.
+
+     Recorders that are doubled bounded will create two constraints in the platypus problem.
+     """
+    count = 0
+    for c in constraints:
+        if c.is_double_bounded_constraint:
+            count += 2
+        elif c.is_constraint:
+            count += 1
+        else:
+            raise ValueError(f'Constraint "{c.name}" has no bounds defined.')
+    return count
+
+
+class PlatypusWrapper(BaseOptimisationWrapper):
+    """ A helper class for running pywr optimisations with platypus.
+    """
+    def __init__(self, *args, **kwargs):
+        super(PlatypusWrapper, self).__init__(*args, **kwargs)
+
+        # To determine the number of variables, etc
+        m = self.model
+
+        # Cache the variables, objectives and constraints
+        variables, variable_map = cache_variable_parameters(m)
+        objectives = cache_objectives(m)
+        constraints = cache_constraints(m)
+
+        if len(variables) < 1:
+            raise ValueError('At least one variable must be defined.')
+
+        if len(objectives) < 1:
+            raise ValueError('At least one objective must be defined.')
+
+        self.problem = platypus.Problem(variable_map[-1], len(objectives), count_constraints(constraints))
+        self.problem.function = self.evaluate
+        self.problem.wrapper = self
+
+        # Setup the problem; subclasses can change this behaviour
+        self._make_variables(variables)
+        self._make_constraints(constraints)
+
+    def _make_variables(self, variables):
+        """Setup the variable types. """
+
+        ix = 0
+        for var in variables:
+            if var.double_size > 0:
+                lower = var.get_double_lower_bounds()
+                upper = var.get_double_upper_bounds()
+                for i in range(var.double_size):
+                    self.problem.types[ix] = platypus.Real(lower[i], upper[i])
+                    ix += 1
+
+            if var.integer_size > 0:
+                lower = var.get_integer_lower_bounds()
+                upper = var.get_integer_upper_bounds()
+                for i in range(var.integer_size):
+                    # Integers are cast to real
+                    self.problem.types[ix] = platypus.Real(lower[i], upper[i])
+                    ix += 1
+
+    def _make_constraints(self, constraints):
+        """ Setup the constraints. """
+
+        ic = 0  # platypus constraint index
+        for c in constraints:
+            if c.is_double_bounded_constraint:
+                # Need to create two constraints
+                self.problem.constraints[ic] = platypus.Constraint('>=', value=c.constraint_lower_bounds)
+                self.problem.constraints[ic + 1] = platypus.Constraint('<=', value=c.constraint_upper_bounds)
+                ic += 2
+            elif c.is_equality_constraint:
+                self.problem.constraints[ic] = platypus.Constraint('==', value=c.constraint_lower_bounds)
+                ic += 1
+            elif c.is_lower_bounded_constraint:
+                self.problem.constraints[ic] = platypus.Constraint('>=', value=c.constraint_lower_bounds)
+                ic += 1
+            elif c.is_upper_bounded_constraint:
+                self.problem.constraints[ic] = platypus.Constraint('<=', value=c.constraint_upper_bounds)
+                ic += 1
+            else:
+                raise RuntimeError(f'The bounds of constraint "{c.name}" could not be identified correctly.')
+
+    def evaluate(self, solution):
+        logger.info('Evaluating solution ...')
+
+        for ivar, var in enumerate(self.model_variables):
+            j = slice(self.model_variable_map[ivar], self.model_variable_map[ivar+1])
+            x = np.array(solution[j])
+            assert len(x) == var.double_size + var.integer_size
+            if var.double_size > 0:
+                var.set_double_variables(np.array(x[:var.double_size]))
+
+            if var.integer_size > 0:
+                ints = np.round(np.array(x[-var.integer_size:])).astype(np.int32)
+                var.set_integer_variables(ints)
+
+        self.run_stats = self.model.run()
+
+        objectives = []
+        for r in self.model_objectives:
+            sign = 1.0 if r.is_objective == 'minimise' else -1.0
+            value = r.aggregated_value()
+            objectives.append(sign*value)
+
+        constraints = []
+        for c in self.model_constraints:
+            x = c.aggregated_value()
+            if c.is_double_bounded_constraint:
+                # Double bounded recorder is translated to two platypus constraints.
+                constraints.extend([x, x])
+            else:
+                constraints.append(x)
+
+        # Return values to the solution
+        logger.info(f'Evaluation completed in {self.run_stats.time_taken:.2f} seconds '
+                    f'({self.run_stats.speed:.2f} ts/s).')
+        if len(constraints) > 0:
+            return objectives, constraints
+        else:
+            return objectives
+
+
+class PywrRandomGenerator(platypus.RandomGenerator):
+    """A Platypus Generator that injects current and/or alternative setups of the Pywr model into the population.
+
+    When use_current is true the first Solution returned from the generate method is taken from the wrapper
+    (i.e. the Pywr model being wrapped) as the current values of the variable Parameters. This allows the population
+    to be seeded with the current model configuration, which is often an initial solution. Additional solutions
+    can be provided in as an iterable of solutions. These can come from an alternative source such as previous
+    optimisation.
+
+    Parameters
+    ==========
+    wrapper : PlatypusWrapper
+        Wrapper from which to grab the current model and decision variables.
+    use_current: Bool
+        Whether to generate an initial solution using the model's current configuration. Default is true.
+        Set this to False and pass some solutions to use pre-generated
+    solutions : List of dicts
+        An iterable of initial solutions to use (default is None). If given these alternative solutions
+        are provided to Platypus in order. Each item in the list should be a dictionary containing keys
+        for each of the variable Parameters in the optimisation. The value of each key should be another
+        dictionary container keys "doubles" and/or "integers" to provide the appropriate values as
+        dictated by the Parameter's type.
+    """
+    def __init__(self, *args, **kwargs):
+        self.wrapper = kwargs.pop('wrapper', None)
+        self.use_current = kwargs.pop('use_current', True)
+        self.solutions = kwargs.pop('solutions', None)
+        super().__init__(*args, **kwargs)
+        self._wrapped_generated = False
+        self._solution_pointer = 0
+
+    def generate(self, problem):
+        solution = None
+        if self.wrapper is not None:
+            if self.use_current and not self._wrapped_generated:
+                solution = platypus.Solution(problem)
+                # Gather the variable values from the wrapper.
+                variables = []
+                for ivar, var in enumerate(self.wrapper.model_variables):
+                    if var.double_size > 0:
+                        variables.extend(np.array(var.get_double_variables(), dtype=np.float64))
+                    if var.integer_size > 0:
+                        variables.extend(np.array(var.get_integer_variables(), dtype=np.int32))
+                solution.variables = variables
+                self._wrapped_generated = True  # Only include one solution with the current config.
+            elif self.solutions is not None and self._solution_pointer < len(self.solutions):
+                # Use one of the given solutions
+                solution = platypus.Solution(problem)
+                given_solution = self.solutions[self._solution_pointer]
+                variables = []
+                for ivar, var in enumerate(self.wrapper.model_variables):
+                    if var.double_size > 0:
+                        variables.extend(np.array(given_solution[var.name]['doubles'], dtype=np.float64))
+                    if var.integer_size > 0:
+                        variables.extend(np.array(given_solution[var.name]['integers'], dtype=np.int32))
+                solution.variables = variables
+                self._solution_pointer += 1  # Increment the internal pointer to return the next solution
+
+        if solution is None:
+            # Default to behaviour of RandomGenerator
+            solution = super().generate(problem)
+        return solution
```

### Comparing `pywr-1.8.0/pywr/optimisation/pygmo.py` & `pywr-1.9.0/pywr/optimisation/pygmo.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,88 +1,88 @@
-import numpy as np
-from . import BaseOptimisationWrapper
-
-import logging
-logger = logging.getLogger(__name__)
-
-
-class PygmoWrapper(BaseOptimisationWrapper):
-
-    def fitness(self, solution):
-        logger.info('Evaluating solution ...')
-
-        for ivar, var in enumerate(self.model_variables):
-            j = slice(self.model_variable_map[ivar], self.model_variable_map[ivar+1])
-            var.set_double_variables(np.array(solution[j]).copy())
-
-        self.model.reset()
-        self.run_stats = self.model.run()
-
-        objectives = []
-        for r in self.model_objectives:
-            sign = 1.0 if r.is_objective == 'minimise' else -1.0
-            value = r.aggregated_value()
-            objectives.append(sign*value)
-
-        # Return separate lists for equality and inequality constraints.
-        # pygmo requires that inequality constraints are all of the form g(x) <= 0
-        # Therefore these are converted to this form from their respective bounds.
-        eq_constraints = []
-        ineq_constraints = []
-        for r in self.model_constraints:
-            x = r.aggregated_value()
-            if r.is_double_bounded_constraint:
-                # Need to create two constraints
-                ineq_constraints.append(r.constraint_lower_bounds - x)
-                ineq_constraints.append(x - r.constraint_upper_bounds)
-            elif r.is_equality_constraint:
-                eq_constraints.append(x)
-            elif r.is_lower_bounded_constraint:
-                ineq_constraints.append(r.constraint_lower_bounds - x)
-            elif r.is_upper_bounded_constraint:
-                ineq_constraints.append(x - r.constraint_upper_bounds)
-            else:
-                raise RuntimeError(f'The bounds if constraint "{r.name}" could not be identified correctly.')
-
-        # Return values to the solution
-        logger.info(f'Evaluation completed in {self.run_stats.time_taken:.2f} seconds '
-                    f'({self.run_stats.speed:.2f} ts/s).')
-        return objectives + eq_constraints + ineq_constraints
-
-    def get_bounds(self):
-        """ Return the variable bounds. """
-        lower = []
-        upper = []
-        for var in self.model_variables:
-
-            if var.double_size > 0:
-                lower.append(var.get_double_lower_bounds())
-                upper.append(var.get_double_upper_bounds())
-
-            if var.integer_size > 0:
-                lower.append(var.get_integer_lower_bounds())
-                upper.append(var.get_integer_upper_bounds())
-
-        lower = np.concatenate(lower)
-        upper = np.concatenate(upper)
-
-        if len(lower) != len(upper):
-            raise ValueError(
-                'Upper and lower bounds are different lengths. Malformed bound data from Parameter:'
-                ' "{}"'.format(var.name))
-
-        return lower, upper
-
-    def get_nobj(self):
-        return len(self.model_objectives)
-
-    def get_nec(self):
-        return len([c for c in self.model_constraints if c.is_equality_constraint])
-
-    def get_nic(self):
-        count = 0
-        for c in self.model_constraints:
-            if c.is_double_bounded_constraint:
-                count += 2
-            elif c.is_lower_bounded_constraint or c.is_upper_bounded_constraint:
-                count += 1
-        return count
+import numpy as np
+from . import BaseOptimisationWrapper
+
+import logging
+logger = logging.getLogger(__name__)
+
+
+class PygmoWrapper(BaseOptimisationWrapper):
+
+    def fitness(self, solution):
+        logger.info('Evaluating solution ...')
+
+        for ivar, var in enumerate(self.model_variables):
+            j = slice(self.model_variable_map[ivar], self.model_variable_map[ivar+1])
+            var.set_double_variables(np.array(solution[j]).copy())
+
+        self.model.reset()
+        self.run_stats = self.model.run()
+
+        objectives = []
+        for r in self.model_objectives:
+            sign = 1.0 if r.is_objective == 'minimise' else -1.0
+            value = r.aggregated_value()
+            objectives.append(sign*value)
+
+        # Return separate lists for equality and inequality constraints.
+        # pygmo requires that inequality constraints are all of the form g(x) <= 0
+        # Therefore these are converted to this form from their respective bounds.
+        eq_constraints = []
+        ineq_constraints = []
+        for r in self.model_constraints:
+            x = r.aggregated_value()
+            if r.is_double_bounded_constraint:
+                # Need to create two constraints
+                ineq_constraints.append(r.constraint_lower_bounds - x)
+                ineq_constraints.append(x - r.constraint_upper_bounds)
+            elif r.is_equality_constraint:
+                eq_constraints.append(x)
+            elif r.is_lower_bounded_constraint:
+                ineq_constraints.append(r.constraint_lower_bounds - x)
+            elif r.is_upper_bounded_constraint:
+                ineq_constraints.append(x - r.constraint_upper_bounds)
+            else:
+                raise RuntimeError(f'The bounds if constraint "{r.name}" could not be identified correctly.')
+
+        # Return values to the solution
+        logger.info(f'Evaluation completed in {self.run_stats.time_taken:.2f} seconds '
+                    f'({self.run_stats.speed:.2f} ts/s).')
+        return objectives + eq_constraints + ineq_constraints
+
+    def get_bounds(self):
+        """ Return the variable bounds. """
+        lower = []
+        upper = []
+        for var in self.model_variables:
+
+            if var.double_size > 0:
+                lower.append(var.get_double_lower_bounds())
+                upper.append(var.get_double_upper_bounds())
+
+            if var.integer_size > 0:
+                lower.append(var.get_integer_lower_bounds())
+                upper.append(var.get_integer_upper_bounds())
+
+        lower = np.concatenate(lower)
+        upper = np.concatenate(upper)
+
+        if len(lower) != len(upper):
+            raise ValueError(
+                'Upper and lower bounds are different lengths. Malformed bound data from Parameter:'
+                ' "{}"'.format(var.name))
+
+        return lower, upper
+
+    def get_nobj(self):
+        return len(self.model_objectives)
+
+    def get_nec(self):
+        return len([c for c in self.model_constraints if c.is_equality_constraint])
+
+    def get_nic(self):
+        count = 0
+        for c in self.model_constraints:
+            if c.is_double_bounded_constraint:
+                count += 2
+            elif c.is_lower_bounded_constraint or c.is_upper_bounded_constraint:
+                count += 1
+        return count
```

### Comparing `pywr-1.8.0/pywr/parameters/_control_curves.pxd` & `pywr-1.9.0/pywr/parameters/_control_curves.pxd`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, Storage, AbstractStorage
-from ._parameters cimport Parameter, IndexParameter
-
-cdef class PiecewiseLinearControlCurve(Parameter):
-    cdef public AbstractStorage storage_node
-    cdef Parameter _control_curve
-    cdef public double below_lower
-    cdef public double below_upper
-    cdef public double above_lower
-    cdef public double above_upper
-    cdef public double minimum
-    cdef public double maximum
-
-cpdef double _interpolate(double current_position, double lower_bound, double upper_bound, double lower_value, double upper_value)
-
-cdef class BaseControlCurveParameter(Parameter):
-    cdef AbstractStorage _storage_node
-    cdef list _control_curves
-
-cdef class ControlCurveInterpolatedParameter(BaseControlCurveParameter):
-    cdef double[:] _values
-    cdef public list parameters
-
-
-cdef class ControlCurvePiecewiseInterpolatedParameter(BaseControlCurveParameter):
-    cdef double[:, :] _values
-    cdef public list parameters
-    cdef public double minimum
-    cdef public double maximum
-
-
-cdef class ControlCurveIndexParameter(IndexParameter):
-    cdef public AbstractStorage storage_node
-    cdef list _control_curves
-
-
-cdef class ControlCurveParameter(BaseControlCurveParameter):
-    cdef double[:] _values
-    cdef public list parameters
-    cdef int[:] _variable_indices
-    cdef double[:] _upper_bounds
-    cdef double[:] _lower_bounds
+from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, Storage, AbstractStorage
+from ._parameters cimport Parameter, IndexParameter
+
+cdef class PiecewiseLinearControlCurve(Parameter):
+    cdef public AbstractStorage storage_node
+    cdef Parameter _control_curve
+    cdef public double below_lower
+    cdef public double below_upper
+    cdef public double above_lower
+    cdef public double above_upper
+    cdef public double minimum
+    cdef public double maximum
+
+cpdef double _interpolate(double current_position, double lower_bound, double upper_bound, double lower_value, double upper_value)
+
+cdef class BaseControlCurveParameter(Parameter):
+    cdef AbstractStorage _storage_node
+    cdef list _control_curves
+
+cdef class ControlCurveInterpolatedParameter(BaseControlCurveParameter):
+    cdef double[:] _values
+    cdef public list parameters
+
+
+cdef class ControlCurvePiecewiseInterpolatedParameter(BaseControlCurveParameter):
+    cdef double[:, :] _values
+    cdef public list parameters
+    cdef public double minimum
+    cdef public double maximum
+
+
+cdef class ControlCurveIndexParameter(IndexParameter):
+    cdef public AbstractStorage storage_node
+    cdef list _control_curves
+
+
+cdef class ControlCurveParameter(BaseControlCurveParameter):
+    cdef double[:] _values
+    cdef public list parameters
+    cdef int[:] _variable_indices
+    cdef double[:] _upper_bounds
+    cdef double[:] _lower_bounds
```

### Comparing `pywr-1.8.0/pywr/parameters/_control_curves.pyx` & `pywr-1.9.0/pywr/parameters/_control_curves.pyx`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,634 +1,634 @@
-import cython
-import numpy as np
-cimport numpy as np
-from .parameters import parameter_registry, ConstantParameter, parameter_property
-from ._parameters import load_parameter, load_parameter_values, Parameter, IndexParameter
-import warnings
-
-# http://stackoverflow.com/a/20031818/1300519
-cdef extern from "numpy/npy_math.h":
-    bint npy_isnan(double x)
-
-cdef class PiecewiseLinearControlCurve(Parameter):
-    """Piecewise function composed of two linear curves
-
-    Parameters
-    ----------
-    model : Model
-    storage_node : Storage
-    control_curve : Parameter
-    values : [(float, float), (float, float)]
-        Iterable of 2-tuples, representing the lower and upper value of the
-        linear interpolation below and above the control curve, respectively.
-    minimum : float
-        The storage considered the bottom of the lower curve, 0-1 (default=0).
-    maximum : float
-        The storage considered the top of the upper curve, 0-1 (default=1).
-    """
-    def __init__(self, model, storage_node, control_curve, values, minimum=0.0, maximum=1.0, *args, **kwargs):
-        warnings.warn("`PiecewiseLinearControlCurve` has been deprecated in favour of "
-                      "`ControlCurvePiecewiseInterpolatedParameter`. It will be removed in a future version "
-                      "of Pywr.", DeprecationWarning)
-        super(PiecewiseLinearControlCurve, self).__init__(model, *args, **kwargs)
-        self._control_curve = None
-        self.storage_node = storage_node
-        self.control_curve = control_curve
-        self.below_lower, self.below_upper = values[0]
-        self.above_lower, self.above_upper = values[1]
-        self.minimum = minimum
-        self.maximum = maximum
-
-    property control_curve:
-        def __get__(self):
-            return self._control_curve
-        def __set__(self, parameter):
-            if self._control_curve:
-                self.children.remove(self._control_curve)
-            self.children.add(parameter)
-            self._control_curve = parameter
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef double value
-        cdef double control_curve = self._control_curve.get_value(scenario_index)
-        cdef double current_pc = self.storage_node._current_pc[scenario_index.global_id]
-        if current_pc > control_curve:
-            value = _interpolate(current_pc, control_curve, self.maximum, self.above_lower, self.above_upper)
-        else:
-            value = _interpolate(current_pc, self.minimum, control_curve, self.below_lower, self.below_upper)
-        return value
-
-    cpdef reset(self):
-        super(PiecewiseLinearControlCurve, self).setup()
-        assert self.maximum > self.minimum
-        assert np.isfinite(self.below_lower)
-        assert np.isfinite(self.below_upper)
-        assert np.isfinite(self.above_lower)
-        assert np.isfinite(self.above_upper)
-
-    @classmethod
-    def load(cls, model, data):
-        storage_node = model._get_node_from_ref(model, data["storage_node"])
-        control_curve = load_parameter(model, data["control_curve"])
-        values = data["values"]
-        kwargs = {}
-        if "minimum" in data.keys():
-            kwargs["minimum"] = data["minimum"]
-        if "maximum" in data.keys():
-            kwargs["maximum"] = data["maximum"]
-        parameter = cls(model, storage_node, control_curve, values, **kwargs)
-        return parameter
-
-PiecewiseLinearControlCurve.register()
-
-
-@cython.cdivision(True)
-cpdef double _interpolate(double current_position, double lower_bound, double upper_bound, double lower_value, double upper_value):
-    """Interpolation function used by PiecewiseLinearControlCurve"""
-    cdef double factor
-    cdef double value
-    if current_position < lower_bound:
-        value = lower_value
-    elif current_position > upper_bound:
-        value = upper_value
-    elif upper_bound == lower_bound:
-        value = lower_value
-    else:
-        factor = (current_position - lower_bound) / (upper_bound - lower_bound)
-        value = lower_value + (upper_value - lower_value) * factor
-    return value
-
-
-cdef class BaseControlCurveParameter(Parameter):
-    """ Base class for all Parameters that rely on a the attached Node containing a control_curve Parameter
-
-    """
-    def __init__(self, model, AbstractStorage storage_node, control_curves, **kwargs):
-        """
-
-        Parameters
-        ----------
-        storage_node : `Storage`
-            An optional `Storage` node that can be used to query the current percentage volume.
-        control_curves : iterable of Parameter objects or single Parameter
-            The Parameter objects to use as a control curve(s).
-        """
-        super(BaseControlCurveParameter, self).__init__(model, **kwargs)
-        self.control_curves = control_curves
-        if storage_node is None:
-            raise ValueError("storage_node is required")
-        self._storage_node = storage_node
-
-    property control_curves:
-        def __get__(self):
-            return self._control_curves
-        def __set__(self, control_curves):
-            # Accept a single Parameter and convert to a list internally
-            if isinstance(control_curves, Parameter):
-                control_curves = [control_curves]
-
-            # remove existing control curves (if any)
-            if self._control_curves is not None:
-                for control_curve in self._control_curves:
-                    control_curve.parents.remove(self)
-
-            _new_control_curves = []
-            for control_curve in control_curves:
-                # Accept numeric inputs and convert to `ConstantParameter`
-                if isinstance(control_curve, (float, int)):
-                    control_curve = ConstantParameter(self.model, control_curve)
-
-                control_curve.parents.add(self)
-                _new_control_curves.append(control_curve)
-            self._control_curves = list(_new_control_curves)
-
-    property storage_node:
-        def __get__(self):
-            return self._storage_node
-        def __set__(self, value):
-            self._storage_node = value
-
-    @classmethod
-    def _load_control_curves(cls, model, data):
-        """ Private class method to load control curve data from dict. """
-
-        control_curves = []
-        if 'control_curve' in data:
-            control_curves.append(load_parameter(model, data.pop('control_curve')))
-        elif 'control_curves' in data:
-            for pdata in data.pop('control_curves'):
-                control_curves.append(load_parameter(model, pdata))
-        return control_curves
-
-    @classmethod
-    def _load_storage_node(cls, model, data):
-        """ Private class method to load storage node from dict. """
-        node = model._get_node_from_ref(model, data.pop("storage_node"))
-        return node
-
-
-BaseControlCurveParameter.register()
-
-
-cdef class ControlCurveInterpolatedParameter(BaseControlCurveParameter):
-    """A control curve Parameter that interpolates between three or more values
-
-    Return values are linearly interpolated between control curves, with the
-    first and last value being 100% and 0% respectively.
-
-    Parameters
-    ----------
-    storage_node : `Storage`
-        The storage node to compare the control curve(s) to.
-    control_curves : list of `Parameter` or floats
-        A list of parameters representing the control curve(s). These are
-        often MonthlyProfileParameters or DailyProfileParameters, but may be
-        any Parameter that returns values between 0.0 and 1.0. If floats are
-        passed they are converted to `ConstantParameter`.
-    values : list of float
-        A list of values to return corresponding to the control curves. The
-        length of the list should be 2 + len(control_curves).
-    parameters : iterable `Parameter` objects or `None`, optional
-        If `values` is `None` then `parameters` can specify a `Parameter` object to use at each
-        of the control curves. The number of parameters should be 2 + len(control_curves)
-
-    Examples
-    --------
-    In the example below the cost of a storage node is related to it's volume.
-    At 100% full the cost is 0. Between 100% and 50% the cost is linearly
-    interpolated between 0 and -5. Between 50% and 30% the cost is interpolated
-    between -5 and -10. Between 30% and 0% the cost is interpolated between -10
-    and -20
-
-    ::
-
-        Volume:  100%             50%      30%       0%
-                  |----------------|--------|--------|
-          Cost:  0.0            -5.0     -10.0   -20.0
-
-
-    >>> storage_node = Storage(model, "reservoir", max_volume=100, initial_volume=100)
-    >>> ccs = [ConstantParameter(0.5), ConstantParameter(0.3)]
-    >>> values = [0.0, -5.0, -10.0, -20.0]
-    >>> cost = ControlCurveInterpolatedParameter(storage_node, ccs, values)
-    >>> storage_node.cost = cost
-    """
-    def __init__(self, model, storage_node, control_curves, values=None, parameters=None, **kwargs):
-        super(ControlCurveInterpolatedParameter, self).__init__(model, storage_node, control_curves, **kwargs)
-        # Expected number of values is number of control curves plus two.
-        nvalues = len(self.control_curves) + 2
-        self.parameters = None
-
-        if values is not None:
-            if len(values) != nvalues:
-                raise ValueError('Length of values should be two more than the number of '
-                                 'control curves ({}).'.format(nvalues))
-            self.values = np.asarray(values)
-
-        elif parameters is not None:
-            if len(parameters) != nvalues:
-                raise ValueError('Length of parameters should be two more than the number of '
-                                 'control curves ({}).'.format(nvalues))
-            self.parameters = list(parameters)
-            # Make sure these parameters depend on this parameter to ensure they are evaluated
-            # in the correct order.
-            for p in self.parameters:
-                p.parents.add(self)
-        else:
-            raise ValueError('One of values or parameters keywords must be given.')
-
-    property values:
-        def __get__(self):
-            return np.array(self._values)
-        def __set__(self, values):
-            self._values = np.array(values)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int j
-        cdef Parameter cc_param, value_param
-        cdef double cc, cc_prev
-        # return the interpolated value for the current level.
-        cdef double current_pc = self._storage_node._current_pc[scenario_index.global_id]
-        cdef double weight
-        cdef double[:] values  # y values to interpolate between in this time-step
-
-        if self.parameters is not None:
-            # If there are parameter use them to gather the interpolation values
-            values = np.empty(len(self.parameters))
-            for j, value_param in enumerate(self.parameters):
-                values[j] = value_param.get_value(scenario_index)
-        else:
-            # Otherwise use the given array of floats.
-            # This makes a reference rather than a copy.
-            values = self._values
-
-        # Bounds check the current pc storage.
-        # Always return the first value if storage above 100% or NaN
-        if current_pc > 1.0 or npy_isnan(current_pc):
-            return values[0]
-        # Always return last value is storage less than 0%
-        if current_pc < 0.0:
-            return values[-1]
-
-        # Assumes control_curves is sorted highest to lowest
-        # First level 100%
-        cc_prev = 1.0
-        for j, cc_param in enumerate(self._control_curves):
-            cc = cc_param.get_value(scenario_index)
-            # If level above control curve then return this level's value
-            if current_pc >= cc:
-                try:
-                    weight = (current_pc - cc) / (cc_prev - cc)
-                except ZeroDivisionError:
-                    # Last two control curves identical; return the next value
-                    return values[j+1]
-                return values[j]*weight + values[j+1]*(1.0 - weight)
-            # Update previous value for next iteration
-            cc_prev = cc
-
-        # Current storage is above none of the control curves
-        # Therefore interpolate between last control curve and bottom
-        cc = 0.0
-        try:
-            weight = (current_pc - cc) / (cc_prev - cc)
-        except ZeroDivisionError:
-            # cc_prev == cc  i.e. last control curve is close to 0%
-            return values[-2]
-        return values[-2]*weight + values[-1]*(1.0 - weight)
-
-    @classmethod
-    def load(cls, model, data):
-        control_curves = super(ControlCurveInterpolatedParameter, cls)._load_control_curves(model, data)
-        storage_node = super(ControlCurveInterpolatedParameter, cls)._load_storage_node(model, data)
-        if "parameters" in data:
-            parameters = [load_parameter(model, p) for p in data.pop("parameters")]
-            values = None
-        else:
-            values = load_parameter_values(model, data)
-            parameters = None
-        return cls(model, storage_node, control_curves, values=values, parameters=parameters)
-
-ControlCurveInterpolatedParameter.register()
-
-
-cdef class ControlCurvePiecewiseInterpolatedParameter(BaseControlCurveParameter):
-    """A control curve Parameter that interpolates between two or more pairs of values.
-
-    Return values are linearly interpolated between a pair of values depending on the current
-    storage. The first pair is used between maximum and the first control curve, the next pair
-    between the first control curve and second control curve, and so on until the last pair is
-    used between the last control curve and the minimum value. The first value in each pair is the
-    value at the upper position, and the second the value at the lower position.
-
-    Parameters
-    ----------
-    storage_node : `Storage`
-        The storage node to compare the control curve(s) to.
-    control_curves : list of `Parameter` or floats
-        A list of parameters representing the control curve(s). These are
-        often MonthlyProfileParameters or DailyProfileParameters, but may be
-        any Parameter that returns values between 0.0 and 1.0. If floats are
-        passed they are converted to `ConstantParameter`.
-    values : 2D array or list of lists
-        A list of value pairs to interpolate between. The length of the list should be 1 + len(control_curves).
-    minimum : float
-        The storage considered the bottom of the lower curve, 0-1 (default=0).
-    maximum : float
-        The storage considered the top of the upper curve, 0-1 (default=1).
-
-    """
-    def __init__(self, model, storage_node, control_curves, values, minimum=0.0, maximum=1.0, **kwargs):
-        super(ControlCurvePiecewiseInterpolatedParameter, self).__init__(model, storage_node, control_curves, **kwargs)
-        self.values = np.array(values, dtype=np.float64)
-        self.minimum = minimum
-        self.maximum = maximum
-
-    property values:
-        def __get__(self):
-            return np.array(self._values)
-        def __set__(self, values):
-            # Expected number of values is number of control curves plus one.
-            nvalues = len(self.control_curves) + 1
-            if len(values) != nvalues:
-                raise ValueError('Length of values should be two more than the number of '
-                                 'control curves ({}).'.format(nvalues))
-            elif len(values[0]) != 2:
-                raise ValueError('The second dimension of values should be of length 2.')
-            self._values = np.array(values)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int j, ncc
-        cdef Parameter cc_param
-        cdef double cc, cc_prev, val
-        # return the interpolated value for the current level.
-        cdef double current_pc = self._storage_node._current_pc[scenario_index.global_id]
-
-        cc_prev = self.maximum
-        for j, cc_param in enumerate(self._control_curves):
-            cc = cc_param.get_value(scenario_index)
-            # If level above control curve then return this level's value
-            if current_pc >= cc:
-                return _interpolate(current_pc, cc, cc_prev, self._values[j, 1], self._values[j, 0])
-
-            # Update previous value for next iteration
-            cc_prev = cc
-
-        # Current storage is above none of the control curves
-        # Therefore interpolate between last control curve and minimum
-        ncc = len(self._control_curves)
-        val = _interpolate(current_pc, self.minimum, cc_prev, self._values[ncc, 1], self._values[ncc, 0])
-        return val
-
-    @classmethod
-    def load(cls, model, data):
-        control_curves = super(ControlCurvePiecewiseInterpolatedParameter, cls)._load_control_curves(model, data)
-        storage_node = super(ControlCurvePiecewiseInterpolatedParameter, cls)._load_storage_node(model, data)
-        values = load_parameter_values(model, data)
-        return cls(model, storage_node, control_curves, values=values, **data)
-
-ControlCurvePiecewiseInterpolatedParameter.register()
-
-
-cdef class ControlCurveIndexParameter(IndexParameter):
-    """Multiple control curve holder which returns an index not a value
-
-    Parameters
-    ----------
-    storage_node : `Storage`
-    control_curves : iterable of `Parameter` instances or floats
-    """
-    def __init__(self, model, storage_node, control_curves, **kwargs):
-        super(ControlCurveIndexParameter, self).__init__(model, **kwargs)
-        self.storage_node = storage_node
-        self.control_curves = control_curves
-
-    property control_curves:
-        def __get__(self):
-            return self._control_curves
-        def __set__(self, control_curves):
-            # Accept a single Parameter and convert to a list internally
-            if isinstance(control_curves, Parameter):
-                control_curves = [control_curves]
-
-            # remove existing control curves (if any)
-            if self._control_curves is not None:
-                for control_curve in self._control_curves:
-                    control_curve.parents.remove(self)
-
-            _new_control_curves = []
-            for control_curve in control_curves:
-                # Accept numeric inputs and convert to `ConstantParameter`
-                if isinstance(control_curve, (float, int)):
-                    control_curve = ConstantParameter(self.model, control_curve)
-
-                control_curve.parents.add(self)
-                _new_control_curves.append(control_curve)
-            self._control_curves = list(_new_control_curves)
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns the index of the first control curve the storage is above
-
-        The index is zero-based. For example, if only one control curve is
-        supplied then the index is either 0 (above) or 1 (below). For two
-        curves the index is either 0 (above both), 1 (in between), or 2 (below
-        both), and so on.
-        """
-        cdef double current_percentage
-        cdef double target_percentage
-        cdef int index, j
-        cdef Parameter control_curve
-        current_percentage = self.storage_node._current_pc[scenario_index.global_id]
-        index = len(self.control_curves)
-        for j, control_curve in enumerate(self.control_curves):
-            target_percentage = control_curve.get_value(scenario_index)
-            if current_percentage >= target_percentage:
-                index = j
-                break
-        return index
-
-    @classmethod
-    def load(cls, model, data):
-        storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
-        control_curves = [load_parameter(model, d) for d in data.pop("control_curves")]
-        return cls(model, storage_node, control_curves, **data)
-ControlCurveIndexParameter.register()
-
-
-cdef class ControlCurveParameter(BaseControlCurveParameter):
-    """ A generic multi-levelled control curve Parameter.
-
-     This parameter can be used to return different values when a `Storage` node's current
-      volumes is at different percentage of `max_volume` relative to predefined control curves.
-      Control curves must be defined in the range [0, 1] corresponding to 0% and 100% volume.
-
-     By default this parameter returns an integer sequence from zero if the first control curve
-      is passed, and incrementing by one for each control curve (or "level") the `Storage` node
-      is below.
-
-    Parameters
-    ----------
-    storage_node : `Storage`
-        An optional `Storage` node that can be used to query the current percentage volume.
-    control_curves : `float`, `int` or `Parameter` object, or iterable thereof
-        The position of the control curves. Internally `float` or `int` types are cast to
-        `ConstantParameter`. Multiple values correspond to multiple control curve positions.
-        These should be specified in descending order.
-    values : array_like or `None`, optional
-        The values to return if the `Storage` object is above the correspond control curve.
-        I.e. the first value is returned if the current volume is above the first control curve,
-        and second value if above the second control curve, and so on. The length of `values`
-        must be one more than than the length of `control_curves`.
-    parameters : iterable `Parameter` objects or `None`, optional
-        If `values` is `None` then `parameters` can specify a `Parameter` object to use at level
-        of the control curves. In the same way as `values` the first `Parameter` is used if
-        `Storage` is above the first control curve, and second `Parameter` if above the
-        second control curve, and so on.
-    variable_indices : iterable of ints, optional
-        A list of indices that correspond to items in `values` which are to be considered variables
-         when `self.is_variable` is True. This mechanism allows a subset of `values` to be variable.
-    lower_bounds, upper_bounds : array_like, optional
-        Bounds of the variables. The length must correspond to the length of `variable_indices`, i.e.
-         there are bounds for each index to be considered as a variable.
-
-    Notes
-    -----
-    If `values` and `parameters` are both `None`, the default, then `values` defaults to
-     a range of integers, starting at zero, one more than length of `control_curves`.
-
-    See also
-    --------
-    `BaseControlCurveParameter`
-
-    """
-    def __init__(self, model, storage_node, control_curves, values=None, parameters=None,
-                 variable_indices=None, upper_bounds=None, lower_bounds=None, **kwargs):
-        super(ControlCurveParameter, self).__init__(model, storage_node, control_curves, **kwargs)
-        # Expected number of values is number of control curves plus one.
-        nvalues = len(self.control_curves) + 1
-        self.parameters = None
-        if values is not None:
-            if len(values) != nvalues:
-                raise ValueError('Length of values should be one more than the number of '
-                                 'control curves ({}).'.format(nvalues))
-            self.values = values
-        elif parameters is not None:
-            if len(parameters) != nvalues:
-                raise ValueError('Length of parameters should be one more than the number of '
-                                 'control curves ({}).'.format(nvalues))
-            self.parameters = list(parameters)
-            # Make sure these parameters depend on this parameter to ensure they are evaluated
-            # in the correct order.
-            for p in self.parameters:
-                p.parents.add(self)
-        else:
-            # No values or parameters given, default to sequence of integers
-            self.values = np.arange(nvalues)
-
-        # Default values
-        self._upper_bounds = None
-        self._lower_bounds = None
-
-        if variable_indices is not None:
-            self.variable_indices = variable_indices
-            self.double_size = len(variable_indices)
-        else:
-            self.double_size = 0
-        # Bounds for use as a variable (i.e. when self.is_variable = True)
-        if upper_bounds is not None:
-            if self.values is None or variable_indices is None:
-                raise ValueError('Upper bounds can only be specified if `values` and `variable_indices` '
-                                 'is not `None`.')
-            if len(upper_bounds) != self.double_size:
-                raise ValueError('Length of upper_bounds should be equal to the length of `variable_indices` '
-                                 '({}).'.format(self.double_size))
-            self._upper_bounds = np.array(upper_bounds)
-
-        if lower_bounds is not None:
-            if self.values is None or variable_indices is None:
-                raise ValueError('Lower bounds can only be specified if `values` and `variable_indices` '
-                                 'is not `None`.')
-            if len(lower_bounds) != self.double_size:
-                raise ValueError('Length of lower_bounds should be equal to the length of `variable_indices` '
-                                 '({}).'.format(self.double_size))
-            self._lower_bounds = np.array(lower_bounds)
-
-    property values:
-        def __get__(self):
-            return np.asarray(self._values)
-        def __set__(self, values):
-            self._values = np.asarray(values, dtype=np.float64)
-
-    property variable_indices:
-        def __get__(self):
-            return np.array(self._variable_indices)
-        def __set__(self, values):
-            self._variable_indices = np.array(values, dtype=np.int)
-
-    @classmethod
-    def load(cls, model, data):
-        control_curves = super(ControlCurveParameter, cls)._load_control_curves(model, data)
-        storage_node = super(ControlCurveParameter, cls)._load_storage_node(model, data)
-
-        parameters = None
-        values = None
-        if 'values' in data:
-            values = load_parameter_values(model, data)
-        elif 'parameters' in data:
-            # Load parameters
-            parameters_data = data['parameters']
-            parameters = []
-            for pdata in parameters_data:
-                parameters.append(load_parameter(model, pdata))
-
-        return cls(model, storage_node, control_curves, values=values, parameters=parameters)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i = scenario_index.global_id
-        cdef int j
-        cdef AbstractStorage node
-        cdef double cc
-        cdef Parameter param, cc_param
-        node = self.node if self.storage_node is None else self.storage_node
-
-        # Assumes control_curves is sorted highest to lowest
-        for j, cc_param in enumerate(self.control_curves):
-            cc = cc_param.get_value(scenario_index)
-            # If level above control curve then return this level's value
-            if node._current_pc[i] >= cc:
-                if self.parameters is not None:
-                    param = self.parameters[j]
-                    return param.get_value(scenario_index)
-                else:
-                    return self._values[j]
-
-        if self.parameters is not None:
-            param = self.parameters[-1]
-            return param.get_value(scenario_index)
-        else:
-            return self._values[-1]
-
-    cpdef set_double_variables(self, double[:] values):
-        cdef int i
-        cdef double v
-
-        if len(values) != len(self.variable_indices):
-            raise ValueError('Number of values must be the same as the number of variable_indices.')
-
-        if self.double_size != 0:
-            for i, v in zip(self.variable_indices, values):
-                self._values[i] = v
-
-    cpdef double[:] get_double_variables(self):
-        cdef int i, j
-        cdef double v
-        cdef double[:] arry = np.empty((len(self.variable_indices), ))
-        for i, j in enumerate(self.variable_indices):
-            arry[i] = self._values[j]
-        return arry
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return self._lower_bounds
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return self._upper_bounds
-
-ControlCurveParameter.register()
+import cython
+import numpy as np
+cimport numpy as np
+from .parameters import parameter_registry, ConstantParameter, parameter_property
+from ._parameters import load_parameter, load_parameter_values, Parameter, IndexParameter
+import warnings
+
+# http://stackoverflow.com/a/20031818/1300519
+cdef extern from "numpy/npy_math.h":
+    bint npy_isnan(double x)
+
+cdef class PiecewiseLinearControlCurve(Parameter):
+    """Piecewise function composed of two linear curves
+
+    Parameters
+    ----------
+    model : Model
+    storage_node : Storage
+    control_curve : Parameter
+    values : [(float, float), (float, float)]
+        Iterable of 2-tuples, representing the lower and upper value of the
+        linear interpolation below and above the control curve, respectively.
+    minimum : float
+        The storage considered the bottom of the lower curve, 0-1 (default=0).
+    maximum : float
+        The storage considered the top of the upper curve, 0-1 (default=1).
+    """
+    def __init__(self, model, storage_node, control_curve, values, minimum=0.0, maximum=1.0, *args, **kwargs):
+        warnings.warn("`PiecewiseLinearControlCurve` has been deprecated in favour of "
+                      "`ControlCurvePiecewiseInterpolatedParameter`. It will be removed in a future version "
+                      "of Pywr.", DeprecationWarning)
+        super(PiecewiseLinearControlCurve, self).__init__(model, *args, **kwargs)
+        self._control_curve = None
+        self.storage_node = storage_node
+        self.control_curve = control_curve
+        self.below_lower, self.below_upper = values[0]
+        self.above_lower, self.above_upper = values[1]
+        self.minimum = minimum
+        self.maximum = maximum
+
+    property control_curve:
+        def __get__(self):
+            return self._control_curve
+        def __set__(self, parameter):
+            if self._control_curve:
+                self.children.remove(self._control_curve)
+            self.children.add(parameter)
+            self._control_curve = parameter
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef double value
+        cdef double control_curve = self._control_curve.get_value(scenario_index)
+        cdef double current_pc = self.storage_node._current_pc[scenario_index.global_id]
+        if current_pc > control_curve:
+            value = _interpolate(current_pc, control_curve, self.maximum, self.above_lower, self.above_upper)
+        else:
+            value = _interpolate(current_pc, self.minimum, control_curve, self.below_lower, self.below_upper)
+        return value
+
+    cpdef reset(self):
+        super(PiecewiseLinearControlCurve, self).setup()
+        assert self.maximum > self.minimum
+        assert np.isfinite(self.below_lower)
+        assert np.isfinite(self.below_upper)
+        assert np.isfinite(self.above_lower)
+        assert np.isfinite(self.above_upper)
+
+    @classmethod
+    def load(cls, model, data):
+        storage_node = model._get_node_from_ref(model, data["storage_node"])
+        control_curve = load_parameter(model, data["control_curve"])
+        values = data["values"]
+        kwargs = {}
+        if "minimum" in data.keys():
+            kwargs["minimum"] = data["minimum"]
+        if "maximum" in data.keys():
+            kwargs["maximum"] = data["maximum"]
+        parameter = cls(model, storage_node, control_curve, values, **kwargs)
+        return parameter
+
+PiecewiseLinearControlCurve.register()
+
+
+@cython.cdivision(True)
+cpdef double _interpolate(double current_position, double lower_bound, double upper_bound, double lower_value, double upper_value):
+    """Interpolation function used by PiecewiseLinearControlCurve"""
+    cdef double factor
+    cdef double value
+    if current_position < lower_bound:
+        value = lower_value
+    elif current_position > upper_bound:
+        value = upper_value
+    elif upper_bound == lower_bound:
+        value = lower_value
+    else:
+        factor = (current_position - lower_bound) / (upper_bound - lower_bound)
+        value = lower_value + (upper_value - lower_value) * factor
+    return value
+
+
+cdef class BaseControlCurveParameter(Parameter):
+    """ Base class for all Parameters that rely on a the attached Node containing a control_curve Parameter
+
+    """
+    def __init__(self, model, AbstractStorage storage_node, control_curves, **kwargs):
+        """
+
+        Parameters
+        ----------
+        storage_node : `Storage`
+            An optional `Storage` node that can be used to query the current percentage volume.
+        control_curves : iterable of Parameter objects or single Parameter
+            The Parameter objects to use as a control curve(s).
+        """
+        super(BaseControlCurveParameter, self).__init__(model, **kwargs)
+        self.control_curves = control_curves
+        if storage_node is None:
+            raise ValueError("storage_node is required")
+        self._storage_node = storage_node
+
+    property control_curves:
+        def __get__(self):
+            return self._control_curves
+        def __set__(self, control_curves):
+            # Accept a single Parameter and convert to a list internally
+            if isinstance(control_curves, Parameter):
+                control_curves = [control_curves]
+
+            # remove existing control curves (if any)
+            if self._control_curves is not None:
+                for control_curve in self._control_curves:
+                    control_curve.parents.remove(self)
+
+            _new_control_curves = []
+            for control_curve in control_curves:
+                # Accept numeric inputs and convert to `ConstantParameter`
+                if isinstance(control_curve, (float, int)):
+                    control_curve = ConstantParameter(self.model, control_curve)
+
+                control_curve.parents.add(self)
+                _new_control_curves.append(control_curve)
+            self._control_curves = list(_new_control_curves)
+
+    property storage_node:
+        def __get__(self):
+            return self._storage_node
+        def __set__(self, value):
+            self._storage_node = value
+
+    @classmethod
+    def _load_control_curves(cls, model, data):
+        """ Private class method to load control curve data from dict. """
+
+        control_curves = []
+        if 'control_curve' in data:
+            control_curves.append(load_parameter(model, data.pop('control_curve')))
+        elif 'control_curves' in data:
+            for pdata in data.pop('control_curves'):
+                control_curves.append(load_parameter(model, pdata))
+        return control_curves
+
+    @classmethod
+    def _load_storage_node(cls, model, data):
+        """ Private class method to load storage node from dict. """
+        node = model._get_node_from_ref(model, data.pop("storage_node"))
+        return node
+
+
+BaseControlCurveParameter.register()
+
+
+cdef class ControlCurveInterpolatedParameter(BaseControlCurveParameter):
+    """A control curve Parameter that interpolates between three or more values
+
+    Return values are linearly interpolated between control curves, with the
+    first and last value being 100% and 0% respectively.
+
+    Parameters
+    ----------
+    storage_node : `Storage`
+        The storage node to compare the control curve(s) to.
+    control_curves : list of `Parameter` or floats
+        A list of parameters representing the control curve(s). These are
+        often MonthlyProfileParameters or DailyProfileParameters, but may be
+        any Parameter that returns values between 0.0 and 1.0. If floats are
+        passed they are converted to `ConstantParameter`.
+    values : list of float
+        A list of values to return corresponding to the control curves. The
+        length of the list should be 2 + len(control_curves).
+    parameters : iterable `Parameter` objects or `None`, optional
+        If `values` is `None` then `parameters` can specify a `Parameter` object to use at each
+        of the control curves. The number of parameters should be 2 + len(control_curves)
+
+    Examples
+    --------
+    In the example below the cost of a storage node is related to it's volume.
+    At 100% full the cost is 0. Between 100% and 50% the cost is linearly
+    interpolated between 0 and -5. Between 50% and 30% the cost is interpolated
+    between -5 and -10. Between 30% and 0% the cost is interpolated between -10
+    and -20
+
+    ::
+
+        Volume:  100%             50%      30%       0%
+                  |----------------|--------|--------|
+          Cost:  0.0            -5.0     -10.0   -20.0
+
+
+    >>> storage_node = Storage(model, "reservoir", max_volume=100, initial_volume=100)
+    >>> ccs = [ConstantParameter(0.5), ConstantParameter(0.3)]
+    >>> values = [0.0, -5.0, -10.0, -20.0]
+    >>> cost = ControlCurveInterpolatedParameter(storage_node, ccs, values)
+    >>> storage_node.cost = cost
+    """
+    def __init__(self, model, storage_node, control_curves, values=None, parameters=None, **kwargs):
+        super(ControlCurveInterpolatedParameter, self).__init__(model, storage_node, control_curves, **kwargs)
+        # Expected number of values is number of control curves plus two.
+        nvalues = len(self.control_curves) + 2
+        self.parameters = None
+
+        if values is not None:
+            if len(values) != nvalues:
+                raise ValueError('Length of values should be two more than the number of '
+                                 'control curves ({}).'.format(nvalues))
+            self.values = np.asarray(values)
+
+        elif parameters is not None:
+            if len(parameters) != nvalues:
+                raise ValueError('Length of parameters should be two more than the number of '
+                                 'control curves ({}).'.format(nvalues))
+            self.parameters = list(parameters)
+            # Make sure these parameters depend on this parameter to ensure they are evaluated
+            # in the correct order.
+            for p in self.parameters:
+                p.parents.add(self)
+        else:
+            raise ValueError('One of values or parameters keywords must be given.')
+
+    property values:
+        def __get__(self):
+            return np.array(self._values)
+        def __set__(self, values):
+            self._values = np.array(values)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int j
+        cdef Parameter cc_param, value_param
+        cdef double cc, cc_prev
+        # return the interpolated value for the current level.
+        cdef double current_pc = self._storage_node._current_pc[scenario_index.global_id]
+        cdef double weight
+        cdef double[:] values  # y values to interpolate between in this time-step
+
+        if self.parameters is not None:
+            # If there are parameter use them to gather the interpolation values
+            values = np.empty(len(self.parameters))
+            for j, value_param in enumerate(self.parameters):
+                values[j] = value_param.get_value(scenario_index)
+        else:
+            # Otherwise use the given array of floats.
+            # This makes a reference rather than a copy.
+            values = self._values
+
+        # Bounds check the current pc storage.
+        # Always return the first value if storage above 100% or NaN
+        if current_pc > 1.0 or npy_isnan(current_pc):
+            return values[0]
+        # Always return last value is storage less than 0%
+        if current_pc < 0.0:
+            return values[-1]
+
+        # Assumes control_curves is sorted highest to lowest
+        # First level 100%
+        cc_prev = 1.0
+        for j, cc_param in enumerate(self._control_curves):
+            cc = cc_param.get_value(scenario_index)
+            # If level above control curve then return this level's value
+            if current_pc >= cc:
+                try:
+                    weight = (current_pc - cc) / (cc_prev - cc)
+                except ZeroDivisionError:
+                    # Last two control curves identical; return the next value
+                    return values[j+1]
+                return values[j]*weight + values[j+1]*(1.0 - weight)
+            # Update previous value for next iteration
+            cc_prev = cc
+
+        # Current storage is above none of the control curves
+        # Therefore interpolate between last control curve and bottom
+        cc = 0.0
+        try:
+            weight = (current_pc - cc) / (cc_prev - cc)
+        except ZeroDivisionError:
+            # cc_prev == cc  i.e. last control curve is close to 0%
+            return values[-2]
+        return values[-2]*weight + values[-1]*(1.0 - weight)
+
+    @classmethod
+    def load(cls, model, data):
+        control_curves = super(ControlCurveInterpolatedParameter, cls)._load_control_curves(model, data)
+        storage_node = super(ControlCurveInterpolatedParameter, cls)._load_storage_node(model, data)
+        if "parameters" in data:
+            parameters = [load_parameter(model, p) for p in data.pop("parameters")]
+            values = None
+        else:
+            values = load_parameter_values(model, data)
+            parameters = None
+        return cls(model, storage_node, control_curves, values=values, parameters=parameters)
+
+ControlCurveInterpolatedParameter.register()
+
+
+cdef class ControlCurvePiecewiseInterpolatedParameter(BaseControlCurveParameter):
+    """A control curve Parameter that interpolates between two or more pairs of values.
+
+    Return values are linearly interpolated between a pair of values depending on the current
+    storage. The first pair is used between maximum and the first control curve, the next pair
+    between the first control curve and second control curve, and so on until the last pair is
+    used between the last control curve and the minimum value. The first value in each pair is the
+    value at the upper position, and the second the value at the lower position.
+
+    Parameters
+    ----------
+    storage_node : `Storage`
+        The storage node to compare the control curve(s) to.
+    control_curves : list of `Parameter` or floats
+        A list of parameters representing the control curve(s). These are
+        often MonthlyProfileParameters or DailyProfileParameters, but may be
+        any Parameter that returns values between 0.0 and 1.0. If floats are
+        passed they are converted to `ConstantParameter`.
+    values : 2D array or list of lists
+        A list of value pairs to interpolate between. The length of the list should be 1 + len(control_curves).
+    minimum : float
+        The storage considered the bottom of the lower curve, 0-1 (default=0).
+    maximum : float
+        The storage considered the top of the upper curve, 0-1 (default=1).
+
+    """
+    def __init__(self, model, storage_node, control_curves, values, minimum=0.0, maximum=1.0, **kwargs):
+        super(ControlCurvePiecewiseInterpolatedParameter, self).__init__(model, storage_node, control_curves, **kwargs)
+        self.values = np.array(values, dtype=np.float64)
+        self.minimum = minimum
+        self.maximum = maximum
+
+    property values:
+        def __get__(self):
+            return np.array(self._values)
+        def __set__(self, values):
+            # Expected number of values is number of control curves plus one.
+            nvalues = len(self.control_curves) + 1
+            if len(values) != nvalues:
+                raise ValueError('Length of values should be two more than the number of '
+                                 'control curves ({}).'.format(nvalues))
+            elif len(values[0]) != 2:
+                raise ValueError('The second dimension of values should be of length 2.')
+            self._values = np.array(values)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int j, ncc
+        cdef Parameter cc_param
+        cdef double cc, cc_prev, val
+        # return the interpolated value for the current level.
+        cdef double current_pc = self._storage_node._current_pc[scenario_index.global_id]
+
+        cc_prev = self.maximum
+        for j, cc_param in enumerate(self._control_curves):
+            cc = cc_param.get_value(scenario_index)
+            # If level above control curve then return this level's value
+            if current_pc >= cc:
+                return _interpolate(current_pc, cc, cc_prev, self._values[j, 1], self._values[j, 0])
+
+            # Update previous value for next iteration
+            cc_prev = cc
+
+        # Current storage is above none of the control curves
+        # Therefore interpolate between last control curve and minimum
+        ncc = len(self._control_curves)
+        val = _interpolate(current_pc, self.minimum, cc_prev, self._values[ncc, 1], self._values[ncc, 0])
+        return val
+
+    @classmethod
+    def load(cls, model, data):
+        control_curves = super(ControlCurvePiecewiseInterpolatedParameter, cls)._load_control_curves(model, data)
+        storage_node = super(ControlCurvePiecewiseInterpolatedParameter, cls)._load_storage_node(model, data)
+        values = load_parameter_values(model, data)
+        return cls(model, storage_node, control_curves, values=values, **data)
+
+ControlCurvePiecewiseInterpolatedParameter.register()
+
+
+cdef class ControlCurveIndexParameter(IndexParameter):
+    """Multiple control curve holder which returns an index not a value
+
+    Parameters
+    ----------
+    storage_node : `Storage`
+    control_curves : iterable of `Parameter` instances or floats
+    """
+    def __init__(self, model, storage_node, control_curves, **kwargs):
+        super(ControlCurveIndexParameter, self).__init__(model, **kwargs)
+        self.storage_node = storage_node
+        self.control_curves = control_curves
+
+    property control_curves:
+        def __get__(self):
+            return self._control_curves
+        def __set__(self, control_curves):
+            # Accept a single Parameter and convert to a list internally
+            if isinstance(control_curves, Parameter):
+                control_curves = [control_curves]
+
+            # remove existing control curves (if any)
+            if self._control_curves is not None:
+                for control_curve in self._control_curves:
+                    control_curve.parents.remove(self)
+
+            _new_control_curves = []
+            for control_curve in control_curves:
+                # Accept numeric inputs and convert to `ConstantParameter`
+                if isinstance(control_curve, (float, int)):
+                    control_curve = ConstantParameter(self.model, control_curve)
+
+                control_curve.parents.add(self)
+                _new_control_curves.append(control_curve)
+            self._control_curves = list(_new_control_curves)
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns the index of the first control curve the storage is above
+
+        The index is zero-based. For example, if only one control curve is
+        supplied then the index is either 0 (above) or 1 (below). For two
+        curves the index is either 0 (above both), 1 (in between), or 2 (below
+        both), and so on.
+        """
+        cdef double current_percentage
+        cdef double target_percentage
+        cdef int index, j
+        cdef Parameter control_curve
+        current_percentage = self.storage_node._current_pc[scenario_index.global_id]
+        index = len(self.control_curves)
+        for j, control_curve in enumerate(self.control_curves):
+            target_percentage = control_curve.get_value(scenario_index)
+            if current_percentage >= target_percentage:
+                index = j
+                break
+        return index
+
+    @classmethod
+    def load(cls, model, data):
+        storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
+        control_curves = [load_parameter(model, d) for d in data.pop("control_curves")]
+        return cls(model, storage_node, control_curves, **data)
+ControlCurveIndexParameter.register()
+
+
+cdef class ControlCurveParameter(BaseControlCurveParameter):
+    """ A generic multi-levelled control curve Parameter.
+
+     This parameter can be used to return different values when a `Storage` node's current
+      volumes is at different percentage of `max_volume` relative to predefined control curves.
+      Control curves must be defined in the range [0, 1] corresponding to 0% and 100% volume.
+
+     By default this parameter returns an integer sequence from zero if the first control curve
+      is passed, and incrementing by one for each control curve (or "level") the `Storage` node
+      is below.
+
+    Parameters
+    ----------
+    storage_node : `Storage`
+        An optional `Storage` node that can be used to query the current percentage volume.
+    control_curves : `float`, `int` or `Parameter` object, or iterable thereof
+        The position of the control curves. Internally `float` or `int` types are cast to
+        `ConstantParameter`. Multiple values correspond to multiple control curve positions.
+        These should be specified in descending order.
+    values : array_like or `None`, optional
+        The values to return if the `Storage` object is above the correspond control curve.
+        I.e. the first value is returned if the current volume is above the first control curve,
+        and second value if above the second control curve, and so on. The length of `values`
+        must be one more than than the length of `control_curves`.
+    parameters : iterable `Parameter` objects or `None`, optional
+        If `values` is `None` then `parameters` can specify a `Parameter` object to use at level
+        of the control curves. In the same way as `values` the first `Parameter` is used if
+        `Storage` is above the first control curve, and second `Parameter` if above the
+        second control curve, and so on.
+    variable_indices : iterable of ints, optional
+        A list of indices that correspond to items in `values` which are to be considered variables
+         when `self.is_variable` is True. This mechanism allows a subset of `values` to be variable.
+    lower_bounds, upper_bounds : array_like, optional
+        Bounds of the variables. The length must correspond to the length of `variable_indices`, i.e.
+         there are bounds for each index to be considered as a variable.
+
+    Notes
+    -----
+    If `values` and `parameters` are both `None`, the default, then `values` defaults to
+     a range of integers, starting at zero, one more than length of `control_curves`.
+
+    See also
+    --------
+    `BaseControlCurveParameter`
+
+    """
+    def __init__(self, model, storage_node, control_curves, values=None, parameters=None,
+                 variable_indices=None, upper_bounds=None, lower_bounds=None, **kwargs):
+        super(ControlCurveParameter, self).__init__(model, storage_node, control_curves, **kwargs)
+        # Expected number of values is number of control curves plus one.
+        nvalues = len(self.control_curves) + 1
+        self.parameters = None
+        if values is not None:
+            if len(values) != nvalues:
+                raise ValueError('Length of values should be one more than the number of '
+                                 'control curves ({}).'.format(nvalues))
+            self.values = values
+        elif parameters is not None:
+            if len(parameters) != nvalues:
+                raise ValueError('Length of parameters should be one more than the number of '
+                                 'control curves ({}).'.format(nvalues))
+            self.parameters = list(parameters)
+            # Make sure these parameters depend on this parameter to ensure they are evaluated
+            # in the correct order.
+            for p in self.parameters:
+                p.parents.add(self)
+        else:
+            # No values or parameters given, default to sequence of integers
+            self.values = np.arange(nvalues)
+
+        # Default values
+        self._upper_bounds = None
+        self._lower_bounds = None
+
+        if variable_indices is not None:
+            self.variable_indices = variable_indices
+            self.double_size = len(variable_indices)
+        else:
+            self.double_size = 0
+        # Bounds for use as a variable (i.e. when self.is_variable = True)
+        if upper_bounds is not None:
+            if self.values is None or variable_indices is None:
+                raise ValueError('Upper bounds can only be specified if `values` and `variable_indices` '
+                                 'is not `None`.')
+            if len(upper_bounds) != self.double_size:
+                raise ValueError('Length of upper_bounds should be equal to the length of `variable_indices` '
+                                 '({}).'.format(self.double_size))
+            self._upper_bounds = np.array(upper_bounds)
+
+        if lower_bounds is not None:
+            if self.values is None or variable_indices is None:
+                raise ValueError('Lower bounds can only be specified if `values` and `variable_indices` '
+                                 'is not `None`.')
+            if len(lower_bounds) != self.double_size:
+                raise ValueError('Length of lower_bounds should be equal to the length of `variable_indices` '
+                                 '({}).'.format(self.double_size))
+            self._lower_bounds = np.array(lower_bounds)
+
+    property values:
+        def __get__(self):
+            return np.asarray(self._values)
+        def __set__(self, values):
+            self._values = np.asarray(values, dtype=np.float64)
+
+    property variable_indices:
+        def __get__(self):
+            return np.array(self._variable_indices)
+        def __set__(self, values):
+            self._variable_indices = np.array(values, dtype=np.int)
+
+    @classmethod
+    def load(cls, model, data):
+        control_curves = super(ControlCurveParameter, cls)._load_control_curves(model, data)
+        storage_node = super(ControlCurveParameter, cls)._load_storage_node(model, data)
+
+        parameters = None
+        values = None
+        if 'values' in data:
+            values = load_parameter_values(model, data)
+        elif 'parameters' in data:
+            # Load parameters
+            parameters_data = data['parameters']
+            parameters = []
+            for pdata in parameters_data:
+                parameters.append(load_parameter(model, pdata))
+
+        return cls(model, storage_node, control_curves, values=values, parameters=parameters)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i = scenario_index.global_id
+        cdef int j
+        cdef AbstractStorage node
+        cdef double cc
+        cdef Parameter param, cc_param
+        node = self.node if self.storage_node is None else self.storage_node
+
+        # Assumes control_curves is sorted highest to lowest
+        for j, cc_param in enumerate(self.control_curves):
+            cc = cc_param.get_value(scenario_index)
+            # If level above control curve then return this level's value
+            if node._current_pc[i] >= cc:
+                if self.parameters is not None:
+                    param = self.parameters[j]
+                    return param.get_value(scenario_index)
+                else:
+                    return self._values[j]
+
+        if self.parameters is not None:
+            param = self.parameters[-1]
+            return param.get_value(scenario_index)
+        else:
+            return self._values[-1]
+
+    cpdef set_double_variables(self, double[:] values):
+        cdef int i
+        cdef double v
+
+        if len(values) != len(self.variable_indices):
+            raise ValueError('Number of values must be the same as the number of variable_indices.')
+
+        if self.double_size != 0:
+            for i, v in zip(self.variable_indices, values):
+                self._values[i] = v
+
+    cpdef double[:] get_double_variables(self):
+        cdef int i, j
+        cdef double v
+        cdef double[:] arry = np.empty((len(self.variable_indices), ))
+        for i, j in enumerate(self.variable_indices):
+            arry[i] = self._values[j]
+        return arry
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return self._lower_bounds
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return self._upper_bounds
+
+ControlCurveParameter.register()
```

### Comparing `pywr-1.8.0/pywr/parameters/_hydropower.pxd` & `pywr-1.9.0/pywr/parameters/_hydropower.pxd`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, Storage, AbstractStorage
-from ._parameters cimport Parameter, IndexParameter
-
-
-
-cdef class HydropowerTargetParameter(Parameter):
-    cdef Parameter _water_elevation_parameter
-    cdef Parameter _target
-    cdef Parameter _max_flow
-    cdef Parameter _min_flow
-    cdef public double min_head
-    cdef public double turbine_elevation
-    cdef public double flow_unit_conversion
-    cdef public double energy_unit_conversion
-    cdef public double density
-    cdef public double efficiency
+from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, Storage, AbstractStorage
+from ._parameters cimport Parameter, IndexParameter
+
+
+
+cdef class HydropowerTargetParameter(Parameter):
+    cdef Parameter _water_elevation_parameter
+    cdef Parameter _target
+    cdef Parameter _max_flow
+    cdef Parameter _min_flow
+    cdef public double min_head
+    cdef public double turbine_elevation
+    cdef public double flow_unit_conversion
+    cdef public double energy_unit_conversion
+    cdef public double density
+    cdef public double efficiency
```

### Comparing `pywr-1.8.0/pywr/parameters/_hydropower.pyx` & `pywr-1.9.0/pywr/parameters/_hydropower.pyx`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,228 +1,228 @@
-""" Parameters for """
-from ._parameters import load_parameter, ConstantParameter
-
-
-cpdef double inverse_hydropower_calculation(double power, double water_elevation, double turbine_elevation, double efficiency,
-                                    double flow_unit_conversion=1.0, double energy_unit_conversion=1e-6,
-                                    double density=1000.0):
-    """
-    Calculate the flow required to produce power using the hydropower equation.
-    
-   
-    Parameters
-    ----------
-    power: double
-        Hydropower requirement in rate in units of energy per day.
-    water_elevation : double
-        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
-        the working head of the turbine.
-    turbine_elevation : double
-        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
-        the working head of the turbine.
-    efficiency : double
-        An efficiency scaling factor for the power output of the turbine.
-    flow_unit_conversion : double (default=1.0)
-        A factor used to transform the units of flow to be compatible with the equation here. This
-        should convert flow to units of $m^3/day$
-    energy_unit_conversion : double (default=1e-6)
-        A factor used to transform the units of power. Defaults to 1e-6 to assuming input of $MJ$/day. 
-    density : double (default=1000)
-        Density of water in $kgm^{-3}$.
-        
-    Returns
-    -------
-    flow : double 
-        Required flow rate of water through the turbine. Converted using `flow_unit_conversion` to 
-        units of $m^3£ per day (not per second).
-    
-    Notes
-    -----
-    The inverse hydropower calculation uses the following equation.
-    
-    .. math:: q = \frac{P}{\rho * g * \deltaH}
-    
-    The energy rate in should be converted to units of energy per day.    
-    
-    """
-    cdef double head
-    cdef double flow
-
-    head = water_elevation - turbine_elevation
-    if head < 0.0:
-        head = 0.0
-
-    # Power
-    try:
-        flow = power / (energy_unit_conversion * density * 9.81 * head * efficiency * flow_unit_conversion)
-    except ZeroDivisionError:
-        flow = float('inf')
-
-    return flow
-
-
-cdef class HydropowerTargetParameter(Parameter):
-    """ A parameter that returns flow from a hydropower generation target.
-
-    This parameter calculates the flow required to generate a particular hydropower production target. It
-    is intended to be used on a node representing a turbine where a particular production target is required
-    each time-step.
-
-    Parameters
-    ----------
-
-    target : Parameter instance
-        Hydropower production target. Units should be in units of energy per day.
-    water_elevation_parameter : Parameter instance (default=None)
-        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
-        the working head of the turbine.
-    max_flow : Parameter instance (default=None)
-        Upper bounds on the calculated flow. If set the flow returned by this parameter is at most the value
-        of the max_flow parameter.
-    min_flow : Parameter instance (default=None)
-        Lower bounds on the calculated flow. If set the flow returned by this parameter is at least the value
-        of the min_flow parameter.
-    min_head : double (default=0.0)
-        Minimum head for flow to occur. If actual head is less than this value zero flow is returned.
-    turbine_elevation : double
-        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
-        the working head of the turbine.
-    efficiency : float (default=1.0)
-        The efficiency of the turbine.
-    density : float (default=1000.0)
-        The density of water.
-    flow_unit_conversion : float (default=1.0)
-        A factor used to transform the units of flow to be compatible with the equation here. This
-        should convert flow to units of :math:`m^3/day`
-    energy_unit_conversion : float (default=1e-6)
-        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
-
-    Notes
-    -----
-    The inverse hydropower calculation uses the following equation.
-
-    .. math:: q = \\frac{P}{\\rho * g * \\delta H}
-
-    The energy rate in should be converted to units of energy per day. The returned flow rate in should is
-    converted from units of :math:`m^3` per day to those used by the model using the `flow_unit_conversion` parameter.
-
-    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
-    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
-    is `None` then the head is simply the turbine elevation.
-
-    See Also
-    --------
-    pywr.recorders.TotalHydroEnergyRecorder
-    pywr.recorders.HydropowerRecorder
-
-    """
-    def __init__(self, model, target, water_elevation_parameter=None, max_flow=None, min_flow=None,
-                 turbine_elevation=0.0, efficiency=1.0, density=1000, min_head=0.0,
-                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
-        super(HydropowerTargetParameter, self).__init__(model, **kwargs)
-
-        self.target = target
-        self.water_elevation_parameter = water_elevation_parameter
-        self.max_flow = max_flow
-        self.min_flow = min_flow
-        self.min_head = min_head
-        self.turbine_elevation = turbine_elevation
-        self.efficiency = efficiency
-        self.density = density
-        self.flow_unit_conversion = flow_unit_conversion
-        self.energy_unit_conversion = energy_unit_conversion
-
-    property water_elevation_parameter:
-        def __get__(self):
-            return self._water_elevation_parameter
-        def __set__(self, parameter):
-            if self._water_elevation_parameter:
-                self.children.remove(self._water_elevation_parameter)
-            self.children.add(parameter)
-            self._water_elevation_parameter = parameter
-
-    property target:
-        def __get__(self):
-            return self._target
-        def __set__(self, parameter):
-            if self._target:
-                self.children.remove(self._target)
-            self.children.add(parameter)
-            self._target = parameter
-
-    property max_flow:
-        def __get__(self):
-            return self._max_flow
-        def __set__(self, parameter):
-            if self._max_flow:
-                self.children.remove(self._max_flow)
-            self.children.add(parameter)
-            self._max_flow = parameter
-
-    property min_flow:
-        def __get__(self):
-            return self._min_flow
-        def __set__(self, parameter):
-            if self._min_flow:
-                self.children.remove(self._min_flow)
-            self.children.add(parameter)
-            self._min_flow = parameter
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i
-        cdef double q, head, power
-
-        power = self._target.get_value(scenario_index)
-
-        if self._water_elevation_parameter is not None:
-            head = self._water_elevation_parameter.get_value(scenario_index)
-            if self.turbine_elevation is not None:
-                head -= self.turbine_elevation
-        elif self.turbine_elevation is not None:
-            head = self.turbine_elevation
-        else:
-            raise ValueError('One or both of storage_node or level must be set.')
-
-        # -ve head is not valid
-        head = max(head, 0.0)
-
-        # Apply minimum head threshold.
-        if head < self.min_head:
-            return 0.0
-
-        # Get the flow from the current node
-        q = inverse_hydropower_calculation(power, head, 0.0, self.efficiency, density=self.density,
-                                           flow_unit_conversion=self.flow_unit_conversion,
-                                           energy_unit_conversion=self.energy_unit_conversion)
-
-        # Bound the flow if required
-        if self._max_flow is not None:
-            q = min(self._max_flow.get_value(scenario_index), q)
-        if self._min_flow is not None:
-            q = max(self._min_flow.get_value(scenario_index), q)
-
-        assert q >= 0.0
-
-        return q
-
-    @classmethod
-    def load(cls, model, data):
-
-        target = load_parameter(model, data.pop("target"))
-        if "water_elevation_parameter" in data:
-            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
-        else:
-            water_elevation_parameter = None
-
-        if "max_flow" in data:
-            max_flow = load_parameter(model, data.pop("max_flow"))
-        else:
-            max_flow = None
-
-        if "min_flow" in data:
-            min_flow = load_parameter(model, data.pop("min_flow"))
-        else:
-            min_flow = None
-
-        return cls(model, target, water_elevation_parameter=water_elevation_parameter,
-                   max_flow=max_flow, min_flow=min_flow, **data)
-HydropowerTargetParameter.register()
+""" Parameters for """
+from ._parameters import load_parameter, ConstantParameter
+
+
+cpdef double inverse_hydropower_calculation(double power, double water_elevation, double turbine_elevation, double efficiency,
+                                    double flow_unit_conversion=1.0, double energy_unit_conversion=1e-6,
+                                    double density=1000.0):
+    """
+    Calculate the flow required to produce power using the hydropower equation.
+    
+   
+    Parameters
+    ----------
+    power: double
+        Hydropower requirement in rate in units of energy per day.
+    water_elevation : double
+        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
+        the working head of the turbine.
+    turbine_elevation : double
+        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
+        the working head of the turbine.
+    efficiency : double
+        An efficiency scaling factor for the power output of the turbine.
+    flow_unit_conversion : double (default=1.0)
+        A factor used to transform the units of flow to be compatible with the equation here. This
+        should convert flow to units of $m^3/day$
+    energy_unit_conversion : double (default=1e-6)
+        A factor used to transform the units of power. Defaults to 1e-6 to assuming input of $MJ$/day. 
+    density : double (default=1000)
+        Density of water in $kgm^{-3}$.
+        
+    Returns
+    -------
+    flow : double 
+        Required flow rate of water through the turbine. Converted using `flow_unit_conversion` to 
+        units of $m^3£ per day (not per second).
+    
+    Notes
+    -----
+    The inverse hydropower calculation uses the following equation.
+    
+    .. math:: q = \frac{P}{\rho * g * \deltaH}
+    
+    The energy rate in should be converted to units of energy per day.    
+    
+    """
+    cdef double head
+    cdef double flow
+
+    head = water_elevation - turbine_elevation
+    if head < 0.0:
+        head = 0.0
+
+    # Power
+    try:
+        flow = power / (energy_unit_conversion * density * 9.81 * head * efficiency * flow_unit_conversion)
+    except ZeroDivisionError:
+        flow = float('inf')
+
+    return flow
+
+
+cdef class HydropowerTargetParameter(Parameter):
+    """ A parameter that returns flow from a hydropower generation target.
+
+    This parameter calculates the flow required to generate a particular hydropower production target. It
+    is intended to be used on a node representing a turbine where a particular production target is required
+    each time-step.
+
+    Parameters
+    ----------
+
+    target : Parameter instance
+        Hydropower production target. Units should be in units of energy per day.
+    water_elevation_parameter : Parameter instance (default=None)
+        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
+        the working head of the turbine.
+    max_flow : Parameter instance (default=None)
+        Upper bounds on the calculated flow. If set the flow returned by this parameter is at most the value
+        of the max_flow parameter.
+    min_flow : Parameter instance (default=None)
+        Lower bounds on the calculated flow. If set the flow returned by this parameter is at least the value
+        of the min_flow parameter.
+    min_head : double (default=0.0)
+        Minimum head for flow to occur. If actual head is less than this value zero flow is returned.
+    turbine_elevation : double
+        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
+        the working head of the turbine.
+    efficiency : float (default=1.0)
+        The efficiency of the turbine.
+    density : float (default=1000.0)
+        The density of water.
+    flow_unit_conversion : float (default=1.0)
+        A factor used to transform the units of flow to be compatible with the equation here. This
+        should convert flow to units of :math:`m^3/day`
+    energy_unit_conversion : float (default=1e-6)
+        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
+
+    Notes
+    -----
+    The inverse hydropower calculation uses the following equation.
+
+    .. math:: q = \\frac{P}{\\rho * g * \\delta H}
+
+    The energy rate in should be converted to units of energy per day. The returned flow rate in should is
+    converted from units of :math:`m^3` per day to those used by the model using the `flow_unit_conversion` parameter.
+
+    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
+    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
+    is `None` then the head is simply the turbine elevation.
+
+    See Also
+    --------
+    pywr.recorders.TotalHydroEnergyRecorder
+    pywr.recorders.HydropowerRecorder
+
+    """
+    def __init__(self, model, target, water_elevation_parameter=None, max_flow=None, min_flow=None,
+                 turbine_elevation=0.0, efficiency=1.0, density=1000, min_head=0.0,
+                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
+        super(HydropowerTargetParameter, self).__init__(model, **kwargs)
+
+        self.target = target
+        self.water_elevation_parameter = water_elevation_parameter
+        self.max_flow = max_flow
+        self.min_flow = min_flow
+        self.min_head = min_head
+        self.turbine_elevation = turbine_elevation
+        self.efficiency = efficiency
+        self.density = density
+        self.flow_unit_conversion = flow_unit_conversion
+        self.energy_unit_conversion = energy_unit_conversion
+
+    property water_elevation_parameter:
+        def __get__(self):
+            return self._water_elevation_parameter
+        def __set__(self, parameter):
+            if self._water_elevation_parameter:
+                self.children.remove(self._water_elevation_parameter)
+            self.children.add(parameter)
+            self._water_elevation_parameter = parameter
+
+    property target:
+        def __get__(self):
+            return self._target
+        def __set__(self, parameter):
+            if self._target:
+                self.children.remove(self._target)
+            self.children.add(parameter)
+            self._target = parameter
+
+    property max_flow:
+        def __get__(self):
+            return self._max_flow
+        def __set__(self, parameter):
+            if self._max_flow:
+                self.children.remove(self._max_flow)
+            self.children.add(parameter)
+            self._max_flow = parameter
+
+    property min_flow:
+        def __get__(self):
+            return self._min_flow
+        def __set__(self, parameter):
+            if self._min_flow:
+                self.children.remove(self._min_flow)
+            self.children.add(parameter)
+            self._min_flow = parameter
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i
+        cdef double q, head, power
+
+        power = self._target.get_value(scenario_index)
+
+        if self._water_elevation_parameter is not None:
+            head = self._water_elevation_parameter.get_value(scenario_index)
+            if self.turbine_elevation is not None:
+                head -= self.turbine_elevation
+        elif self.turbine_elevation is not None:
+            head = self.turbine_elevation
+        else:
+            raise ValueError('One or both of storage_node or level must be set.')
+
+        # -ve head is not valid
+        head = max(head, 0.0)
+
+        # Apply minimum head threshold.
+        if head < self.min_head:
+            return 0.0
+
+        # Get the flow from the current node
+        q = inverse_hydropower_calculation(power, head, 0.0, self.efficiency, density=self.density,
+                                           flow_unit_conversion=self.flow_unit_conversion,
+                                           energy_unit_conversion=self.energy_unit_conversion)
+
+        # Bound the flow if required
+        if self._max_flow is not None:
+            q = min(self._max_flow.get_value(scenario_index), q)
+        if self._min_flow is not None:
+            q = max(self._min_flow.get_value(scenario_index), q)
+
+        assert q >= 0.0
+
+        return q
+
+    @classmethod
+    def load(cls, model, data):
+
+        target = load_parameter(model, data.pop("target"))
+        if "water_elevation_parameter" in data:
+            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
+        else:
+            water_elevation_parameter = None
+
+        if "max_flow" in data:
+            max_flow = load_parameter(model, data.pop("max_flow"))
+        else:
+            max_flow = None
+
+        if "min_flow" in data:
+            min_flow = load_parameter(model, data.pop("min_flow"))
+        else:
+            min_flow = None
+
+        return cls(model, target, water_elevation_parameter=water_elevation_parameter,
+                   max_flow=max_flow, min_flow=min_flow, **data)
+HydropowerTargetParameter.register()
```

### Comparing `pywr-1.8.0/pywr/parameters/_parameters.pxd` & `pywr-1.9.0/pywr/parameters/_parameters.pxd`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-from pywr.recorders._recorders cimport Recorder
-from pywr._component cimport Component
-from pywr._core cimport Timestep, Scenario, ScenarioIndex, ScenarioCollection, AbstractNode, Node
-
-cdef class Parameter(Component):
-    cdef public int double_size
-    cdef public int integer_size
-    cdef int _size
-    cdef public bint is_variable
-    cdef AbstractNode _node
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1
-    cdef double[:] __values
-    cdef calc_values(self, Timestep ts)
-    cpdef double get_value(self, ScenarioIndex scenario_index)
-    cpdef double[:] get_all_values(self)
-
-    # New variable API
-    cpdef set_double_variables(self, double[:] values)
-    cpdef double[:] get_double_variables(self)
-    cpdef double[:] get_double_lower_bounds(self)
-    cpdef double[:] get_double_upper_bounds(self)
-    cpdef set_integer_variables(self, int[:] values)
-    cpdef int[:] get_integer_variables(self)
-    cpdef int[:] get_integer_lower_bounds(self)
-    cpdef int[:] get_integer_upper_bounds(self)
-
-
-
-cdef class ConstantParameter(Parameter):
-    cdef double _value
-    cdef public double scale, offset
-    cdef double[:] _lower_bounds
-    cdef double[:] _upper_bounds
-
-cdef class DataFrameParameter(Parameter):
-    cdef double[:,:] _values
-    cdef public Scenario scenario
-    cdef int _scenario_index
-    cdef public object dataframe
-
-cdef class ArrayIndexedParameter(Parameter):
-    cdef double[:] values
-
-cdef class ArrayIndexedScenarioParameter(Parameter):
-    cdef Scenario _scenario
-    cdef double[:, :] values
-    cdef int _scenario_index
-
-cdef class ConstantScenarioParameter(Parameter):
-    cdef Scenario _scenario
-    cdef double[:] _values
-    cdef int _scenario_index
-
-cdef class ArrayIndexedScenarioMonthlyFactorsParameter(Parameter):
-    cdef double[:] _values
-    cdef double[:, :] _factors
-    cdef Scenario _scenario
-    cdef int _scenario_index
-
-
-
-cdef class DailyProfileParameter(Parameter):
-    cdef double[:] _values
-
-cdef class WeeklyProfileParameter(Parameter):
-    cdef double[:] _values
-
-cdef class MonthlyProfileParameter(Parameter):
-    cdef double[:] _values
-    cdef double[:] _interp_values
-    cdef double[:] _lower_bounds
-    cdef double[:] _upper_bounds
-    cdef public object interp_day
-    cpdef _interpolate(self)
-
-cdef class ScenarioMonthlyProfileParameter(Parameter):
-    cdef double[:, :] _values
-    cdef Scenario _scenario
-    cdef int _scenario_index
-
-cdef class ScenarioDailyProfileParameter(Parameter):
-    cdef double[:, :] _values
-    cdef Scenario _scenario
-    cdef int _scenario_index
-
-cdef class ScenarioWeeklyProfileParameter(Parameter):
-    cdef double[:, :] _values
-    cdef Scenario _scenario
-    cdef int _scenario_index
-
-cdef class UniformDrawdownProfileParameter(Parameter):
-    cdef public int reset_day
-    cdef public int reset_month
-    cdef int _reset_idoy
-
-cdef class RbfProfileParameter(Parameter):
-    cdef double[:] _values
-    cdef double[:] _interp_values
-    cdef double min_value
-    cdef double max_value
-    cdef int[:] _days_of_year
-    cdef double[:] _lower_bounds
-    cdef double[:] _upper_bounds
-    cdef int[:] _doy_lower_bounds
-    cdef int[:] _doy_upper_bounds
-    cdef readonly int variable_days_of_year_range
-    cdef public object rbf
-    cdef public object rbf_kwargs
-    cpdef _interpolate(self)
-
-cdef class IndexParameter(Parameter):
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
-    cdef int[:] __indices
-    cpdef int get_index(self, ScenarioIndex scenario_index)
-    cpdef int[:] get_all_indices(self)
-
-cdef class ConstantScenarioIndexParameter(IndexParameter):
-    cdef Scenario _scenario
-    cdef int[:] _values
-    cdef int _scenario_index
-
-cdef class TablesArrayParameter(IndexParameter):
-    cdef double[:, :] _values_dbl
-    cdef int[:, :] _values_int
-    cdef public Scenario scenario
-    cdef public object h5file
-    cdef public object h5store
-    cdef public object node
-    cdef public object where
-
-    cdef int _scenario_index
-    cdef int[:] _scenario_ids
-
-cdef class IndexedArrayParameter(Parameter):
-    cdef public IndexParameter index_parameter
-    cdef public list params
-
-cdef class AnnualHarmonicSeriesParameter(Parameter):
-    cdef public double mean
-    cdef double[:] _amplitudes, _phases
-    cdef double _mean_lower_bounds, _mean_upper_bounds
-    cdef double[:] _amplitude_lower_bounds, _amplitude_upper_bounds
-    cdef double[:] _phase_lower_bounds, _phase_upper_bounds
-    cdef double _value_cache
-    cdef int _ts_index_cache
-
-cdef class AggregatedParameter(Parameter):
-    # This is a list rather than a set due to floating point arithmetic.
-    # The order is important for maintaining determinism.
-    cdef public list parameters
-    cdef object _agg_user_func
-    cdef int _agg_func
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
-    cpdef add(self, Parameter parameter)
-    cpdef remove(self, Parameter parameter)
-
-cdef class AggregatedIndexParameter(IndexParameter):
-    # This is a list; see above AggregatedParameter.
-    cdef public list parameters
-    cdef object _agg_user_func
-    cdef int _agg_func
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
-    cpdef add(self, Parameter parameter)
-    cpdef remove(self, Parameter parameter)
-
-
-cdef class DivisionParameter(Parameter):
-    cdef Parameter _numerator
-    cdef Parameter _denominator
-
-
-cdef class NegativeParameter(Parameter):
-    cdef public Parameter parameter
-
-cdef class MaxParameter(Parameter):
-    cdef public Parameter parameter
-    cdef public double threshold
-
-cdef class NegativeMaxParameter(MaxParameter):
-    pass
-
-cdef class MinParameter(Parameter):
-    cdef public Parameter parameter
-    cdef public double threshold
-
-cdef class NegativeMinParameter(MinParameter):
-    pass
-
-cdef class OffsetParameter(Parameter):
-    cdef public Parameter parameter
-    cdef public double offset
-    cdef double[:] _lower_bounds
-    cdef double[:] _upper_bounds
-
-cdef class DeficitParameter(Parameter):
-    cdef public Node node
-
-cdef class FlowParameter(Parameter):
-    cdef public Node node
-    cdef double[:] __next_values
-    cdef public double initial_value
-
-cdef class PiecewiseIntegralParameter(Parameter):
-    cdef public double[:] x
-    cdef public double[:] y
-    cdef public Parameter parameter
-
-cdef class FlowDelayParameter(Parameter):
-    cdef public AbstractNode node
-    cdef public int days
-    cdef public int timesteps
-    cdef public double initial_flow
-    cdef double[:, :] _memory
-    cdef public int _memory_pointer
-
-cdef class DiscountFactorParameter(Parameter):
-    cdef public double rate
-    cdef public int base_year
+from pywr.recorders._recorders cimport Recorder
+from pywr._component cimport Component
+from pywr._core cimport Timestep, Scenario, ScenarioIndex, ScenarioCollection, AbstractNode, Node
+
+cdef class Parameter(Component):
+    cdef public int double_size
+    cdef public int integer_size
+    cdef int _size
+    cdef public bint is_variable
+    cdef AbstractNode _node
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1
+    cdef double[:] __values
+    cdef calc_values(self, Timestep ts)
+    cpdef double get_value(self, ScenarioIndex scenario_index)
+    cpdef double[:] get_all_values(self)
+
+    # New variable API
+    cpdef set_double_variables(self, double[:] values)
+    cpdef double[:] get_double_variables(self)
+    cpdef double[:] get_double_lower_bounds(self)
+    cpdef double[:] get_double_upper_bounds(self)
+    cpdef set_integer_variables(self, int[:] values)
+    cpdef int[:] get_integer_variables(self)
+    cpdef int[:] get_integer_lower_bounds(self)
+    cpdef int[:] get_integer_upper_bounds(self)
+
+
+
+cdef class ConstantParameter(Parameter):
+    cdef double _value
+    cdef public double scale, offset
+    cdef double[:] _lower_bounds
+    cdef double[:] _upper_bounds
+
+cdef class DataFrameParameter(Parameter):
+    cdef double[:,:] _values
+    cdef public Scenario scenario
+    cdef int _scenario_index
+    cdef public object dataframe
+
+cdef class ArrayIndexedParameter(Parameter):
+    cdef double[:] values
+
+cdef class ArrayIndexedScenarioParameter(Parameter):
+    cdef Scenario _scenario
+    cdef double[:, :] values
+    cdef int _scenario_index
+
+cdef class ConstantScenarioParameter(Parameter):
+    cdef Scenario _scenario
+    cdef double[:] _values
+    cdef int _scenario_index
+
+cdef class ArrayIndexedScenarioMonthlyFactorsParameter(Parameter):
+    cdef double[:] _values
+    cdef double[:, :] _factors
+    cdef Scenario _scenario
+    cdef int _scenario_index
+
+
+
+cdef class DailyProfileParameter(Parameter):
+    cdef double[:] _values
+
+cdef class WeeklyProfileParameter(Parameter):
+    cdef double[:] _values
+
+cdef class MonthlyProfileParameter(Parameter):
+    cdef double[:] _values
+    cdef double[:] _interp_values
+    cdef double[:] _lower_bounds
+    cdef double[:] _upper_bounds
+    cdef public object interp_day
+    cpdef _interpolate(self)
+
+cdef class ScenarioMonthlyProfileParameter(Parameter):
+    cdef double[:, :] _values
+    cdef Scenario _scenario
+    cdef int _scenario_index
+
+cdef class ScenarioDailyProfileParameter(Parameter):
+    cdef double[:, :] _values
+    cdef Scenario _scenario
+    cdef int _scenario_index
+
+cdef class ScenarioWeeklyProfileParameter(Parameter):
+    cdef double[:, :] _values
+    cdef Scenario _scenario
+    cdef int _scenario_index
+
+cdef class UniformDrawdownProfileParameter(Parameter):
+    cdef public int reset_day
+    cdef public int reset_month
+    cdef int _reset_idoy
+
+cdef class RbfProfileParameter(Parameter):
+    cdef double[:] _values
+    cdef double[:] _interp_values
+    cdef double min_value
+    cdef double max_value
+    cdef int[:] _days_of_year
+    cdef double[:] _lower_bounds
+    cdef double[:] _upper_bounds
+    cdef int[:] _doy_lower_bounds
+    cdef int[:] _doy_upper_bounds
+    cdef readonly int variable_days_of_year_range
+    cdef public object rbf
+    cdef public object rbf_kwargs
+    cpdef _interpolate(self)
+
+cdef class IndexParameter(Parameter):
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
+    cdef int[:] __indices
+    cpdef int get_index(self, ScenarioIndex scenario_index)
+    cpdef int[:] get_all_indices(self)
+
+cdef class ConstantScenarioIndexParameter(IndexParameter):
+    cdef Scenario _scenario
+    cdef int[:] _values
+    cdef int _scenario_index
+
+cdef class TablesArrayParameter(IndexParameter):
+    cdef double[:, :] _values_dbl
+    cdef int[:, :] _values_int
+    cdef public Scenario scenario
+    cdef public object h5file
+    cdef public object h5store
+    cdef public object node
+    cdef public object where
+
+    cdef int _scenario_index
+    cdef int[:] _scenario_ids
+
+cdef class IndexedArrayParameter(Parameter):
+    cdef public IndexParameter index_parameter
+    cdef public list params
+
+cdef class AnnualHarmonicSeriesParameter(Parameter):
+    cdef public double mean
+    cdef double[:] _amplitudes, _phases
+    cdef double _mean_lower_bounds, _mean_upper_bounds
+    cdef double[:] _amplitude_lower_bounds, _amplitude_upper_bounds
+    cdef double[:] _phase_lower_bounds, _phase_upper_bounds
+    cdef double _value_cache
+    cdef int _ts_index_cache
+
+cdef class AggregatedParameter(Parameter):
+    # This is a list rather than a set due to floating point arithmetic.
+    # The order is important for maintaining determinism.
+    cdef public list parameters
+    cdef object _agg_user_func
+    cdef int _agg_func
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
+    cpdef add(self, Parameter parameter)
+    cpdef remove(self, Parameter parameter)
+
+cdef class AggregatedIndexParameter(IndexParameter):
+    # This is a list; see above AggregatedParameter.
+    cdef public list parameters
+    cdef object _agg_user_func
+    cdef int _agg_func
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
+    cpdef add(self, Parameter parameter)
+    cpdef remove(self, Parameter parameter)
+
+
+cdef class DivisionParameter(Parameter):
+    cdef Parameter _numerator
+    cdef Parameter _denominator
+
+
+cdef class NegativeParameter(Parameter):
+    cdef public Parameter parameter
+
+cdef class MaxParameter(Parameter):
+    cdef public Parameter parameter
+    cdef public double threshold
+
+cdef class NegativeMaxParameter(MaxParameter):
+    pass
+
+cdef class MinParameter(Parameter):
+    cdef public Parameter parameter
+    cdef public double threshold
+
+cdef class NegativeMinParameter(MinParameter):
+    pass
+
+cdef class OffsetParameter(Parameter):
+    cdef public Parameter parameter
+    cdef public double offset
+    cdef double[:] _lower_bounds
+    cdef double[:] _upper_bounds
+
+cdef class DeficitParameter(Parameter):
+    cdef public Node node
+
+cdef class FlowParameter(Parameter):
+    cdef public Node node
+    cdef double[:] __next_values
+    cdef public double initial_value
+
+cdef class PiecewiseIntegralParameter(Parameter):
+    cdef public double[:] x
+    cdef public double[:] y
+    cdef public Parameter parameter
+
+cdef class FlowDelayParameter(Parameter):
+    cdef public AbstractNode node
+    cdef public int days
+    cdef public int timesteps
+    cdef public double initial_flow
+    cdef double[:, :] _memory
+    cdef public int _memory_pointer
+
+cdef class DiscountFactorParameter(Parameter):
+    cdef public double rate
+    cdef public int base_year
```

### Comparing `pywr-1.8.0/pywr/parameters/_parameters.pyx` & `pywr-1.9.0/pywr/parameters/_parameters.pyx`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,2120 +1,2120 @@
-import os
-import numpy as np
-cimport numpy as np
-from scipy.interpolate import Rbf
-import pandas
-import json
-import calendar
-from libc.math cimport cos, M_PI
-from libc.limits cimport INT_MIN, INT_MAX
-from pywr.h5tools import H5Store
-from pywr.hashes import check_hash
-from .._core cimport is_leap_year
-from ..dataframe_tools import align_and_resample_dataframe, load_dataframe, read_dataframe
-import warnings
-
-
-parameter_registry = {}
-
-
-class UnutilisedDataWarning(Warning):
-    """ Simple warning to indicate that not all data has been used. """
-    pass
-
-class TypeNotFoundError(KeyError):
-    """
-      Key Error, specifically designed for when the 'type' key is not found
-      in a dataset. This takes the data value and outputs a summary of it, to
-      aid in debugging.
-    """
-    def __init__(self, data):
-        #Try to print out some sensible amount of data without overloading
-        #the terminal with data. 1000 chars should be enough to get an idea
-        #of what the data looks like. If more than 1000 chars, do a pandas-style
-        #summary using ...
-        data_str = json.dumps(data)
-        if len(data_str) > 1000:
-            data_summary = f"{data_str[:500]} ... {data_str[-500:]}"
-        else:
-            data_summary = data_str
-
-        return f"Unable to find key 'type' in {data_summary}"
-
-cdef class Parameter(Component):
-    def __init__(self, *args, is_variable=False, **kwargs):
-        super(Parameter, self).__init__(*args, **kwargs)
-        self.is_variable = is_variable
-        self.double_size = 0
-        self.integer_size = 0
-
-    @classmethod
-    def register(cls):
-        parameter_registry[cls.__name__.lower()] = cls
-
-    @classmethod
-    def unregister(cls):
-        del(parameter_registry[cls.__name__.lower()])
-
-    cpdef setup(self):
-        super(Parameter, self).setup()
-        cdef int num_comb
-        if self.model.scenarios.combinations:
-            num_comb = len(self.model.scenarios.combinations)
-        else:
-            num_comb = 1
-        self.__values = np.empty([num_comb], np.float64)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        raise NotImplementedError("Parameter must be subclassed")
-
-    cdef calc_values(self, Timestep timestep):
-        # default implementation calls Parameter.value in loop
-        cdef ScenarioIndex scenario_index
-        cdef ScenarioCollection scenario_collection = self.model.scenarios
-        for scenario_index in scenario_collection.combinations:
-            self.__values[<int>(scenario_index.global_id)] = self.value(timestep, scenario_index)
-
-    cpdef double get_value(self, ScenarioIndex scenario_index):
-        return self.__values[<int>(scenario_index.global_id)]
-
-    cpdef double[:] get_all_values(self):
-        return self.__values
-
-    cpdef set_double_variables(self, double[:] values):
-        raise NotImplementedError()
-
-    cpdef double[:] get_double_variables(self):
-        raise NotImplementedError()
-
-    cpdef double[:] get_double_lower_bounds(self):
-        raise NotImplementedError()
-
-    cpdef double[:] get_double_upper_bounds(self):
-        raise NotImplementedError()
-
-    cpdef set_integer_variables(self, int[:] values):
-        raise NotImplementedError()
-
-    cpdef int[:] get_integer_variables(self):
-        raise NotImplementedError()
-
-    cpdef int[:] get_integer_lower_bounds(self):
-        raise NotImplementedError()
-
-    cpdef int[:] get_integer_upper_bounds(self):
-        raise NotImplementedError()
-
-    property size:
-        def __get__(self):
-            warnings.warn("Use of the `size` property on Parameters has been deprecated."
-                          "Please use either `double_size` or `integer_size` instead.", DeprecationWarning)
-            return self.double_size
-
-        def __set__(self, value):
-            warnings.warn("Use of the `size` property on Parameters has been deprecated."
-                          "Please use either `double_size` or `integer_size` instead.", DeprecationWarning)
-            self.double_size = value
-
-    @classmethod
-    def load(cls, model, data):
-        # If a scenario is given don't pass this to the load values methods
-        scenario = data.pop('scenario', None)
-
-        values = load_parameter_values(model, data)
-        data.pop("values", None)
-        data.pop("url", None)
-        name = data.pop("name", None)
-        comment = data.pop("comment", None)
-
-        if scenario is not None:
-            scenario = model.scenarios[scenario]
-            # Only pass scenario object if one provided; most Parameter subclasses
-            # do not accept a scenario argument.
-            return cls(model, scenario=scenario, values=values, name=name, comment=None, **data)
-        else:
-            return cls(model, values=values, name=name, comment=None, **data)
-Parameter.register()
-
-
-cdef class ConstantParameter(Parameter):
-    def __init__(self, model, value, lower_bounds=0.0, upper_bounds=np.inf, scale=1.0, offset=0.0, **kwargs):
-        super(ConstantParameter, self).__init__(model, **kwargs)
-        self._value = value
-        self.scale = scale
-        self.offset = offset
-        self.double_size = 1
-        self.integer_size = 0
-        self._lower_bounds = np.ones(self.double_size) * lower_bounds
-        self._upper_bounds = np.ones(self.double_size) * upper_bounds
-
-    cdef calc_values(self, Timestep timestep):
-        # constant parameter can just set the entire array to one value
-        self.__values[...] = self.offset + self._value * self.scale
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._value
-
-    cpdef set_double_variables(self, double[:] values):
-        self._value = values[0]
-
-    cpdef double[:] get_double_variables(self):
-        return np.array([self._value, ], dtype=np.float64)
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return self._lower_bounds
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return self._upper_bounds
-
-    @classmethod
-    def load(cls, model, data):
-        if "value" in data:
-            value = data.pop("value")
-        else:
-            value = load_parameter_values(model, data)
-        parameter = cls(model, value, **data)
-        return parameter
-
-ConstantParameter.register()
-
-
-cdef class DataFrameParameter(Parameter):
-    """Timeseries parameter with automatic alignment and resampling
-
-    Parameters
-    ----------
-    model : pywr.model.Model
-    dataframe : pandas.DataFrame or pandas.Series
-    scenario: pywr._core.Scenario (optional)
-    """
-    def __init__(self, model, dataframe, scenario=None, **kwargs):
-        super(DataFrameParameter, self).__init__(model, *kwargs)
-        self.dataframe = dataframe
-        self.scenario = scenario
-
-    cpdef setup(self):
-        super(DataFrameParameter, self).setup()
-        # align and resample the dataframe
-        dataframe_resampled = align_and_resample_dataframe(self.dataframe, self.model.timestepper.datetime_index)
-        if dataframe_resampled.ndim == 1:
-            dataframe_resampled = pandas.DataFrame(dataframe_resampled)
-        # dataframe should now have the correct number of timesteps for the model
-        if len(dataframe_resampled) != len(self.model.timestepper):
-            raise ValueError("Aligning DataFrame failed with a different length compared with model timesteps.")
-        # check that if a 2D DataFrame is given that we also have a scenario assigned with it
-        if dataframe_resampled.ndim == 2 and dataframe_resampled.shape[1] > 1:
-            if self.scenario is None:
-                raise ValueError("Scenario must be given for a DataFrame input with multiple columns.")
-            if self.scenario.size != dataframe_resampled.shape[1]:
-                raise ValueError("Scenario size ({}) is different to the number of columns ({}) "
-                                 "in the DataFrame input.".format(self.scenario.size, dataframe_resampled.shape[1]))
-        self._values = dataframe_resampled.values.astype(np.float64)
-        if self.scenario is not None:
-            self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
-
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        cdef double value
-        if self.scenario is not None:
-            value = self._values[<int>(timestep.index), <int>(scenario_index._indices[self._scenario_index])]
-        else:
-            value = self._values[<int>(timestep.index), 0]
-        return value
-
-    @classmethod
-    def load(cls, model, data):
-        scenario = data.pop('scenario', None)
-        if scenario is not None:
-            scenario = model.scenarios[scenario]
-        df = load_dataframe(model, data)
-        return cls(model, df, scenario=scenario, **data)
-
-DataFrameParameter.register()
-
-cdef class ArrayIndexedParameter(Parameter):
-    """Time varying parameter using an array and Timestep.index
-
-    The values in this parameter are constant across all scenarios.
-    """
-    def __init__(self, model, values, *args, **kwargs):
-        super(ArrayIndexedParameter, self).__init__(model, *args, **kwargs)
-        self.values = np.asarray(values, dtype=np.float64)
-
-    cdef calc_values(self, Timestep ts):
-        # constant parameter can just set the entire array to one value
-        self.__values[...] = self.values[ts.index]
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        """Returns the value of the parameter at a given timestep
-        """
-        return self.values[ts.index]
-ArrayIndexedParameter.register()
-
-
-cdef class ArrayIndexedScenarioParameter(Parameter):
-    """A Scenario varying Parameter
-
-    The values in this parameter are vary in time based on index and vary within a single Scenario.
-    """
-    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
-        """
-        values should be an iterable that is the same length as scenario.size
-        """
-        super(ArrayIndexedScenarioParameter, self).__init__(model, *args, **kwargs)
-        cdef int i
-        values = np.asarray(values, dtype=np.float64)
-        if values.ndim != 2:
-            raise ValueError("Values must be two dimensional.")
-        if scenario._size != values.shape[1]:
-            raise ValueError("The size of the second dimension of values must equal the size of the scenario.")
-        self.values = values
-        self._scenario = scenario
-
-    cpdef setup(self):
-        super(ArrayIndexedScenarioParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        # This is a bit confusing.
-        # scenario_indices contains the current scenario number for all
-        # the Scenario objects in the model run. We have cached the
-        # position of self._scenario in self._scenario_index to lookup the
-        # correct number to use in this instance.
-        return self.values[ts.index, scenario_index._indices[self._scenario_index]]
-
-ArrayIndexedScenarioParameter.register()
-
-
-cdef class TablesArrayParameter(IndexParameter):
-    def __init__(self, model, h5file, node, where='/', scenario=None, **kwargs):
-        """
-        This Parameter reads array data from a PyTables HDF database.
-
-        The parameter reads data using the PyTables array interface and therefore
-        does not require loading the entire dataset in to memory. This is useful
-        for large model runs.
-
-        Parameters
-        ----------
-        h5file : tables.File or filename
-            The tables file handle or filename to attach the CArray objects to. If a
-            filename is given the object will open and close the file handles.
-        node : string
-            Name of the node in the tables database to read data from
-        where : string
-            Path to read the node from.
-        scenario : Scenario
-            Scenario to use as the second index in the array.
-        """
-        super(TablesArrayParameter, self).__init__(model, **kwargs)
-
-        self.h5file = h5file
-        self.h5store = None
-        self.node = node
-        self.where = where
-        self.scenario = scenario
-
-        # Private attributes, initialised during setup()
-        self._values_dbl = None  # Stores the loaded data if float
-        self._values_int = None  # Stores the loaded data if integer
-        # If a scenario is present this is the index in the model list of scenarios
-        self._scenario_index = -1
-        self._scenario_ids = None  # Lookup of scenario index to the loaded data index
-
-    cpdef setup(self):
-        cdef Py_ssize_t n, i
-
-        super(TablesArrayParameter, self).setup()
-        self._scenario_index = -1
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        if self.scenario is not None:
-            self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
-
-        self.h5store = H5Store(self.h5file, None, "r")
-        node = self.h5store.file.get_node(self.where, self.node)
-        if not node.dtype in (np.float32, np.float64, np.int8, np.int16, np.int32):
-            raise TypeError("Unexpected dtype in array: {}".format(node.dtype))
-
-        # check the shape of the data is valid
-        if self.scenario is not None:
-            if node.shape[1] < self.scenario.size:
-                raise IndexError('The length of the second dimension ({:d}) of the tables node ({}:{}) '
-                                 'should be the same as the size of the specified Scenario ({:d}).'
-                                 .format(node.shape[1], node._v_file.filename, node._v_pathname, self.scenario.size))
-            elif node.shape[1] > self.scenario.size:
-                warnings.warn('The length of the second dimension ({:d}) of the tables node ({}:{}) '
-                              'is greater than the size of the specified Scenario ({:d}). '
-                              'Not all data is being used!'.format(node.shape[1], node._v_file.filename, node._v_pathname, self.scenario.size),
-                              UnutilisedDataWarning)
-        if node.shape[0] < len(self.model.timestepper):
-            raise IndexError('The length of the first dimension ({:d}) of the tables node ({}:{}) '
-                             'should be equal to or greater than the number of timesteps.'
-                             .format(node.shape[0], node._v_file.filename, node._v_pathname, len(self.model.timestepper)))
-        elif node.shape[0] > len(self.model.timestepper):
-            warnings.warn('The length of the first dimension ({:d}) of the tables node ({}:{}) '
-                          'is greater than the number of timesteps. Not all data is being used!'
-                          .format(node.shape[0], node._v_file.filename, node._v_pathname, len(self.model.timestepper)),
-                          UnutilisedDataWarning)
-
-        # detect data type and read into memoryview
-        self._values_dbl = None
-        self._values_int = None
-        if self.scenario:
-            # if possible, only load the data required
-            scenario_indices = None
-            # Default to index that is just out of bounds to cause IndexError if something goes wrong
-            self._scenario_ids = np.ones(self.scenario.size, dtype=np.int32) * self.scenario.size
-
-            # Calculate the scenario indices to load dependning on how scenario combinations are defined.
-            if self.model.scenarios.user_combinations:
-                scenario_indices = set()
-                for user_combination in self.model.scenarios.user_combinations:
-                    scenario_indices.add(user_combination[self._scenario_index])
-                scenario_indices = sorted(list(scenario_indices))
-            elif self.scenario.slice:
-                scenario_indices = range(*self.scenario.slice.indices(self.scenario.slice.stop))
-            else:
-                # scenario is defined, but all data required
-                self._scenario_ids = None
-
-            if scenario_indices is not None:
-                # Now load only the required data
-                for n, i in enumerate(scenario_indices):
-                    self._scenario_ids[i] = n
-
-                if node.dtype in (np.float32, np.float64):
-                    self._values_dbl = node[:len(self.model.timestepper), scenario_indices].astype(np.float64)
-                else:
-                    self._values_int = node[:len(self.model.timestepper), scenario_indices].astype(np.int32)
-
-        if node.dtype in (np.float32, np.float64):
-            if self._values_dbl is None:
-                self._values_dbl = node.read().astype(np.float64)
-            # negative values are often erroneous
-            if np.min(self._values_dbl) < 0.0:
-                warnings.warn('Negative values in input file "{}" from node: {}'.format(self.h5file, self.node))
-            if not np.all(np.isfinite(self._values_dbl)):
-                raise ValueError('Non-finite values in input file "{}" from node: {}'.format(self.h5file, self.node))
-        else:
-            if self._values_int is None:
-                self._values_int = node.read().astype(np.int32)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef Py_ssize_t i = ts.index
-        cdef Py_ssize_t j
-        if self._values_dbl is None:
-            return float(self.index(ts, scenario_index))
-        # Support 1D and 2D indexing when scenario is or is not given.
-        if self._scenario_index == -1:
-            return self._values_dbl[i, 0]
-        else:
-            j = scenario_index._indices[self._scenario_index]
-            if self._scenario_ids is not None:
-                j = self._scenario_ids[j]
-            return self._values_dbl[i, j]
-
-    cpdef int index(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef Py_ssize_t i = ts.index
-        cdef Py_ssize_t j
-        if self._values_int is None:
-            return int(self.value(ts, scenario_index))
-        # Support 1D and 2D indexing when scenario is or is not given.
-        if self._scenario_index == -1:
-            return self._values_int[i, 0]
-        else:
-            j = scenario_index._indices[self._scenario_index]
-            if self._scenario_ids is not None:
-                j = self._scenario_ids[j]
-            return self._values_int[i, j]
-
-    cpdef finish(self):
-        self.h5store = None
-
-    @classmethod
-    def load(cls, model, data):
-        scenario = data.pop('scenario', None)
-        if scenario is not None:
-            scenario = model.scenarios[scenario]
-
-        url = data.pop('url')
-        if not os.path.isabs(url) and model.path is not None:
-            url = os.path.join(model.path, url)
-        node = data.pop('node')
-        where = data.pop('where', '/')
-
-        # Check hashes if given before reading the data
-        checksums = data.pop('checksum', {})
-        for algo, hash in checksums.items():
-            check_hash(url, hash, algorithm=algo)
-
-        return cls(model, url, node, where=where, scenario=scenario)
-TablesArrayParameter.register()
-
-
-cdef class ConstantScenarioParameter(Parameter):
-    """A Scenario varying Parameter
-
-    The values in this parameter are constant in time, but vary within a single Scenario.
-    """
-    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
-        """
-        values should be an iterable that is the same length as scenario.size
-        """
-        super(ConstantScenarioParameter, self).__init__(model, *args, **kwargs)
-        cdef int i
-        if scenario._size != len(values):
-            raise ValueError("The number of values must equal the size of the scenario.")
-        self._values = np.empty(scenario._size)
-        for i in range(scenario._size):
-            self._values[i] = values[i]
-        self._scenario = scenario
-
-    cpdef setup(self):
-        super(ConstantScenarioParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        # This is a bit confusing.
-        # scenario_indices contains the current scenario number for all
-        # the Scenario objects in the model run. We have cached the
-        # position of self._scenario in self._scenario_index to lookup the
-        # correct number to use in this instance.
-        return self._values[scenario_index._indices[self._scenario_index]]
-ConstantScenarioParameter.register()
-
-
-cdef class ArrayIndexedScenarioMonthlyFactorsParameter(Parameter):
-    """Time varying parameter using an array and Timestep.index with
-    multiplicative factors per Scenario
-    """
-    def __init__(self, model, Scenario scenario, values, factors, *args, **kwargs):
-        """
-        values is the baseline timeseries data that is perturbed by a factor. The
-        factor is taken from factors which is shape (scenario.size, 12). Therefore
-        factors vary with the individual scenarios in scenario and month.
-        """
-        super(ArrayIndexedScenarioMonthlyFactorsParameter, self).__init__(model, *args, **kwargs)
-
-        values = np.asarray(values, dtype=np.float64)
-        factors = np.asarray(factors, dtype=np.float64)
-        if factors.ndim != 2:
-            raise ValueError("Factors must be two dimensional.")
-
-        if factors.shape[0] != scenario._size:
-            raise ValueError("First dimension of factors must be the same size as scenario.")
-        if factors.shape[1] != 12:
-            raise ValueError("Second dimension of factors must be 12.")
-        self._scenario = scenario
-        self._values = values
-        self._factors = factors
-
-    cpdef setup(self):
-        super(ArrayIndexedScenarioMonthlyFactorsParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        # This is a bit confusing.
-        # scenario_indices contains the current scenario number for all
-        # the Scenario objects in the model run. We have cached the
-        # position of self._scenario in self._scenario_index to lookup the
-        # correct number to use in this instance.
-        cdef int imth = ts.month-1
-        cdef int i = scenario_index._indices[self._scenario_index]
-        return self._values[ts.index]*self._factors[i, imth]
-
-    @classmethod
-    def load(cls, model, data):
-        scenario = data.pop("scenario", None)
-        if scenario is not None:
-            scenario = model.scenarios[scenario]
-
-        if isinstance(data["values"], list):
-            values = np.asarray(data["values"], np.float64)
-        elif isinstance(data["values"], dict):
-            values = load_parameter_values(model, data["values"])
-        else:
-            raise TypeError("Unexpected type for \"values\" in {}".format(cls.__name__))
-
-        if isinstance(data["factors"], list):
-            factors = np.asarray(data["factors"], np.float64)
-        elif isinstance(data["factors"], dict):
-            factors = load_parameter_values(model, data["factors"])
-        else:
-            raise TypeError("Unexpected type for \"factors\" in {}".format(cls.__name__))
-
-        return cls(model, scenario, values, factors)
-
-ArrayIndexedScenarioMonthlyFactorsParameter.register()
-
-
-cdef class DailyProfileParameter(Parameter):
-    """ An annual profile consisting of daily values.
-
-    This parameter provides a repeating annual profile with a daily resolution. A total of 366 values
-    must be provided. These values are coerced to a `numpy.array` internally.
-
-    Parameters
-    ----------
-    values : iterable, array
-        The 366 values that represent the daily profile.
-
-    """
-    def __init__(self, model, values, *args, **kwargs):
-        super(DailyProfileParameter, self).__init__(model, *args, **kwargs)
-        v = np.squeeze(np.array(values))
-        if v.ndim != 1:
-            raise ValueError("values must be 1-dimensional.")
-        if len(values) != 366:
-            raise ValueError("366 values must be given for a daily profile.")
-        self._values = v
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._values[ts.dayofyear_index]
-DailyProfileParameter.register()
-
-cdef class WeeklyProfileParameter(Parameter):
-    """Weekly profile (52-week year)
-
-    The last week of the year will have more than 7 days, as 365 / 7 is not whole.
-    """
-    def __init__(self, model, values, *args, **kwargs):
-        super(WeeklyProfileParameter, self).__init__(model, *args, **kwargs)
-        v = np.squeeze(np.array(values))
-        if v.ndim != 1:
-            raise ValueError("values must be 1-dimensional.")
-        if len(values) == 53:
-            values = values[:52]
-            warnings.warn("Truncating 53 week profile to 52 weeks.")
-        if len(values) != 52:
-            raise ValueError("52 values must be given for a weekly profile.")
-        self._values = v
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._values[ts.week_index]
-WeeklyProfileParameter.register()
-
-
-cdef class MonthlyProfileParameter(Parameter):
-    """Parameter which provides a monthly profile.
-
-    The monthly profile returns a different value based on the month of the current
-    time-step. By default this creates a piecewise profile with a step change at the
-    beginning of each month. An optional `interp_day` keyword can instead create a
-    linearly interpolated daily profile assuming the given values correspond to either
-    the first or last day of the month.
-
-    Parameters
-    ----------
-    values : iterable, array
-        The 12 values that represent the monthly profile.
-    lower_bounds : float (default=0.0)
-        The lower bounds of the monthly profile values when used during optimisation.
-    upper_bounds : float (default=np.inf)
-        The upper bounds of the monthly profile values when used during optimisation.
-    inter_day : str or None (default=None)
-        If `interp_day` is None then no interpolation is undertaken, and the parameter
-         returns values representing a piecewise monthly profile. Otherwise `interp_day`
-         must be a string of either "first" or "last" representing which day of the month
-         each of the 12 values represents. The parameter then returns linearly
-         interpolated values between the given day of the month.
-
-
-    See also
-    --------
-    ScenarioMonthlyProfileParameter
-    ArrayIndexedScenarioMonthlyFactorsParameter
-    """
-    def __init__(self, model, values, lower_bounds=0.0, upper_bounds=np.inf, interp_day=None, **kwargs):
-        super(MonthlyProfileParameter, self).__init__(model, **kwargs)
-        self.double_size = 12
-        self.integer_size = 0
-        if len(values) != self.double_size:
-            raise ValueError("12 values must be given for a monthly profile.")
-        self._values = np.array(values)
-        self.interp_day = interp_day
-        self._lower_bounds = np.ones(self.double_size)*lower_bounds
-        self._upper_bounds = np.ones(self.double_size)*upper_bounds
-
-    cpdef reset(self):
-        Parameter.reset(self)
-        # The interpolated profile is recalculated during reset so that
-        # it will update when the _values array is updated via `set_double_variables`
-        # and the model is rerun. I.e. during optimisation (where setup is not redone).
-        if self.interp_day is not None:
-            self._interpolate()
-
-    cpdef _interpolate(self):
-
-        # Create an array to save the daily profile in.
-        self._interp_values = np.zeros(366)
-        cdef int i = 0
-        cdef int mth
-
-        # Create interpolation knots depending on values
-        if self.interp_day == 'first':
-            x = [1]  # First month
-            y = []
-            for mth in range(1, 13):
-                x.append(x[-1] + calendar.monthrange(2015, mth)[1])
-                y.append(self._values[mth-1])
-            y.append(self._values[0])
-        elif self.interp_day == 'last':
-            x = [0]  # End of previous year
-            y = [self._values[11]]  # Use value from December
-            for mth in range(1, 13):
-                x.append(x[-1] + calendar.monthrange(2015, mth)[1])
-                y.append(self._values[mth-1])
-        else:
-            raise ValueError(f'Interpolation day "{self.interp_day}" not supported.')
-
-        # Do the interpolation
-        values = np.interp(np.arange(365) + 1, x, y)
-        # Make the daily profile of 366 values repeating the same value for 28th & 29th Feb.
-        for i in range(365):
-            if i < 58:
-                self._interp_values[i] = values[i]
-            elif i == 58:
-                self._interp_values[i] = values[i]
-                self._interp_values[i+1] = values[i]
-            elif i > 58:
-                self._interp_values[i+1] = values[i]
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        if self.interp_day is None:
-            return self._values[ts.month-1]
-        else:
-            return self._interp_values[ts.dayofyear_index]
-
-    cpdef set_double_variables(self, double[:] values):
-        self._values[...] = values
-
-    cpdef double[:] get_double_variables(self):
-        # Make sure we return a copy of the data instead of a view.
-        return np.array(self._values).copy()
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return self._lower_bounds
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return self._upper_bounds
-MonthlyProfileParameter.register()
-
-
-cdef class ScenarioMonthlyProfileParameter(Parameter):
-    """ Parameter that provides a monthly profile per scenario
-
-    Behaviour is the same as `MonthlyProfileParameter` except a different
-    profile is returned for each ensemble in a given scenario.
-
-    See also
-    --------
-    MonthlyProfileParameter
-    ArrayIndexedScenarioMonthlyFactorsParameter
-    """
-    def __init__(self, model, Scenario scenario, values, **kwargs):
-        super(ScenarioMonthlyProfileParameter, self).__init__(model, **kwargs)
-
-        if values.ndim != 2:
-            raise ValueError("Factors must be two dimensional.")
-
-        if scenario._size != values.shape[0]:
-            raise ValueError("First dimension of factors must be the same size as scenario.")
-        if values.shape[1] != 12:
-            raise ValueError("Second dimension of factors must be 12.")
-        self._scenario = scenario
-        self._values = np.array(values)
-
-    cpdef setup(self):
-        super(ScenarioMonthlyProfileParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._values[scenario_index._indices[self._scenario_index], ts.month-1]
-
-ScenarioMonthlyProfileParameter.register()
-
-cdef class ScenarioWeeklyProfileParameter(Parameter):
-    """Parameter that provides a weekly profile per scenario
-
-    This parameter provides a repeating annual profile with a weekly resolution. A
-    different profile is returned for each member of a given scenario
-
-    Parameters
-    ----------
-    scenario: Scenario
-        Scenario object over which different profiles should be provided.
-    values : iterable, array
-        Length of 1st dimension should equal the number of members in the scenario object
-        and the length of the second dimension should be 52
-
-    """
-    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
-        super().__init__(model, *args, **kwargs)
-        values = np.array(values)
-        if values.ndim != 2:
-            raise ValueError("Factors must be two dimensional.")
-        if scenario._size != values.shape[0]:
-            raise ValueError("First dimension of factors must be the same size as scenario.")
-        if values.shape[1] != 52:
-            raise ValueError("52 values must be given for a weekly profile.")
-        self._values = values
-        self._scenario = scenario
-
-    cpdef setup(self):
-        super(ScenarioWeeklyProfileParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._values[scenario_index._indices[self._scenario_index], ts.week_index]
-
-ScenarioWeeklyProfileParameter.register()
-
-cdef class ScenarioDailyProfileParameter(Parameter):
-    """Parameter which provides a daily profile per scenario.
-
-    This parameter provides a repeating annual profile with a daily resolution. A
-    different profile is returned for each member of a given scenario
-
-    Parameters
-    ----------
-    scenario: Scenario
-        Scenario object over which different profiles should be provided
-    values : iterable, array
-        Length of 1st dimension should equal the number of members in the scenario object
-        and the length of the second dimension should be 366
-
-    """
-    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
-        super().__init__(model, *args, **kwargs)
-        values = np.array(values)
-        if values.ndim != 2:
-            raise ValueError("Factors must be two dimensional.")
-        if scenario._size != values.shape[0]:
-            raise ValueError("First dimension of factors must be the same size as scenario.")
-        if values.shape[1] != 366:
-            raise ValueError("366 values must be given for a daily profile.")
-        self._values = values
-        self._scenario = scenario
-
-    cpdef setup(self):
-        super(ScenarioDailyProfileParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i = ts.dayofyear_index
-        return self._values[scenario_index._indices[self._scenario_index], i]
-
-ScenarioDailyProfileParameter.register()
-
-
-cdef class UniformDrawdownProfileParameter(Parameter):
-    """Parameter which provides a uniformly reducing value from one to zero.
-
-     This parameter is intended to be used with an `AnnualVirtualStorage` node to provide a profile
-     that represents perfect average utilisation of the annual volume. It returns a value of 1 on the
-     reset day, and subsequently reduces by 1/366 every day afterward.
-
-    Parameters
-    ----------
-    reset_day: int
-        The day of the month (1-31) to reset the volume to the initial value.
-    reset_month: int
-        The month of the year (1-12) to reset the volume to the initial value.
-
-    See also
-    --------
-    AnnualVirtualStorage
-    """
-    def __init__(self, model, reset_day=1, reset_month=1, **kwargs):
-        super().__init__(model, **kwargs)
-        self.reset_day = reset_day
-        self.reset_month = reset_month
-
-    cpdef reset(self):
-        super(UniformDrawdownProfileParameter, self).reset()
-        # Reset day of the year based on a leap year.
-        # Note that this is zero-based
-        self._reset_idoy = pandas.Period(year=2016, month=self.reset_month, day=self.reset_day, freq='D').dayofyear - 1
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int current_idoy = ts.dayofyear_index
-        cdef int total_days_in_period
-        cdef int days_into_period
-        cdef int year = ts.year
-
-        days_into_period = current_idoy - self._reset_idoy
-        if days_into_period < 0:
-            # We're not past the reset day yet; use the previous year
-            year -= 1
-
-        if self._reset_idoy > 59:
-            # Reset occurs after February therefore next year's February might be a leap year?
-            year += 1
-
-        # Determine the number of days in the period based on whether there is a leap year or not in the current period
-        if is_leap_year(year):
-            total_days_in_period = 366
-        else:
-            total_days_in_period = 365
-
-        # Now determine number of days we're into the period if it has wrapped around to a new year
-        if days_into_period < 0:
-            days_into_period += 366
-            # Need to adjust for post 29th Feb in non-leap years.
-            # Recall `current_idoy` was incremented by 1 if it is a non-leap already (hence comparison to 59)
-            if not is_leap_year(ts.year) and current_idoy > 59:
-                days_into_period -= 1
-
-        return 1.0 - days_into_period / total_days_in_period
-
-    @classmethod
-    def load(cls, model, data):
-        return cls(model, **data)
-UniformDrawdownProfileParameter.register()
-
-
-cdef class RbfProfileParameter(Parameter):
-    """Parameter which interpolates a daily profile using a radial basis function (RBF).
-
-    The daily profile is computed during model `reset` using a radial basis function with
-    day-of-year as the independent variables. The days of the year are defined by the user
-    alongside the values to use on each of those days for the interpolation. The first
-    day of the years should always be one, and its value is repeated as the 366th value.
-    In addition the second and penultimate values are mirrored to encourage a consistent
-    gradient to appear across the boundary. The RBF calculations are undertaken using
-    the `scipy.interpolate.Rbf` object, please refer to Scipy's documentation for more
-    information.
-
-    Parameters
-    ----------
-    days_of_year : iterable, integer
-        The days of the year at which the interpolation values are defined. The first
-        value should be one.
-    values : iterable, float
-        Values to use for interpolation corresponding to the `days_of_year`.
-    lower_bounds : float (default=0.0)
-        The lower bounds of the values when used during optimisation.
-    upper_bounds : float (default=np.inf)
-        The upper bounds of the values when used during optimisation.
-    variable_days_of_year_range : int (default=0)
-        The maximum bounds (positive or negative) for the days of year during optimisation. A non-zero value
-        will cause the days of the year values to be exposed as integer variables (except the first value which
-        remains at day 1). This value is bounds on those variables as maximum shift from the given `days_of_year`.
-    min_value, max_value : float
-        Optionally cap the interpolated daily profile to a minimum and/or maximum value. The default values
-        are negative and positive infinity for minimum and maximum respectively.
-    rbf_kwargs: Optional, dict
-        Optional dictionary of keyword arguments to base to the Rbf object.
-
-    """
-    def __init__(self, model, days_of_year, values, lower_bounds=0.0, upper_bounds=np.inf, rbf_kwargs=None,
-                 variable_days_of_year_range=0, min_value=-np.inf, max_value=np.inf, **kwargs):
-        super(RbfProfileParameter, self).__init__(model, **kwargs)
-
-        if len(days_of_year) != len(values):
-            raise ValueError(f"The length of values ({len(values)}) must equal the length of "
-                             f"`days_of_year` ({len(days_of_year)}).")
-
-        self.variable_days_of_year_range = variable_days_of_year_range
-        self.double_size = len(values)
-        self._values = np.array(values, dtype=np.float64)
-        self.days_of_year = days_of_year
-        self.min_value = min_value
-        self.max_value = max_value
-        self._lower_bounds = np.ones(self.double_size)*lower_bounds
-        self._upper_bounds = np.ones(self.double_size)*upper_bounds
-
-        if self.variable_days_of_year_range > 0:
-            if np.any(np.diff(self.days_of_year) <= 2*self.variable_days_of_year_range):
-                raise ValueError(f"The days of the year are too close together for the given "
-                                 f"`variable_days_of_year_range`. This could cause the optimised days"
-                                 f"of the year to overlap and become out of order.  Either increase the"
-                                 f"spacing of the days of the year or reduce `variable_days_of_year_range` to"
-                                 f"less than half the closest distance between the days of the year.")
-            self.integer_size = len(values) - 1
-            self._doy_lower_bounds = np.array([d - self.variable_days_of_year_range
-                                               for d in self.days_of_year[1:]], dtype=np.int32)
-            self._doy_upper_bounds = np.array([d + self.variable_days_of_year_range
-                                               for d in self.days_of_year[1:]], dtype=np.int32)
-        else:
-            self.integer_size = 0
-
-        self.rbf_kwargs = rbf_kwargs if rbf_kwargs is not None else {}
-
-    property days_of_year:
-        def __get__(self):
-            return np.array(self._days_of_year)
-        def __set__(self, values):
-            values = np.array(values, dtype=np.int32)
-            if values[0] != 1:
-                raise ValueError('The first day of the years must be 1.')
-            if len(values) < 3:
-                raise ValueError('At least 3 days of the year are required.')
-            if np.any(np.diff(values) <= 0):
-                raise ValueError('The days of the year should be strictly monotonically increasing.')
-            if np.any(0 > values > 365):
-                raise ValueError('Days of the years should be between 1 and 365 inclusive.')
-            self._days_of_year = values
-
-    cpdef reset(self):
-        Parameter.reset(self)
-        # The interpolated profile is recalculated during reset so that
-        # it will update when the _values array is updated via `set_double_variables`
-        # and the model is rerun. I.e. during optimisation (where setup is not redone).
-        self._interpolate()
-
-    cpdef _interpolate(self):
-        cdef int i
-        cdef double[:] values
-        cdef double v
-
-        days_of_year = list(self._days_of_year)
-        # Append day 365 to the list and mirror the penultimate and second DOY at the start and end
-        # of the list respectively. This helps ensure the gradient is roughly the same across the boundary
-        # between days 365 and 0.
-        days_of_year = [days_of_year[-1]-365] + list(days_of_year) + [366, 366+days_of_year[1]-1]
-        # Create the corresponding y values including the mirrored entries
-        y = list(self._values)
-        y = [y[-1]] + y + [y[0], y[1]]
-        rbfi = Rbf(days_of_year, y)
-
-        # Do the interpolation
-        values = rbfi(np.arange(365) + 1)
-
-        # Create an array to save the daily profile in.
-        self._interp_values = np.zeros(366)
-        # Make the daily profile of 366 values repeating the same value for 28th & 29th Feb.
-        for i in range(365):
-            v = max(min(values[i], self.max_value), self.min_value)
-            if i < 58:
-                self._interp_values[i] = v
-            elif i == 58:
-                self._interp_values[i] = v
-                self._interp_values[i+1] = v
-            elif i > 58:
-                self._interp_values[i+1] = v
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i = ts.dayofyear_index
-        return self._interp_values[i]
-
-    # Double variables are for the known interpolation values (y-axis)
-    cpdef set_double_variables(self, double[:] values):
-        self._values[...] = values
-
-    cpdef double[:] get_double_variables(self):
-        # Make sure we return a copy of the data instead of a view.
-        return np.array(self._values).copy()
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return self._lower_bounds
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return self._upper_bounds
-
-    # Integer variables are for the days of the year positions (if optimised)
-    cpdef set_integer_variables(self, int[:] values):
-        self.days_of_year = [1] + np.array(values).tolist()
-
-    cpdef int[:] get_integer_variables(self):
-        return np.array(self.days_of_year[1:], dtype=np.int32)
-
-    cpdef int[:] get_integer_lower_bounds(self):
-        return self._doy_lower_bounds
-
-    cpdef int[:] get_integer_upper_bounds(self):
-        return self._doy_upper_bounds
-
-    @classmethod
-    def load(cls, model, data):
-        return cls(model, **data)
-RbfProfileParameter.register()
-
-
-cdef class IndexParameter(Parameter):
-    """Base parameter providing an `index` method
-
-    See also
-    --------
-    IndexedArrayParameter
-    ControlCurveIndexParameter
-    """
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns the current index as a float"""
-        # return index as a float
-        return float(self.get_index(scenario_index))
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns the current index"""
-        # return index as an integer
-        return 0
-
-    cpdef setup(self):
-        super(IndexParameter, self).setup()
-        cdef int num_comb
-        if self.model.scenarios.combinations:
-            num_comb = len(self.model.scenarios.combinations)
-        else:
-            num_comb = 1
-        self.__indices = np.empty([num_comb], np.int32)
-
-    cdef calc_values(self, Timestep timestep):
-        cdef ScenarioIndex scenario_index
-        cdef ScenarioCollection scenario_collection = self.model.scenarios
-        for scenario_index in scenario_collection.combinations:
-            self.__indices[<int>(scenario_index.global_id)] = self.index(timestep, scenario_index)
-            self.__values[<int>(scenario_index.global_id)] = self.value(timestep, scenario_index)
-
-    cpdef int get_index(self, ScenarioIndex scenario_index):
-        return self.__indices[<int>(scenario_index.global_id)]
-
-    cpdef int[:] get_all_indices(self):
-        return self.__indices
-IndexParameter.register()
-
-
-cdef class ConstantScenarioIndexParameter(IndexParameter):
-    """A Scenario varying IndexParameter
-
-    The values in this parameter are constant in time, but vary within a single Scenario.
-    """
-    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
-        """
-        values should be an iterable that is the same length as scenario.size
-        """
-        super(ConstantScenarioIndexParameter, self).__init__(model, *args, **kwargs)
-        cdef int i
-        if scenario._size != len(values):
-            raise ValueError("The number of values must equal the size of the scenario.")
-        self._values = np.empty(scenario._size, dtype=np.int32)
-        for i in range(scenario._size):
-            self._values[i] = values[i]
-        self._scenario = scenario
-
-    cpdef setup(self):
-        super(ConstantScenarioIndexParameter, self).setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        # This is a bit confusing.
-        # scenario_indices contains the current scenario number for all
-        # the Scenario objects in the model run. We have cached the
-        # position of self._scenario in self._scenario_index to lookup the
-        # correct number to use in this instance.
-        return self._values[scenario_index._indices[self._scenario_index]]
-ConstantScenarioIndexParameter.register()
-
-
-cdef class IndexedArrayParameter(Parameter):
-    """Parameter which uses an IndexParameter to index an array of Parameters
-
-    An example use of this parameter is to return a demand saving factor (as
-    a float) based on the current demand saving level (calculated by an
-    `IndexParameter`).
-
-    Parameters
-    ----------
-    index_parameter : `IndexParameter`
-    params : iterable of `Parameters` or floats
-
-
-    Notes
-    -----
-    Float arguments `params` are converted to `ConstantParameter`
-    """
-    def __init__(self, model, index_parameter, params, **kwargs):
-        super(IndexedArrayParameter, self).__init__(model, **kwargs)
-        assert(isinstance(index_parameter, IndexParameter))
-        self.index_parameter = index_parameter
-        self.children.add(index_parameter)
-
-        self.params = []
-        for p in params:
-            if not isinstance(p, Parameter):
-                from pywr.parameters import ConstantParameter
-                p = ConstantParameter(model, p)
-            self.params.append(p)
-
-        for param in self.params:
-            self.children.add(param)
-        self.children.add(index_parameter)
-
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns the value of the Parameter at the current index"""
-        cdef int index
-        index = self.index_parameter.get_index(scenario_index)
-        cdef Parameter parameter = self.params[index]
-        return parameter.get_value(scenario_index)
-
-    @classmethod
-    def load(cls, model, data):
-        index_parameter = load_parameter(model, data.pop("index_parameter"))
-        try:
-            parameters = data.pop("params")
-        except KeyError:
-            parameters = data.pop("parameters")
-        parameters = [load_parameter(model, parameter_data) for parameter_data in parameters]
-        return cls(model, index_parameter, parameters, **data)
-IndexedArrayParameter.register()
-
-
-cdef class AnnualHarmonicSeriesParameter(Parameter):
-    """ A `Parameter` which returns the value from an annual harmonic series
-
-    This `Parameter` comprises a series N cosine function with a period of 365
-     days. The calculation is performed using the Julien day of the year minus 1
-     This causes a small discontinuity in non-leap years.
-
-    .. math:: f(t) = A + \sum_{n=1}^N A_n\cdot \cos((2\pi nt)/365+\phi_n)
-
-    Parameters
-    ----------
-
-    mean : float
-        Mean value for the series (i.e. the position of zeroth harmonic)
-    amplitudes : array_like
-        The amplitudes for the N harmonic cosine functions. Must be the same
-        length as phases.
-    phases : array_like
-        The phase shift of the N harmonic cosine functions. Must be the same
-        length as amplitudes.
-
-    """
-    def __init__(self, model, mean, amplitudes, phases, *args, **kwargs):
-        if len(amplitudes) != len(phases):
-            raise ValueError("The number  of amplitudes and phases must be the same.")
-        n = len(amplitudes)
-        self.mean = mean
-        self._amplitudes = np.array(amplitudes)
-        self._phases = np.array(phases)
-
-        self._mean_lower_bounds = kwargs.pop('mean_lower_bounds', 0.0)
-        self._mean_upper_bounds = kwargs.pop('mean_upper_bounds', np.inf)
-        self._amplitude_lower_bounds = np.ones(n)*kwargs.pop('amplitude_lower_bounds', 0.0)
-        self._amplitude_upper_bounds = np.ones(n)*kwargs.pop('amplitude_upper_bounds', np.inf)
-        self._phase_lower_bounds = np.ones(n)*kwargs.pop('phase_lower_bounds', 0.0)
-        self._phase_upper_bounds = np.ones(n)*kwargs.pop('phase_upper_bounds', np.pi*2)
-        super(AnnualHarmonicSeriesParameter, self).__init__(model, *args, **kwargs)
-        # Size must be set after call to super.
-        self.double_size = 1 + 2*n
-        self._value_cache = 0.0
-        self._ts_index_cache = -1
-
-    @classmethod
-    def load(cls, model, data):
-        mean = data.pop('mean')
-        amplitudes = data.pop('amplitudes')
-        phases = data.pop('phases')
-
-        return cls(model, mean, amplitudes, phases, **data)
-
-    property amplitudes:
-        def __get__(self):
-            return np.asarray(self._amplitudes)
-
-    property phases:
-        def __get__(self):
-            return np.asarray(self._phases)
-
-    cpdef reset(self):
-        Parameter.reset(self)
-        self._value_cache = 0.0
-        self._ts_index_cache = -1
-
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        cdef int ts_index = timestep.index
-        cdef int doy = timestep.dayofyear - 1
-        cdef int n = self._amplitudes.shape[0]
-        cdef int i
-        cdef double val
-        if ts_index == self._ts_index_cache:
-            val = self._value_cache
-        else:
-            val = self.mean
-            for i in range(n):
-                val += self._amplitudes[i]*cos(doy*(i+1)*M_PI*2/365 + self._phases[i])
-            self._value_cache = val
-            self._ts_index_cache = ts_index
-        return val
-
-    cpdef set_double_variables(self, double[:] values):
-        n = len(self.amplitudes)
-        self.mean = values[0]
-        self._amplitudes[...] = values[1:n+1]
-        self._phases[...] = values[n+1:]
-
-    cpdef double[:] get_double_variables(self):
-        return np.r_[np.array([self.mean, ]), np.array(self.amplitudes), np.array(self.phases)]
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return np.r_[self._mean_lower_bounds, self._amplitude_lower_bounds, self._phase_lower_bounds]
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return np.r_[self._mean_upper_bounds, self._amplitude_upper_bounds, self._phase_upper_bounds]
-AnnualHarmonicSeriesParameter.register()
-
-cdef enum AggFuncs:
-    SUM = 0
-    MIN = 1
-    MAX = 2
-    MEAN = 3
-    PRODUCT = 4
-    CUSTOM = 5
-    ANY = 6
-    ALL = 7
-    MEDIAN = 8
-_agg_func_lookup = {
-    "sum": AggFuncs.SUM,
-    "min": AggFuncs.MIN,
-    "max": AggFuncs.MAX,
-    "mean": AggFuncs.MEAN,
-    "product": AggFuncs.PRODUCT,
-    "custom": AggFuncs.CUSTOM,
-    "any": AggFuncs.ANY,
-    "all": AggFuncs.ALL,
-    "median": AggFuncs.MEDIAN,
-}
-_agg_func_lookup_reverse = {v: k for k, v in _agg_func_lookup.items()}
-
-def wrap_const(model, value):
-    if isinstance(value, (int, float)):
-        value = ConstantParameter(model, value)
-    return value
-
-
-cdef class AggregatedParameter(Parameter):
-    """A collection of IndexParameters
-
-    This class behaves like a set. Parameters can be added or removed from it.
-    It's value is the value of it's child parameters aggregated using a
-    aggregating function (e.g. sum).
-
-    Parameters
-    ----------
-    parameters : iterable of `IndexParameter`
-        The parameters to aggregate
-    agg_func : callable or str
-        The aggregation function. Must be one of {"sum", "min", "max", "mean",
-        "product"}, or a callable function which accepts a list of values.
-    """
-    def __init__(self, model, parameters, agg_func=None, **kwargs):
-        super(AggregatedParameter, self).__init__(model, **kwargs)
-        self.agg_func = agg_func
-        self.parameters = list(parameters)
-        for parameter in self.parameters:
-            self.children.add(parameter)
-
-    @classmethod
-    def load(cls, model, data):
-        parameters_data = data.pop("parameters")
-        parameters = []
-        for pdata in parameters_data:
-            parameter = load_parameter(model, pdata)
-            parameters.append(wrap_const(model, parameter))
-
-        agg_func = data.pop("agg_func", None)
-        return cls(model, parameters=parameters, agg_func=agg_func, **data)
-
-    property agg_func:
-        def __get__(self):
-            if self._agg_func == AggFuncs.CUSTOM:
-                return self._agg_user_func
-            return _agg_func_lookup_reverse[self._agg_func]
-        def __set__(self, agg_func):
-            self._agg_user_func = None
-            if isinstance(agg_func, str):
-                agg_func = _agg_func_lookup[agg_func.lower()]
-            elif callable(agg_func):
-                self._agg_user_func = agg_func
-                agg_func = AggFuncs.CUSTOM
-            else:
-                raise ValueError("Unrecognised aggregation function: \"{}\".".format(agg_func))
-            self._agg_func = agg_func
-
-    cpdef add(self, Parameter parameter):
-        self.parameters.append(parameter)
-        parameter.parents.add(self)
-
-    cpdef remove(self, Parameter parameter):
-        self.parameters.remove(parameter)
-        parameter.parent.remove(self)
-
-    def __len__(self):
-        return len(self.parameters)
-
-    cpdef setup(self):
-        super(AggregatedParameter, self).setup()
-        assert(len(self.parameters))
-
-    cdef calc_values(self, Timestep timestep):
-        cdef Parameter parameter
-        cdef double[:] accum = self.__values  # View of the underlying location for the data
-        cdef double[:] values
-        cdef int i
-        cdef int nparam
-        cdef int n = accum.shape[0]
-        cdef ScenarioIndex scenario_index
-
-        if self._agg_func == AggFuncs.PRODUCT:
-            accum[...] = 1.0
-            for parameter in self.parameters:
-                values = parameter.get_all_values()
-                for i in range(n):
-                    accum[i] *= values[i]
-        elif self._agg_func == AggFuncs.SUM:
-            accum[...] = 0.0
-            for parameter in self.parameters:
-                values = parameter.get_all_values()
-                for i in range(n):
-                    accum[i] += values[i]
-        elif self._agg_func == AggFuncs.MAX:
-            accum[...] = np.NINF
-            for parameter in self.parameters:
-                values = parameter.get_all_values()
-                for i in range(n):
-                    if values[i] > accum[i]:
-                        accum[i] = values[i]
-        elif self._agg_func == AggFuncs.MIN:
-            accum[...] = np.PINF
-            for parameter in self.parameters:
-                values = parameter.get_all_values()
-                for i in range(n):
-                    if values[i] < accum[i]:
-                        accum[i] = values[i]
-        elif self._agg_func == AggFuncs.MEAN:
-            accum[...] = 0.0
-            for parameter in self.parameters:
-                values = parameter.get_all_values()
-                for i in range(n):
-                    accum[i] += values[i]
-
-            nparam = len(self.parameters)
-            for i in range(n):
-                accum[i] /= nparam
-
-        elif self._agg_func == AggFuncs.MEDIAN:
-            for i, scenario_index in enumerate(self.model.scenarios.combinations):
-                accum[i] = np.median([parameter.get_value(scenario_index) for parameter in self.parameters])
-        elif self._agg_func == AggFuncs.CUSTOM:
-            for i, scenario_index in enumerate(self.model.scenarios.combinations):
-                accum[i] = self._agg_user_func([parameter.get_value(scenario_index) for parameter in self.parameters])
-        else:
-            raise ValueError("Unsupported aggregation function.")
-AggregatedParameter.register()
-
-cdef class AggregatedIndexParameter(IndexParameter):
-    """A collection of IndexParameters
-
-    This class behaves like a set. Parameters can be added or removed from it.
-    Its index is the index of it's child parameters aggregated using a
-    aggregating function (e.g. sum).
-
-    Parameters
-    ----------
-    parameters : iterable of `IndexParameter`
-        The parameters to aggregate
-    agg_func : callable or str
-        The aggregation function. Must be one of {"sum", "min", "max", "any",
-        "all", "product"}, or a callable function which accepts a list of values.
-    """
-    def __init__(self, model, parameters, agg_func=None, **kwargs):
-        super(AggregatedIndexParameter, self).__init__(model, **kwargs)
-        self.agg_func = agg_func
-        self.parameters = list(parameters)
-        for parameter in self.parameters:
-            self.children.add(parameter)
-
-    @classmethod
-    def load(cls, model, data):
-        parameters_data = data.pop("parameters")
-        parameters = list()
-        for pdata in parameters_data:
-            parameter = load_parameter(model, pdata)
-            parameters.append(wrap_const(model, parameter))
-
-        agg_func = data.pop("agg_func", None)
-        return cls(model, parameters=parameters, agg_func=agg_func, **data)
-
-    property agg_func:
-        def __get__(self):
-            if self._agg_func == AggFuncs.CUSTOM:
-               return self._agg_user_func
-            return _agg_func_lookup_reverse[self._agg_func]
-        def __set__(self, agg_func):
-            self._agg_user_func = None
-            if isinstance(agg_func, str):
-                agg_func = _agg_func_lookup[agg_func.lower()]
-            elif callable(agg_func):
-                self._agg_user_func = agg_func
-                agg_func = AggFuncs.CUSTOM
-            else:
-                raise ValueError("Unrecognised aggregation function: \"{}\".".format(agg_func))
-            self._agg_func = agg_func
-
-    cpdef add(self, Parameter parameter):
-        self.parameters.append(parameter)
-        parameter.parents.add(self)
-
-    cpdef remove(self, Parameter parameter):
-        self.parameters.remove(parameter)
-        parameter.parent.remove(self)
-
-    def __len__(self):
-        return len(self.parameters)
-
-    cpdef setup(self):
-        super(AggregatedIndexParameter, self).setup()
-        assert len(self.parameters)
-        assert all([isinstance(parameter, IndexParameter) for parameter in self.parameters])
-
-    cdef calc_values(self, Timestep timestep):
-        cdef IndexParameter parameter
-        cdef int[:] accum = self.__indices  # View of the underlying location for the data
-        cdef int[:] values
-        cdef int i
-        cdef int nparam
-        cdef int n = accum.shape[0]
-        cdef ScenarioIndex scenario_index
-
-        if self._agg_func == AggFuncs.PRODUCT:
-            accum[...] = 1
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    accum[i] *= values[i]
-        elif self._agg_func == AggFuncs.SUM:
-            accum[...] = 0
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    accum[i] += values[i]
-        elif self._agg_func == AggFuncs.MAX:
-            accum[...] = INT_MIN
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    if values[i] > accum[i]:
-                        accum[i] = values[i]
-        elif self._agg_func == AggFuncs.MIN:
-            accum[...] = INT_MAX
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    if values[i] < accum[i]:
-                        accum[i] = values[i]
-        elif self._agg_func == AggFuncs.ANY:
-            accum[...] = 0
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    if values[i]:
-                        accum[i] = 1
-        elif self._agg_func == AggFuncs.ALL:
-            accum[...] = 1
-            for parameter in self.parameters:
-                values = parameter.get_all_indices()
-                for i in range(n):
-                    if not values[i]:
-                        accum[i] = 0
-
-        elif self._agg_func == AggFuncs.CUSTOM:
-            for i, scenario_index in enumerate(self.model.scenarios.combinations):
-                accum[i] = self._agg_user_func([parameter.get_index(scenario_index) for parameter in self.parameters])
-        else:
-            raise ValueError("Unsupported aggregation function.")
-
-        # Finally set the float values
-        for i in range(n):
-            self.__values[i] = accum[i]
-
-
-AggregatedIndexParameter.register()
-
-
-cdef class DivisionParameter(Parameter):
-    """ Parameter that divides one `Parameter` by another.
-
-    Parameters
-    ----------
-    denominator : `Parameter`
-        The parameter to use as the denominator (or divisor).
-    numerator : `Parameter`
-        The parameter to use as the numerator (or dividend).
-    """
-    def __init__(self, model, numerator, denominator, **kwargs):
-        super().__init__(model, **kwargs)
-        self._numerator = None
-        self._denominator = None
-        self.numerator = numerator
-        self.denominator = denominator
-
-    property numerator:
-        def __get__(self):
-            return self._numerator
-        def __set__(self, parameter):
-            # remove any existing parameter
-            if self._numerator is not None:
-                self._numerator.parents.remove(self)
-
-            self._numerator = parameter
-            self.children.add(parameter)
-
-    property denominator:
-        def __get__(self):
-            return self._denominator
-        def __set__(self, parameter):
-            # remove any existing parameter
-            if self._denominator is not None:
-                self._denominator.parents.remove(self)
-
-            self._denominator = parameter
-            self.children.add(parameter)
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = self._numerator.__values[i] / self._denominator.__values[i]
-
-    @classmethod
-    def load(cls, model, data):
-        numerator = load_parameter(model, data.pop("numerator"))
-        denominator = load_parameter(model, data.pop("denominator"))
-        return cls(model, numerator, denominator, **data)
-DivisionParameter.register()
-
-
-cdef class NegativeParameter(Parameter):
-    """ Parameter that takes negative of another `Parameter`
-
-    Parameters
-    ----------
-    parameter : `Parameter`
-        The parameter to to compare with the float.
-    """
-    def __init__(self, model, parameter, *args, **kwargs):
-        super(NegativeParameter, self).__init__(model, *args, **kwargs)
-        self.parameter = parameter
-        self.children.add(parameter)
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = -self.parameter.__values[i]
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-NegativeParameter.register()
-
-
-cdef class MaxParameter(Parameter):
-    """ Parameter that takes maximum of another `Parameter` and constant value (threshold)
-
-    This class is a more efficient version of `AggregatedParameter` where
-    a single `Parameter` is compared to constant value.
-
-    Parameters
-    ----------
-    parameter : `Parameter`
-        The parameter to to compare with the float.
-    threshold : float (default=0.0)
-        The threshold value to compare with the given parameter.
-    """
-    def __init__(self, model, parameter, threshold=0.0, *args, **kwargs):
-        super(MaxParameter, self).__init__(model, *args, **kwargs)
-        self.parameter = parameter
-        self.children.add(parameter)
-        self.threshold = threshold
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = max(self.parameter.__values[i], self.threshold)
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-MaxParameter.register()
-
-
-cdef class NegativeMaxParameter(MaxParameter):
-    """ Parameter that takes maximum of the negative of a `Parameter` and constant value (threshold) """
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = max(-self.parameter.__values[i], self.threshold)
-
-NegativeMaxParameter.register()
-
-
-cdef class MinParameter(Parameter):
-    """ Parameter that takes minimum of another `Parameter` and constant value (threshold)
-
-    This class is a more efficient version of `AggregatedParameter` where
-    a single `Parameter` is compared to constant value.
-
-    Parameters
-    ----------
-    parameter : `Parameter`
-        The parameter to to compare with the float.
-    threshold : float (default=0.0)
-        The threshold value to compare with the given parameter.
-    """
-    def __init__(self, model, parameter, threshold=0.0, *args, **kwargs):
-        super(MinParameter, self).__init__(model, *args, **kwargs)
-        self.parameter = parameter
-        self.children.add(parameter)
-        self.threshold = threshold
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = min(self.parameter.__values[i], self.threshold)
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-MinParameter.register()
-
-
-cdef class NegativeMinParameter(MinParameter):
-    """ Parameter that takes minimum of the negative of a `Parameter` and constant value (threshold) """
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = min(-self.parameter.__values[i], self.threshold)
-NegativeMinParameter.register()
-
-
-cdef class OffsetParameter(Parameter):
-    """Parameter that offsets another `Parameter` by a constant value.
-
-    This class is a more efficient version of `AggregatedParameter` where
-    a single `Parameter` is offset by a constant value.
-
-    Parameters
-    ----------
-    parameter : `Parameter`
-        The parameter to compare with the float.
-    offset : float (default=0.0)
-        The offset to apply to the value returned by `parameter`.
-    lower_bounds : float (default=0.0)
-        The lower bounds of the offset when used during optimisation.
-    upper_bounds : float (default=np.inf)
-        The upper bounds of the offset when used during optimisation.
-    """
-    def __init__(self, model, parameter, offset=0.0, lower_bounds=0.0, upper_bounds=np.inf, *args, **kwargs):
-        super(OffsetParameter, self).__init__(model, *args, **kwargs)
-        self.parameter = parameter
-        self.children.add(parameter)
-        self.offset = offset
-        self.double_size = 1
-        self._lower_bounds = np.ones(self.double_size) * lower_bounds
-        self._upper_bounds = np.ones(self.double_size) * upper_bounds
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        cdef int n = self.__values.shape[0]
-
-        for i in range(n):
-            self.__values[i] = self.parameter.__values[i] + self.offset
-
-    cpdef set_double_variables(self, double[:] values):
-        self.offset = values[0]
-
-    cpdef double[:] get_double_variables(self):
-        return np.array([self.offset, ], dtype=np.float64)
-
-    cpdef double[:] get_double_lower_bounds(self):
-        return self._lower_bounds
-
-    cpdef double[:] get_double_upper_bounds(self):
-        return self._upper_bounds
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-OffsetParameter.register()
-
-
-cdef class DeficitParameter(Parameter):
-    """Parameter track the deficit (max_flow - actual flow) of a Node
-
-    Parameters
-    ----------
-    model : pywr.model.Model
-    node : Node
-      The node that will have it's deficit tracked
-
-    Notes
-    -----
-    This parameter is a little unusual in that it's value is calculated during
-    the after method, not calc_values. It is intended to be used in combination
-    with a recorder (e.g. NumpyArrayNodeRecorder) to record the deficit (
-    defined as requested - actual flow) at a node. Note that this means
-    recording this parameter does *not* give you the value that was used by
-    the solver in this timestep. Alternatively, this parameter can be used
-    in the model by other parameters and will evaluate to *yesterdays* deficit,
-    where the deficit in the zeroth timestep is zero.
-    """
-    def __init__(self, model, node, *args, **kwargs):
-        super(DeficitParameter, self).__init__(model, *args, **kwargs)
-        self.node = node
-
-    cpdef reset(self):
-        self.__values[...] = 0.0
-
-    cdef calc_values(self, Timestep timestep):
-        pass # calculation done in after
-
-    cpdef after(self):
-        cdef double[:] max_flow
-        cdef int i
-        if self.node._max_flow_param is None:
-            for i in range(0, self.node._flow.shape[0]):
-                self.__values[i] = self.node._max_flow - self.node._flow[i]
-        else:
-            max_flow = self.node._max_flow_param.get_all_values()
-            for i in range(0, self.node._flow.shape[0]):
-                self.__values[i] = max_flow[i] - self.node._flow[i]
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        return cls(model, node=node, **data)
-
-DeficitParameter.register()
-
-
-cdef class FlowParameter(Parameter):
-    """Parameter that provides the flow from a node from the previous time-step.
-
-    Parameters
-    ----------
-    model : pywr.model.Model
-    node : Node
-      The node that will have its flow tracked
-    initial_value : float (default=0.0)
-      The value to return on the first  time-step before the node has any past flow.
-
-    Notes
-    -----
-    This parameter keeps track of the previous time step's flow on the given node. These
-    values can be used in calculations for the current timestep as though this was any
-    other parameter.
-    """
-    def __init__(self, model, node, *args, **kwargs):
-        self.initial_value = kwargs.pop('initial_value', 0)
-        super().__init__(model, *args, **kwargs)
-        self.node = node
-
-    cpdef setup(self):
-        super(FlowParameter, self).setup()
-        cdef int num_comb
-        if self.model.scenarios.combinations:
-            num_comb = len(self.model.scenarios.combinations)
-        else:
-            num_comb = 1
-        self.__next_values = np.empty([num_comb], np.float64)
-
-    cpdef reset(self):
-        self.__next_values[...] = self.initial_value
-        self.__values[...] = 0.0
-
-    cdef calc_values(self, Timestep timestep):
-        cdef int i
-        for i in range(self.__values.shape[0]):
-            self.__values[i] = self.__next_values[i]
-
-    cpdef after(self):
-        cdef int i
-        for i in range(self.node._flow.shape[0]):
-            self.__next_values[i] = self.node._flow[i]
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        return cls(model, node=node, **data)
-FlowParameter.register()
-
-
-cdef class PiecewiseIntegralParameter(Parameter):
-    """Parameter that integrates a piecewise function.
-
-    This parameter calculates the integral of a piecewise function. The
-    piecewise function is given as two arrays (`x` and `y`) and is assumed to
-    start from (0, 0). The values of `x` should be monotonically increasing
-    and greater than zero.
-
-    Parameters
-    ----------
-    parameter : `Parameter`
-        The parameter the defines the right hand bounds of the integration.
-    x : iterable of doubles
-    y : iterable of doubles
-
-    """
-    def __init__(self, model, parameter, x, y, *args, **kwargs):
-        super().__init__(model, *args, **kwargs)
-        self.parameter = parameter
-        self.children.add(parameter)
-        self.x = np.array(x, dtype=float)
-        self.y = np.array(y, dtype=float)
-
-    cpdef setup(self):
-        super(PiecewiseIntegralParameter, self).setup()
-
-        if len(self.x) != len(self.y):
-            raise ValueError('The length of `x` and `y` should be the same.')
-
-        if np.any(np.diff(self.x) < 0):
-            raise ValueError('The array `x` should be monotonically increasing.')
-
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        cdef double integral = 0.0
-        cdef double x = self.parameter.get_value(scenario_index)
-        cdef int i
-        cdef double dx, prev_x
-
-        prev_x = 0
-        for i in range(self.x.shape[0]):
-            if x < self.x[i]:
-                dx = x - prev_x
-            else:
-                dx = self.x[i] - prev_x
-
-            if dx < 0.0:
-                break
-            else:
-                integral += dx * self.y[i]
-            prev_x = self.x[i]
-        return integral
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop('parameter'))
-        return cls(model, parameter, **data)
-PiecewiseIntegralParameter.register()
-
-
-cdef class FlowDelayParameter(Parameter):
-    """Parameter that returns the delayed flow for a node after a given number of timesteps or days
-    
-    Parameters
-    ----------
-    model : `pywr.model.Model`
-    node: Node
-        The node to delay for.
-    timesteps: int
-        Number of timesteps to delay the flow.
-    days: int
-        Number of days to delay the flow. Specifying a number of days (instead of a number
-        of timesteps) is only valid if the number of days is exactly divisible by the model timestep length.
-    initial_flow: float
-        Flow value to return for initial model timesteps prior to any delayed flow being available. This
-        value is constant across all delayed timesteps and any model scenarios. Default is 0.0.
-    """
-
-    def __init__(self, model, node, *args, **kwargs):  
-        self.node = node
-        self.timesteps = kwargs.pop('timesteps', 0)
-        self.days = kwargs.pop('days', 0)
-        self.initial_flow = kwargs.pop('initial_flow', 0.0)
-        super().__init__(model, *args, **kwargs)
-
-    cpdef setup(self):
-        super(FlowDelayParameter, self).setup()
-        cdef int r
-        if self.days > 0:
-            r = self.days % self.model.timestepper.delta
-            if r == 0:
-                self.timesteps = self.days / self.model.timestepper.delta
-            else:
-                raise ValueError('The delay defined as number of days is not exactly divisible by the timestep delta.')
-        if self.timesteps < 1:
-            raise ValueError('The number of time-steps for a FlowDelayParameter node must be greater than one.')
-        self._memory = np.zeros((self.timesteps,  len(self.model.scenarios.combinations)))
-        self._memory_pointer = 0
-
-    cpdef reset(self):
-        self._memory[...] = self.initial_flow 
-        self._memory_pointer = 0
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return self._memory[self._memory_pointer, scenario_index.global_id]
-
-    cpdef after(self):
-        for i in range(self._memory.shape[1]):
-            self._memory[self._memory_pointer, i] = self.node._flow[i]
-        if self.timesteps > 1:
-            self._memory_pointer = (self._memory_pointer + 1) % self.timesteps 
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        return cls(model, node, **data)
-
-FlowDelayParameter.register()
-
-
-cdef class DiscountFactorParameter(Parameter):
-    """Parameter that returns the current discount factor based on discount rate and a base year.
-
-    Parameters
-    ----------
-    discount_rate : float
-        Discount rate (expressed as 0 - 1) used calculate discount factor for each year.
-    base_year : int
-        Discounting base year (i.e. the year with a discount factor equal to 1.0).
-    """
-
-    def __init__(self, model, rate, base_year, **kwargs):
-        super(DiscountFactorParameter, self).__init__(model, **kwargs)
-        self.rate = rate
-        self.base_year = base_year
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        return 1 / pow(1.0 + self.rate, ts.year - self.base_year)
-
-    @classmethod
-    def load(cls, model, data):
-        return cls(model, **data)
-
-DiscountFactorParameter.register()
-
-
-def get_parameter_from_registry(parameter_type):
-    key = parameter_type.lower()
-    try:
-        return parameter_registry[key]
-    except KeyError:
-        pass
-    if key.endswith("parameter"):
-        key.replace("parameter", "")
-    else:
-        key = key + "parameter"
-    try:
-        return parameter_registry[key]
-    except KeyError:
-        raise TypeError('Unknown parameter type: "{}"'.format(parameter_type))
-
-
-def load_parameter(model, data, parameter_name=None):
-    """Load a parameter from a dict"""
-    if isinstance(data, str):
-        # parameter is a reference
-        try:
-            parameter = model.parameters[data]
-        except KeyError:
-            parameter = None
-        if parameter is None:
-            if hasattr(model, "_parameters_to_load"):
-                # we're still in the process of loading data from JSON and
-                # the parameter requested hasn't been loaded yet - do it now
-                name = data
-                try:
-                    data = model._parameters_to_load.pop(name)
-                except KeyError:
-                    raise KeyError("Unknown parameter: '{}'".format(data))
-                parameter = load_parameter(model, data)
-            else:
-                raise KeyError("Unknown parameter: '{}'".format(data))
-    elif isinstance(data, (float, int)) or data is None:
-        # parameter is a constant
-        parameter = data
-    else:
-        # parameter is dynamic
-
-        try:
-             parameter_type = data['type']
-        except KeyError:
-            #raise custom exception that makes the error a bit easier to interpret
-            raise TypeNotFoundError(data)
-
-        try:
-            parameter_name = data["name"]
-        except:
-            pass
-
-        cls = get_parameter_from_registry(parameter_type)
-
-        kwargs = dict([(k,v) for k,v in data.items()])
-        del(kwargs["type"])
-        if "name" in kwargs:
-            del(kwargs["name"])
-        parameter = cls.load(model, kwargs)
-
-    if parameter_name is not None:
-        # TODO FIXME: memory leak if parameter is subsequently removed from the model
-        parameter.name = parameter_name
-        model.parameters[parameter_name] = parameter
-
-    return parameter
-
-
-def load_parameter_values(model, data, values_key='values', url_key='url',
-                          table_key='table'):
-    """ Function to load values from a data dictionary.
-
-    This function tries to load values in to a `np.ndarray` if 'values_key' is
-    in 'data'. Otherwise it tries to `load_dataframe` from a 'url' key.
-
-    Parameters
-    ----------
-    model - `Model` instance
-    data - dict
-    values_key - str
-        Key in data to load values directly to a `np.ndarray`
-    url_key - str
-        Key in data to load values directly from an external file reference (using pandas)
-    table_key - str
-        Key in data to load values directly from an external file reference (using pandas)
-    """
-    if values_key in data:
-        # values are given as an array
-        values = np.array(data.pop(values_key), np.float64)
-    elif url_key in data or table_key in data:
-        df = load_dataframe(model, data)
-        try:
-            # If it's a DataFrame we coerce to a numpy array
-            values = df.values
-        except AttributeError:
-            values = df
-        values = np.squeeze(values.astype(np.float64))
-    else:
-        # Try to get some useful information about the parameter for the error message
-        name = data.get('name', None)
-        ptype = data.get('type', None)
-        raise ValueError("Parameter ('{name}' of type '{ptype}' is missing a valid key to load its values. "
-                         "Please provide either a '{}', '{}' or '{}' entry.".format(values_key, url_key, table_key, name=name, ptype=ptype))
-    return values
+import os
+import numpy as np
+cimport numpy as np
+from scipy.interpolate import Rbf
+import pandas
+import json
+import calendar
+from libc.math cimport cos, M_PI
+from libc.limits cimport INT_MIN, INT_MAX
+from pywr.h5tools import H5Store
+from pywr.hashes import check_hash
+from .._core cimport is_leap_year
+from ..dataframe_tools import align_and_resample_dataframe, load_dataframe, read_dataframe
+import warnings
+
+
+parameter_registry = {}
+
+
+class UnutilisedDataWarning(Warning):
+    """ Simple warning to indicate that not all data has been used. """
+    pass
+
+class TypeNotFoundError(KeyError):
+    """
+      Key Error, specifically designed for when the 'type' key is not found
+      in a dataset. This takes the data value and outputs a summary of it, to
+      aid in debugging.
+    """
+    def __init__(self, data):
+        #Try to print out some sensible amount of data without overloading
+        #the terminal with data. 1000 chars should be enough to get an idea
+        #of what the data looks like. If more than 1000 chars, do a pandas-style
+        #summary using ...
+        data_str = json.dumps(data)
+        if len(data_str) > 1000:
+            data_summary = f"{data_str[:500]} ... {data_str[-500:]}"
+        else:
+            data_summary = data_str
+
+        return f"Unable to find key 'type' in {data_summary}"
+
+cdef class Parameter(Component):
+    def __init__(self, *args, is_variable=False, **kwargs):
+        super(Parameter, self).__init__(*args, **kwargs)
+        self.is_variable = is_variable
+        self.double_size = 0
+        self.integer_size = 0
+
+    @classmethod
+    def register(cls):
+        parameter_registry[cls.__name__.lower()] = cls
+
+    @classmethod
+    def unregister(cls):
+        del(parameter_registry[cls.__name__.lower()])
+
+    cpdef setup(self):
+        super(Parameter, self).setup()
+        cdef int num_comb
+        if self.model.scenarios.combinations:
+            num_comb = len(self.model.scenarios.combinations)
+        else:
+            num_comb = 1
+        self.__values = np.empty([num_comb], np.float64)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        raise NotImplementedError("Parameter must be subclassed")
+
+    cdef calc_values(self, Timestep timestep):
+        # default implementation calls Parameter.value in loop
+        cdef ScenarioIndex scenario_index
+        cdef ScenarioCollection scenario_collection = self.model.scenarios
+        for scenario_index in scenario_collection.combinations:
+            self.__values[<int>(scenario_index.global_id)] = self.value(timestep, scenario_index)
+
+    cpdef double get_value(self, ScenarioIndex scenario_index):
+        return self.__values[<int>(scenario_index.global_id)]
+
+    cpdef double[:] get_all_values(self):
+        return self.__values
+
+    cpdef set_double_variables(self, double[:] values):
+        raise NotImplementedError()
+
+    cpdef double[:] get_double_variables(self):
+        raise NotImplementedError()
+
+    cpdef double[:] get_double_lower_bounds(self):
+        raise NotImplementedError()
+
+    cpdef double[:] get_double_upper_bounds(self):
+        raise NotImplementedError()
+
+    cpdef set_integer_variables(self, int[:] values):
+        raise NotImplementedError()
+
+    cpdef int[:] get_integer_variables(self):
+        raise NotImplementedError()
+
+    cpdef int[:] get_integer_lower_bounds(self):
+        raise NotImplementedError()
+
+    cpdef int[:] get_integer_upper_bounds(self):
+        raise NotImplementedError()
+
+    property size:
+        def __get__(self):
+            warnings.warn("Use of the `size` property on Parameters has been deprecated."
+                          "Please use either `double_size` or `integer_size` instead.", DeprecationWarning)
+            return self.double_size
+
+        def __set__(self, value):
+            warnings.warn("Use of the `size` property on Parameters has been deprecated."
+                          "Please use either `double_size` or `integer_size` instead.", DeprecationWarning)
+            self.double_size = value
+
+    @classmethod
+    def load(cls, model, data):
+        # If a scenario is given don't pass this to the load values methods
+        scenario = data.pop('scenario', None)
+
+        values = load_parameter_values(model, data)
+        data.pop("values", None)
+        data.pop("url", None)
+        name = data.pop("name", None)
+        comment = data.pop("comment", None)
+
+        if scenario is not None:
+            scenario = model.scenarios[scenario]
+            # Only pass scenario object if one provided; most Parameter subclasses
+            # do not accept a scenario argument.
+            return cls(model, scenario=scenario, values=values, name=name, comment=None, **data)
+        else:
+            return cls(model, values=values, name=name, comment=None, **data)
+Parameter.register()
+
+
+cdef class ConstantParameter(Parameter):
+    def __init__(self, model, value, lower_bounds=0.0, upper_bounds=np.inf, scale=1.0, offset=0.0, **kwargs):
+        super(ConstantParameter, self).__init__(model, **kwargs)
+        self._value = value
+        self.scale = scale
+        self.offset = offset
+        self.double_size = 1
+        self.integer_size = 0
+        self._lower_bounds = np.ones(self.double_size) * lower_bounds
+        self._upper_bounds = np.ones(self.double_size) * upper_bounds
+
+    cdef calc_values(self, Timestep timestep):
+        # constant parameter can just set the entire array to one value
+        self.__values[...] = self.offset + self._value * self.scale
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._value
+
+    cpdef set_double_variables(self, double[:] values):
+        self._value = values[0]
+
+    cpdef double[:] get_double_variables(self):
+        return np.array([self._value, ], dtype=np.float64)
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return self._lower_bounds
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return self._upper_bounds
+
+    @classmethod
+    def load(cls, model, data):
+        if "value" in data:
+            value = data.pop("value")
+        else:
+            value = load_parameter_values(model, data)
+        parameter = cls(model, value, **data)
+        return parameter
+
+ConstantParameter.register()
+
+
+cdef class DataFrameParameter(Parameter):
+    """Timeseries parameter with automatic alignment and resampling
+
+    Parameters
+    ----------
+    model : pywr.model.Model
+    dataframe : pandas.DataFrame or pandas.Series
+    scenario: pywr._core.Scenario (optional)
+    """
+    def __init__(self, model, dataframe, scenario=None, **kwargs):
+        super(DataFrameParameter, self).__init__(model, *kwargs)
+        self.dataframe = dataframe
+        self.scenario = scenario
+
+    cpdef setup(self):
+        super(DataFrameParameter, self).setup()
+        # align and resample the dataframe
+        dataframe_resampled = align_and_resample_dataframe(self.dataframe, self.model.timestepper.datetime_index)
+        if dataframe_resampled.ndim == 1:
+            dataframe_resampled = pandas.DataFrame(dataframe_resampled)
+        # dataframe should now have the correct number of timesteps for the model
+        if len(dataframe_resampled) != len(self.model.timestepper):
+            raise ValueError("Aligning DataFrame failed with a different length compared with model timesteps.")
+        # check that if a 2D DataFrame is given that we also have a scenario assigned with it
+        if dataframe_resampled.ndim == 2 and dataframe_resampled.shape[1] > 1:
+            if self.scenario is None:
+                raise ValueError("Scenario must be given for a DataFrame input with multiple columns.")
+            if self.scenario.size != dataframe_resampled.shape[1]:
+                raise ValueError("Scenario size ({}) is different to the number of columns ({}) "
+                                 "in the DataFrame input.".format(self.scenario.size, dataframe_resampled.shape[1]))
+        self._values = dataframe_resampled.values.astype(np.float64)
+        if self.scenario is not None:
+            self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
+
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        cdef double value
+        if self.scenario is not None:
+            value = self._values[<int>(timestep.index), <int>(scenario_index._indices[self._scenario_index])]
+        else:
+            value = self._values[<int>(timestep.index), 0]
+        return value
+
+    @classmethod
+    def load(cls, model, data):
+        scenario = data.pop('scenario', None)
+        if scenario is not None:
+            scenario = model.scenarios[scenario]
+        df = load_dataframe(model, data)
+        return cls(model, df, scenario=scenario, **data)
+
+DataFrameParameter.register()
+
+cdef class ArrayIndexedParameter(Parameter):
+    """Time varying parameter using an array and Timestep.index
+
+    The values in this parameter are constant across all scenarios.
+    """
+    def __init__(self, model, values, *args, **kwargs):
+        super(ArrayIndexedParameter, self).__init__(model, *args, **kwargs)
+        self.values = np.asarray(values, dtype=np.float64)
+
+    cdef calc_values(self, Timestep ts):
+        # constant parameter can just set the entire array to one value
+        self.__values[...] = self.values[ts.index]
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        """Returns the value of the parameter at a given timestep
+        """
+        return self.values[ts.index]
+ArrayIndexedParameter.register()
+
+
+cdef class ArrayIndexedScenarioParameter(Parameter):
+    """A Scenario varying Parameter
+
+    The values in this parameter are vary in time based on index and vary within a single Scenario.
+    """
+    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
+        """
+        values should be an iterable that is the same length as scenario.size
+        """
+        super(ArrayIndexedScenarioParameter, self).__init__(model, *args, **kwargs)
+        cdef int i
+        values = np.asarray(values, dtype=np.float64)
+        if values.ndim != 2:
+            raise ValueError("Values must be two dimensional.")
+        if scenario._size != values.shape[1]:
+            raise ValueError("The size of the second dimension of values must equal the size of the scenario.")
+        self.values = values
+        self._scenario = scenario
+
+    cpdef setup(self):
+        super(ArrayIndexedScenarioParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        # This is a bit confusing.
+        # scenario_indices contains the current scenario number for all
+        # the Scenario objects in the model run. We have cached the
+        # position of self._scenario in self._scenario_index to lookup the
+        # correct number to use in this instance.
+        return self.values[ts.index, scenario_index._indices[self._scenario_index]]
+
+ArrayIndexedScenarioParameter.register()
+
+
+cdef class TablesArrayParameter(IndexParameter):
+    def __init__(self, model, h5file, node, where='/', scenario=None, **kwargs):
+        """
+        This Parameter reads array data from a PyTables HDF database.
+
+        The parameter reads data using the PyTables array interface and therefore
+        does not require loading the entire dataset in to memory. This is useful
+        for large model runs.
+
+        Parameters
+        ----------
+        h5file : tables.File or filename
+            The tables file handle or filename to attach the CArray objects to. If a
+            filename is given the object will open and close the file handles.
+        node : string
+            Name of the node in the tables database to read data from
+        where : string
+            Path to read the node from.
+        scenario : Scenario
+            Scenario to use as the second index in the array.
+        """
+        super(TablesArrayParameter, self).__init__(model, **kwargs)
+
+        self.h5file = h5file
+        self.h5store = None
+        self.node = node
+        self.where = where
+        self.scenario = scenario
+
+        # Private attributes, initialised during setup()
+        self._values_dbl = None  # Stores the loaded data if float
+        self._values_int = None  # Stores the loaded data if integer
+        # If a scenario is present this is the index in the model list of scenarios
+        self._scenario_index = -1
+        self._scenario_ids = None  # Lookup of scenario index to the loaded data index
+
+    cpdef setup(self):
+        cdef Py_ssize_t n, i
+
+        super(TablesArrayParameter, self).setup()
+        self._scenario_index = -1
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        if self.scenario is not None:
+            self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
+
+        self.h5store = H5Store(self.h5file, None, "r")
+        node = self.h5store.file.get_node(self.where, self.node)
+        if not node.dtype in (np.float32, np.float64, np.int8, np.int16, np.int32):
+            raise TypeError("Unexpected dtype in array: {}".format(node.dtype))
+
+        # check the shape of the data is valid
+        if self.scenario is not None:
+            if node.shape[1] < self.scenario.size:
+                raise IndexError('The length of the second dimension ({:d}) of the tables node ({}:{}) '
+                                 'should be the same as the size of the specified Scenario ({:d}).'
+                                 .format(node.shape[1], node._v_file.filename, node._v_pathname, self.scenario.size))
+            elif node.shape[1] > self.scenario.size:
+                warnings.warn('The length of the second dimension ({:d}) of the tables node ({}:{}) '
+                              'is greater than the size of the specified Scenario ({:d}). '
+                              'Not all data is being used!'.format(node.shape[1], node._v_file.filename, node._v_pathname, self.scenario.size),
+                              UnutilisedDataWarning)
+        if node.shape[0] < len(self.model.timestepper):
+            raise IndexError('The length of the first dimension ({:d}) of the tables node ({}:{}) '
+                             'should be equal to or greater than the number of timesteps.'
+                             .format(node.shape[0], node._v_file.filename, node._v_pathname, len(self.model.timestepper)))
+        elif node.shape[0] > len(self.model.timestepper):
+            warnings.warn('The length of the first dimension ({:d}) of the tables node ({}:{}) '
+                          'is greater than the number of timesteps. Not all data is being used!'
+                          .format(node.shape[0], node._v_file.filename, node._v_pathname, len(self.model.timestepper)),
+                          UnutilisedDataWarning)
+
+        # detect data type and read into memoryview
+        self._values_dbl = None
+        self._values_int = None
+        if self.scenario:
+            # if possible, only load the data required
+            scenario_indices = None
+            # Default to index that is just out of bounds to cause IndexError if something goes wrong
+            self._scenario_ids = np.ones(self.scenario.size, dtype=np.int32) * self.scenario.size
+
+            # Calculate the scenario indices to load dependning on how scenario combinations are defined.
+            if self.model.scenarios.user_combinations:
+                scenario_indices = set()
+                for user_combination in self.model.scenarios.user_combinations:
+                    scenario_indices.add(user_combination[self._scenario_index])
+                scenario_indices = sorted(list(scenario_indices))
+            elif self.scenario.slice:
+                scenario_indices = range(*self.scenario.slice.indices(self.scenario.slice.stop))
+            else:
+                # scenario is defined, but all data required
+                self._scenario_ids = None
+
+            if scenario_indices is not None:
+                # Now load only the required data
+                for n, i in enumerate(scenario_indices):
+                    self._scenario_ids[i] = n
+
+                if node.dtype in (np.float32, np.float64):
+                    self._values_dbl = node[:len(self.model.timestepper), scenario_indices].astype(np.float64)
+                else:
+                    self._values_int = node[:len(self.model.timestepper), scenario_indices].astype(np.int32)
+
+        if node.dtype in (np.float32, np.float64):
+            if self._values_dbl is None:
+                self._values_dbl = node.read().astype(np.float64)
+            # negative values are often erroneous
+            if np.min(self._values_dbl) < 0.0:
+                warnings.warn('Negative values in input file "{}" from node: {}'.format(self.h5file, self.node))
+            if not np.all(np.isfinite(self._values_dbl)):
+                raise ValueError('Non-finite values in input file "{}" from node: {}'.format(self.h5file, self.node))
+        else:
+            if self._values_int is None:
+                self._values_int = node.read().astype(np.int32)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef Py_ssize_t i = ts.index
+        cdef Py_ssize_t j
+        if self._values_dbl is None:
+            return float(self.index(ts, scenario_index))
+        # Support 1D and 2D indexing when scenario is or is not given.
+        if self._scenario_index == -1:
+            return self._values_dbl[i, 0]
+        else:
+            j = scenario_index._indices[self._scenario_index]
+            if self._scenario_ids is not None:
+                j = self._scenario_ids[j]
+            return self._values_dbl[i, j]
+
+    cpdef int index(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef Py_ssize_t i = ts.index
+        cdef Py_ssize_t j
+        if self._values_int is None:
+            return int(self.value(ts, scenario_index))
+        # Support 1D and 2D indexing when scenario is or is not given.
+        if self._scenario_index == -1:
+            return self._values_int[i, 0]
+        else:
+            j = scenario_index._indices[self._scenario_index]
+            if self._scenario_ids is not None:
+                j = self._scenario_ids[j]
+            return self._values_int[i, j]
+
+    cpdef finish(self):
+        self.h5store = None
+
+    @classmethod
+    def load(cls, model, data):
+        scenario = data.pop('scenario', None)
+        if scenario is not None:
+            scenario = model.scenarios[scenario]
+
+        url = data.pop('url')
+        if not os.path.isabs(url) and model.path is not None:
+            url = os.path.join(model.path, url)
+        node = data.pop('node')
+        where = data.pop('where', '/')
+
+        # Check hashes if given before reading the data
+        checksums = data.pop('checksum', {})
+        for algo, hash in checksums.items():
+            check_hash(url, hash, algorithm=algo)
+
+        return cls(model, url, node, where=where, scenario=scenario)
+TablesArrayParameter.register()
+
+
+cdef class ConstantScenarioParameter(Parameter):
+    """A Scenario varying Parameter
+
+    The values in this parameter are constant in time, but vary within a single Scenario.
+    """
+    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
+        """
+        values should be an iterable that is the same length as scenario.size
+        """
+        super(ConstantScenarioParameter, self).__init__(model, *args, **kwargs)
+        cdef int i
+        if scenario._size != len(values):
+            raise ValueError("The number of values must equal the size of the scenario.")
+        self._values = np.empty(scenario._size)
+        for i in range(scenario._size):
+            self._values[i] = values[i]
+        self._scenario = scenario
+
+    cpdef setup(self):
+        super(ConstantScenarioParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        # This is a bit confusing.
+        # scenario_indices contains the current scenario number for all
+        # the Scenario objects in the model run. We have cached the
+        # position of self._scenario in self._scenario_index to lookup the
+        # correct number to use in this instance.
+        return self._values[scenario_index._indices[self._scenario_index]]
+ConstantScenarioParameter.register()
+
+
+cdef class ArrayIndexedScenarioMonthlyFactorsParameter(Parameter):
+    """Time varying parameter using an array and Timestep.index with
+    multiplicative factors per Scenario
+    """
+    def __init__(self, model, Scenario scenario, values, factors, *args, **kwargs):
+        """
+        values is the baseline timeseries data that is perturbed by a factor. The
+        factor is taken from factors which is shape (scenario.size, 12). Therefore
+        factors vary with the individual scenarios in scenario and month.
+        """
+        super(ArrayIndexedScenarioMonthlyFactorsParameter, self).__init__(model, *args, **kwargs)
+
+        values = np.asarray(values, dtype=np.float64)
+        factors = np.asarray(factors, dtype=np.float64)
+        if factors.ndim != 2:
+            raise ValueError("Factors must be two dimensional.")
+
+        if factors.shape[0] != scenario._size:
+            raise ValueError("First dimension of factors must be the same size as scenario.")
+        if factors.shape[1] != 12:
+            raise ValueError("Second dimension of factors must be 12.")
+        self._scenario = scenario
+        self._values = values
+        self._factors = factors
+
+    cpdef setup(self):
+        super(ArrayIndexedScenarioMonthlyFactorsParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        # This is a bit confusing.
+        # scenario_indices contains the current scenario number for all
+        # the Scenario objects in the model run. We have cached the
+        # position of self._scenario in self._scenario_index to lookup the
+        # correct number to use in this instance.
+        cdef int imth = ts.month-1
+        cdef int i = scenario_index._indices[self._scenario_index]
+        return self._values[ts.index]*self._factors[i, imth]
+
+    @classmethod
+    def load(cls, model, data):
+        scenario = data.pop("scenario", None)
+        if scenario is not None:
+            scenario = model.scenarios[scenario]
+
+        if isinstance(data["values"], list):
+            values = np.asarray(data["values"], np.float64)
+        elif isinstance(data["values"], dict):
+            values = load_parameter_values(model, data["values"])
+        else:
+            raise TypeError("Unexpected type for \"values\" in {}".format(cls.__name__))
+
+        if isinstance(data["factors"], list):
+            factors = np.asarray(data["factors"], np.float64)
+        elif isinstance(data["factors"], dict):
+            factors = load_parameter_values(model, data["factors"])
+        else:
+            raise TypeError("Unexpected type for \"factors\" in {}".format(cls.__name__))
+
+        return cls(model, scenario, values, factors)
+
+ArrayIndexedScenarioMonthlyFactorsParameter.register()
+
+
+cdef class DailyProfileParameter(Parameter):
+    """ An annual profile consisting of daily values.
+
+    This parameter provides a repeating annual profile with a daily resolution. A total of 366 values
+    must be provided. These values are coerced to a `numpy.array` internally.
+
+    Parameters
+    ----------
+    values : iterable, array
+        The 366 values that represent the daily profile.
+
+    """
+    def __init__(self, model, values, *args, **kwargs):
+        super(DailyProfileParameter, self).__init__(model, *args, **kwargs)
+        v = np.squeeze(np.array(values))
+        if v.ndim != 1:
+            raise ValueError("values must be 1-dimensional.")
+        if len(values) != 366:
+            raise ValueError("366 values must be given for a daily profile.")
+        self._values = v
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._values[ts.dayofyear_index]
+DailyProfileParameter.register()
+
+cdef class WeeklyProfileParameter(Parameter):
+    """Weekly profile (52-week year)
+
+    The last week of the year will have more than 7 days, as 365 / 7 is not whole.
+    """
+    def __init__(self, model, values, *args, **kwargs):
+        super(WeeklyProfileParameter, self).__init__(model, *args, **kwargs)
+        v = np.squeeze(np.array(values))
+        if v.ndim != 1:
+            raise ValueError("values must be 1-dimensional.")
+        if len(values) == 53:
+            values = values[:52]
+            warnings.warn("Truncating 53 week profile to 52 weeks.")
+        if len(values) != 52:
+            raise ValueError("52 values must be given for a weekly profile.")
+        self._values = v
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._values[ts.week_index]
+WeeklyProfileParameter.register()
+
+
+cdef class MonthlyProfileParameter(Parameter):
+    """Parameter which provides a monthly profile.
+
+    The monthly profile returns a different value based on the month of the current
+    time-step. By default this creates a piecewise profile with a step change at the
+    beginning of each month. An optional `interp_day` keyword can instead create a
+    linearly interpolated daily profile assuming the given values correspond to either
+    the first or last day of the month.
+
+    Parameters
+    ----------
+    values : iterable, array
+        The 12 values that represent the monthly profile.
+    lower_bounds : float (default=0.0)
+        The lower bounds of the monthly profile values when used during optimisation.
+    upper_bounds : float (default=np.inf)
+        The upper bounds of the monthly profile values when used during optimisation.
+    inter_day : str or None (default=None)
+        If `interp_day` is None then no interpolation is undertaken, and the parameter
+         returns values representing a piecewise monthly profile. Otherwise `interp_day`
+         must be a string of either "first" or "last" representing which day of the month
+         each of the 12 values represents. The parameter then returns linearly
+         interpolated values between the given day of the month.
+
+
+    See also
+    --------
+    ScenarioMonthlyProfileParameter
+    ArrayIndexedScenarioMonthlyFactorsParameter
+    """
+    def __init__(self, model, values, lower_bounds=0.0, upper_bounds=np.inf, interp_day=None, **kwargs):
+        super(MonthlyProfileParameter, self).__init__(model, **kwargs)
+        self.double_size = 12
+        self.integer_size = 0
+        if len(values) != self.double_size:
+            raise ValueError("12 values must be given for a monthly profile.")
+        self._values = np.array(values)
+        self.interp_day = interp_day
+        self._lower_bounds = np.ones(self.double_size)*lower_bounds
+        self._upper_bounds = np.ones(self.double_size)*upper_bounds
+
+    cpdef reset(self):
+        Parameter.reset(self)
+        # The interpolated profile is recalculated during reset so that
+        # it will update when the _values array is updated via `set_double_variables`
+        # and the model is rerun. I.e. during optimisation (where setup is not redone).
+        if self.interp_day is not None:
+            self._interpolate()
+
+    cpdef _interpolate(self):
+
+        # Create an array to save the daily profile in.
+        self._interp_values = np.zeros(366)
+        cdef int i = 0
+        cdef int mth
+
+        # Create interpolation knots depending on values
+        if self.interp_day == 'first':
+            x = [1]  # First month
+            y = []
+            for mth in range(1, 13):
+                x.append(x[-1] + calendar.monthrange(2015, mth)[1])
+                y.append(self._values[mth-1])
+            y.append(self._values[0])
+        elif self.interp_day == 'last':
+            x = [0]  # End of previous year
+            y = [self._values[11]]  # Use value from December
+            for mth in range(1, 13):
+                x.append(x[-1] + calendar.monthrange(2015, mth)[1])
+                y.append(self._values[mth-1])
+        else:
+            raise ValueError(f'Interpolation day "{self.interp_day}" not supported.')
+
+        # Do the interpolation
+        values = np.interp(np.arange(365) + 1, x, y)
+        # Make the daily profile of 366 values repeating the same value for 28th & 29th Feb.
+        for i in range(365):
+            if i < 58:
+                self._interp_values[i] = values[i]
+            elif i == 58:
+                self._interp_values[i] = values[i]
+                self._interp_values[i+1] = values[i]
+            elif i > 58:
+                self._interp_values[i+1] = values[i]
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        if self.interp_day is None:
+            return self._values[ts.month-1]
+        else:
+            return self._interp_values[ts.dayofyear_index]
+
+    cpdef set_double_variables(self, double[:] values):
+        self._values[...] = values
+
+    cpdef double[:] get_double_variables(self):
+        # Make sure we return a copy of the data instead of a view.
+        return np.array(self._values).copy()
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return self._lower_bounds
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return self._upper_bounds
+MonthlyProfileParameter.register()
+
+
+cdef class ScenarioMonthlyProfileParameter(Parameter):
+    """ Parameter that provides a monthly profile per scenario
+
+    Behaviour is the same as `MonthlyProfileParameter` except a different
+    profile is returned for each ensemble in a given scenario.
+
+    See also
+    --------
+    MonthlyProfileParameter
+    ArrayIndexedScenarioMonthlyFactorsParameter
+    """
+    def __init__(self, model, Scenario scenario, values, **kwargs):
+        super(ScenarioMonthlyProfileParameter, self).__init__(model, **kwargs)
+
+        if values.ndim != 2:
+            raise ValueError("Factors must be two dimensional.")
+
+        if scenario._size != values.shape[0]:
+            raise ValueError("First dimension of factors must be the same size as scenario.")
+        if values.shape[1] != 12:
+            raise ValueError("Second dimension of factors must be 12.")
+        self._scenario = scenario
+        self._values = np.array(values)
+
+    cpdef setup(self):
+        super(ScenarioMonthlyProfileParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._values[scenario_index._indices[self._scenario_index], ts.month-1]
+
+ScenarioMonthlyProfileParameter.register()
+
+cdef class ScenarioWeeklyProfileParameter(Parameter):
+    """Parameter that provides a weekly profile per scenario
+
+    This parameter provides a repeating annual profile with a weekly resolution. A
+    different profile is returned for each member of a given scenario
+
+    Parameters
+    ----------
+    scenario: Scenario
+        Scenario object over which different profiles should be provided.
+    values : iterable, array
+        Length of 1st dimension should equal the number of members in the scenario object
+        and the length of the second dimension should be 52
+
+    """
+    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
+        super().__init__(model, *args, **kwargs)
+        values = np.array(values)
+        if values.ndim != 2:
+            raise ValueError("Factors must be two dimensional.")
+        if scenario._size != values.shape[0]:
+            raise ValueError("First dimension of factors must be the same size as scenario.")
+        if values.shape[1] != 52:
+            raise ValueError("52 values must be given for a weekly profile.")
+        self._values = values
+        self._scenario = scenario
+
+    cpdef setup(self):
+        super(ScenarioWeeklyProfileParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._values[scenario_index._indices[self._scenario_index], ts.week_index]
+
+ScenarioWeeklyProfileParameter.register()
+
+cdef class ScenarioDailyProfileParameter(Parameter):
+    """Parameter which provides a daily profile per scenario.
+
+    This parameter provides a repeating annual profile with a daily resolution. A
+    different profile is returned for each member of a given scenario
+
+    Parameters
+    ----------
+    scenario: Scenario
+        Scenario object over which different profiles should be provided
+    values : iterable, array
+        Length of 1st dimension should equal the number of members in the scenario object
+        and the length of the second dimension should be 366
+
+    """
+    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
+        super().__init__(model, *args, **kwargs)
+        values = np.array(values)
+        if values.ndim != 2:
+            raise ValueError("Factors must be two dimensional.")
+        if scenario._size != values.shape[0]:
+            raise ValueError("First dimension of factors must be the same size as scenario.")
+        if values.shape[1] != 366:
+            raise ValueError("366 values must be given for a daily profile.")
+        self._values = values
+        self._scenario = scenario
+
+    cpdef setup(self):
+        super(ScenarioDailyProfileParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i = ts.dayofyear_index
+        return self._values[scenario_index._indices[self._scenario_index], i]
+
+ScenarioDailyProfileParameter.register()
+
+
+cdef class UniformDrawdownProfileParameter(Parameter):
+    """Parameter which provides a uniformly reducing value from one to zero.
+
+     This parameter is intended to be used with an `AnnualVirtualStorage` node to provide a profile
+     that represents perfect average utilisation of the annual volume. It returns a value of 1 on the
+     reset day, and subsequently reduces by 1/366 every day afterward.
+
+    Parameters
+    ----------
+    reset_day: int
+        The day of the month (1-31) to reset the volume to the initial value.
+    reset_month: int
+        The month of the year (1-12) to reset the volume to the initial value.
+
+    See also
+    --------
+    AnnualVirtualStorage
+    """
+    def __init__(self, model, reset_day=1, reset_month=1, **kwargs):
+        super().__init__(model, **kwargs)
+        self.reset_day = reset_day
+        self.reset_month = reset_month
+
+    cpdef reset(self):
+        super(UniformDrawdownProfileParameter, self).reset()
+        # Reset day of the year based on a leap year.
+        # Note that this is zero-based
+        self._reset_idoy = pandas.Period(year=2016, month=self.reset_month, day=self.reset_day, freq='D').dayofyear - 1
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int current_idoy = ts.dayofyear_index
+        cdef int total_days_in_period
+        cdef int days_into_period
+        cdef int year = ts.year
+
+        days_into_period = current_idoy - self._reset_idoy
+        if days_into_period < 0:
+            # We're not past the reset day yet; use the previous year
+            year -= 1
+
+        if self._reset_idoy > 59:
+            # Reset occurs after February therefore next year's February might be a leap year?
+            year += 1
+
+        # Determine the number of days in the period based on whether there is a leap year or not in the current period
+        if is_leap_year(year):
+            total_days_in_period = 366
+        else:
+            total_days_in_period = 365
+
+        # Now determine number of days we're into the period if it has wrapped around to a new year
+        if days_into_period < 0:
+            days_into_period += 366
+            # Need to adjust for post 29th Feb in non-leap years.
+            # Recall `current_idoy` was incremented by 1 if it is a non-leap already (hence comparison to 59)
+            if not is_leap_year(ts.year) and current_idoy > 59:
+                days_into_period -= 1
+
+        return 1.0 - days_into_period / total_days_in_period
+
+    @classmethod
+    def load(cls, model, data):
+        return cls(model, **data)
+UniformDrawdownProfileParameter.register()
+
+
+cdef class RbfProfileParameter(Parameter):
+    """Parameter which interpolates a daily profile using a radial basis function (RBF).
+
+    The daily profile is computed during model `reset` using a radial basis function with
+    day-of-year as the independent variables. The days of the year are defined by the user
+    alongside the values to use on each of those days for the interpolation. The first
+    day of the years should always be one, and its value is repeated as the 366th value.
+    In addition the second and penultimate values are mirrored to encourage a consistent
+    gradient to appear across the boundary. The RBF calculations are undertaken using
+    the `scipy.interpolate.Rbf` object, please refer to Scipy's documentation for more
+    information.
+
+    Parameters
+    ----------
+    days_of_year : iterable, integer
+        The days of the year at which the interpolation values are defined. The first
+        value should be one.
+    values : iterable, float
+        Values to use for interpolation corresponding to the `days_of_year`.
+    lower_bounds : float (default=0.0)
+        The lower bounds of the values when used during optimisation.
+    upper_bounds : float (default=np.inf)
+        The upper bounds of the values when used during optimisation.
+    variable_days_of_year_range : int (default=0)
+        The maximum bounds (positive or negative) for the days of year during optimisation. A non-zero value
+        will cause the days of the year values to be exposed as integer variables (except the first value which
+        remains at day 1). This value is bounds on those variables as maximum shift from the given `days_of_year`.
+    min_value, max_value : float
+        Optionally cap the interpolated daily profile to a minimum and/or maximum value. The default values
+        are negative and positive infinity for minimum and maximum respectively.
+    rbf_kwargs: Optional, dict
+        Optional dictionary of keyword arguments to base to the Rbf object.
+
+    """
+    def __init__(self, model, days_of_year, values, lower_bounds=0.0, upper_bounds=np.inf, rbf_kwargs=None,
+                 variable_days_of_year_range=0, min_value=-np.inf, max_value=np.inf, **kwargs):
+        super(RbfProfileParameter, self).__init__(model, **kwargs)
+
+        if len(days_of_year) != len(values):
+            raise ValueError(f"The length of values ({len(values)}) must equal the length of "
+                             f"`days_of_year` ({len(days_of_year)}).")
+
+        self.variable_days_of_year_range = variable_days_of_year_range
+        self.double_size = len(values)
+        self._values = np.array(values, dtype=np.float64)
+        self.days_of_year = days_of_year
+        self.min_value = min_value
+        self.max_value = max_value
+        self._lower_bounds = np.ones(self.double_size)*lower_bounds
+        self._upper_bounds = np.ones(self.double_size)*upper_bounds
+
+        if self.variable_days_of_year_range > 0:
+            if np.any(np.diff(self.days_of_year) <= 2*self.variable_days_of_year_range):
+                raise ValueError(f"The days of the year are too close together for the given "
+                                 f"`variable_days_of_year_range`. This could cause the optimised days"
+                                 f"of the year to overlap and become out of order.  Either increase the"
+                                 f"spacing of the days of the year or reduce `variable_days_of_year_range` to"
+                                 f"less than half the closest distance between the days of the year.")
+            self.integer_size = len(values) - 1
+            self._doy_lower_bounds = np.array([d - self.variable_days_of_year_range
+                                               for d in self.days_of_year[1:]], dtype=np.int32)
+            self._doy_upper_bounds = np.array([d + self.variable_days_of_year_range
+                                               for d in self.days_of_year[1:]], dtype=np.int32)
+        else:
+            self.integer_size = 0
+
+        self.rbf_kwargs = rbf_kwargs if rbf_kwargs is not None else {}
+
+    property days_of_year:
+        def __get__(self):
+            return np.array(self._days_of_year)
+        def __set__(self, values):
+            values = np.array(values, dtype=np.int32)
+            if values[0] != 1:
+                raise ValueError('The first day of the years must be 1.')
+            if len(values) < 3:
+                raise ValueError('At least 3 days of the year are required.')
+            if np.any(np.diff(values) <= 0):
+                raise ValueError('The days of the year should be strictly monotonically increasing.')
+            if np.any(0 > values > 365):
+                raise ValueError('Days of the years should be between 1 and 365 inclusive.')
+            self._days_of_year = values
+
+    cpdef reset(self):
+        Parameter.reset(self)
+        # The interpolated profile is recalculated during reset so that
+        # it will update when the _values array is updated via `set_double_variables`
+        # and the model is rerun. I.e. during optimisation (where setup is not redone).
+        self._interpolate()
+
+    cpdef _interpolate(self):
+        cdef int i
+        cdef double[:] values
+        cdef double v
+
+        days_of_year = list(self._days_of_year)
+        # Append day 365 to the list and mirror the penultimate and second DOY at the start and end
+        # of the list respectively. This helps ensure the gradient is roughly the same across the boundary
+        # between days 365 and 0.
+        days_of_year = [days_of_year[-1]-365] + list(days_of_year) + [366, 366+days_of_year[1]-1]
+        # Create the corresponding y values including the mirrored entries
+        y = list(self._values)
+        y = [y[-1]] + y + [y[0], y[1]]
+        rbfi = Rbf(days_of_year, y)
+
+        # Do the interpolation
+        values = rbfi(np.arange(365) + 1)
+
+        # Create an array to save the daily profile in.
+        self._interp_values = np.zeros(366)
+        # Make the daily profile of 366 values repeating the same value for 28th & 29th Feb.
+        for i in range(365):
+            v = max(min(values[i], self.max_value), self.min_value)
+            if i < 58:
+                self._interp_values[i] = v
+            elif i == 58:
+                self._interp_values[i] = v
+                self._interp_values[i+1] = v
+            elif i > 58:
+                self._interp_values[i+1] = v
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i = ts.dayofyear_index
+        return self._interp_values[i]
+
+    # Double variables are for the known interpolation values (y-axis)
+    cpdef set_double_variables(self, double[:] values):
+        self._values[...] = values
+
+    cpdef double[:] get_double_variables(self):
+        # Make sure we return a copy of the data instead of a view.
+        return np.array(self._values).copy()
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return self._lower_bounds
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return self._upper_bounds
+
+    # Integer variables are for the days of the year positions (if optimised)
+    cpdef set_integer_variables(self, int[:] values):
+        self.days_of_year = [1] + np.array(values).tolist()
+
+    cpdef int[:] get_integer_variables(self):
+        return np.array(self.days_of_year[1:], dtype=np.int32)
+
+    cpdef int[:] get_integer_lower_bounds(self):
+        return self._doy_lower_bounds
+
+    cpdef int[:] get_integer_upper_bounds(self):
+        return self._doy_upper_bounds
+
+    @classmethod
+    def load(cls, model, data):
+        return cls(model, **data)
+RbfProfileParameter.register()
+
+
+cdef class IndexParameter(Parameter):
+    """Base parameter providing an `index` method
+
+    See also
+    --------
+    IndexedArrayParameter
+    ControlCurveIndexParameter
+    """
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns the current index as a float"""
+        # return index as a float
+        return float(self.get_index(scenario_index))
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns the current index"""
+        # return index as an integer
+        return 0
+
+    cpdef setup(self):
+        super(IndexParameter, self).setup()
+        cdef int num_comb
+        if self.model.scenarios.combinations:
+            num_comb = len(self.model.scenarios.combinations)
+        else:
+            num_comb = 1
+        self.__indices = np.empty([num_comb], np.int32)
+
+    cdef calc_values(self, Timestep timestep):
+        cdef ScenarioIndex scenario_index
+        cdef ScenarioCollection scenario_collection = self.model.scenarios
+        for scenario_index in scenario_collection.combinations:
+            self.__indices[<int>(scenario_index.global_id)] = self.index(timestep, scenario_index)
+            self.__values[<int>(scenario_index.global_id)] = self.value(timestep, scenario_index)
+
+    cpdef int get_index(self, ScenarioIndex scenario_index):
+        return self.__indices[<int>(scenario_index.global_id)]
+
+    cpdef int[:] get_all_indices(self):
+        return self.__indices
+IndexParameter.register()
+
+
+cdef class ConstantScenarioIndexParameter(IndexParameter):
+    """A Scenario varying IndexParameter
+
+    The values in this parameter are constant in time, but vary within a single Scenario.
+    """
+    def __init__(self, model, Scenario scenario, values, *args, **kwargs):
+        """
+        values should be an iterable that is the same length as scenario.size
+        """
+        super(ConstantScenarioIndexParameter, self).__init__(model, *args, **kwargs)
+        cdef int i
+        if scenario._size != len(values):
+            raise ValueError("The number of values must equal the size of the scenario.")
+        self._values = np.empty(scenario._size, dtype=np.int32)
+        for i in range(scenario._size):
+            self._values[i] = values[i]
+        self._scenario = scenario
+
+    cpdef setup(self):
+        super(ConstantScenarioIndexParameter, self).setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self._scenario)
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        # This is a bit confusing.
+        # scenario_indices contains the current scenario number for all
+        # the Scenario objects in the model run. We have cached the
+        # position of self._scenario in self._scenario_index to lookup the
+        # correct number to use in this instance.
+        return self._values[scenario_index._indices[self._scenario_index]]
+ConstantScenarioIndexParameter.register()
+
+
+cdef class IndexedArrayParameter(Parameter):
+    """Parameter which uses an IndexParameter to index an array of Parameters
+
+    An example use of this parameter is to return a demand saving factor (as
+    a float) based on the current demand saving level (calculated by an
+    `IndexParameter`).
+
+    Parameters
+    ----------
+    index_parameter : `IndexParameter`
+    params : iterable of `Parameters` or floats
+
+
+    Notes
+    -----
+    Float arguments `params` are converted to `ConstantParameter`
+    """
+    def __init__(self, model, index_parameter, params, **kwargs):
+        super(IndexedArrayParameter, self).__init__(model, **kwargs)
+        assert(isinstance(index_parameter, IndexParameter))
+        self.index_parameter = index_parameter
+        self.children.add(index_parameter)
+
+        self.params = []
+        for p in params:
+            if not isinstance(p, Parameter):
+                from pywr.parameters import ConstantParameter
+                p = ConstantParameter(model, p)
+            self.params.append(p)
+
+        for param in self.params:
+            self.children.add(param)
+        self.children.add(index_parameter)
+
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns the value of the Parameter at the current index"""
+        cdef int index
+        index = self.index_parameter.get_index(scenario_index)
+        cdef Parameter parameter = self.params[index]
+        return parameter.get_value(scenario_index)
+
+    @classmethod
+    def load(cls, model, data):
+        index_parameter = load_parameter(model, data.pop("index_parameter"))
+        try:
+            parameters = data.pop("params")
+        except KeyError:
+            parameters = data.pop("parameters")
+        parameters = [load_parameter(model, parameter_data) for parameter_data in parameters]
+        return cls(model, index_parameter, parameters, **data)
+IndexedArrayParameter.register()
+
+
+cdef class AnnualHarmonicSeriesParameter(Parameter):
+    """ A `Parameter` which returns the value from an annual harmonic series
+
+    This `Parameter` comprises a series N cosine function with a period of 365
+     days. The calculation is performed using the Julien day of the year minus 1
+     This causes a small discontinuity in non-leap years.
+
+    .. math:: f(t) = A + \sum_{n=1}^N A_n\cdot \cos((2\pi nt)/365+\phi_n)
+
+    Parameters
+    ----------
+
+    mean : float
+        Mean value for the series (i.e. the position of zeroth harmonic)
+    amplitudes : array_like
+        The amplitudes for the N harmonic cosine functions. Must be the same
+        length as phases.
+    phases : array_like
+        The phase shift of the N harmonic cosine functions. Must be the same
+        length as amplitudes.
+
+    """
+    def __init__(self, model, mean, amplitudes, phases, *args, **kwargs):
+        if len(amplitudes) != len(phases):
+            raise ValueError("The number  of amplitudes and phases must be the same.")
+        n = len(amplitudes)
+        self.mean = mean
+        self._amplitudes = np.array(amplitudes)
+        self._phases = np.array(phases)
+
+        self._mean_lower_bounds = kwargs.pop('mean_lower_bounds', 0.0)
+        self._mean_upper_bounds = kwargs.pop('mean_upper_bounds', np.inf)
+        self._amplitude_lower_bounds = np.ones(n)*kwargs.pop('amplitude_lower_bounds', 0.0)
+        self._amplitude_upper_bounds = np.ones(n)*kwargs.pop('amplitude_upper_bounds', np.inf)
+        self._phase_lower_bounds = np.ones(n)*kwargs.pop('phase_lower_bounds', 0.0)
+        self._phase_upper_bounds = np.ones(n)*kwargs.pop('phase_upper_bounds', np.pi*2)
+        super(AnnualHarmonicSeriesParameter, self).__init__(model, *args, **kwargs)
+        # Size must be set after call to super.
+        self.double_size = 1 + 2*n
+        self._value_cache = 0.0
+        self._ts_index_cache = -1
+
+    @classmethod
+    def load(cls, model, data):
+        mean = data.pop('mean')
+        amplitudes = data.pop('amplitudes')
+        phases = data.pop('phases')
+
+        return cls(model, mean, amplitudes, phases, **data)
+
+    property amplitudes:
+        def __get__(self):
+            return np.asarray(self._amplitudes)
+
+    property phases:
+        def __get__(self):
+            return np.asarray(self._phases)
+
+    cpdef reset(self):
+        Parameter.reset(self)
+        self._value_cache = 0.0
+        self._ts_index_cache = -1
+
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        cdef int ts_index = timestep.index
+        cdef int doy = timestep.dayofyear - 1
+        cdef int n = self._amplitudes.shape[0]
+        cdef int i
+        cdef double val
+        if ts_index == self._ts_index_cache:
+            val = self._value_cache
+        else:
+            val = self.mean
+            for i in range(n):
+                val += self._amplitudes[i]*cos(doy*(i+1)*M_PI*2/365 + self._phases[i])
+            self._value_cache = val
+            self._ts_index_cache = ts_index
+        return val
+
+    cpdef set_double_variables(self, double[:] values):
+        n = len(self.amplitudes)
+        self.mean = values[0]
+        self._amplitudes[...] = values[1:n+1]
+        self._phases[...] = values[n+1:]
+
+    cpdef double[:] get_double_variables(self):
+        return np.r_[np.array([self.mean, ]), np.array(self.amplitudes), np.array(self.phases)]
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return np.r_[self._mean_lower_bounds, self._amplitude_lower_bounds, self._phase_lower_bounds]
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return np.r_[self._mean_upper_bounds, self._amplitude_upper_bounds, self._phase_upper_bounds]
+AnnualHarmonicSeriesParameter.register()
+
+cdef enum AggFuncs:
+    SUM = 0
+    MIN = 1
+    MAX = 2
+    MEAN = 3
+    PRODUCT = 4
+    CUSTOM = 5
+    ANY = 6
+    ALL = 7
+    MEDIAN = 8
+_agg_func_lookup = {
+    "sum": AggFuncs.SUM,
+    "min": AggFuncs.MIN,
+    "max": AggFuncs.MAX,
+    "mean": AggFuncs.MEAN,
+    "product": AggFuncs.PRODUCT,
+    "custom": AggFuncs.CUSTOM,
+    "any": AggFuncs.ANY,
+    "all": AggFuncs.ALL,
+    "median": AggFuncs.MEDIAN,
+}
+_agg_func_lookup_reverse = {v: k for k, v in _agg_func_lookup.items()}
+
+def wrap_const(model, value):
+    if isinstance(value, (int, float)):
+        value = ConstantParameter(model, value)
+    return value
+
+
+cdef class AggregatedParameter(Parameter):
+    """A collection of IndexParameters
+
+    This class behaves like a set. Parameters can be added or removed from it.
+    It's value is the value of it's child parameters aggregated using a
+    aggregating function (e.g. sum).
+
+    Parameters
+    ----------
+    parameters : iterable of `IndexParameter`
+        The parameters to aggregate
+    agg_func : callable or str
+        The aggregation function. Must be one of {"sum", "min", "max", "mean",
+        "product"}, or a callable function which accepts a list of values.
+    """
+    def __init__(self, model, parameters, agg_func=None, **kwargs):
+        super(AggregatedParameter, self).__init__(model, **kwargs)
+        self.agg_func = agg_func
+        self.parameters = list(parameters)
+        for parameter in self.parameters:
+            self.children.add(parameter)
+
+    @classmethod
+    def load(cls, model, data):
+        parameters_data = data.pop("parameters")
+        parameters = []
+        for pdata in parameters_data:
+            parameter = load_parameter(model, pdata)
+            parameters.append(wrap_const(model, parameter))
+
+        agg_func = data.pop("agg_func", None)
+        return cls(model, parameters=parameters, agg_func=agg_func, **data)
+
+    property agg_func:
+        def __get__(self):
+            if self._agg_func == AggFuncs.CUSTOM:
+                return self._agg_user_func
+            return _agg_func_lookup_reverse[self._agg_func]
+        def __set__(self, agg_func):
+            self._agg_user_func = None
+            if isinstance(agg_func, str):
+                agg_func = _agg_func_lookup[agg_func.lower()]
+            elif callable(agg_func):
+                self._agg_user_func = agg_func
+                agg_func = AggFuncs.CUSTOM
+            else:
+                raise ValueError("Unrecognised aggregation function: \"{}\".".format(agg_func))
+            self._agg_func = agg_func
+
+    cpdef add(self, Parameter parameter):
+        self.parameters.append(parameter)
+        parameter.parents.add(self)
+
+    cpdef remove(self, Parameter parameter):
+        self.parameters.remove(parameter)
+        parameter.parent.remove(self)
+
+    def __len__(self):
+        return len(self.parameters)
+
+    cpdef setup(self):
+        super(AggregatedParameter, self).setup()
+        assert(len(self.parameters))
+
+    cdef calc_values(self, Timestep timestep):
+        cdef Parameter parameter
+        cdef double[:] accum = self.__values  # View of the underlying location for the data
+        cdef double[:] values
+        cdef int i
+        cdef int nparam
+        cdef int n = accum.shape[0]
+        cdef ScenarioIndex scenario_index
+
+        if self._agg_func == AggFuncs.PRODUCT:
+            accum[...] = 1.0
+            for parameter in self.parameters:
+                values = parameter.get_all_values()
+                for i in range(n):
+                    accum[i] *= values[i]
+        elif self._agg_func == AggFuncs.SUM:
+            accum[...] = 0.0
+            for parameter in self.parameters:
+                values = parameter.get_all_values()
+                for i in range(n):
+                    accum[i] += values[i]
+        elif self._agg_func == AggFuncs.MAX:
+            accum[...] = np.NINF
+            for parameter in self.parameters:
+                values = parameter.get_all_values()
+                for i in range(n):
+                    if values[i] > accum[i]:
+                        accum[i] = values[i]
+        elif self._agg_func == AggFuncs.MIN:
+            accum[...] = np.PINF
+            for parameter in self.parameters:
+                values = parameter.get_all_values()
+                for i in range(n):
+                    if values[i] < accum[i]:
+                        accum[i] = values[i]
+        elif self._agg_func == AggFuncs.MEAN:
+            accum[...] = 0.0
+            for parameter in self.parameters:
+                values = parameter.get_all_values()
+                for i in range(n):
+                    accum[i] += values[i]
+
+            nparam = len(self.parameters)
+            for i in range(n):
+                accum[i] /= nparam
+
+        elif self._agg_func == AggFuncs.MEDIAN:
+            for i, scenario_index in enumerate(self.model.scenarios.combinations):
+                accum[i] = np.median([parameter.get_value(scenario_index) for parameter in self.parameters])
+        elif self._agg_func == AggFuncs.CUSTOM:
+            for i, scenario_index in enumerate(self.model.scenarios.combinations):
+                accum[i] = self._agg_user_func([parameter.get_value(scenario_index) for parameter in self.parameters])
+        else:
+            raise ValueError("Unsupported aggregation function.")
+AggregatedParameter.register()
+
+cdef class AggregatedIndexParameter(IndexParameter):
+    """A collection of IndexParameters
+
+    This class behaves like a set. Parameters can be added or removed from it.
+    Its index is the index of it's child parameters aggregated using a
+    aggregating function (e.g. sum).
+
+    Parameters
+    ----------
+    parameters : iterable of `IndexParameter`
+        The parameters to aggregate
+    agg_func : callable or str
+        The aggregation function. Must be one of {"sum", "min", "max", "any",
+        "all", "product"}, or a callable function which accepts a list of values.
+    """
+    def __init__(self, model, parameters, agg_func=None, **kwargs):
+        super(AggregatedIndexParameter, self).__init__(model, **kwargs)
+        self.agg_func = agg_func
+        self.parameters = list(parameters)
+        for parameter in self.parameters:
+            self.children.add(parameter)
+
+    @classmethod
+    def load(cls, model, data):
+        parameters_data = data.pop("parameters")
+        parameters = list()
+        for pdata in parameters_data:
+            parameter = load_parameter(model, pdata)
+            parameters.append(wrap_const(model, parameter))
+
+        agg_func = data.pop("agg_func", None)
+        return cls(model, parameters=parameters, agg_func=agg_func, **data)
+
+    property agg_func:
+        def __get__(self):
+            if self._agg_func == AggFuncs.CUSTOM:
+               return self._agg_user_func
+            return _agg_func_lookup_reverse[self._agg_func]
+        def __set__(self, agg_func):
+            self._agg_user_func = None
+            if isinstance(agg_func, str):
+                agg_func = _agg_func_lookup[agg_func.lower()]
+            elif callable(agg_func):
+                self._agg_user_func = agg_func
+                agg_func = AggFuncs.CUSTOM
+            else:
+                raise ValueError("Unrecognised aggregation function: \"{}\".".format(agg_func))
+            self._agg_func = agg_func
+
+    cpdef add(self, Parameter parameter):
+        self.parameters.append(parameter)
+        parameter.parents.add(self)
+
+    cpdef remove(self, Parameter parameter):
+        self.parameters.remove(parameter)
+        parameter.parent.remove(self)
+
+    def __len__(self):
+        return len(self.parameters)
+
+    cpdef setup(self):
+        super(AggregatedIndexParameter, self).setup()
+        assert len(self.parameters)
+        assert all([isinstance(parameter, IndexParameter) for parameter in self.parameters])
+
+    cdef calc_values(self, Timestep timestep):
+        cdef IndexParameter parameter
+        cdef int[:] accum = self.__indices  # View of the underlying location for the data
+        cdef int[:] values
+        cdef int i
+        cdef int nparam
+        cdef int n = accum.shape[0]
+        cdef ScenarioIndex scenario_index
+
+        if self._agg_func == AggFuncs.PRODUCT:
+            accum[...] = 1
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    accum[i] *= values[i]
+        elif self._agg_func == AggFuncs.SUM:
+            accum[...] = 0
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    accum[i] += values[i]
+        elif self._agg_func == AggFuncs.MAX:
+            accum[...] = INT_MIN
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    if values[i] > accum[i]:
+                        accum[i] = values[i]
+        elif self._agg_func == AggFuncs.MIN:
+            accum[...] = INT_MAX
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    if values[i] < accum[i]:
+                        accum[i] = values[i]
+        elif self._agg_func == AggFuncs.ANY:
+            accum[...] = 0
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    if values[i]:
+                        accum[i] = 1
+        elif self._agg_func == AggFuncs.ALL:
+            accum[...] = 1
+            for parameter in self.parameters:
+                values = parameter.get_all_indices()
+                for i in range(n):
+                    if not values[i]:
+                        accum[i] = 0
+
+        elif self._agg_func == AggFuncs.CUSTOM:
+            for i, scenario_index in enumerate(self.model.scenarios.combinations):
+                accum[i] = self._agg_user_func([parameter.get_index(scenario_index) for parameter in self.parameters])
+        else:
+            raise ValueError("Unsupported aggregation function.")
+
+        # Finally set the float values
+        for i in range(n):
+            self.__values[i] = accum[i]
+
+
+AggregatedIndexParameter.register()
+
+
+cdef class DivisionParameter(Parameter):
+    """ Parameter that divides one `Parameter` by another.
+
+    Parameters
+    ----------
+    denominator : `Parameter`
+        The parameter to use as the denominator (or divisor).
+    numerator : `Parameter`
+        The parameter to use as the numerator (or dividend).
+    """
+    def __init__(self, model, numerator, denominator, **kwargs):
+        super().__init__(model, **kwargs)
+        self._numerator = None
+        self._denominator = None
+        self.numerator = numerator
+        self.denominator = denominator
+
+    property numerator:
+        def __get__(self):
+            return self._numerator
+        def __set__(self, parameter):
+            # remove any existing parameter
+            if self._numerator is not None:
+                self._numerator.parents.remove(self)
+
+            self._numerator = parameter
+            self.children.add(parameter)
+
+    property denominator:
+        def __get__(self):
+            return self._denominator
+        def __set__(self, parameter):
+            # remove any existing parameter
+            if self._denominator is not None:
+                self._denominator.parents.remove(self)
+
+            self._denominator = parameter
+            self.children.add(parameter)
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = self._numerator.__values[i] / self._denominator.__values[i]
+
+    @classmethod
+    def load(cls, model, data):
+        numerator = load_parameter(model, data.pop("numerator"))
+        denominator = load_parameter(model, data.pop("denominator"))
+        return cls(model, numerator, denominator, **data)
+DivisionParameter.register()
+
+
+cdef class NegativeParameter(Parameter):
+    """ Parameter that takes negative of another `Parameter`
+
+    Parameters
+    ----------
+    parameter : `Parameter`
+        The parameter to to compare with the float.
+    """
+    def __init__(self, model, parameter, *args, **kwargs):
+        super(NegativeParameter, self).__init__(model, *args, **kwargs)
+        self.parameter = parameter
+        self.children.add(parameter)
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = -self.parameter.__values[i]
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+NegativeParameter.register()
+
+
+cdef class MaxParameter(Parameter):
+    """ Parameter that takes maximum of another `Parameter` and constant value (threshold)
+
+    This class is a more efficient version of `AggregatedParameter` where
+    a single `Parameter` is compared to constant value.
+
+    Parameters
+    ----------
+    parameter : `Parameter`
+        The parameter to to compare with the float.
+    threshold : float (default=0.0)
+        The threshold value to compare with the given parameter.
+    """
+    def __init__(self, model, parameter, threshold=0.0, *args, **kwargs):
+        super(MaxParameter, self).__init__(model, *args, **kwargs)
+        self.parameter = parameter
+        self.children.add(parameter)
+        self.threshold = threshold
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = max(self.parameter.__values[i], self.threshold)
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+MaxParameter.register()
+
+
+cdef class NegativeMaxParameter(MaxParameter):
+    """ Parameter that takes maximum of the negative of a `Parameter` and constant value (threshold) """
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = max(-self.parameter.__values[i], self.threshold)
+
+NegativeMaxParameter.register()
+
+
+cdef class MinParameter(Parameter):
+    """ Parameter that takes minimum of another `Parameter` and constant value (threshold)
+
+    This class is a more efficient version of `AggregatedParameter` where
+    a single `Parameter` is compared to constant value.
+
+    Parameters
+    ----------
+    parameter : `Parameter`
+        The parameter to to compare with the float.
+    threshold : float (default=0.0)
+        The threshold value to compare with the given parameter.
+    """
+    def __init__(self, model, parameter, threshold=0.0, *args, **kwargs):
+        super(MinParameter, self).__init__(model, *args, **kwargs)
+        self.parameter = parameter
+        self.children.add(parameter)
+        self.threshold = threshold
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = min(self.parameter.__values[i], self.threshold)
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+MinParameter.register()
+
+
+cdef class NegativeMinParameter(MinParameter):
+    """ Parameter that takes minimum of the negative of a `Parameter` and constant value (threshold) """
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = min(-self.parameter.__values[i], self.threshold)
+NegativeMinParameter.register()
+
+
+cdef class OffsetParameter(Parameter):
+    """Parameter that offsets another `Parameter` by a constant value.
+
+    This class is a more efficient version of `AggregatedParameter` where
+    a single `Parameter` is offset by a constant value.
+
+    Parameters
+    ----------
+    parameter : `Parameter`
+        The parameter to compare with the float.
+    offset : float (default=0.0)
+        The offset to apply to the value returned by `parameter`.
+    lower_bounds : float (default=0.0)
+        The lower bounds of the offset when used during optimisation.
+    upper_bounds : float (default=np.inf)
+        The upper bounds of the offset when used during optimisation.
+    """
+    def __init__(self, model, parameter, offset=0.0, lower_bounds=0.0, upper_bounds=np.inf, *args, **kwargs):
+        super(OffsetParameter, self).__init__(model, *args, **kwargs)
+        self.parameter = parameter
+        self.children.add(parameter)
+        self.offset = offset
+        self.double_size = 1
+        self._lower_bounds = np.ones(self.double_size) * lower_bounds
+        self._upper_bounds = np.ones(self.double_size) * upper_bounds
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        cdef int n = self.__values.shape[0]
+
+        for i in range(n):
+            self.__values[i] = self.parameter.__values[i] + self.offset
+
+    cpdef set_double_variables(self, double[:] values):
+        self.offset = values[0]
+
+    cpdef double[:] get_double_variables(self):
+        return np.array([self.offset, ], dtype=np.float64)
+
+    cpdef double[:] get_double_lower_bounds(self):
+        return self._lower_bounds
+
+    cpdef double[:] get_double_upper_bounds(self):
+        return self._upper_bounds
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+OffsetParameter.register()
+
+
+cdef class DeficitParameter(Parameter):
+    """Parameter track the deficit (max_flow - actual flow) of a Node
+
+    Parameters
+    ----------
+    model : pywr.model.Model
+    node : Node
+      The node that will have it's deficit tracked
+
+    Notes
+    -----
+    This parameter is a little unusual in that it's value is calculated during
+    the after method, not calc_values. It is intended to be used in combination
+    with a recorder (e.g. NumpyArrayNodeRecorder) to record the deficit (
+    defined as requested - actual flow) at a node. Note that this means
+    recording this parameter does *not* give you the value that was used by
+    the solver in this timestep. Alternatively, this parameter can be used
+    in the model by other parameters and will evaluate to *yesterdays* deficit,
+    where the deficit in the zeroth timestep is zero.
+    """
+    def __init__(self, model, node, *args, **kwargs):
+        super(DeficitParameter, self).__init__(model, *args, **kwargs)
+        self.node = node
+
+    cpdef reset(self):
+        self.__values[...] = 0.0
+
+    cdef calc_values(self, Timestep timestep):
+        pass # calculation done in after
+
+    cpdef after(self):
+        cdef double[:] max_flow
+        cdef int i
+        if self.node._max_flow_param is None:
+            for i in range(0, self.node._flow.shape[0]):
+                self.__values[i] = self.node._max_flow - self.node._flow[i]
+        else:
+            max_flow = self.node._max_flow_param.get_all_values()
+            for i in range(0, self.node._flow.shape[0]):
+                self.__values[i] = max_flow[i] - self.node._flow[i]
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        return cls(model, node=node, **data)
+
+DeficitParameter.register()
+
+
+cdef class FlowParameter(Parameter):
+    """Parameter that provides the flow from a node from the previous time-step.
+
+    Parameters
+    ----------
+    model : pywr.model.Model
+    node : Node
+      The node that will have its flow tracked
+    initial_value : float (default=0.0)
+      The value to return on the first  time-step before the node has any past flow.
+
+    Notes
+    -----
+    This parameter keeps track of the previous time step's flow on the given node. These
+    values can be used in calculations for the current timestep as though this was any
+    other parameter.
+    """
+    def __init__(self, model, node, *args, **kwargs):
+        self.initial_value = kwargs.pop('initial_value', 0)
+        super().__init__(model, *args, **kwargs)
+        self.node = node
+
+    cpdef setup(self):
+        super(FlowParameter, self).setup()
+        cdef int num_comb
+        if self.model.scenarios.combinations:
+            num_comb = len(self.model.scenarios.combinations)
+        else:
+            num_comb = 1
+        self.__next_values = np.empty([num_comb], np.float64)
+
+    cpdef reset(self):
+        self.__next_values[...] = self.initial_value
+        self.__values[...] = 0.0
+
+    cdef calc_values(self, Timestep timestep):
+        cdef int i
+        for i in range(self.__values.shape[0]):
+            self.__values[i] = self.__next_values[i]
+
+    cpdef after(self):
+        cdef int i
+        for i in range(self.node._flow.shape[0]):
+            self.__next_values[i] = self.node._flow[i]
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        return cls(model, node=node, **data)
+FlowParameter.register()
+
+
+cdef class PiecewiseIntegralParameter(Parameter):
+    """Parameter that integrates a piecewise function.
+
+    This parameter calculates the integral of a piecewise function. The
+    piecewise function is given as two arrays (`x` and `y`) and is assumed to
+    start from (0, 0). The values of `x` should be monotonically increasing
+    and greater than zero.
+
+    Parameters
+    ----------
+    parameter : `Parameter`
+        The parameter the defines the right hand bounds of the integration.
+    x : iterable of doubles
+    y : iterable of doubles
+
+    """
+    def __init__(self, model, parameter, x, y, *args, **kwargs):
+        super().__init__(model, *args, **kwargs)
+        self.parameter = parameter
+        self.children.add(parameter)
+        self.x = np.array(x, dtype=float)
+        self.y = np.array(y, dtype=float)
+
+    cpdef setup(self):
+        super(PiecewiseIntegralParameter, self).setup()
+
+        if len(self.x) != len(self.y):
+            raise ValueError('The length of `x` and `y` should be the same.')
+
+        if np.any(np.diff(self.x) < 0):
+            raise ValueError('The array `x` should be monotonically increasing.')
+
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        cdef double integral = 0.0
+        cdef double x = self.parameter.get_value(scenario_index)
+        cdef int i
+        cdef double dx, prev_x
+
+        prev_x = 0
+        for i in range(self.x.shape[0]):
+            if x < self.x[i]:
+                dx = x - prev_x
+            else:
+                dx = self.x[i] - prev_x
+
+            if dx < 0.0:
+                break
+            else:
+                integral += dx * self.y[i]
+            prev_x = self.x[i]
+        return integral
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop('parameter'))
+        return cls(model, parameter, **data)
+PiecewiseIntegralParameter.register()
+
+
+cdef class FlowDelayParameter(Parameter):
+    """Parameter that returns the delayed flow for a node after a given number of timesteps or days
+    
+    Parameters
+    ----------
+    model : `pywr.model.Model`
+    node: Node
+        The node to delay for.
+    timesteps: int
+        Number of timesteps to delay the flow.
+    days: int
+        Number of days to delay the flow. Specifying a number of days (instead of a number
+        of timesteps) is only valid if the number of days is exactly divisible by the model timestep length.
+    initial_flow: float
+        Flow value to return for initial model timesteps prior to any delayed flow being available. This
+        value is constant across all delayed timesteps and any model scenarios. Default is 0.0.
+    """
+
+    def __init__(self, model, node, *args, **kwargs):  
+        self.node = node
+        self.timesteps = kwargs.pop('timesteps', 0)
+        self.days = kwargs.pop('days', 0)
+        self.initial_flow = kwargs.pop('initial_flow', 0.0)
+        super().__init__(model, *args, **kwargs)
+
+    cpdef setup(self):
+        super(FlowDelayParameter, self).setup()
+        cdef int r
+        if self.days > 0:
+            r = self.days % self.model.timestepper.delta
+            if r == 0:
+                self.timesteps = self.days / self.model.timestepper.delta
+            else:
+                raise ValueError('The delay defined as number of days is not exactly divisible by the timestep delta.')
+        if self.timesteps < 1:
+            raise ValueError('The number of time-steps for a FlowDelayParameter node must be greater than one.')
+        self._memory = np.zeros((self.timesteps,  len(self.model.scenarios.combinations)))
+        self._memory_pointer = 0
+
+    cpdef reset(self):
+        self._memory[...] = self.initial_flow 
+        self._memory_pointer = 0
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return self._memory[self._memory_pointer, scenario_index.global_id]
+
+    cpdef after(self):
+        for i in range(self._memory.shape[1]):
+            self._memory[self._memory_pointer, i] = self.node._flow[i]
+        if self.timesteps > 1:
+            self._memory_pointer = (self._memory_pointer + 1) % self.timesteps 
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        return cls(model, node, **data)
+
+FlowDelayParameter.register()
+
+
+cdef class DiscountFactorParameter(Parameter):
+    """Parameter that returns the current discount factor based on discount rate and a base year.
+
+    Parameters
+    ----------
+    discount_rate : float
+        Discount rate (expressed as 0 - 1) used calculate discount factor for each year.
+    base_year : int
+        Discounting base year (i.e. the year with a discount factor equal to 1.0).
+    """
+
+    def __init__(self, model, rate, base_year, **kwargs):
+        super(DiscountFactorParameter, self).__init__(model, **kwargs)
+        self.rate = rate
+        self.base_year = base_year
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        return 1 / pow(1.0 + self.rate, ts.year - self.base_year)
+
+    @classmethod
+    def load(cls, model, data):
+        return cls(model, **data)
+
+DiscountFactorParameter.register()
+
+
+def get_parameter_from_registry(parameter_type):
+    key = parameter_type.lower()
+    try:
+        return parameter_registry[key]
+    except KeyError:
+        pass
+    if key.endswith("parameter"):
+        key.replace("parameter", "")
+    else:
+        key = key + "parameter"
+    try:
+        return parameter_registry[key]
+    except KeyError:
+        raise TypeError('Unknown parameter type: "{}"'.format(parameter_type))
+
+
+def load_parameter(model, data, parameter_name=None):
+    """Load a parameter from a dict"""
+    if isinstance(data, str):
+        # parameter is a reference
+        try:
+            parameter = model.parameters[data]
+        except KeyError:
+            parameter = None
+        if parameter is None:
+            if hasattr(model, "_parameters_to_load"):
+                # we're still in the process of loading data from JSON and
+                # the parameter requested hasn't been loaded yet - do it now
+                name = data
+                try:
+                    data = model._parameters_to_load.pop(name)
+                except KeyError:
+                    raise KeyError("Unknown parameter: '{}'".format(data))
+                parameter = load_parameter(model, data)
+            else:
+                raise KeyError("Unknown parameter: '{}'".format(data))
+    elif isinstance(data, (float, int)) or data is None:
+        # parameter is a constant
+        parameter = data
+    else:
+        # parameter is dynamic
+
+        try:
+             parameter_type = data['type']
+        except KeyError:
+            #raise custom exception that makes the error a bit easier to interpret
+            raise TypeNotFoundError(data)
+
+        try:
+            parameter_name = data["name"]
+        except:
+            pass
+
+        cls = get_parameter_from_registry(parameter_type)
+
+        kwargs = dict([(k,v) for k,v in data.items()])
+        del(kwargs["type"])
+        if "name" in kwargs:
+            del(kwargs["name"])
+        parameter = cls.load(model, kwargs)
+
+    if parameter_name is not None:
+        # TODO FIXME: memory leak if parameter is subsequently removed from the model
+        parameter.name = parameter_name
+        model.parameters[parameter_name] = parameter
+
+    return parameter
+
+
+def load_parameter_values(model, data, values_key='values', url_key='url',
+                          table_key='table'):
+    """ Function to load values from a data dictionary.
+
+    This function tries to load values in to a `np.ndarray` if 'values_key' is
+    in 'data'. Otherwise it tries to `load_dataframe` from a 'url' key.
+
+    Parameters
+    ----------
+    model - `Model` instance
+    data - dict
+    values_key - str
+        Key in data to load values directly to a `np.ndarray`
+    url_key - str
+        Key in data to load values directly from an external file reference (using pandas)
+    table_key - str
+        Key in data to load values directly from an external file reference (using pandas)
+    """
+    if values_key in data:
+        # values are given as an array
+        values = np.array(data.pop(values_key), np.float64)
+    elif url_key in data or table_key in data:
+        df = load_dataframe(model, data)
+        try:
+            # If it's a DataFrame we coerce to a numpy array
+            values = df.values
+        except AttributeError:
+            values = df
+        values = np.squeeze(values.astype(np.float64))
+    else:
+        # Try to get some useful information about the parameter for the error message
+        name = data.get('name', None)
+        ptype = data.get('type', None)
+        raise ValueError("Parameter ('{name}' of type '{ptype}' is missing a valid key to load its values. "
+                         "Please provide either a '{}', '{}' or '{}' entry.".format(values_key, url_key, table_key, name=name, ptype=ptype))
+    return values
```

### Comparing `pywr-1.8.0/pywr/parameters/_polynomial.pyx` & `pywr-1.9.0/pywr/parameters/_polynomial.pyx`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,176 +1,176 @@
-""" Module contains Polynomial Parameters """
-from ._parameters import load_parameter
-import numpy as np
-cimport numpy as np
-
-
-cdef class Polynomial1DParameter(Parameter):
-    """ Parameter that returns the result of 1D polynomial evaluation
-
-    The input to the polynomial can be either:
-     - The previous flow of the attached node (default)
-     - The previous flow of another `AbstractNode`
-     - The current storage of an `AbstractStorage` node
-     - The current value of another `Parameter`
-
-    Parameters
-    ----------
-    coefficients : array like
-        The 1 dimensional array of polynomial coefficients.
-    node : `AbstractNode`
-        An optional `AbstractNode` the flow of which is used to evaluate the polynomial.
-    storage_node : `Storage`
-        An optional `Storage` node the volume of which is used to evaluate the polynomial.
-    parameter : iterable of Parameter objects or single Parameter
-        An optional `Parameter` the value of which is used to evaluate the polynomial.
-    use_proportional_volume : bool
-        An optional boolean only used with a `Storage` node to switch between using absolute
-         or proportional volume when evaluating the polynomial.
-    scale : float
-        An optional scaling factor to apply to the polynomial input before calculation. This is
-         applied before any offset.
-    offset : float
-        An optional offset to apply to the polynomial input before calculation. This is applied after
-         and scaling.
-    """
-    def __init__(self, model, coefficients, *args, **kwargs):
-        self.coefficients = np.array(coefficients, dtype=np.float64)
-        self._other_node = kwargs.pop('node', None)
-        self._storage_node = kwargs.pop('storage_node', None)
-        self._parameter = kwargs.pop('parameter', None)
-        self.use_proportional_volume = kwargs.pop('use_proportional_volume', False)
-        self.offset = kwargs.pop('offset', 0.0)
-        self.scale = kwargs.pop('scale', 1.0)
-        # Check only one of the above is given
-        arg_check = [
-            self._other_node is not None,
-            self._storage_node is not None,
-            self._parameter is not None,
-        ]
-        # Check we haven't been given an ambiguous number of objects
-        if arg_check.count(True) > 1:
-            raise ValueError('Only one of "node", "storage_node" or "parameter" keywords should be given.')
-
-        super(Polynomial1DParameter, self).__init__(model, *args, **kwargs)
-
-        # Finally register parent relationship if parameter is given
-        if self._parameter is not None:
-            self.children.add(self._parameter)
-
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i
-        cdef double x, y
-
-        # Get the 'x' value to put in the polynomial
-        if self._parameter is not None:
-            x = self._parameter.__values[scenario_index.global_id]
-        elif self._storage_node is not None:
-            if self.use_proportional_volume:
-                x = self._storage_node._current_pc[scenario_index.global_id]
-            else:
-                x = self._storage_node._volume[scenario_index.global_id]
-        elif self._other_node is not None:
-            x = self._other_node._flow[scenario_index.global_id]
-        else:
-            x = self._node._flow[scenario_index.global_id]
-
-        # Apply scaling and offset
-        x = x*self.scale + self.offset
-        # No calculate polynomial
-        y = 0.0
-        for i in range(self.coefficients.shape[0]):
-            y += self.coefficients[i]*x**i
-        return y
-
-    @classmethod
-    def load(cls, model, data):
-        node = None
-        if 'node' in data:
-            node = model._get_node_from_ref(model, data.pop("node"))
-        storage_node = None
-        if 'storage_node' in data:
-            storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
-        parameter = None
-        if 'parameter' in data:
-            parameter = load_parameter(model, data.pop("parameter"))
-
-        coefficients = data.pop("coefficients")
-        parameter = cls(model, coefficients, node=node, storage_node=storage_node, parameter=parameter, **data)
-        return parameter
-Polynomial1DParameter.register()
-
-
-cdef class Polynomial2DStorageParameter(Parameter):
-    """ Parameter that returns the result of 2D polynomial evaluation
-
-    The 2 dimensions of the polynomial are the volume of a `Storage` node and
-    the current value of a `Parameter` respectively. Both must be given to this parameter.
-
-    Parameters
-    ----------
-    coefficients : array like
-        The 2 dimensional array of polynomial coefficients.
-    storage_node : `Storage`
-        A `Storage` node the volume of which is used to evaluate the polynomial.
-    parameter : iterable of Parameter objects or single Parameter
-        An `Parameter` the value of which is used to evaluate the polynomial.
-    use_proportional_volume : bool
-        An optional boolean only used with a `Storage` node to switch between using absolute
-         or proportional volume when evaluating the polynomial.
-    storage_scale : float
-        An optional scaling factor to apply to the storage value before calculation. This is
-         applied before any offset.
-    storage_offset : float
-        An optional offset to apply to the storage value before calculation. This is applied after
-         and scaling
-    parameter_scale : float
-        An optional scaling factor to apply to the parameter value before calculation. This is
-         applied before any offset.
-    parameter_offset : float
-        An optional offset to apply to the parameter value before calculation. This is applied after
-         and scaling
-    """
-    def __init__(self, model, coefficients, storage_node, parameter, *args, **kwargs):
-        self.coefficients = np.array(coefficients, dtype=np.float64)
-        self._storage_node = storage_node
-        self._parameter = parameter
-        self.use_proportional_volume = kwargs.pop('use_proportional_volume', False)
-        self.storage_offset = kwargs.pop('storage_offset', 0.0)
-        self.storage_scale = kwargs.pop('storage_scale', 1.0)
-        self.parameter_offset = kwargs.pop('parameter_offset', 0.0)
-        self.parameter_scale = kwargs.pop('parameter_scale', 1.0)
-        super(Polynomial2DStorageParameter, self).__init__(model, *args, **kwargs)
-
-        # Register parameter relationships
-        self.children.add(parameter)
-
-    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
-        cdef int i, j
-        cdef double x, y, z
-
-        # Storage volume is 1st dimension
-        if self.use_proportional_volume:
-            x = self._storage_node._current_pc[scenario_index.global_id]
-        else:
-            x = self._storage_node._volume[scenario_index.global_id]
-        # Parameter value is 2nd dimension
-        y = self._parameter.__values[scenario_index.global_id]
-
-        # Apply scaling and offset
-        x = self.storage_scale*x + self.storage_offset
-        y = self.parameter_scale*y + self.parameter_offset
-        z = 0.0
-        for i in range(self.coefficients.shape[0]):
-            for j in range(self.coefficients.shape[1]):
-                z += self.coefficients[i, j]*x**i*y**j
-        return z
-
-    @classmethod
-    def load(cls, model, data):
-        storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
-        parameter = load_parameter(model, data.pop("parameter"))
-        coefficients = data.pop("coefficients")
-        parameter = cls(model, coefficients, storage_node, parameter, **data)
-        return parameter
-Polynomial2DStorageParameter.register()
+""" Module contains Polynomial Parameters """
+from ._parameters import load_parameter
+import numpy as np
+cimport numpy as np
+
+
+cdef class Polynomial1DParameter(Parameter):
+    """ Parameter that returns the result of 1D polynomial evaluation
+
+    The input to the polynomial can be either:
+     - The previous flow of the attached node (default)
+     - The previous flow of another `AbstractNode`
+     - The current storage of an `AbstractStorage` node
+     - The current value of another `Parameter`
+
+    Parameters
+    ----------
+    coefficients : array like
+        The 1 dimensional array of polynomial coefficients.
+    node : `AbstractNode`
+        An optional `AbstractNode` the flow of which is used to evaluate the polynomial.
+    storage_node : `Storage`
+        An optional `Storage` node the volume of which is used to evaluate the polynomial.
+    parameter : iterable of Parameter objects or single Parameter
+        An optional `Parameter` the value of which is used to evaluate the polynomial.
+    use_proportional_volume : bool
+        An optional boolean only used with a `Storage` node to switch between using absolute
+         or proportional volume when evaluating the polynomial.
+    scale : float
+        An optional scaling factor to apply to the polynomial input before calculation. This is
+         applied before any offset.
+    offset : float
+        An optional offset to apply to the polynomial input before calculation. This is applied after
+         and scaling.
+    """
+    def __init__(self, model, coefficients, *args, **kwargs):
+        self.coefficients = np.array(coefficients, dtype=np.float64)
+        self._other_node = kwargs.pop('node', None)
+        self._storage_node = kwargs.pop('storage_node', None)
+        self._parameter = kwargs.pop('parameter', None)
+        self.use_proportional_volume = kwargs.pop('use_proportional_volume', False)
+        self.offset = kwargs.pop('offset', 0.0)
+        self.scale = kwargs.pop('scale', 1.0)
+        # Check only one of the above is given
+        arg_check = [
+            self._other_node is not None,
+            self._storage_node is not None,
+            self._parameter is not None,
+        ]
+        # Check we haven't been given an ambiguous number of objects
+        if arg_check.count(True) > 1:
+            raise ValueError('Only one of "node", "storage_node" or "parameter" keywords should be given.')
+
+        super(Polynomial1DParameter, self).__init__(model, *args, **kwargs)
+
+        # Finally register parent relationship if parameter is given
+        if self._parameter is not None:
+            self.children.add(self._parameter)
+
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i
+        cdef double x, y
+
+        # Get the 'x' value to put in the polynomial
+        if self._parameter is not None:
+            x = self._parameter.__values[scenario_index.global_id]
+        elif self._storage_node is not None:
+            if self.use_proportional_volume:
+                x = self._storage_node._current_pc[scenario_index.global_id]
+            else:
+                x = self._storage_node._volume[scenario_index.global_id]
+        elif self._other_node is not None:
+            x = self._other_node._flow[scenario_index.global_id]
+        else:
+            x = self._node._flow[scenario_index.global_id]
+
+        # Apply scaling and offset
+        x = x*self.scale + self.offset
+        # No calculate polynomial
+        y = 0.0
+        for i in range(self.coefficients.shape[0]):
+            y += self.coefficients[i]*x**i
+        return y
+
+    @classmethod
+    def load(cls, model, data):
+        node = None
+        if 'node' in data:
+            node = model._get_node_from_ref(model, data.pop("node"))
+        storage_node = None
+        if 'storage_node' in data:
+            storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
+        parameter = None
+        if 'parameter' in data:
+            parameter = load_parameter(model, data.pop("parameter"))
+
+        coefficients = data.pop("coefficients")
+        parameter = cls(model, coefficients, node=node, storage_node=storage_node, parameter=parameter, **data)
+        return parameter
+Polynomial1DParameter.register()
+
+
+cdef class Polynomial2DStorageParameter(Parameter):
+    """ Parameter that returns the result of 2D polynomial evaluation
+
+    The 2 dimensions of the polynomial are the volume of a `Storage` node and
+    the current value of a `Parameter` respectively. Both must be given to this parameter.
+
+    Parameters
+    ----------
+    coefficients : array like
+        The 2 dimensional array of polynomial coefficients.
+    storage_node : `Storage`
+        A `Storage` node the volume of which is used to evaluate the polynomial.
+    parameter : iterable of Parameter objects or single Parameter
+        An `Parameter` the value of which is used to evaluate the polynomial.
+    use_proportional_volume : bool
+        An optional boolean only used with a `Storage` node to switch between using absolute
+         or proportional volume when evaluating the polynomial.
+    storage_scale : float
+        An optional scaling factor to apply to the storage value before calculation. This is
+         applied before any offset.
+    storage_offset : float
+        An optional offset to apply to the storage value before calculation. This is applied after
+         and scaling
+    parameter_scale : float
+        An optional scaling factor to apply to the parameter value before calculation. This is
+         applied before any offset.
+    parameter_offset : float
+        An optional offset to apply to the parameter value before calculation. This is applied after
+         and scaling
+    """
+    def __init__(self, model, coefficients, storage_node, parameter, *args, **kwargs):
+        self.coefficients = np.array(coefficients, dtype=np.float64)
+        self._storage_node = storage_node
+        self._parameter = parameter
+        self.use_proportional_volume = kwargs.pop('use_proportional_volume', False)
+        self.storage_offset = kwargs.pop('storage_offset', 0.0)
+        self.storage_scale = kwargs.pop('storage_scale', 1.0)
+        self.parameter_offset = kwargs.pop('parameter_offset', 0.0)
+        self.parameter_scale = kwargs.pop('parameter_scale', 1.0)
+        super(Polynomial2DStorageParameter, self).__init__(model, *args, **kwargs)
+
+        # Register parameter relationships
+        self.children.add(parameter)
+
+    cpdef double value(self, Timestep ts, ScenarioIndex scenario_index) except? -1:
+        cdef int i, j
+        cdef double x, y, z
+
+        # Storage volume is 1st dimension
+        if self.use_proportional_volume:
+            x = self._storage_node._current_pc[scenario_index.global_id]
+        else:
+            x = self._storage_node._volume[scenario_index.global_id]
+        # Parameter value is 2nd dimension
+        y = self._parameter.__values[scenario_index.global_id]
+
+        # Apply scaling and offset
+        x = self.storage_scale*x + self.storage_offset
+        y = self.parameter_scale*y + self.parameter_offset
+        z = 0.0
+        for i in range(self.coefficients.shape[0]):
+            for j in range(self.coefficients.shape[1]):
+                z += self.coefficients[i, j]*x**i*y**j
+        return z
+
+    @classmethod
+    def load(cls, model, data):
+        storage_node = model._get_node_from_ref(model, data.pop("storage_node"))
+        parameter = load_parameter(model, data.pop("parameter"))
+        coefficients = data.pop("coefficients")
+        parameter = cls(model, coefficients, storage_node, parameter, **data)
+        return parameter
+Polynomial2DStorageParameter.register()
```

### Comparing `pywr-1.8.0/pywr/parameters/_thresholds.pxd` & `pywr-1.9.0/pywr/parameters/_thresholds.pxd`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-from ._parameters cimport IndexParameter, Parameter
-from pywr.recorders._recorders cimport Recorder
-from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, AbstractStorage
-cimport numpy as np
-ctypedef np.uint8_t uint8
-
-
-cdef class AbstractThresholdParameter(IndexParameter):
-    cdef public double _threshold
-    cdef public Parameter _threshold_parameter
-    cdef double[:] values
-    cdef int predicate
-    cdef public bint ratchet
-    cdef uint8[:] _triggered
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
-
-cdef class StorageThresholdParameter(AbstractThresholdParameter):
-    cdef public AbstractStorage storage
-
-cdef class NodeThresholdParameter(AbstractThresholdParameter):
-    cdef public AbstractNode node
-
-cdef class ParameterThresholdParameter(AbstractThresholdParameter):
-    cdef public Parameter param
-
-cdef class RecorderThresholdParameter(AbstractThresholdParameter):
-    cdef public Recorder recorder
-    cdef public initial_value
-
-cdef class CurrentYearThresholdParameter(AbstractThresholdParameter):
-    pass
-
-cdef class CurrentOrdinalDayThresholdParameter(AbstractThresholdParameter):
-    pass
+from ._parameters cimport IndexParameter, Parameter
+from pywr.recorders._recorders cimport Recorder
+from .._core cimport Timestep, Scenario, ScenarioIndex, AbstractNode, AbstractStorage
+cimport numpy as np
+ctypedef np.uint8_t uint8
+
+
+cdef class AbstractThresholdParameter(IndexParameter):
+    cdef public double _threshold
+    cdef public Parameter _threshold_parameter
+    cdef double[:] values
+    cdef int predicate
+    cdef public bint ratchet
+    cdef uint8[:] _triggered
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1
+
+cdef class StorageThresholdParameter(AbstractThresholdParameter):
+    cdef public AbstractStorage storage
+
+cdef class NodeThresholdParameter(AbstractThresholdParameter):
+    cdef public AbstractNode node
+
+cdef class ParameterThresholdParameter(AbstractThresholdParameter):
+    cdef public Parameter param
+
+cdef class RecorderThresholdParameter(AbstractThresholdParameter):
+    cdef public Recorder recorder
+    cdef public initial_value
+
+cdef class CurrentYearThresholdParameter(AbstractThresholdParameter):
+    pass
+
+cdef class CurrentOrdinalDayThresholdParameter(AbstractThresholdParameter):
+    pass
```

### Comparing `pywr-1.8.0/pywr/parameters/_thresholds.pyx` & `pywr-1.9.0/pywr/parameters/_thresholds.pyx`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,285 +1,285 @@
-from ._parameters import load_parameter
-cimport numpy as np
-import numpy as np
-
-cdef enum Predicates:
-    LT = 0
-    GT = 1
-    EQ = 2
-    LE = 3
-    GE = 4
-_predicate_lookup = {
-    "LT": Predicates.LT, "<": Predicates.LT,
-    "GT": Predicates.GT, ">": Predicates.GT,
-    "EQ": Predicates.EQ, "=": Predicates.EQ,
-    "LE": Predicates.LE, "<=": Predicates.LE,
-    "GE": Predicates.GE, ">=": Predicates.GE,
-}
-
-cdef class AbstractThresholdParameter(IndexParameter):
-    """ Base class for parameters returning one of two values depending on other state.
-
-    Parameters
-    ----------
-    threshold : double or Parameter
-        Threshold to compare the value of the recorder to
-    values : iterable of doubles
-        If the predicate evaluates False the zeroth value is returned,
-        otherwise the first value is returned.
-    predicate : string
-        One of {"LT", "GT", "EQ", "LE", "GE"}.
-    ratchet : bool
-        If true the parameter behaves like a ratchet. Once it is triggered first
-        it stays in the triggered position (default=False).
-
-    Methods
-    -------
-    value(timestep, scenario_index)
-        Returns a value from the `values` attribute, using the index.
-    index(timestep, scenario_index)
-        Returns 1 if the predicate evaluates True, else 0.
-
-    Notes
-    -----
-    On the first day of the model run the recorder will not have a value for
-    the previous day. In this case the predicate evaluates to True.
-
-    """
-    def __init__(self, model, threshold, *args, values=None, predicate=None, ratchet=False, **kwargs):
-        super(AbstractThresholdParameter, self).__init__(model, *args, **kwargs)
-        self.threshold = threshold
-        if values is None:
-            self.values = None
-        else:
-            self.values = np.array(values, np.float64)
-        if predicate is None:
-            predicate = Predicates.LT
-        elif isinstance(predicate, str):
-            predicate = _predicate_lookup[predicate.upper()]
-        self.predicate = predicate
-        self.ratchet = ratchet
-
-    cpdef setup(self):
-        super(AbstractThresholdParameter, self).setup()
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        self._triggered = np.empty(ncomb, dtype=np.uint8)
-
-    cpdef reset(self):
-        super(AbstractThresholdParameter, self).reset()
-        self._triggered[...] = 0
-
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        raise NotImplementedError()
-
-    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns a value from the values attribute, using the index"""
-        cdef int ind = self.get_index(scenario_index)
-        cdef double v
-        if self.values is not None:
-            v = self.values[ind]
-        else:
-            return np.nan
-        return v
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns 1 if the predicate evalutes True, else 0"""
-        cdef double x
-        cdef bint ind, triggered
-
-        triggered = self._triggered[scenario_index.global_id]
-
-        # Return triggered state if ratchet is enabled.
-        if self.ratchet and triggered:
-            return triggered
-
-        x = self._value_to_compare(timestep, scenario_index)
-
-        cdef double threshold
-        if self._threshold_parameter is not None:
-            threshold = self._threshold_parameter.value(timestep, scenario_index)
-        else:
-            threshold = self._threshold
-
-        if self.predicate == Predicates.LT:
-            ind = x < threshold
-        elif self.predicate == Predicates.GT:
-            ind = x > threshold
-        elif self.predicate == Predicates.LE:
-            ind = x <= threshold
-        elif self.predicate == Predicates.GE:
-            ind = x >= threshold
-        else:
-            ind = x == threshold
-
-        self._triggered[scenario_index.global_id] = max(ind, triggered)
-        return ind
-
-    property threshold:
-        def __get__(self):
-            if self._threshold_parameter is not None:
-                return self._threshold_parameter
-            else:
-                return self._threshold
-
-        def __set__(self, value):
-            if self._threshold_parameter is not None:
-                self.children.remove(self._threshold_parameter)
-                self._threshold_parameter = None
-            if isinstance(value, Parameter):
-                self._threshold_parameter = value
-                self.children.add(self._threshold_parameter)
-            else:
-                self._threshold = value
-
-cdef class StorageThresholdParameter(AbstractThresholdParameter):
-    """ Returns one of two values depending on current volume in a Storage node
-
-    Parameters
-    ----------
-    recorder : `pywr.core.AbstractStorage`
-
-    """
-    def __init__(self, model, AbstractStorage storage, *args, **kwargs):
-        super(StorageThresholdParameter, self).__init__(model, *args, **kwargs)
-        self.storage = storage
-
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        return self.storage._volume[scenario_index.global_id]
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("storage_node"))
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, node, threshold, values=values, predicate=predicate, **data)
-StorageThresholdParameter.register()
-
-
-cdef class NodeThresholdParameter(AbstractThresholdParameter):
-    """ Returns one of two values depending on previous flow in a node
-
-    Parameters
-    ----------
-    recorder : `pywr.core.AbstractNode`
-
-    """
-    def __init__(self, model, AbstractNode node, *args, **kwargs):
-        super(NodeThresholdParameter, self).__init__(model, *args, **kwargs)
-        self.node = node
-
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        return self.node._prev_flow[scenario_index.global_id]
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        if timestep.index == 0:
-            # previous flow on initial timestep is undefined
-            return 0
-        return AbstractThresholdParameter.index(self, timestep, scenario_index)
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, node, threshold, values=values, predicate=predicate, **data)
-NodeThresholdParameter.register()
-
-
-cdef class ParameterThresholdParameter(AbstractThresholdParameter):
-    """ Returns one of two values depending on the value of a Parameter
-
-    Parameters
-    ----------
-    recorder : `pywr.core.AbstractNode`
-
-    """
-    def __init__(self, model, Parameter param, *args, **kwargs):
-        super(ParameterThresholdParameter, self).__init__(model, *args, **kwargs)
-        self.param = param
-        self.children.add(param)
-
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        return self.param.get_value(scenario_index)
-
-    @classmethod
-    def load(cls, model, data):
-        param = load_parameter(model, data.pop('parameter'))
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, param, threshold, values=values, predicate=predicate, **data)
-ParameterThresholdParameter.register()
-
-
-cdef class RecorderThresholdParameter(AbstractThresholdParameter):
-    """Returns one of two values depending on a Recorder value and a threshold
-
-    Parameters
-    ----------
-    recorder : `pywr.recorder.Recorder`
-
-    """
-
-    def __init__(self,  model, Recorder recorder, *args, initial_value=1, **kwargs):
-        super(RecorderThresholdParameter, self).__init__(model, *args, **kwargs)
-        self.recorder = recorder
-        self.recorder.parents.add(self)
-        self.initial_value = initial_value
-
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        # TODO Make this a more general API on Recorder
-        return self.recorder.data[timestep.index - 1, scenario_index.global_id]
-
-    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        """Returns 1 if the predicate evalutes True, else 0"""
-        cdef int index = timestep.index
-        cdef int ind
-        if index == 0:
-            # on the first day the recorder doesn't have a value so we have no
-            # threshold to compare to
-            ind = self.initial_value
-        else:
-            ind = super(RecorderThresholdParameter, self).index(timestep, scenario_index)
-        return ind
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.recorders._recorders import load_recorder  # delayed to prevent circular reference
-        recorder = load_recorder(model, data.pop("recorder"))
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, recorder, threshold, values=values, predicate=predicate, **data)
-RecorderThresholdParameter.register()
-
-
-cdef class CurrentYearThresholdParameter(AbstractThresholdParameter):
-    """ Returns one of two values depending on the year of the current timestep..
-    """
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        return float(timestep.year)
-
-    @classmethod
-    def load(cls, model, data):
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, threshold, values=values, predicate=predicate, **data)
-CurrentYearThresholdParameter.register()
-
-
-cdef class CurrentOrdinalDayThresholdParameter(AbstractThresholdParameter):
-    """ Returns one of two values depending on the ordinal of the current timestep.
-    """
-    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
-        return float(timestep.datetime.toordinal())
-
-    @classmethod
-    def load(cls, model, data):
-        threshold = load_parameter(model, data.pop("threshold"))
-        values = data.pop("values", None)
-        predicate = data.pop("predicate", None)
-        return cls(model, threshold, values=values, predicate=predicate, **data)
-CurrentOrdinalDayThresholdParameter.register()
+from ._parameters import load_parameter
+cimport numpy as np
+import numpy as np
+
+cdef enum Predicates:
+    LT = 0
+    GT = 1
+    EQ = 2
+    LE = 3
+    GE = 4
+_predicate_lookup = {
+    "LT": Predicates.LT, "<": Predicates.LT,
+    "GT": Predicates.GT, ">": Predicates.GT,
+    "EQ": Predicates.EQ, "=": Predicates.EQ,
+    "LE": Predicates.LE, "<=": Predicates.LE,
+    "GE": Predicates.GE, ">=": Predicates.GE,
+}
+
+cdef class AbstractThresholdParameter(IndexParameter):
+    """ Base class for parameters returning one of two values depending on other state.
+
+    Parameters
+    ----------
+    threshold : double or Parameter
+        Threshold to compare the value of the recorder to
+    values : iterable of doubles
+        If the predicate evaluates False the zeroth value is returned,
+        otherwise the first value is returned.
+    predicate : string
+        One of {"LT", "GT", "EQ", "LE", "GE"}.
+    ratchet : bool
+        If true the parameter behaves like a ratchet. Once it is triggered first
+        it stays in the triggered position (default=False).
+
+    Methods
+    -------
+    value(timestep, scenario_index)
+        Returns a value from the `values` attribute, using the index.
+    index(timestep, scenario_index)
+        Returns 1 if the predicate evaluates True, else 0.
+
+    Notes
+    -----
+    On the first day of the model run the recorder will not have a value for
+    the previous day. In this case the predicate evaluates to True.
+
+    """
+    def __init__(self, model, threshold, *args, values=None, predicate=None, ratchet=False, **kwargs):
+        super(AbstractThresholdParameter, self).__init__(model, *args, **kwargs)
+        self.threshold = threshold
+        if values is None:
+            self.values = None
+        else:
+            self.values = np.array(values, np.float64)
+        if predicate is None:
+            predicate = Predicates.LT
+        elif isinstance(predicate, str):
+            predicate = _predicate_lookup[predicate.upper()]
+        self.predicate = predicate
+        self.ratchet = ratchet
+
+    cpdef setup(self):
+        super(AbstractThresholdParameter, self).setup()
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        self._triggered = np.empty(ncomb, dtype=np.uint8)
+
+    cpdef reset(self):
+        super(AbstractThresholdParameter, self).reset()
+        self._triggered[...] = 0
+
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        raise NotImplementedError()
+
+    cpdef double value(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns a value from the values attribute, using the index"""
+        cdef int ind = self.get_index(scenario_index)
+        cdef double v
+        if self.values is not None:
+            v = self.values[ind]
+        else:
+            return np.nan
+        return v
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns 1 if the predicate evalutes True, else 0"""
+        cdef double x
+        cdef bint ind, triggered
+
+        triggered = self._triggered[scenario_index.global_id]
+
+        # Return triggered state if ratchet is enabled.
+        if self.ratchet and triggered:
+            return triggered
+
+        x = self._value_to_compare(timestep, scenario_index)
+
+        cdef double threshold
+        if self._threshold_parameter is not None:
+            threshold = self._threshold_parameter.value(timestep, scenario_index)
+        else:
+            threshold = self._threshold
+
+        if self.predicate == Predicates.LT:
+            ind = x < threshold
+        elif self.predicate == Predicates.GT:
+            ind = x > threshold
+        elif self.predicate == Predicates.LE:
+            ind = x <= threshold
+        elif self.predicate == Predicates.GE:
+            ind = x >= threshold
+        else:
+            ind = x == threshold
+
+        self._triggered[scenario_index.global_id] = max(ind, triggered)
+        return ind
+
+    property threshold:
+        def __get__(self):
+            if self._threshold_parameter is not None:
+                return self._threshold_parameter
+            else:
+                return self._threshold
+
+        def __set__(self, value):
+            if self._threshold_parameter is not None:
+                self.children.remove(self._threshold_parameter)
+                self._threshold_parameter = None
+            if isinstance(value, Parameter):
+                self._threshold_parameter = value
+                self.children.add(self._threshold_parameter)
+            else:
+                self._threshold = value
+
+cdef class StorageThresholdParameter(AbstractThresholdParameter):
+    """ Returns one of two values depending on current volume in a Storage node
+
+    Parameters
+    ----------
+    recorder : `pywr.core.AbstractStorage`
+
+    """
+    def __init__(self, model, AbstractStorage storage, *args, **kwargs):
+        super(StorageThresholdParameter, self).__init__(model, *args, **kwargs)
+        self.storage = storage
+
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        return self.storage._volume[scenario_index.global_id]
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("storage_node"))
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, node, threshold, values=values, predicate=predicate, **data)
+StorageThresholdParameter.register()
+
+
+cdef class NodeThresholdParameter(AbstractThresholdParameter):
+    """ Returns one of two values depending on previous flow in a node
+
+    Parameters
+    ----------
+    recorder : `pywr.core.AbstractNode`
+
+    """
+    def __init__(self, model, AbstractNode node, *args, **kwargs):
+        super(NodeThresholdParameter, self).__init__(model, *args, **kwargs)
+        self.node = node
+
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        return self.node._prev_flow[scenario_index.global_id]
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        if timestep.index == 0:
+            # previous flow on initial timestep is undefined
+            return 0
+        return AbstractThresholdParameter.index(self, timestep, scenario_index)
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, node, threshold, values=values, predicate=predicate, **data)
+NodeThresholdParameter.register()
+
+
+cdef class ParameterThresholdParameter(AbstractThresholdParameter):
+    """ Returns one of two values depending on the value of a Parameter
+
+    Parameters
+    ----------
+    recorder : `pywr.core.AbstractNode`
+
+    """
+    def __init__(self, model, Parameter param, *args, **kwargs):
+        super(ParameterThresholdParameter, self).__init__(model, *args, **kwargs)
+        self.param = param
+        self.children.add(param)
+
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        return self.param.get_value(scenario_index)
+
+    @classmethod
+    def load(cls, model, data):
+        param = load_parameter(model, data.pop('parameter'))
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, param, threshold, values=values, predicate=predicate, **data)
+ParameterThresholdParameter.register()
+
+
+cdef class RecorderThresholdParameter(AbstractThresholdParameter):
+    """Returns one of two values depending on a Recorder value and a threshold
+
+    Parameters
+    ----------
+    recorder : `pywr.recorder.Recorder`
+
+    """
+
+    def __init__(self,  model, Recorder recorder, *args, initial_value=1, **kwargs):
+        super(RecorderThresholdParameter, self).__init__(model, *args, **kwargs)
+        self.recorder = recorder
+        self.recorder.parents.add(self)
+        self.initial_value = initial_value
+
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        # TODO Make this a more general API on Recorder
+        return self.recorder.data[timestep.index - 1, scenario_index.global_id]
+
+    cpdef int index(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        """Returns 1 if the predicate evalutes True, else 0"""
+        cdef int index = timestep.index
+        cdef int ind
+        if index == 0:
+            # on the first day the recorder doesn't have a value so we have no
+            # threshold to compare to
+            ind = self.initial_value
+        else:
+            ind = super(RecorderThresholdParameter, self).index(timestep, scenario_index)
+        return ind
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.recorders._recorders import load_recorder  # delayed to prevent circular reference
+        recorder = load_recorder(model, data.pop("recorder"))
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, recorder, threshold, values=values, predicate=predicate, **data)
+RecorderThresholdParameter.register()
+
+
+cdef class CurrentYearThresholdParameter(AbstractThresholdParameter):
+    """ Returns one of two values depending on the year of the current timestep..
+    """
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        return float(timestep.year)
+
+    @classmethod
+    def load(cls, model, data):
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, threshold, values=values, predicate=predicate, **data)
+CurrentYearThresholdParameter.register()
+
+
+cdef class CurrentOrdinalDayThresholdParameter(AbstractThresholdParameter):
+    """ Returns one of two values depending on the ordinal of the current timestep.
+    """
+    cpdef double _value_to_compare(self, Timestep timestep, ScenarioIndex scenario_index) except? -1:
+        return float(timestep.datetime.toordinal())
+
+    @classmethod
+    def load(cls, model, data):
+        threshold = load_parameter(model, data.pop("threshold"))
+        values = data.pop("values", None)
+        predicate = data.pop("predicate", None)
+        return cls(model, threshold, values=values, predicate=predicate, **data)
+CurrentOrdinalDayThresholdParameter.register()
```

### Comparing `pywr-1.8.0/pywr/parameters/groundwater.py` & `pywr-1.9.0/pywr/parameters/groundwater.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-# -*- coding: utf-8 -*-
-from ._parameters import Parameter
-from ..core import Storage
-import numpy as np
-
-
-class KeatingStreamFlowParameter(Parameter):
-    """
-    A flow Parameter that returns stream flow from an Aquifer based on the groundwater level.
-
-    The approach is based on the lumped parameter model by Keating (1982). This Parameter calculates the
-     perennial and winterbourne stream flow components based on the Storage node. It requires the Storage
-     node to have a valid level Parameter.
-
-    In contrast to the Keating paper a general coefficient is provided for calibration. The Keating approach
-     utilised a coefficient of $B/L$ where "$B$ and $L$ are the dimensions of the aquifer block parallel
-     and perpendicular to the stream."
-
-    Keating, T. (1982), A Lumped Parameter Model of a Chalk Aquifer-Stream System in Hampshire,
-      United Kingdom. Ground Water, 20: 430–436. doi:10.1111/j.1745-6584.1982.tb02763.x
-    """
-    def __init__(self, model, storage_node, levels, transmissivity, coefficient=1.0, **kwargs):
-        """
-
-        :param storage_node:
-        :param levels:
-        :param transmissivity:
-        :param coefficient:
-        """
-        super(KeatingStreamFlowParameter, self).__init__(model, **kwargs)
-        self.storage_node = storage_node
-
-        if len(levels) != len(transmissivity):
-            raise ValueError('The number of transmissivity values must equal the number of levels.')
-
-        self.levels = np.array(levels)
-        self.transmissivity = np.array(transmissivity)
-        self.coefficient = coefficient
-
-    def value(self, ts, scenario_index):
-        # Get the current level of the aquifer
-        # TODO: this is a HACK - we can't use get_level/get_value as there is
-        #       no way to define the parent/child relationship
-        level = self.storage_node.level.value(ts, scenario_index)
-
-        # Coefficient
-        C = self.coefficient
-
-        # Calculate flow at each stream level
-        Q = 0.0
-        for n, stream_flow_level in enumerate(self.levels):
-            T = self.transmissivity[n]
-            if level > stream_flow_level:
-                Q += 2 * T * C * (level - stream_flow_level)
-
-        return Q
+# -*- coding: utf-8 -*-
+from ._parameters import Parameter
+from ..core import Storage
+import numpy as np
+
+
+class KeatingStreamFlowParameter(Parameter):
+    """
+    A flow Parameter that returns stream flow from an Aquifer based on the groundwater level.
+
+    The approach is based on the lumped parameter model by Keating (1982). This Parameter calculates the
+     perennial and winterbourne stream flow components based on the Storage node. It requires the Storage
+     node to have a valid level Parameter.
+
+    In contrast to the Keating paper a general coefficient is provided for calibration. The Keating approach
+     utilised a coefficient of $B/L$ where "$B$ and $L$ are the dimensions of the aquifer block parallel
+     and perpendicular to the stream."
+
+    Keating, T. (1982), A Lumped Parameter Model of a Chalk Aquifer-Stream System in Hampshire,
+      United Kingdom. Ground Water, 20: 430–436. doi:10.1111/j.1745-6584.1982.tb02763.x
+    """
+    def __init__(self, model, storage_node, levels, transmissivity, coefficient=1.0, **kwargs):
+        """
+
+        :param storage_node:
+        :param levels:
+        :param transmissivity:
+        :param coefficient:
+        """
+        super(KeatingStreamFlowParameter, self).__init__(model, **kwargs)
+        self.storage_node = storage_node
+
+        if len(levels) != len(transmissivity):
+            raise ValueError('The number of transmissivity values must equal the number of levels.')
+
+        self.levels = np.array(levels)
+        self.transmissivity = np.array(transmissivity)
+        self.coefficient = coefficient
+
+    def value(self, ts, scenario_index):
+        # Get the current level of the aquifer
+        # TODO: this is a HACK - we can't use get_level/get_value as there is
+        #       no way to define the parent/child relationship
+        level = self.storage_node.level.value(ts, scenario_index)
+
+        # Coefficient
+        C = self.coefficient
+
+        # Calculate flow at each stream level
+        Q = 0.0
+        for n, stream_flow_level in enumerate(self.levels):
+            T = self.transmissivity[n]
+            if level > stream_flow_level:
+                Q += 2 * T * C * (level - stream_flow_level)
+
+        return Q
```

### Comparing `pywr-1.8.0/pywr/parameters/licenses.py` & `pywr-1.9.0/pywr/parameters/licenses.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,210 +1,210 @@
-#!/usr/bin/env python
-
-import calendar, datetime
-from ._parameters import Parameter as BaseParameter
-import numpy as np
-
-inf = float('inf')
-
-class License(BaseParameter):
-    """Base license class from which others inherit
-
-    This class should not be instantiated directly. Instead, use one of the
-    subclasses (e.g. DailyLicense).
-    """
-    def __new__(cls, *args, **kwargs):
-        if cls is License:
-            raise TypeError('License cannot be instantiated directly')
-        else:
-            return BaseParameter.__new__(cls)
-
-    def __init__(self, model, node, **kwargs):
-        super(License, self).__init__(model, **kwargs)
-        self._node = node
-
-    def resource_state(self, timestep):
-        raise NotImplementedError()
-
-
-class TimestepLicense(License):
-    """License limiting volume for a single timestep
-
-    This is the simplest kind of license. The volume available each timestep
-    is a fixed value. There is no resource state, as use today does not
-    impact availability tomorrow.
-    """
-    def __init__(self, model, node, amount, **kwargs):
-        """Initialise a new TimestepLicense
-
-        Parameters
-        ----------
-        amount : float
-            The maximum volume available in each timestep
-        """
-        super(TimestepLicense, self).__init__(model, node, **kwargs)
-        self._amount = amount
-
-    def value(self, timestep, scenario_index):
-        return self._amount
-
-    def resource_state(self, timestep):
-        return None
-TimestepLicense.register()
-
-
-# for now, assume a daily timestep
-# in the future this will need to be more clever
-class DailyLicense(TimestepLicense):
-    pass
-
-class StorageLicense(License):
-    def __init__(self, model, node, amount, **kwargs):
-        """A license with a volume to be spent over multiple timesteps
-
-        This class should not be instantiated directly. Instead, use one of the
-        subclasses such as AnnualLicense.
-
-        Parameters
-        ----------
-        amount : float
-            The volume of water available in each period
-        """
-        super(StorageLicense, self).__init__(model, node, **kwargs)
-        self._amount = amount
-
-    def setup(self):
-        super(StorageLicense, self).setup()
-        # Create a state array for the remaining licence volume.
-        self._remaining = np.ones(len(self.model.scenarios.combinations))*self._amount
-
-    def value(self, timestep, scenario_index):
-        return self._remaining[scenario_index.global_id]
-
-    def after(self):
-        timestep = self.model.timestepper.current
-        self._remaining -= self._node.flow * timestep.days
-        self._remaining[self._remaining < 0] = 0.0
-
-    def reset(self):
-        self._remaining[...] = self._amount
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        amount = data.pop("amount")
-        return cls(model, node, amount=amount, **data)
-
-StorageLicense.register()
-
-
-class AnnualLicense(StorageLicense):
-    """An annual license that apportions remaining volume equally for the rest of the year
-
-    value = (volume remaining) / (days remaining) * (timestep length)
-
-    Parameters
-    ----------
-    node : Node
-        The node that consumes the licence
-    amount : float
-        The total annual volume for this license
-    """
-    def __init__(self, *args, **kwargs):
-        super(AnnualLicense, self).__init__(*args, **kwargs)
-        # Record year ready to reset licence when the year changes.
-        self._prev_year = None
-
-    def value(self, timestep, scenario_index):
-        i = scenario_index.global_id
-        day_of_year = timestep.dayofyear
-        days_in_year = 365 + int(calendar.isleap(timestep.year))
-        if day_of_year == days_in_year:
-            return self._remaining[i]
-        else:
-            days_remaining = days_in_year - (day_of_year - 1)
-            return self._remaining[i] / days_remaining
-
-    def before(self):
-        # Reset licence if year changes.
-        timestep = self.model.timestepper.current
-        if self._prev_year != timestep.year:
-            self.reset()
-
-            # The number of days in the year before the first timestep of that year
-            days_before_reset = timestep.dayofyear - 1
-            # Adjust the license by the rate in previous timestep. This is needed for timesteps greater
-            # than 1 day where the license reset is not exactly on the anniversary
-            self._remaining[...] -= days_before_reset*self._node.prev_flow
-
-            self._prev_year = timestep.year
-AnnualLicense.register()
-
-
-class AnnualExponentialLicense(AnnualLicense):
-    """ An annual license that returns a value based on an exponential function of the license's current state.
-
-    The exponential function takes the form,
-
-    .. math::
-        f(t) = \mathit{max_value}e^{-x/k}
-
-    Where :math:`x` is the ratio of actual daily averaged remaining license (as calculated by AnnualLicense) to the
-    expected daily averaged remaining licence. I.e. if the license is on track the ratio is 1.0.
-    """
-    def __init__(self, model, node, amount, max_value, k=1.0, **kwargs):
-        """
-
-        Parameters
-        ----------
-        amount : float
-            The total annual volume for this license
-        max_value : float
-            The maximum value that can be returned. This is used to scale the exponential function
-        k : float
-            A scale factor for the exponent of the exponential function
-        """
-        super(AnnualExponentialLicense, self).__init__(model, node, amount, **kwargs)
-        self._max_value = max_value
-        self._k = k
-
-    def value(self, timestep, scenario_index):
-        remaining = super(AnnualExponentialLicense, self).value(timestep, scenario_index)
-        expected = self._amount / (365 + int(calendar.isleap(timestep.year)))
-        x = remaining / expected
-        return self._max_value * np.exp(-x / self._k)
-AnnualExponentialLicense.register()
-
-
-class AnnualHyperbolaLicense(AnnualLicense):
-    """ An annual license that returns a value based on an hyperbola (1/x) function of the license's current state.
-
-    The hyperbola function takes the form,
-
-    .. math::
-        f(t) = \mathit{value}/x
-
-    Where :math:`x` is the ratio of actual daily averaged remaining license (as calculated by AnnualLicense) to the
-    expected daily averaged remaining licence. I.e. if the license is on track the ratio is 1.0.
-    """
-    def __init__(self, model, node, amount, value, **kwargs):
-        """
-
-        Parameters
-        ----------
-        amount : float
-            The total annual volume for this license
-        value : float
-            The value used to scale the hyperbola function
-        """
-        super(AnnualHyperbolaLicense, self).__init__(model, node, amount, **kwargs)
-        self._value = value
-
-    def value(self, timestep, scenario_index):
-        remaining = super(AnnualHyperbolaLicense, self).value(timestep, scenario_index)
-        expected = self._amount / (365 + int(calendar.isleap(timestep.year)))
-        x = remaining / expected
-        try:
-            return self._value / x
-        except ZeroDivisionError:
-            return inf
-AnnualHyperbolaLicense.register()
+#!/usr/bin/env python
+
+import calendar, datetime
+from ._parameters import Parameter as BaseParameter
+import numpy as np
+
+inf = float('inf')
+
+class License(BaseParameter):
+    """Base license class from which others inherit
+
+    This class should not be instantiated directly. Instead, use one of the
+    subclasses (e.g. DailyLicense).
+    """
+    def __new__(cls, *args, **kwargs):
+        if cls is License:
+            raise TypeError('License cannot be instantiated directly')
+        else:
+            return BaseParameter.__new__(cls)
+
+    def __init__(self, model, node, **kwargs):
+        super(License, self).__init__(model, **kwargs)
+        self._node = node
+
+    def resource_state(self, timestep):
+        raise NotImplementedError()
+
+
+class TimestepLicense(License):
+    """License limiting volume for a single timestep
+
+    This is the simplest kind of license. The volume available each timestep
+    is a fixed value. There is no resource state, as use today does not
+    impact availability tomorrow.
+    """
+    def __init__(self, model, node, amount, **kwargs):
+        """Initialise a new TimestepLicense
+
+        Parameters
+        ----------
+        amount : float
+            The maximum volume available in each timestep
+        """
+        super(TimestepLicense, self).__init__(model, node, **kwargs)
+        self._amount = amount
+
+    def value(self, timestep, scenario_index):
+        return self._amount
+
+    def resource_state(self, timestep):
+        return None
+TimestepLicense.register()
+
+
+# for now, assume a daily timestep
+# in the future this will need to be more clever
+class DailyLicense(TimestepLicense):
+    pass
+
+class StorageLicense(License):
+    def __init__(self, model, node, amount, **kwargs):
+        """A license with a volume to be spent over multiple timesteps
+
+        This class should not be instantiated directly. Instead, use one of the
+        subclasses such as AnnualLicense.
+
+        Parameters
+        ----------
+        amount : float
+            The volume of water available in each period
+        """
+        super(StorageLicense, self).__init__(model, node, **kwargs)
+        self._amount = amount
+
+    def setup(self):
+        super(StorageLicense, self).setup()
+        # Create a state array for the remaining licence volume.
+        self._remaining = np.ones(len(self.model.scenarios.combinations))*self._amount
+
+    def value(self, timestep, scenario_index):
+        return self._remaining[scenario_index.global_id]
+
+    def after(self):
+        timestep = self.model.timestepper.current
+        self._remaining -= self._node.flow * timestep.days
+        self._remaining[self._remaining < 0] = 0.0
+
+    def reset(self):
+        self._remaining[...] = self._amount
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        amount = data.pop("amount")
+        return cls(model, node, amount=amount, **data)
+
+StorageLicense.register()
+
+
+class AnnualLicense(StorageLicense):
+    """An annual license that apportions remaining volume equally for the rest of the year
+
+    value = (volume remaining) / (days remaining) * (timestep length)
+
+    Parameters
+    ----------
+    node : Node
+        The node that consumes the licence
+    amount : float
+        The total annual volume for this license
+    """
+    def __init__(self, *args, **kwargs):
+        super(AnnualLicense, self).__init__(*args, **kwargs)
+        # Record year ready to reset licence when the year changes.
+        self._prev_year = None
+
+    def value(self, timestep, scenario_index):
+        i = scenario_index.global_id
+        day_of_year = timestep.dayofyear
+        days_in_year = 365 + int(calendar.isleap(timestep.year))
+        if day_of_year == days_in_year:
+            return self._remaining[i]
+        else:
+            days_remaining = days_in_year - (day_of_year - 1)
+            return self._remaining[i] / days_remaining
+
+    def before(self):
+        # Reset licence if year changes.
+        timestep = self.model.timestepper.current
+        if self._prev_year != timestep.year:
+            self.reset()
+
+            # The number of days in the year before the first timestep of that year
+            days_before_reset = timestep.dayofyear - 1
+            # Adjust the license by the rate in previous timestep. This is needed for timesteps greater
+            # than 1 day where the license reset is not exactly on the anniversary
+            self._remaining[...] -= days_before_reset*self._node.prev_flow
+
+            self._prev_year = timestep.year
+AnnualLicense.register()
+
+
+class AnnualExponentialLicense(AnnualLicense):
+    """ An annual license that returns a value based on an exponential function of the license's current state.
+
+    The exponential function takes the form,
+
+    .. math::
+        f(t) = \mathit{max_value}e^{-x/k}
+
+    Where :math:`x` is the ratio of actual daily averaged remaining license (as calculated by AnnualLicense) to the
+    expected daily averaged remaining licence. I.e. if the license is on track the ratio is 1.0.
+    """
+    def __init__(self, model, node, amount, max_value, k=1.0, **kwargs):
+        """
+
+        Parameters
+        ----------
+        amount : float
+            The total annual volume for this license
+        max_value : float
+            The maximum value that can be returned. This is used to scale the exponential function
+        k : float
+            A scale factor for the exponent of the exponential function
+        """
+        super(AnnualExponentialLicense, self).__init__(model, node, amount, **kwargs)
+        self._max_value = max_value
+        self._k = k
+
+    def value(self, timestep, scenario_index):
+        remaining = super(AnnualExponentialLicense, self).value(timestep, scenario_index)
+        expected = self._amount / (365 + int(calendar.isleap(timestep.year)))
+        x = remaining / expected
+        return self._max_value * np.exp(-x / self._k)
+AnnualExponentialLicense.register()
+
+
+class AnnualHyperbolaLicense(AnnualLicense):
+    """ An annual license that returns a value based on an hyperbola (1/x) function of the license's current state.
+
+    The hyperbola function takes the form,
+
+    .. math::
+        f(t) = \mathit{value}/x
+
+    Where :math:`x` is the ratio of actual daily averaged remaining license (as calculated by AnnualLicense) to the
+    expected daily averaged remaining licence. I.e. if the license is on track the ratio is 1.0.
+    """
+    def __init__(self, model, node, amount, value, **kwargs):
+        """
+
+        Parameters
+        ----------
+        amount : float
+            The total annual volume for this license
+        value : float
+            The value used to scale the hyperbola function
+        """
+        super(AnnualHyperbolaLicense, self).__init__(model, node, amount, **kwargs)
+        self._value = value
+
+    def value(self, timestep, scenario_index):
+        remaining = super(AnnualHyperbolaLicense, self).value(timestep, scenario_index)
+        expected = self._amount / (365 + int(calendar.isleap(timestep.year)))
+        x = remaining / expected
+        try:
+            return self._value / x
+        except ZeroDivisionError:
+            return inf
+AnnualHyperbolaLicense.register()
```

### Comparing `pywr-1.8.0/pywr/parameters/parameters.py` & `pywr-1.9.0/pywr/parameters/parameters.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,323 +1,323 @@
-import os
-import datetime
-from ..parameter_property import parameter_property
-from ._parameters import (
-    Parameter, parameter_registry, UnutilisedDataWarning, ConstantParameter,
-    ConstantScenarioParameter, ConstantScenarioIndexParameter, AnnualHarmonicSeriesParameter,
-    ArrayIndexedParameter, ConstantScenarioParameter, IndexedArrayParameter,
-    ArrayIndexedScenarioMonthlyFactorsParameter, TablesArrayParameter,
-    DailyProfileParameter, MonthlyProfileParameter, WeeklyProfileParameter,
-    ArrayIndexedScenarioParameter, ScenarioMonthlyProfileParameter, ScenarioDailyProfileParameter,
-    ScenarioWeeklyProfileParameter, align_and_resample_dataframe, DataFrameParameter,
-    IndexParameter, AggregatedParameter, AggregatedIndexParameter, PiecewiseIntegralParameter,
-    DiscountFactorParameter, NegativeParameter, MaxParameter, NegativeMaxParameter, MinParameter,
-    NegativeMinParameter, DeficitParameter, DivisionParameter, FlowDelayParameter, OffsetParameter,
-    RbfProfileParameter, UniformDrawdownProfileParameter, load_parameter, load_parameter_values, load_dataframe)
-
-from . import licenses
-from ._polynomial import Polynomial1DParameter, Polynomial2DStorageParameter
-from ._thresholds import (
-    AbstractThresholdParameter, StorageThresholdParameter, NodeThresholdParameter, ParameterThresholdParameter,
-    RecorderThresholdParameter, CurrentYearThresholdParameter, CurrentOrdinalDayThresholdParameter
-)
-from ._hydropower import HydropowerTargetParameter
-import numpy as np
-from scipy.interpolate import interp1d
-from scipy.integrate import quad
-import pandas
-
-
-class FunctionParameter(Parameter):
-    def __init__(self, model, parent, func, *args, **kwargs):
-        super(FunctionParameter, self).__init__(model, *args, **kwargs)
-        self._parent = parent
-        self._func = func
-
-    def value(self, ts, scenario_index):
-        return self._func(self._parent, ts, scenario_index)
-FunctionParameter.register()
-
-
-class ScaledProfileParameter(Parameter):
-    def __init__(self, model, scale, profile, *args, **kwargs):
-        super(ScaledProfileParameter, self).__init__(model, *args, **kwargs)
-        self.scale = scale
-
-        profile.parents.add(self)
-        self.profile = profile
-
-    @classmethod
-    def load(cls, model, data):
-        scale = float(data.pop("scale"))
-        profile = load_parameter(model, data.pop("profile"))
-        return cls(model, scale, profile, **data)
-
-    def value(self, ts, si):
-        p = self.profile.get_value(si)
-        return self.scale * p
-ScaledProfileParameter.register()
-
-
-class AbstractInterpolatedParameter(Parameter):
-    def __init__(self, model, x, y, interp_kwargs=None, **kwargs):
-        super(AbstractInterpolatedParameter, self).__init__(model, **kwargs)
-        self.x = x
-        self.y = y
-        self.interp = None
-        default_interp_kwargs = dict(kind='linear', bounds_error=True)
-        if interp_kwargs is not None:
-            # Overwrite or add to defaults with given values
-            default_interp_kwargs.update(interp_kwargs)
-        self.interp_kwargs = default_interp_kwargs
-
-    def _value_to_interpolate(self, ts, scenario_index):
-        raise NotImplementedError()
-
-    def setup(self):
-        super(AbstractInterpolatedParameter, self).setup()
-        self.interp = interp1d(self.x, self.y, **self.interp_kwargs)
-
-    def value(self, ts, scenario_index):
-        v = self._value_to_interpolate(ts, scenario_index)
-        return self.interp(v)
-
-
-class InterpolatedParameter(AbstractInterpolatedParameter):
-    """
-    Parameter value is equal to the interpolation of another parameter
-
-    Example
-    -------
-    >>> x = [0, 5, 10, 20]
-    >>> y = [0, 10, 30, -5]
-    >>> p1 = ConstantParameter(model, 9.3) # or something more interesting
-    >>> p2 = InterpolatedParameter(model, p1, x, y, interp_kwargs={"kind": "linear"})
-    """
-    def __init__(self, model, parameter, x, y, interp_kwargs=None, **kwargs):
-        super(InterpolatedParameter, self).__init__(model, x, y, interp_kwargs, **kwargs)
-        self._parameter = None
-        self.parameter = parameter
-
-    parameter = parameter_property("_parameter")
-
-    def _value_to_interpolate(self, ts, scenario_index):
-        return self._parameter.get_value(scenario_index)
-
-    @classmethod
-    def load(cls, model, data):
-        parameter = load_parameter(model, data.pop("parameter"))
-        x = np.array(data.pop("x"))
-        y = np.array(data.pop("y"))
-        kind = data.pop("kind", "linear")
-        return cls(model, parameter, x, y, interp_kwargs={'kind': kind})
-InterpolatedParameter.register()
-
-
-class InterpolatedVolumeParameter(AbstractInterpolatedParameter):
-    """
-    Generic interpolation parameter calculated from current volume
-
-    Parameters
-    ----------
-    node: Node
-        Storage node to provide input volume values to interpolation calculation
-    volumes: array_like
-        x coordinates of the data points for interpolation.
-    values : array_like
-        y coordinates of the data points for interpolation.
-    interp_kwargs : dict
-        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
-        for interpolation.
-    """
-    def __init__(self, model, node, volumes, values, interp_kwargs=None, **kwargs):
-        super(InterpolatedVolumeParameter, self).__init__(model, volumes, values, interp_kwargs, **kwargs)
-        self._node = node
-
-    def _value_to_interpolate(self, ts, scenario_index):
-        return self._node.volume[scenario_index.global_id]
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        volumes = np.array(data.pop("volumes"))
-        values = np.array(data.pop("values"))
-        kind = data.pop("kind", "linear")
-        return cls(model, node, volumes, values, interp_kwargs={'kind': kind})
-InterpolatedVolumeParameter.register()
-
-
-class InterpolatedFlowParameter(AbstractInterpolatedParameter):
-    """
-    Generic interpolation parameter that uses a node's flow at the previous time-step for interpolation.
-
-    Parameters
-    ----------
-    node: Node
-        Node to provide input flow values to interpolation caluculation
-    flows: array_like
-        x coordinates of the data points for interpolation.
-    values : array_like
-        y coordinates of the data points for interpolation.
-    interp_kwargs : dict
-        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
-        for interpolation.
-    """
-    def __init__(self, model, node, flows, values, interp_kwargs=None, **kwargs):
-        super().__init__(model, flows, values, interp_kwargs, **kwargs)
-        self._node = node
-
-    def _value_to_interpolate(self, ts, scenario_index):
-        return self._node.prev_flow[scenario_index.global_id]
-
-    @classmethod
-    def load(cls, model, data):
-        node = model._get_node_from_ref(model, data.pop("node"))
-        flows = np.array(data.pop("flows"))
-        values = np.array(data.pop("values"))
-        kind = data.pop("kind", "linear")
-        return cls(model, node, flows, values, interp_kwargs={'kind': kind})
-InterpolatedFlowParameter.register()
-
-
-class InterpolatedQuadratureParameter(AbstractInterpolatedParameter):
-    """Parameter value is equal to the quadrature of the interpolation of another parameter
-
-    Parameters
-    ----------
-    upper_parameter : Parameter
-        Upper value of the interpolated interval to integrate over.
-    x : array_like
-        x coordinates of the data points for interpolation.
-    y : array_like
-        y coordinates of the data points for interpolation.
-    lower_parameter : Parameter or None
-        Lower value of the interpolated interval to integrate over. Can be `None` in which
-        case the lower value of interval is zero.
-    interp_kwargs : dict
-        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
-        for interpolation.
-
-    Example
-    -------
-    >>> x = [0, 5, 10, 20]
-    >>> y = [0, 10, 30, -5]
-    >>> p1 = ConstantParameter(model, 9.3) # or something more interesting
-    >>> p2 = InterpolatedQuadratureParameter(model, p1, x, y, interp_kwargs={"kind": "linear"})
-    """
-    def __init__(self, model, upper_parameter, x, y, lower_parameter=None, interp_kwargs=None, **kwargs):
-        super().__init__(model, x, y, interp_kwargs, **kwargs)
-        self._upper_parameter = None
-        self.upper_parameter = upper_parameter
-        self._lower_parameter = None
-        self.lower_parameter = lower_parameter
-
-    upper_parameter = parameter_property("_upper_parameter")
-    lower_parameter = parameter_property("_lower_parameter")
-
-    def _value_to_interpolate(self, ts, scenario_index):
-        return self._upper_parameter.get_value(scenario_index)
-
-    def value(self, ts, scenario_index):
-        a = 0
-        if self._lower_parameter is not None:
-            a = self._lower_parameter.get_value(scenario_index)
-        b = self._value_to_interpolate(ts, scenario_index)
-
-        cost, err = quad(self.interp, a, b)
-        return cost
-
-    @classmethod
-    def load(cls, model, data):
-        upper_parameter = load_parameter(model, data.pop("upper_parameter"))
-        lower_parameter = load_parameter(model, data.pop("lower_parameter", None))
-        x = np.array(data.pop("x"))
-        y = np.array(data.pop("y"))
-        kind = data.pop("kind", "linear")
-        return cls(model, upper_parameter, x, y, lower_parameter=lower_parameter,
-                   interp_kwargs={'kind': kind})
-InterpolatedQuadratureParameter.register()
-
-
-class ScenarioWrapperParameter(Parameter):
-    """Parameter that utilises a different child parameter in each scenario ensemble.
-
-    This parameter is used to switch between different child parameters based on different
-    ensembles in a given `Scenario`. It can be used to vary data in a non-scenario aware
-    parameter type across multiple scenario ensembles. For example, many of control curve or
-    interpolation parameters do not explicitly support scenarios. This parameter can be used
-    to test multiple control curve definitions as part of a single simulation.
-
-    Parameters
-    ----------
-    scenario : Scenario
-        The scenario instance which is used to select the parameters.
-    parameters : iterable of Parameter instances
-        The child parameters that are used in each of `scenario`'s ensembles. The number
-        of parameters must equal the size of the given scenario.
-
-    """
-    def __init__(self, model, scenario, parameters, **kwargs):
-        super().__init__(model, **kwargs)
-        if scenario.size != len(parameters):
-            raise ValueError("The number of parameters must equal the size of the scenario.")
-        self.scenario = scenario
-        self.parameters = []
-        for p in parameters:
-            self.children.add(p)
-            self.parameters.append(p)
-        # Initialise internal attributes
-        self._scenario_index = None
-
-    def setup(self):
-        super().setup()
-        # This setup must find out the index of self._scenario in the model
-        # so that it can return the correct value in value()
-        self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
-
-    def value(self, ts, scenario_index):
-        # This is a bit confusing.
-        # scenario_indices contains the current scenario number for all
-        # the Scenario objects in the model run. We have cached the
-        # position of self._scenario in self._scenario_index to lookup the
-        # correct number to use in this instance.
-        parameter = self.parameters[scenario_index.indices[self._scenario_index]]
-        return parameter.get_value(scenario_index)
-
-    @classmethod
-    def load(cls, model, data):
-        scenario = model.scenarios[data.pop('scenario')]
-
-        parameters = [load_parameter(model, p) for p in data.pop('parameters')]
-        return cls(model, scenario, parameters, **data)
-ScenarioWrapperParameter.register()
-
-
-def pop_kwarg_parameter(kwargs, key, default):
-    """Pop a parameter from the keyword arguments dictionary
-
-    Parameters
-    ----------
-    kwargs : dict
-        A keyword arguments dictionary
-    key : string
-        The argument name, e.g. 'flow'
-    default : object
-        The default value to use if the dictionary does not have that key
-
-    Returns a Parameter
-    """
-    value = kwargs.pop(key, default)
-    if isinstance(value, Parameter):
-        return value
-    elif callable(value):
-        # TODO this is broken?
-        return FunctionParameter(self, value)
-    else:
-        return value
-
-
-class PropertiesDict(dict):
-    def __setitem__(self, key, value):
-        if not isinstance(value, Property):
-            value = ConstantParameter(value)
-        dict.__setitem__(self, key, value)
+import os
+import datetime
+from ..parameter_property import parameter_property
+from ._parameters import (
+    Parameter, parameter_registry, UnutilisedDataWarning, ConstantParameter,
+    ConstantScenarioParameter, ConstantScenarioIndexParameter, AnnualHarmonicSeriesParameter,
+    ArrayIndexedParameter, ConstantScenarioParameter, IndexedArrayParameter,
+    ArrayIndexedScenarioMonthlyFactorsParameter, TablesArrayParameter,
+    DailyProfileParameter, MonthlyProfileParameter, WeeklyProfileParameter,
+    ArrayIndexedScenarioParameter, ScenarioMonthlyProfileParameter, ScenarioDailyProfileParameter,
+    ScenarioWeeklyProfileParameter, align_and_resample_dataframe, DataFrameParameter,
+    IndexParameter, AggregatedParameter, AggregatedIndexParameter, PiecewiseIntegralParameter,
+    DiscountFactorParameter, NegativeParameter, MaxParameter, NegativeMaxParameter, MinParameter,
+    NegativeMinParameter, DeficitParameter, DivisionParameter, FlowDelayParameter, OffsetParameter,
+    RbfProfileParameter, UniformDrawdownProfileParameter, load_parameter, load_parameter_values, load_dataframe)
+
+from . import licenses
+from ._polynomial import Polynomial1DParameter, Polynomial2DStorageParameter
+from ._thresholds import (
+    AbstractThresholdParameter, StorageThresholdParameter, NodeThresholdParameter, ParameterThresholdParameter,
+    RecorderThresholdParameter, CurrentYearThresholdParameter, CurrentOrdinalDayThresholdParameter
+)
+from ._hydropower import HydropowerTargetParameter
+import numpy as np
+from scipy.interpolate import interp1d
+from scipy.integrate import quad
+import pandas
+
+
+class FunctionParameter(Parameter):
+    def __init__(self, model, parent, func, *args, **kwargs):
+        super(FunctionParameter, self).__init__(model, *args, **kwargs)
+        self._parent = parent
+        self._func = func
+
+    def value(self, ts, scenario_index):
+        return self._func(self._parent, ts, scenario_index)
+FunctionParameter.register()
+
+
+class ScaledProfileParameter(Parameter):
+    def __init__(self, model, scale, profile, *args, **kwargs):
+        super(ScaledProfileParameter, self).__init__(model, *args, **kwargs)
+        self.scale = scale
+
+        profile.parents.add(self)
+        self.profile = profile
+
+    @classmethod
+    def load(cls, model, data):
+        scale = float(data.pop("scale"))
+        profile = load_parameter(model, data.pop("profile"))
+        return cls(model, scale, profile, **data)
+
+    def value(self, ts, si):
+        p = self.profile.get_value(si)
+        return self.scale * p
+ScaledProfileParameter.register()
+
+
+class AbstractInterpolatedParameter(Parameter):
+    def __init__(self, model, x, y, interp_kwargs=None, **kwargs):
+        super(AbstractInterpolatedParameter, self).__init__(model, **kwargs)
+        self.x = x
+        self.y = y
+        self.interp = None
+        default_interp_kwargs = dict(kind='linear', bounds_error=True)
+        if interp_kwargs is not None:
+            # Overwrite or add to defaults with given values
+            default_interp_kwargs.update(interp_kwargs)
+        self.interp_kwargs = default_interp_kwargs
+
+    def _value_to_interpolate(self, ts, scenario_index):
+        raise NotImplementedError()
+
+    def setup(self):
+        super(AbstractInterpolatedParameter, self).setup()
+        self.interp = interp1d(self.x, self.y, **self.interp_kwargs)
+
+    def value(self, ts, scenario_index):
+        v = self._value_to_interpolate(ts, scenario_index)
+        return self.interp(v)
+
+
+class InterpolatedParameter(AbstractInterpolatedParameter):
+    """
+    Parameter value is equal to the interpolation of another parameter
+
+    Example
+    -------
+    >>> x = [0, 5, 10, 20]
+    >>> y = [0, 10, 30, -5]
+    >>> p1 = ConstantParameter(model, 9.3) # or something more interesting
+    >>> p2 = InterpolatedParameter(model, p1, x, y, interp_kwargs={"kind": "linear"})
+    """
+    def __init__(self, model, parameter, x, y, interp_kwargs=None, **kwargs):
+        super(InterpolatedParameter, self).__init__(model, x, y, interp_kwargs, **kwargs)
+        self._parameter = None
+        self.parameter = parameter
+
+    parameter = parameter_property("_parameter")
+
+    def _value_to_interpolate(self, ts, scenario_index):
+        return self._parameter.get_value(scenario_index)
+
+    @classmethod
+    def load(cls, model, data):
+        parameter = load_parameter(model, data.pop("parameter"))
+        x = np.array(data.pop("x"))
+        y = np.array(data.pop("y"))
+        kind = data.pop("kind", "linear")
+        return cls(model, parameter, x, y, interp_kwargs={'kind': kind})
+InterpolatedParameter.register()
+
+
+class InterpolatedVolumeParameter(AbstractInterpolatedParameter):
+    """
+    Generic interpolation parameter calculated from current volume
+
+    Parameters
+    ----------
+    node: Node
+        Storage node to provide input volume values to interpolation calculation
+    volumes: array_like
+        x coordinates of the data points for interpolation.
+    values : array_like
+        y coordinates of the data points for interpolation.
+    interp_kwargs : dict
+        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
+        for interpolation.
+    """
+    def __init__(self, model, node, volumes, values, interp_kwargs=None, **kwargs):
+        super(InterpolatedVolumeParameter, self).__init__(model, volumes, values, interp_kwargs, **kwargs)
+        self._node = node
+
+    def _value_to_interpolate(self, ts, scenario_index):
+        return self._node.volume[scenario_index.global_id]
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        volumes = np.array(data.pop("volumes"))
+        values = np.array(data.pop("values"))
+        kind = data.pop("kind", "linear")
+        return cls(model, node, volumes, values, interp_kwargs={'kind': kind})
+InterpolatedVolumeParameter.register()
+
+
+class InterpolatedFlowParameter(AbstractInterpolatedParameter):
+    """
+    Generic interpolation parameter that uses a node's flow at the previous time-step for interpolation.
+
+    Parameters
+    ----------
+    node: Node
+        Node to provide input flow values to interpolation caluculation
+    flows: array_like
+        x coordinates of the data points for interpolation.
+    values : array_like
+        y coordinates of the data points for interpolation.
+    interp_kwargs : dict
+        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
+        for interpolation.
+    """
+    def __init__(self, model, node, flows, values, interp_kwargs=None, **kwargs):
+        super().__init__(model, flows, values, interp_kwargs, **kwargs)
+        self._node = node
+
+    def _value_to_interpolate(self, ts, scenario_index):
+        return self._node.prev_flow[scenario_index.global_id]
+
+    @classmethod
+    def load(cls, model, data):
+        node = model._get_node_from_ref(model, data.pop("node"))
+        flows = np.array(data.pop("flows"))
+        values = np.array(data.pop("values"))
+        kind = data.pop("kind", "linear")
+        return cls(model, node, flows, values, interp_kwargs={'kind': kind})
+InterpolatedFlowParameter.register()
+
+
+class InterpolatedQuadratureParameter(AbstractInterpolatedParameter):
+    """Parameter value is equal to the quadrature of the interpolation of another parameter
+
+    Parameters
+    ----------
+    upper_parameter : Parameter
+        Upper value of the interpolated interval to integrate over.
+    x : array_like
+        x coordinates of the data points for interpolation.
+    y : array_like
+        y coordinates of the data points for interpolation.
+    lower_parameter : Parameter or None
+        Lower value of the interpolated interval to integrate over. Can be `None` in which
+        case the lower value of interval is zero.
+    interp_kwargs : dict
+        Dictionary of keyword arguments to pass to `scipy.interpolate.interp1d` class and used
+        for interpolation.
+
+    Example
+    -------
+    >>> x = [0, 5, 10, 20]
+    >>> y = [0, 10, 30, -5]
+    >>> p1 = ConstantParameter(model, 9.3) # or something more interesting
+    >>> p2 = InterpolatedQuadratureParameter(model, p1, x, y, interp_kwargs={"kind": "linear"})
+    """
+    def __init__(self, model, upper_parameter, x, y, lower_parameter=None, interp_kwargs=None, **kwargs):
+        super().__init__(model, x, y, interp_kwargs, **kwargs)
+        self._upper_parameter = None
+        self.upper_parameter = upper_parameter
+        self._lower_parameter = None
+        self.lower_parameter = lower_parameter
+
+    upper_parameter = parameter_property("_upper_parameter")
+    lower_parameter = parameter_property("_lower_parameter")
+
+    def _value_to_interpolate(self, ts, scenario_index):
+        return self._upper_parameter.get_value(scenario_index)
+
+    def value(self, ts, scenario_index):
+        a = 0
+        if self._lower_parameter is not None:
+            a = self._lower_parameter.get_value(scenario_index)
+        b = self._value_to_interpolate(ts, scenario_index)
+
+        cost, err = quad(self.interp, a, b)
+        return cost
+
+    @classmethod
+    def load(cls, model, data):
+        upper_parameter = load_parameter(model, data.pop("upper_parameter"))
+        lower_parameter = load_parameter(model, data.pop("lower_parameter", None))
+        x = np.array(data.pop("x"))
+        y = np.array(data.pop("y"))
+        kind = data.pop("kind", "linear")
+        return cls(model, upper_parameter, x, y, lower_parameter=lower_parameter,
+                   interp_kwargs={'kind': kind})
+InterpolatedQuadratureParameter.register()
+
+
+class ScenarioWrapperParameter(Parameter):
+    """Parameter that utilises a different child parameter in each scenario ensemble.
+
+    This parameter is used to switch between different child parameters based on different
+    ensembles in a given `Scenario`. It can be used to vary data in a non-scenario aware
+    parameter type across multiple scenario ensembles. For example, many of control curve or
+    interpolation parameters do not explicitly support scenarios. This parameter can be used
+    to test multiple control curve definitions as part of a single simulation.
+
+    Parameters
+    ----------
+    scenario : Scenario
+        The scenario instance which is used to select the parameters.
+    parameters : iterable of Parameter instances
+        The child parameters that are used in each of `scenario`'s ensembles. The number
+        of parameters must equal the size of the given scenario.
+
+    """
+    def __init__(self, model, scenario, parameters, **kwargs):
+        super().__init__(model, **kwargs)
+        if scenario.size != len(parameters):
+            raise ValueError("The number of parameters must equal the size of the scenario.")
+        self.scenario = scenario
+        self.parameters = []
+        for p in parameters:
+            self.children.add(p)
+            self.parameters.append(p)
+        # Initialise internal attributes
+        self._scenario_index = None
+
+    def setup(self):
+        super().setup()
+        # This setup must find out the index of self._scenario in the model
+        # so that it can return the correct value in value()
+        self._scenario_index = self.model.scenarios.get_scenario_index(self.scenario)
+
+    def value(self, ts, scenario_index):
+        # This is a bit confusing.
+        # scenario_indices contains the current scenario number for all
+        # the Scenario objects in the model run. We have cached the
+        # position of self._scenario in self._scenario_index to lookup the
+        # correct number to use in this instance.
+        parameter = self.parameters[scenario_index.indices[self._scenario_index]]
+        return parameter.get_value(scenario_index)
+
+    @classmethod
+    def load(cls, model, data):
+        scenario = model.scenarios[data.pop('scenario')]
+
+        parameters = [load_parameter(model, p) for p in data.pop('parameters')]
+        return cls(model, scenario, parameters, **data)
+ScenarioWrapperParameter.register()
+
+
+def pop_kwarg_parameter(kwargs, key, default):
+    """Pop a parameter from the keyword arguments dictionary
+
+    Parameters
+    ----------
+    kwargs : dict
+        A keyword arguments dictionary
+    key : string
+        The argument name, e.g. 'flow'
+    default : object
+        The default value to use if the dictionary does not have that key
+
+    Returns a Parameter
+    """
+    value = kwargs.pop(key, default)
+    if isinstance(value, Parameter):
+        return value
+    elif callable(value):
+        # TODO this is broken?
+        return FunctionParameter(self, value)
+    else:
+        return value
+
+
+class PropertiesDict(dict):
+    def __setitem__(self, key, value):
+        if not isinstance(value, Property):
+            value = ConstantParameter(value)
+        dict.__setitem__(self, key, value)
```

### Comparing `pywr-1.8.0/pywr/recorders/_hydropower.pxd` & `pywr-1.9.0/pywr/recorders/_hydropower.pxd`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-from ._recorders cimport NumpyArrayNodeRecorder, BaseConstantNodeRecorder
-from pywr.parameters._parameters cimport Parameter
-from .._core cimport Timestep, Scenario, ScenarioIndex
-
-
-cdef class HydropowerRecorder(NumpyArrayNodeRecorder):
-    cdef Parameter _water_elevation_parameter
-    cdef public double turbine_elevation
-    cdef public double flow_unit_conversion
-    cdef public double energy_unit_conversion
-    cdef public double density
-    cdef public double efficiency
-
-
-cdef class TotalHydroEnergyRecorder(BaseConstantNodeRecorder):
-    cdef Parameter _water_elevation_parameter
-    cdef public double turbine_elevation
-    cdef public double flow_unit_conversion
-    cdef public double energy_unit_conversion
-    cdef public double density
+from ._recorders cimport NumpyArrayNodeRecorder, BaseConstantNodeRecorder
+from pywr.parameters._parameters cimport Parameter
+from .._core cimport Timestep, Scenario, ScenarioIndex
+
+
+cdef class HydropowerRecorder(NumpyArrayNodeRecorder):
+    cdef Parameter _water_elevation_parameter
+    cdef public double turbine_elevation
+    cdef public double flow_unit_conversion
+    cdef public double energy_unit_conversion
+    cdef public double density
+    cdef public double efficiency
+
+
+cdef class TotalHydroEnergyRecorder(BaseConstantNodeRecorder):
+    cdef Parameter _water_elevation_parameter
+    cdef public double turbine_elevation
+    cdef public double flow_unit_conversion
+    cdef public double energy_unit_conversion
+    cdef public double density
     cdef public double efficiency
```

### Comparing `pywr-1.8.0/pywr/recorders/_hydropower.pyx` & `pywr-1.9.0/pywr/recorders/_hydropower.pyx`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,271 +1,271 @@
-from ._recorders import NumpyArrayNodeRecorder
-
-
-cpdef double hydropower_calculation(double flow, double water_elevation, double turbine_elevation, double efficiency,
-                                    double flow_unit_conversion=1.0, double energy_unit_conversion=1e-6,
-                                    double density=1000.0):
-    """
-    Calculate the total power produced using the hydropower equation.
-    
-   
-    Parameters
-    ----------
-    flow : double 
-        Flow rate of water through the turbine. Should be converted using `flow_unit_conversion` to 
-        units of $m^3£ per day (not per second).
-    water_elevation : double
-        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
-        the working head of the turbine.
-    turbine_elevation : double
-        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
-        the working head of the turbine.
-    efficiency : double
-        An efficiency scaling factor for the power output of the turbine.
-    flow_unit_conversion : double (default=1.0)
-        A factor used to transform the units of flow to be compatible with the equation here. This
-        should convert flow to units of $m^3/day$
-    energy_unit_conversion : double (default=1e-6)
-        A factor used to transform the units of power. Defaults to 1e-6 to return $MJ$/day. 
-    density : double (default=1000)
-        Density of water in $kgm^{-3}$.
-        
-    Returns
-    -------
-    power : double
-        Hydropower production rate in units of energy per day.
-    
-    Notes
-    -----
-    The hydropower calculation uses the following equation.
-    
-    .. math:: P = \rho * g * \deltaH * q
-    
-    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.    
-    
-    """
-    cdef double head
-    cdef double power
-    cdef double q
-
-    head = water_elevation - turbine_elevation
-    if head < 0.0:
-        head = 0.0
-
-    # Convert flow to correct units (typically to m3/day)
-    q = flow * flow_unit_conversion
-    # Power
-    power = density * q * 9.81 * head * efficiency
-
-    return power * energy_unit_conversion
-
-
-cdef class HydropowerRecorder(NumpyArrayNodeRecorder):
-    """ Calculates the power production using the hydropower equation
-
-    This recorder saves an array of the hydrpower production in each timestep. It can be converted to a dataframe
-    after a model run has completed. It does not calculate total energy production.
-
-    Parameters
-    ----------
-
-    water_elevation_parameter : Parameter instance (default=None)
-        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
-        the working head of the turbine.
-    turbine_elevation : double
-        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
-        the working head of the turbine.
-    efficiency : float (default=1.0)
-        The efficiency of the turbine.
-    density : float (default=1000.0)
-        The density of water.
-    flow_unit_conversion : float (default=1.0)
-        A factor used to transform the units of flow to be compatible with the equation here. This
-        should convert flow to units of :math:`m^3/day`
-    energy_unit_conversion : float (default=1e-6)
-        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
-
-    Notes
-    -----
-    The hydropower calculation uses the following equation.
-
-    .. math:: P = \\rho * g * \\delta H * q
-
-    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.
-
-    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
-    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
-    is `None` then the head is simply the turbine elevation.
-
-
-    See Also
-    --------
-    TotalHydroEnergyRecorder
-    pywr.parameters.HydropowerTargetParameter
-
-    """
-    def __init__(self, model, node, water_elevation_parameter=None, turbine_elevation=0.0, efficiency=1.0, density=1000,
-                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
-        super(HydropowerRecorder, self).__init__(model, node, **kwargs)
-
-        self.water_elevation_parameter = water_elevation_parameter
-        self.turbine_elevation = turbine_elevation
-        self.efficiency = efficiency
-        self.density = density
-        self.flow_unit_conversion = flow_unit_conversion
-        self.energy_unit_conversion = energy_unit_conversion
-
-    property water_elevation_parameter:
-        def __get__(self):
-            return self._water_elevation_parameter
-        def __set__(self, parameter):
-            if self._water_elevation_parameter:
-                self.children.remove(self._water_elevation_parameter)
-            self.children.add(parameter)
-            self._water_elevation_parameter = parameter        
-
-    cpdef after(self):
-        cdef int i
-        cdef double q, head, power
-        cdef Timestep ts = self.model.timestepper.current
-        cdef ScenarioIndex scenario_index
-        flow = self.node.flow
-
-        for scenario_index in self.model.scenarios.combinations:
-
-            if self._water_elevation_parameter is not None:
-                head = self._water_elevation_parameter.get_value(scenario_index)
-                if self.turbine_elevation is not None:
-                    head -= self.turbine_elevation
-            elif self.turbine_elevation is not None:
-                head = self.turbine_elevation
-            else:
-                raise ValueError('One or both of storage_node or level must be set.')
-
-            # -ve head is not valid
-            head = max(head, 0.0)
-            # Get the flow from the current node
-            q = self._node._flow[scenario_index.global_id]
-            power = hydropower_calculation(q, head, 0.0, self.efficiency, density=self.density,
-                                             flow_unit_conversion=self.flow_unit_conversion,
-                                             energy_unit_conversion=self.energy_unit_conversion)
-
-            self._data[ts.index, scenario_index.global_id] = power
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.parameters import load_parameter
-        node = model._get_node_from_ref(model, data.pop("node"))
-        if "water_elevation_parameter" in data:
-            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
-        else:
-            water_elevation_parameter = None
-
-        return cls(model, node, water_elevation_parameter=water_elevation_parameter, **data)
-HydropowerRecorder.register()
-
-
-cdef class TotalHydroEnergyRecorder(BaseConstantNodeRecorder):
-    """ Calculates the total energy production using the hydropower equation from a model run.
-
-    This recorder saves the total energy production in each scenario during a model run. It does not save a timeseries
-    or power, but rather total energy.
-
-    Parameters
-    ----------
-
-    water_elevation_parameter : Parameter instance (default=None)
-        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
-        the working head of the turbine.
-    turbine_elevation : double
-        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
-        the working head of the turbine.
-    efficiency : float (default=1.0)
-        The efficiency of the turbine.
-    density : float (default=1000.0)
-        The density of water.
-    flow_unit_conversion : float (default=1.0)
-        A factor used to transform the units of flow to be compatible with the equation here. This
-        should convert flow to units of :math:`m^3/day`
-    energy_unit_conversion : float (default=1e-6)
-        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
-
-    Notes
-    -----
-    The hydropower calculation uses the following equation.
-
-    .. math:: P = \\rho * g * \\delta H * q
-
-    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.
-
-    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
-    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
-    is `None` then the head is simply the turbine elevation.
-
-
-    See Also
-    --------
-    HydropowerRecorder
-    pywr.parameters.HydropowerTargetParameter
-
-    """
-    def __init__(self, model, node, water_elevation_parameter=None, turbine_elevation=0.0, efficiency=1.0, density=1000,
-                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
-        super(TotalHydroEnergyRecorder, self).__init__(model, node, **kwargs)
-
-        self.water_elevation_parameter = water_elevation_parameter
-        self.turbine_elevation = turbine_elevation
-        self.efficiency = efficiency
-        self.density = density
-        self.flow_unit_conversion = flow_unit_conversion
-        self.energy_unit_conversion = energy_unit_conversion
-
-    property water_elevation_parameter:
-        def __get__(self):
-            return self._water_elevation_parameter
-        def __set__(self, parameter):
-            if self._water_elevation_parameter:
-                self.children.remove(self._water_elevation_parameter)
-            self.children.add(parameter)
-            self._water_elevation_parameter = parameter
-
-    cpdef after(self):
-        """ Calculate the  """
-        cdef int i
-        cdef double q, head, power
-        cdef Timestep ts = self.model.timestepper.current
-        cdef double days = ts.days
-        cdef ScenarioIndex scenario_index
-        flow = self.node.flow
-
-        for scenario_index in self.model.scenarios.combinations:
-
-            if self._water_elevation_parameter is not None:
-                head = self._water_elevation_parameter.get_value(scenario_index)
-                if self.turbine_elevation is not None:
-                    head -= self.turbine_elevation
-            elif self.turbine_elevation is not None:
-                head = self.turbine_elevation
-            else:
-                raise ValueError('One or both of storage_node or level must be set.')
-
-            # -ve head is not valid
-            head = max(head, 0.0)
-            # Get the flow from the current node
-            q = self._node._flow[scenario_index.global_id]
-            power = hydropower_calculation(q, head, 0.0, self.efficiency, density=self.density,
-                                             flow_unit_conversion=self.flow_unit_conversion,
-                                             energy_unit_conversion=self.energy_unit_conversion)
-
-            self._values[scenario_index.global_id] += power * days
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.parameters import load_parameter
-        node = model._get_node_from_ref(model, data.pop("node"))
-        if "water_elevation_parameter" in data:
-            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
-        else:
-            water_elevation_parameter = None
-
-        return cls(model, node, water_elevation_parameter=water_elevation_parameter, **data)
+from ._recorders import NumpyArrayNodeRecorder
+
+
+cpdef double hydropower_calculation(double flow, double water_elevation, double turbine_elevation, double efficiency,
+                                    double flow_unit_conversion=1.0, double energy_unit_conversion=1e-6,
+                                    double density=1000.0):
+    """
+    Calculate the total power produced using the hydropower equation.
+    
+   
+    Parameters
+    ----------
+    flow : double 
+        Flow rate of water through the turbine. Should be converted using `flow_unit_conversion` to 
+        units of $m^3£ per day (not per second).
+    water_elevation : double
+        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
+        the working head of the turbine.
+    turbine_elevation : double
+        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
+        the working head of the turbine.
+    efficiency : double
+        An efficiency scaling factor for the power output of the turbine.
+    flow_unit_conversion : double (default=1.0)
+        A factor used to transform the units of flow to be compatible with the equation here. This
+        should convert flow to units of $m^3/day$
+    energy_unit_conversion : double (default=1e-6)
+        A factor used to transform the units of power. Defaults to 1e-6 to return $MJ$/day. 
+    density : double (default=1000)
+        Density of water in $kgm^{-3}$.
+        
+    Returns
+    -------
+    power : double
+        Hydropower production rate in units of energy per day.
+    
+    Notes
+    -----
+    The hydropower calculation uses the following equation.
+    
+    .. math:: P = \rho * g * \deltaH * q
+    
+    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.    
+    
+    """
+    cdef double head
+    cdef double power
+    cdef double q
+
+    head = water_elevation - turbine_elevation
+    if head < 0.0:
+        head = 0.0
+
+    # Convert flow to correct units (typically to m3/day)
+    q = flow * flow_unit_conversion
+    # Power
+    power = density * q * 9.81 * head * efficiency
+
+    return power * energy_unit_conversion
+
+
+cdef class HydropowerRecorder(NumpyArrayNodeRecorder):
+    """ Calculates the power production using the hydropower equation
+
+    This recorder saves an array of the hydrpower production in each timestep. It can be converted to a dataframe
+    after a model run has completed. It does not calculate total energy production.
+
+    Parameters
+    ----------
+
+    water_elevation_parameter : Parameter instance (default=None)
+        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
+        the working head of the turbine.
+    turbine_elevation : double
+        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
+        the working head of the turbine.
+    efficiency : float (default=1.0)
+        The efficiency of the turbine.
+    density : float (default=1000.0)
+        The density of water.
+    flow_unit_conversion : float (default=1.0)
+        A factor used to transform the units of flow to be compatible with the equation here. This
+        should convert flow to units of :math:`m^3/day`
+    energy_unit_conversion : float (default=1e-6)
+        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
+
+    Notes
+    -----
+    The hydropower calculation uses the following equation.
+
+    .. math:: P = \\rho * g * \\delta H * q
+
+    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.
+
+    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
+    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
+    is `None` then the head is simply the turbine elevation.
+
+
+    See Also
+    --------
+    TotalHydroEnergyRecorder
+    pywr.parameters.HydropowerTargetParameter
+
+    """
+    def __init__(self, model, node, water_elevation_parameter=None, turbine_elevation=0.0, efficiency=1.0, density=1000,
+                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
+        super(HydropowerRecorder, self).__init__(model, node, **kwargs)
+
+        self.water_elevation_parameter = water_elevation_parameter
+        self.turbine_elevation = turbine_elevation
+        self.efficiency = efficiency
+        self.density = density
+        self.flow_unit_conversion = flow_unit_conversion
+        self.energy_unit_conversion = energy_unit_conversion
+
+    property water_elevation_parameter:
+        def __get__(self):
+            return self._water_elevation_parameter
+        def __set__(self, parameter):
+            if self._water_elevation_parameter:
+                self.children.remove(self._water_elevation_parameter)
+            self.children.add(parameter)
+            self._water_elevation_parameter = parameter        
+
+    cpdef after(self):
+        cdef int i
+        cdef double q, head, power
+        cdef Timestep ts = self.model.timestepper.current
+        cdef ScenarioIndex scenario_index
+        flow = self.node.flow
+
+        for scenario_index in self.model.scenarios.combinations:
+
+            if self._water_elevation_parameter is not None:
+                head = self._water_elevation_parameter.get_value(scenario_index)
+                if self.turbine_elevation is not None:
+                    head -= self.turbine_elevation
+            elif self.turbine_elevation is not None:
+                head = self.turbine_elevation
+            else:
+                raise ValueError('One or both of storage_node or level must be set.')
+
+            # -ve head is not valid
+            head = max(head, 0.0)
+            # Get the flow from the current node
+            q = self._node._flow[scenario_index.global_id]
+            power = hydropower_calculation(q, head, 0.0, self.efficiency, density=self.density,
+                                             flow_unit_conversion=self.flow_unit_conversion,
+                                             energy_unit_conversion=self.energy_unit_conversion)
+
+            self._data[ts.index, scenario_index.global_id] = power
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.parameters import load_parameter
+        node = model._get_node_from_ref(model, data.pop("node"))
+        if "water_elevation_parameter" in data:
+            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
+        else:
+            water_elevation_parameter = None
+
+        return cls(model, node, water_elevation_parameter=water_elevation_parameter, **data)
+HydropowerRecorder.register()
+
+
+cdef class TotalHydroEnergyRecorder(BaseConstantNodeRecorder):
+    """ Calculates the total energy production using the hydropower equation from a model run.
+
+    This recorder saves the total energy production in each scenario during a model run. It does not save a timeseries
+    or power, but rather total energy.
+
+    Parameters
+    ----------
+
+    water_elevation_parameter : Parameter instance (default=None)
+        Elevation of water entering the turbine. The difference of this value with the `turbine_elevation` gives
+        the working head of the turbine.
+    turbine_elevation : double
+        Elevation of the turbine itself. The difference between the `water_elevation` and this value gives
+        the working head of the turbine.
+    efficiency : float (default=1.0)
+        The efficiency of the turbine.
+    density : float (default=1000.0)
+        The density of water.
+    flow_unit_conversion : float (default=1.0)
+        A factor used to transform the units of flow to be compatible with the equation here. This
+        should convert flow to units of :math:`m^3/day`
+    energy_unit_conversion : float (default=1e-6)
+        A factor used to transform the units of total energy. Defaults to 1e-6 to return :math:`MJ`.
+
+    Notes
+    -----
+    The hydropower calculation uses the following equation.
+
+    .. math:: P = \\rho * g * \\delta H * q
+
+    The flow rate in should be converted to units of :math:`m^3` per day using the `flow_unit_conversion` parameter.
+
+    Head is calculated from the given `water_elevation_parameter` and `turbine_elevation` value. If water elevation
+    is given then head is the difference in elevation between the water and the turbine. If water elevation parameter
+    is `None` then the head is simply the turbine elevation.
+
+
+    See Also
+    --------
+    HydropowerRecorder
+    pywr.parameters.HydropowerTargetParameter
+
+    """
+    def __init__(self, model, node, water_elevation_parameter=None, turbine_elevation=0.0, efficiency=1.0, density=1000,
+                 flow_unit_conversion=1.0, energy_unit_conversion=1e-6, **kwargs):
+        super(TotalHydroEnergyRecorder, self).__init__(model, node, **kwargs)
+
+        self.water_elevation_parameter = water_elevation_parameter
+        self.turbine_elevation = turbine_elevation
+        self.efficiency = efficiency
+        self.density = density
+        self.flow_unit_conversion = flow_unit_conversion
+        self.energy_unit_conversion = energy_unit_conversion
+
+    property water_elevation_parameter:
+        def __get__(self):
+            return self._water_elevation_parameter
+        def __set__(self, parameter):
+            if self._water_elevation_parameter:
+                self.children.remove(self._water_elevation_parameter)
+            self.children.add(parameter)
+            self._water_elevation_parameter = parameter
+
+    cpdef after(self):
+        """ Calculate the  """
+        cdef int i
+        cdef double q, head, power
+        cdef Timestep ts = self.model.timestepper.current
+        cdef double days = ts.days
+        cdef ScenarioIndex scenario_index
+        flow = self.node.flow
+
+        for scenario_index in self.model.scenarios.combinations:
+
+            if self._water_elevation_parameter is not None:
+                head = self._water_elevation_parameter.get_value(scenario_index)
+                if self.turbine_elevation is not None:
+                    head -= self.turbine_elevation
+            elif self.turbine_elevation is not None:
+                head = self.turbine_elevation
+            else:
+                raise ValueError('One or both of storage_node or level must be set.')
+
+            # -ve head is not valid
+            head = max(head, 0.0)
+            # Get the flow from the current node
+            q = self._node._flow[scenario_index.global_id]
+            power = hydropower_calculation(q, head, 0.0, self.efficiency, density=self.density,
+                                             flow_unit_conversion=self.flow_unit_conversion,
+                                             energy_unit_conversion=self.energy_unit_conversion)
+
+            self._values[scenario_index.global_id] += power * days
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.parameters import load_parameter
+        node = model._get_node_from_ref(model, data.pop("node"))
+        if "water_elevation_parameter" in data:
+            water_elevation_parameter = load_parameter(model, data.pop("water_elevation_parameter"))
+        else:
+            water_elevation_parameter = None
+
+        return cls(model, node, water_elevation_parameter=water_elevation_parameter, **data)
 TotalHydroEnergyRecorder.register()
```

### Comparing `pywr-1.8.0/pywr/recorders/_recorders.pyx` & `pywr-1.9.0/pywr/recorders/_recorders.pyx`

 * *Files 19% similar despite different names*

```diff
@@ -1,2131 +1,2135 @@
-import numpy as np
-cimport numpy as np
-from scipy.stats import percentileofscore
-import pandas as pd
-import warnings
-
-
-recorder_registry = {}
-
-cdef enum AggFuncs:
-    SUM = 0
-    MIN = 1
-    MAX = 2
-    MEAN = 3
-    MEDIAN = 4
-    PRODUCT = 5
-    CUSTOM = 6
-    PERCENTILE = 7
-    PERCENTILEOFSCORE = 8
-    COUNT_NONZERO = 9
-_agg_func_lookup = {
-    "sum": AggFuncs.SUM,
-    "min": AggFuncs.MIN,
-    "max": AggFuncs.MAX,
-    "mean": AggFuncs.MEAN,
-    "median": AggFuncs.MEDIAN,
-    "product": AggFuncs.PRODUCT,
-    "custom": AggFuncs.CUSTOM,
-    "percentile": AggFuncs.PERCENTILE,
-    "percentileofscore": AggFuncs.PERCENTILEOFSCORE,
-    "count_nonzero": AggFuncs.COUNT_NONZERO
-}
-_agg_func_lookup_reverse = {v: k for k, v in _agg_func_lookup.items()}
-
-cdef enum ObjDirection:
-    NONE = 0
-    MAXIMISE = 1
-    MINIMISE = 2
-_obj_direction_lookup = {
-    "maximize": ObjDirection.MAXIMISE,
-    "maximise": ObjDirection.MAXIMISE,
-    "max": ObjDirection.MAXIMISE,
-    "minimise": ObjDirection.MINIMISE,
-    "minimize": ObjDirection.MINIMISE,
-    "min": ObjDirection.MINIMISE,
-}
-
-cdef class Aggregator:
-    """Utility class for computing aggregate values.
-
-    Users are unlikely to use this class directly. Instead `Recorder` sub-classes will use this functionality
-    to aggregate their results across different dimensions (e.g. time, scenarios, etc.).
-
-    Parameters
-    ==========
-    func : str, dict or callable
-        The aggregation function to use. Can be a string or dict defining aggregation functions, or a callable
-        custom function that performs aggregation.
-
-        When a string it can be one of: "sum", "min", "max", "mean", "median", "product", or "count_nonzero". These
-        strings map to and cause the aggregator to use the corresponding `numpy` functions.
-
-        A dict can be provided containing a "func" key, and optional "args" and "kwargs" keys. The value of "func"
-        should be a string corresponding to the aforementioned numpy function names with the additional options of
-        "percentile" and "percentileofscore". These latter two functions require additional arguments (the percentile
-        and score) to function and must be provided as the values in either the "args" or "kwargs" keys of the
-        dictionary. Please refer to the corresponding numpy (or scipy) function definitions for documentation on these
-        arguments.
-
-        Finally, a callable function can be given. This function must accept either a 1D or 2D numpy array as the
-        first argument, and support the "axis" keyword as integer value that determines which axis over which the
-        function should apply aggregation. The axis keyword is only supplied when a 2D array is given. Therefore,`
-        the callable function should behave in a similar fashion to the numpy functions.
-
-    Examples
-    ========
-    >>> Aggregator("sum")
-    >>> Aggregator({"func": "percentile", "args": [95],"kwargs": {}})
-    >>> Aggregator({"func": "percentileofscore", "kwargs": {"score": 0.5, "kind": "rank"}})
-
-    """
-    def __init__(self, func):
-        self.func = func
-
-    property func:
-        def __get__(self):
-            if self._func == AggFuncs.CUSTOM:
-                return self._user_func
-            return _agg_func_lookup_reverse[self._func]
-        def __set__(self, func):
-            self._user_func = None
-            func_args = []
-            func_kwargs = {}
-            if isinstance(func, str):
-                func_type = _agg_func_lookup[func.lower()]
-            elif isinstance(func, dict):
-                func_type = _agg_func_lookup[func['func']]
-                func_args = func.get('args', [])
-                func_kwargs = func.get('kwargs', {})
-            elif callable(func):
-                self._user_func = func
-                func_type = AggFuncs.CUSTOM
-            else:
-                raise ValueError("Unrecognised aggregation function: \"{}\".".format(func))
-            self._func = func_type
-            self.func_args = func_args
-            self.func_kwargs = func_kwargs
-
-    cpdef double aggregate_1d(self, double[:] data, ignore_nan=False) except *:
-        """Compute an aggregated value across 1D array.
-        """
-        cdef double[:] values = data
-
-        if ignore_nan:
-            values = np.array(values)[~np.isnan(values)]
-
-        if self._func == AggFuncs.PRODUCT:
-            return np.product(values)
-        elif self._func == AggFuncs.SUM:
-            return np.sum(values)
-        elif self._func == AggFuncs.MAX:
-            return np.max(values)
-        elif self._func == AggFuncs.MIN:
-            return np.min(values)
-        elif self._func == AggFuncs.MEAN:
-            return np.mean(values)
-        elif self._func == AggFuncs.MEDIAN:
-            return np.median(values)
-        elif self._func == AggFuncs.CUSTOM:
-            return self._user_func(np.array(values))
-        elif self._func == AggFuncs.PERCENTILE:
-            return np.percentile(values, *self.func_args, **self.func_kwargs)
-        elif self._func == AggFuncs.PERCENTILEOFSCORE:
-            return percentileofscore(values, *self.func_args, **self.func_kwargs)
-        elif self._func == AggFuncs.COUNT_NONZERO:
-            return np.count_nonzero(values)
-        else:
-            raise ValueError('Aggregation function code "{}" not recognised.'.format(self._func))
-
-    cpdef double[:] aggregate_2d(self, double[:, :] data, axis=0, ignore_nan=False) except *:
-        """Compute an aggregated value along an axis of a 2D array.
-        """
-        cdef double[:, :] values = data
-        cdef Py_ssize_t i
-
-        if ignore_nan:
-            values = np.array(values)[~np.isnan(values)]
-
-        if self._func == AggFuncs.PRODUCT:
-            return np.product(values, axis=axis)
-        elif self._func == AggFuncs.SUM:
-            return np.sum(values, axis=axis)
-        elif self._func == AggFuncs.MAX:
-            return np.max(values, axis=axis)
-        elif self._func == AggFuncs.MIN:
-            return np.min(values, axis=axis)
-        elif self._func == AggFuncs.MEAN:
-            return np.mean(values, axis=axis)
-        elif self._func == AggFuncs.MEDIAN:
-            return np.median(values, axis=axis)
-        elif self._func == AggFuncs.CUSTOM:
-            return self._user_func(np.array(values), axis=axis)
-        elif self._func == AggFuncs.PERCENTILE:
-            return np.percentile(values, *self.func_args, axis=axis, **self.func_kwargs)
-        elif self._func == AggFuncs.PERCENTILEOFSCORE:
-            # percentileofscore doesn't support the axis argument
-            # we must therefore iterate over the array
-            if axis == 0:
-                out = np.empty(data.shape[1])
-                for i in range(data.shape[1]):
-                    out[i] = percentileofscore(values[:, i], *self.func_args, **self.func_kwargs)
-            elif axis == 1:
-                out = np.empty(data.shape[0])
-                for i in range(data.shape[0]):
-                    out[i] = percentileofscore(values[i, :], *self.func_args, **self.func_kwargs)
-            else:
-                raise ValueError('Axis "{}" not recognised for percentileofscore function.'.format(axis))
-            return out
-        elif self._func == AggFuncs.COUNT_NONZERO:
-            return np.count_nonzero(values, axis=axis).astype(np.float64)
-        else:
-            raise ValueError('Aggregation function code "{}" not recognised.'.format(self._func))
-
-
-cdef class Recorder(Component):
-    """Base class for recording information from a `pywr.model.Model`.
-
-    Recorder components are used to calculate, aggregate and save data from a simulation. This
-    base class provides the basic functionality for all recorders.
-
-    Parameters
-    ==========
-    model : `pywr.core.Model`
-    agg_func : str or callable (default="mean")
-        Scenario aggregation function to use when `aggregated_value` is called.
-    name : str (default=None)
-        Name of the recorder.
-    comment : str (default=None)
-        Comment or description of the recorder.
-    ignore_nan : bool (default=False)
-        Flag to ignore NaN values when calling `aggregated_value`.
-    is_objective : {None, 'maximize', 'maximise', 'max', 'minimize', 'minimise', 'min'}
-        Flag to denote the direction, if any, of optimisation undertaken with this recorder.
-    epsilon : float (default=1.0)
-        Epsilon distance used by some optimisation algorithms.
-    constraint_lower_bounds, constraint_upper_bounds : double (default=None)
-        The value(s) to use for lower and upper bound definitions. These values determine whether the recorder
-        instance is marked as a constraint. Either bound can be `None` (the default) to disable the respective
-        bound. If both bounds are `None` then the `is_constraint` property will return `False`. The lower bound must
-        be strictly less than the upper bound. An equality constraint can be created by setting both bounds to the
-        same value.
-
-        The constraint bounds are not used during model simulation. Instead they are intended for use by optimisation
-        wrappers (or other external tools) to define constrained optimisation problems.
-    """
-    def __init__(self, model, agg_func="mean", ignore_nan=False, is_objective=None, epsilon=1.0,
-                 name=None, constraint_lower_bounds=None, constraint_upper_bounds=None, **kwargs):
-        # Default the constraints internal values to be +/- inf.
-        # This ensures the bounds checking works later in the init.
-        self._constraint_lower_bounds = -np.inf
-        self._constraint_upper_bounds = np.inf
-        if name is None:
-            name = self.__class__.__name__.lower()
-        super(Recorder, self).__init__(model, name=name, **kwargs)
-        self.ignore_nan = ignore_nan
-        self.is_objective = is_objective
-        self.epsilon = epsilon
-        self.constraint_lower_bounds = constraint_lower_bounds
-        self.constraint_upper_bounds = constraint_upper_bounds
-        # Create the aggregator for scenarios
-        self._scenario_aggregator = Aggregator(agg_func)
-
-    property agg_func:
-        def __get__(self):
-            return self._scenario_aggregator.func
-        def __set__(self, agg_func):
-            self._scenario_aggregator.func = agg_func
-
-    property is_objective:
-        def __set__(self, value):
-            if value is None:
-                self._is_objective = ObjDirection.NONE
-            else:
-                self._is_objective = _obj_direction_lookup[value]
-        def __get__(self):
-            if self._is_objective == ObjDirection.NONE:
-                return None
-            elif self._is_objective == ObjDirection.MAXIMISE:
-                return 'maximise'
-            elif self._is_objective == ObjDirection.MINIMISE:
-                return 'minimise'
-            else:
-                raise ValueError("Objective direction type not recognised.")
-
-    property constraint_lower_bounds:
-        def __set__(self, value):
-            if value is None:
-                self._constraint_lower_bounds = -np.inf
-            else:
-                if self.constraint_upper_bounds is not None and value > self.constraint_upper_bounds:
-                    raise ValueError('Lower bounds can not be larger than the upper bounds.')
-                self._constraint_lower_bounds = value
-        def __get__(self):
-            if np.isneginf(self._constraint_lower_bounds):
-                return None
-            else:
-                return self._constraint_lower_bounds
-
-    property constraint_upper_bounds:
-        def __set__(self, value):
-            if value is None:
-                self._constraint_upper_bounds = np.inf
-            else:
-                if self.constraint_lower_bounds is not None and value < self.constraint_lower_bounds:
-                    raise ValueError('Upper bounds can not be smaller than the lower bounds.')
-                self._constraint_upper_bounds = value
-        def __get__(self):
-            if np.isinf(self._constraint_upper_bounds):
-                return None
-            else:
-                return self._constraint_upper_bounds
-
-    @property
-    def is_equality_constraint(self):
-        """Returns true if upper and lower constraint bounds are both defined and equal to one another."""
-        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is not None and \
-               self.constraint_lower_bounds == self.constraint_upper_bounds
-
-    @property
-    def is_double_bounded_constraint(self):
-        """Returns true if upper and lower constraint bounds are both defined and not-equal to one another."""
-        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is not None and \
-               self.constraint_lower_bounds != self.constraint_upper_bounds
-
-    @property
-    def is_lower_bounded_constraint(self):
-        """Returns true if lower constraint bounds is defined and upper constraint bounds is not."""
-        return self.constraint_upper_bounds is None and self.constraint_lower_bounds is not None
-
-    @property
-    def is_upper_bounded_constraint(self):
-        """Returns true if upper constraint bounds is defined and lower constraint bounds is not."""
-        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is None
-
-    @property
-    def is_constraint(self):
-        """Returns true if either upper or lower constraint bounds is defined."""
-        return self.constraint_upper_bounds is not None or self.constraint_lower_bounds is not None
-
-    def is_constraint_violated(self):
-        """Returns true if the value from this Recorder violates its constraint bounds.
-
-        If no constraint bounds are defined (i.e. self.is_constraint == False) then a ValueError is raised.
-        """
-        value = self.aggregated_value()
-        if self.is_equality_constraint:
-            feasible = value == self.constraint_lower_bounds
-        elif self.is_double_bounded_constraint:
-            feasible = self.constraint_lower_bounds <= value <= self.constraint_upper_bounds
-        elif self.is_lower_bounded_constraint:
-            feasible = self.constraint_lower_bounds <= value
-        elif self.is_upper_bounded_constraint:
-            feasible = value <= self.constraint_upper_bounds
-        else:
-            raise ValueError(f'Recorder "{self.name}" has no constraint bounds defined.')
-        return not feasible
-
-    def __repr__(self):
-        return '<{} "{}">'.format(self.__class__.__name__, self.name)
-
-    cpdef double aggregated_value(self) except *:
-        cdef double[:] values = self.values()
-        return self._scenario_aggregator.aggregate_1d(values)
-
-    cpdef double[:] values(self) except *:
-        raise NotImplementedError()
-
-    @classmethod
-    def load(cls, model, data):
-        try:
-            node_name = data["node"]
-        except KeyError:
-            pass
-        else:
-            data["node"] = model._get_node_from_ref(model, node_name)
-        return cls(model, **data)
-
-    @classmethod
-    def register(cls):
-        recorder_registry[cls.__name__.lower()] = cls
-
-    @classmethod
-    def unregister(cls):
-        del(recorder_registry[cls.__name__.lower()])
-
-cdef class AggregatedRecorder(Recorder):
-    """
-    This Recorder is used to aggregate across multiple other Recorder objects.
-
-    The class provides a method to produce a complex aggregated recorder by taking
-    the results of other records. The `.values()` method first collects unaggregated values
-    from the provided recorders. These are then aggregated on a per scenario basis and returned
-    by this classes `.values()` method. This method allows `AggregatedRecorder` to be used as
-    a recorder for in other `AggregatedRecorder` instances.
-
-    By default the same `agg_func` function is used for both steps, but an optional
-    `recorder_agg_func` can undertake a different aggregation across scenarios. For
-    example summing recorders per scenario, and then taking a mean of the sum totals.
-
-    Parameters
-    ==========
-    model : `pywr.core.Model`
-    recorders: iterable of `Recorder` objects.
-        The other `Recorder` instances to perform aggregation over.
-    agg_func : str or callable, optional
-        Scenario aggregation function to use when `aggregated_value` is called (default="mean").
-    recorder_agg_func : str or callable, optional
-        Recorder aggregation function to use when `aggregated_value` is called (default=`agg_func`).
-    """
-    def __init__(self, model, recorders, **kwargs):
-        # Optional different method for aggregating across self.recorders scenarios
-        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
-
-        if isinstance(agg_func, str):
-            agg_func = _agg_func_lookup[agg_func.lower()]
-        elif callable(agg_func):
-            self.recorder_agg_func = agg_func
-            agg_func = AggFuncs.CUSTOM
-        else:
-            raise ValueError("Unrecognised recorder aggregation function: \"{}\".".format(agg_func))
-        self._recorder_agg_func = agg_func
-
-        super(AggregatedRecorder, self).__init__(model, **kwargs)
-        self.recorders = list(recorders)
-
-        for rec in self.recorders:
-            self.children.add(rec)
-
-    cpdef double[:] values(self) except *:
-        cdef Recorder recorder
-        cdef double[:] value, value2
-        assert(len(self.recorders))
-        cdef int n = len(self.model.scenarios.combinations)
-        cdef int i
-
-        if self._recorder_agg_func == AggFuncs.PRODUCT:
-            value = np.ones(n, np.float64)
-            for recorder in self.recorders:
-                value2 = recorder.values()
-                for i in range(n):
-                    value[i] *= value2[i]
-        elif self._recorder_agg_func == AggFuncs.SUM:
-            value = np.zeros(n, np.float64)
-            for recorder in self.recorders:
-                value2 = recorder.values()
-                for i in range(n):
-                    value[i] += value2[i]
-        elif self._recorder_agg_func == AggFuncs.MAX:
-            value = np.empty(n)
-            value[:] = np.NINF
-            for recorder in self.recorders:
-                value2 = recorder.values()
-                for i in range(n):
-                    if value2[i] > value[i]:
-                        value[i] = value2[i]
-        elif self._recorder_agg_func == AggFuncs.MIN:
-            value = np.empty(n)
-            value[:] = np.PINF
-            for recorder in self.recorders:
-                value2 = recorder.values()
-                for i in range(n):
-                    if value2[i] < value[i]:
-                        value[i] = value2[i]
-        elif self._recorder_agg_func == AggFuncs.MEAN:
-            value = np.zeros(n, np.float64)
-            for recorder in self.recorders:
-                value2 = recorder.values()
-                for i in range(n):
-                    value[i] += value2[i]
-            for i in range(n):
-                value[i] /= len(self.recorders)
-        else:
-            value = self.recorder_agg_func([recorder.values() for recorder in self.recorders], axis=0)
-        return value
-
-    @classmethod
-    def load(cls, model, data):
-        recorder_names = data.pop("recorders")
-        recorders = [load_recorder(model, name) for name in recorder_names]
-        rec = cls(model, recorders, **data)
-        return rec
-
-AggregatedRecorder.register()
-
-
-cdef class NodeRecorder(Recorder):
-    def __init__(self, model, AbstractNode node, name=None, **kwargs):
-        if name is None:
-            name = "{}.{}".format(self.__class__.__name__.lower(), node.name)
-        super(NodeRecorder, self).__init__(model, name=name, **kwargs)
-        self._node = node
-        node._recorders.append(self)
-
-    cpdef double[:] values(self) except *:
-        return self._node._flow
-
-    property node:
-        def __get__(self):
-            return self._node
-
-    def __repr__(self):
-        return '<{} on {} "{}">'.format(self.__class__.__name__, self.node, self.name)
-
-NodeRecorder.register()
-
-
-cdef class StorageRecorder(Recorder):
-    def __init__(self, model, AbstractStorage node, name=None, **kwargs):
-        if name is None:
-            name = "{}.{}".format(self.__class__.__name__.lower(), node.name)
-        super(StorageRecorder, self).__init__(model, name=name, **kwargs)
-        self._node = node
-        node._recorders.append(self)
-
-    cpdef double[:] values(self) except *:
-        return self._node._volume
-
-    property node:
-        def __get__(self):
-            return self._node
-
-    def __repr__(self):
-        return '<{} on {} "{}">'.format(self.__class__.__name__, self.node, self.name)
-
-StorageRecorder.register()
-
-
-cdef class ParameterRecorder(Recorder):
-    """Base class for recorders that track `Parameter` values.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.Parameter`
-        The parameter to record.
-    name : str (optional)
-        The name of the recorder
-    """
-    def __init__(self, model, Parameter param, name=None, **kwargs):
-        if name is None:
-            name = "{}.{}".format(self.__class__.__name__.lower(), param.name)
-        super(ParameterRecorder, self).__init__(model, name=name, **kwargs)
-        self._param = param
-        param.parents.add(self)
-
-    property parameter:
-        def __get__(self):
-            return self._param
-
-    def __repr__(self):
-        return '<{} on {} "{}" ({})>'.format(self.__class__.__name__, repr(self.parameter), self.name, hex(id(self)))
-
-    def __str__(self):
-        return '<{} on {} "{}">'.format(self.__class__.__name__, self.parameter, self.name)
-
-    @classmethod
-    def load(cls, model, data):
-        # when the parameter being recorder is defined inline (i.e. not in the
-        # parameters section, but within the node) we need to make sure the
-        # node has been loaded first
-        try:
-            node_name = data["node"]
-        except KeyError:
-            node = None
-        else:
-            del(data["node"])
-            node = model._get_node_from_ref(model, node_name)
-        from pywr.parameters import load_parameter
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-
-ParameterRecorder.register()
-
-
-cdef class IndexParameterRecorder(Recorder):
-    def __init__(self, model, IndexParameter param, name=None, **kwargs):
-        if name is None:
-            name = "{}.{}".format(self.__class__.__name__.lower(), param.name)
-        super(IndexParameterRecorder, self).__init__(model, name=name, **kwargs)
-        self._param = param
-        param.parents.add(self)
-
-    property parameter:
-        def __get__(self):
-            return self._param
-
-    def __repr__(self):
-        return '<{} on {} "{}" ({})>'.format(self.__class__.__name__, repr(self.parameter), self.name, hex(id(self)))
-
-    def __str__(self):
-        return '<{} on {} "{}">'.format(self.__class__.__name__, self.parameter, self.name)
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.parameters import load_parameter
-        parameter = load_parameter(model, data.pop("parameter"))
-        return cls(model, parameter, **data)
-
-IndexParameterRecorder.register()
-
-
-cdef class NumpyArrayNodeRecorder(NodeRecorder):
-    """Recorder for timeseries information from a `Node`.
-
-    This class stores flow from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    factor: float (default=1.0)
-        A factor can be provided to scale the total flow (e.g. for calculating operational costs).
-
-    See also
-    --------
-    NumpyArrayNodeDeficitRecorder
-    NumpyArrayNodeSuppliedRatioRecorder
-    NumpyArrayNodeCurtailmentRatioRecorder
-    """
-    def __init__(self, model, AbstractNode node, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
-        factor = kwargs.pop('factor', 1.0)
-        super(NumpyArrayNodeRecorder, self).__init__(model, node, **kwargs)
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-        self.factor = factor
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        cdef int nts = len(self.model.timestepper)
-        self._data = np.zeros((nts, ncomb))
-
-    cpdef reset(self):
-        self._data[:, :] = 0.0
-
-    cpdef after(self):
-        cdef int i
-        cdef Timestep ts = self.model.timestepper.current
-        for i in range(self._data.shape[1]):
-            self._data[ts.index, i] = self._node._flow[i]*self.factor
-        return 0
-
-    property data:
-        def __get__(self, ):
-            return np.array(self._data)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-
-NumpyArrayNodeRecorder.register()
-
-
-cdef class NumpyArrayNodeDeficitRecorder(NumpyArrayNodeRecorder):
-    """Recorder for timeseries of deficit from a `Node`.
-
-    This class stores deficit from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-
-    Notes
-    -----
-    Deficit is calculated as the difference between `max_flow` and `self.node.flow` (i.e. the actual
-    flow allocated during the time-step)::
-
-        deficit = max_flow - actual_flow
-
-    See also
-    --------
-    NumpyArrayNodeRecorder
-    NumpyArrayNodeSuppliedRatioRecorder
-    NumpyArrayNodeCurtailmentRatioRecorder
-    """
-    cpdef after(self):
-        cdef double max_flow
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef Node node = self._node
-        for scenario_index in self.model.scenarios.combinations:
-            max_flow = node.get_max_flow(scenario_index)
-            self._data[ts.index,scenario_index.global_id] = max_flow - node._flow[scenario_index.global_id]
-        return 0
-NumpyArrayNodeDeficitRecorder.register()
-
-
-cdef class NumpyArrayNodeSuppliedRatioRecorder(NumpyArrayNodeRecorder):
-    """Recorder for timeseries of ratio of supplied flow from a `Node`.
-
-    This class stores supply ratio from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-
-    Notes
-    -----
-    Supply ratio is calculated calculated as the ratio of `self.node.flow` to `self.node.max_flow`
-    for each time-step::
-
-        supply_ratio = actual_flow / max_flow
-
-    See also
-    --------
-    NumpyArrayNodeRecorder
-    NumpyArrayNodeDeficitRecorder
-    NumpyArrayNodeCurtailmentRatioRecorder
-    """
-    cpdef after(self):
-        cdef double max_flow
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef Node node = self._node
-        for scenario_index in self.model.scenarios.combinations:
-            max_flow = node.get_max_flow(scenario_index)
-            self._data[ts.index,scenario_index.global_id] = node._flow[scenario_index.global_id] / max_flow
-        return 0
-NumpyArrayNodeSuppliedRatioRecorder.register()
-
-
-cdef class NumpyArrayNodeCurtailmentRatioRecorder(NumpyArrayNodeRecorder):
-    """Recorder for timeseries of curtailment ratio from a `Node`.
-
-    This class stores curtailment ratio from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-
-    Notes
-    -----
-    Curtailment ratio is calculated calculated as one minues the ratio of `self.node.flow` to
-    `self.node.max_flow` for each time-step::
-
-        curtailment_ratio = 1 - actual_flow / max_flow
-
-    See also
-    --------
-    NumpyArrayNodeRecorder
-    NumpyArrayNodeDeficitRecorder
-    NumpyArrayNodeSuppliedRatioRecorder
-    """
-    cpdef after(self):
-        cdef double max_flow
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef Node node = self._node
-        for scenario_index in self.model.scenarios.combinations:
-            max_flow = node.get_max_flow(scenario_index)
-            self._data[ts.index,scenario_index.global_id] = 1 - node._flow[scenario_index.global_id] / max_flow
-NumpyArrayNodeCurtailmentRatioRecorder.register()
-
-
-cdef class FlowDurationCurveRecorder(NumpyArrayNodeRecorder):
-    """
-    This recorder calculates a flow duration curve for each scenario.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        The node to record
-    percentiles : array
-        The percentiles to use in the calculation of the flow duration curve.
-        Values must be in the range 0-100.
-    agg_func: str, optional
-        function used for aggregating the FDC across percentiles.
-        Numpy style functions that support an axis argument are supported.
-    fdc_agg_func: str, optional
-        optional different function for aggregating across scenarios.
-    """
-    def __init__(self, model, AbstractNode node, percentiles, **kwargs):
-
-        # Optional different method for aggregating across percentiles
-        if 'fdc_agg_func' in kwargs:
-            # Support previous behaviour
-            warnings.warn('The "fdc_agg_func" key is deprecated for defining the temporal '
-                          'aggregation in {}. Please "temporal_agg_func" instead.'
-                          .format(self.__class__.__name__))
-            if "temporal_agg_func" in kwargs:
-                raise ValueError('Both "fdc_agg_func" and "temporal_agg_func" keywords given.'
-                                 'This is ambiguous. Please use "temporal_agg_func" only.')
-            kwargs["temporal_agg_func"] = kwargs.pop("fdc_agg_func")
-
-        super(FlowDurationCurveRecorder, self).__init__(model, node, **kwargs)
-        self._percentiles = np.asarray(percentiles, dtype=np.float64)
-
-    cpdef finish(self):
-        self._fdc = np.percentile(np.asarray(self._data), np.asarray(self._percentiles), axis=0)
-
-    property fdc:
-        def __get__(self, ):
-            return np.array(self._fdc)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._fdc, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self._percentiles
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=np.array(self.fdc), index=index, columns=sc_index)
-
-FlowDurationCurveRecorder.register()
-
-
-cdef class SeasonalFlowDurationCurveRecorder(FlowDurationCurveRecorder):
-    """
-    This recorder calculates a flow duration curve for each scenario for a given season
-    specified in months.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        The node to record
-    percentiles : array
-        The percentiles to use in the calculation of the flow duration curve.
-        Values must be in the range 0-100.
-    agg_func: str, optional
-        function used for aggregating the FDC across percentiles.
-        Numpy style functions that support an axis argument are supported.
-    fdc_agg_func: str, optional
-        optional different function for aggregating across scenarios.
-    months: array
-        The numeric values of the months the flow duration curve should be calculated for.
-    """
-
-    def __init__(self, model, AbstractNode node, percentiles, months, **kwargs):
-        super(SeasonalFlowDurationCurveRecorder, self).__init__(model, node, percentiles, **kwargs)
-        self._months = set(months)
-
-    cpdef finish(self):
-        # this is a def method rather than cpdef because closures inside cpdef functions are not supported yet.
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-
-        df = pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-        mask = np.asarray(df.index.map(self.is_season))
-        self._fdc = np.percentile(df.loc[mask, :], np.asarray(self._percentiles), axis=0)
-
-    def is_season(self, x):
-        return x.month in self._months
-
-SeasonalFlowDurationCurveRecorder.register()
-
-cdef class FlowDurationCurveDeviationRecorder(FlowDurationCurveRecorder):
-    """
-    This recorder calculates a Flow Duration Curve (FDC) for each scenario and then
-    calculates their deviation from upper and lower target FDCs. The 2nd dimension of the target
-    duration curves and percentiles list must be of the same length and have the same
-    order (high to low values or low to high values).
-
-    Deviation is calculated as positive if actual FDC is above the upper target or below the lower
-    target. If actual FDC falls between the upper and lower targets zero deviation is returned.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        The node to record
-    percentiles : array
-        The percentiles to use in the calculation of the flow duration curve.
-        Values must be in the range 0-100.
-    lower_target_fdc : array
-        The lower FDC against which the scenario FDCs are compared
-    upper_target_fdc : array
-        The upper FDC against which the scenario FDCs are compared
-    agg_func: str, optional
-        Function used for aggregating the FDC deviations across percentiles.
-        Numpy style functions that support an axis argument are supported.
-    fdc_agg_func: str, optional
-        Optional different function for aggregating across scenarios.
-
-    """
-    def __init__(self, model, AbstractNode node, percentiles, lower_target_fdc, upper_target_fdc, scenario=None, **kwargs):
-        super(FlowDurationCurveDeviationRecorder, self).__init__(model, node, percentiles, **kwargs)
-
-        lower_target = np.array(lower_target_fdc, dtype=np.float64)
-        if lower_target.ndim < 2:
-            lower_target = lower_target[:, np.newaxis]
-
-        upper_target = np.array(upper_target_fdc, dtype=np.float64)
-        if upper_target.ndim < 2:
-            upper_target = upper_target[:, np.newaxis]
-
-        self._lower_target_fdc = lower_target
-        self._upper_target_fdc = upper_target
-        self.scenario = scenario
-        if len(self._percentiles) != self._lower_target_fdc.shape[0]:
-            raise ValueError("The lengths of the lower target FDC and the percentiles list do not match")
-        if len(self._percentiles) != self._upper_target_fdc.shape[0]:
-            raise ValueError("The lengths of the upper target FDC and the percentiles list do not match")
-
-    cpdef setup(self):
-        super(FlowDurationCurveDeviationRecorder, self).setup()
-        # Check target FDC is the correct size; this is done in setup rather than __init__
-        # because the scenarios might change after the Recorder is created.
-        if self.scenario is not None:
-            if self._lower_target_fdc.shape[1] != self.scenario.size:
-                raise ValueError('The number of lower target FDCs does not match the size ({}) of scenario "{}"'.format(self.scenario.size, self.scenario.name))
-            if self._upper_target_fdc.shape[1] != self.scenario.size:
-                raise ValueError('The number of upper target FDCs does not match the size ({}) of scenario "{}"'.format(self.scenario.size, self.scenario.name))
-        else:
-            if self._lower_target_fdc.shape[1] > 1 and \
-                    self._lower_target_fdc.shape[1] != len(self.model.scenarios.combinations):
-                raise ValueError("The number of lower target FDCs does not match the number of scenarios")
-            if self._upper_target_fdc.shape[1] > 1 and \
-                    self._upper_target_fdc.shape[1] != len(self.model.scenarios.combinations):
-                raise ValueError("The number of upper target FDCs does not match the number of scenarios")
-
-    cpdef finish(self):
-        super(FlowDurationCurveDeviationRecorder, self).finish()
-
-        cdef int i, j, jl, ju, k, sc_index
-        cdef ScenarioIndex scenario_index
-        cdef double[:] utrgt_fdc, ltrgt_fdc
-        cdef double udev, ldev
-
-        # We have to do this the slow way by iterating through all scenario combinations
-        if self.scenario is not None:
-            sc_index = self.model.scenarios.get_scenario_index(self.scenario)
-
-        self._fdc_deviations = np.empty((self._lower_target_fdc.shape[0], len(self.model.scenarios.combinations)), dtype=np.float64)
-        for i, scenario_index in enumerate(self.model.scenarios.combinations):
-
-            if self.scenario is not None:
-                # Get the scenario specific ensemble id for this combination
-                j = scenario_index._indices[sc_index]
-            else:
-                j = scenario_index.global_id
-
-            if self._lower_target_fdc.shape[1] == 1:
-                jl = 0
-            else:
-                jl = j
-
-            if self._upper_target_fdc.shape[1] == 1:
-                ju = 0
-            else:
-                ju = j
-
-            # Cache the target FDC to use in this combination
-            ltrgt_fdc = self._lower_target_fdc[:, jl]
-            utrgt_fdc = self._upper_target_fdc[:, ju]
-            # Finally calculate deviation
-            for k in range(ltrgt_fdc.shape[0]):
-                try:
-                    # upper deviation (+ve when flow higher than upper target)
-                    udev = (self._fdc[k, i] - utrgt_fdc[k])  / utrgt_fdc[k]
-                    # lower deviation (+ve when flow less than lower target)
-                    ldev = (ltrgt_fdc[k] - self._fdc[k, i])  / ltrgt_fdc[k]
-                    # Overall deviation is the worst of upper and lower, but if both
-                    # are negative (i.e. FDC is between upper and lower) there is zero deviation
-                    self._fdc_deviations[k, i] = max(udev, ldev, 0.0)
-                except ZeroDivisionError:
-                    self._fdc_deviations[k, i] = np.nan
-
-    property fdc_deviations:
-        def __get__(self, ):
-            return np.array(self._fdc_deviations)
-
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._fdc_deviations, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self, return_fdc=False):
-        """ Return a `pandas.DataFrame` of the deviations from the target FDCs
-
-        Parameters
-        ----------
-        return_fdc : bool (default=False)
-            If true returns a tuple of two dataframes. The first is the deviations, the second
-            is the actual FDC.
-        """
-        index = self._percentiles
-        sc_index = self.model.scenarios.multiindex
-
-        df = pd.DataFrame(data=np.array(self._fdc_deviations), index=index, columns=sc_index)
-        if return_fdc:
-            return df, super(FlowDurationCurveDeviationRecorder, self).to_dataframe()
-        else:
-            return df
-
-FlowDurationCurveDeviationRecorder.register()
-
-
-cdef class NumpyArrayAbstractStorageRecorder(StorageRecorder):
-    def __init__(self, model, AbstractStorage node, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
-        super().__init__(model, node, **kwargs)
-
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        cdef int nts = len(self.model.timestepper)
-        self._data = np.zeros((nts, ncomb))
-
-    cpdef reset(self):
-        self._data[:, :] = 0.0
-
-    cpdef after(self):
-        raise NotImplementedError()
-
-    property data:
-        def __get__(self, ):
-            return np.array(self._data)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-
-
-cdef class NumpyArrayStorageRecorder(NumpyArrayAbstractStorageRecorder):
-    """Recorder for timeseries information from a `Storage` node.
-
-    This class stores volume from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    proportional : bool
-        Whether to record proportional [0, 1.0] or absolute storage volumes (default=False).
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    def __init__(self, *args, **kwargs):
-        # Optional different method for aggregating across time.
-        self.proportional = kwargs.pop('proportional', False)
-        super().__init__(*args, **kwargs)
-
-    cpdef after(self):
-        cdef int i
-        cdef Timestep ts = self.model.timestepper.current
-        for i in range(self._data.shape[1]):
-            if self.proportional:
-                self._data[ts.index, i] = self._node._current_pc[i]
-            else:
-                self._data[ts.index, i] = self._node._volume[i]
-        return 0
-NumpyArrayStorageRecorder.register()
-
-
-cdef class StorageDurationCurveRecorder(NumpyArrayStorageRecorder):
-    """
-    This recorder calculates a storage duration curve for each scenario.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.AbstractStorage`
-        The node to record
-    percentiles : array
-        The percentiles to use in the calculation of the flow duration curve.
-        Values must be in the range 0-100.
-    agg_func: str, optional
-        function used for aggregating the FDC across percentiles.
-        Numpy style functions that support an axis argument are supported.
-    sdc_agg_func: str, optional
-        optional different function for aggregating across scenarios.
-
-    """
-
-    def __init__(self, model, AbstractStorage node, percentiles, **kwargs):
-
-        if "sdc_agg_func" in kwargs:
-            # Support previous behaviour
-            warnings.warn('The "sdc_agg_func" key is deprecated for defining the temporal '
-                          'aggregation in {}. Please "temporal_agg_func" instead.'
-                          .format(self.__class__.__name__))
-            if "temporal_agg_func" in kwargs:
-                raise ValueError('Both "sdc_agg_func" and "temporal_agg_func" keywords given.'
-                                 'This is ambiguous. Please use "temporal_agg_func" only.')
-            kwargs["temporal_agg_func"] = kwargs.pop("sdc_agg_func")
-
-        super(StorageDurationCurveRecorder, self).__init__(model, node, **kwargs)
-        self._percentiles = np.asarray(percentiles, dtype=np.float64)
-
-
-    cpdef finish(self):
-        self._sdc = np.percentile(np.asarray(self._data), np.asarray(self._percentiles), axis=0)
-
-    property sdc:
-        def __get__(self, ):
-            return np.array(self._sdc)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._sdc, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self._percentiles
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=self.sdc, index=index, columns=sc_index)
-
-StorageDurationCurveRecorder.register()
-
-cdef class NumpyArrayLevelRecorder(NumpyArrayAbstractStorageRecorder):
-    """Recorder for level timeseries from a `Storage` node.
-
-    This class stores level from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    cpdef after(self):
-        cdef int i
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef Storage node = self._node
-        for i, scenario_index in enumerate(self.model.scenarios.combinations):
-            self._data[ts.index, i] = node.get_level(scenario_index)
-        return 0
-NumpyArrayLevelRecorder.register()
-
-
-cdef class NumpyArrayAreaRecorder(NumpyArrayAbstractStorageRecorder):
-    """Recorder for area timeseries from a `Storage` node.
-
-    This class stores area from a specific node for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        Node instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    cpdef after(self):
-        cdef int i
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef Storage node = self._node
-        for i, scenario_index in enumerate(self.model.scenarios.combinations):
-            self._data[ts.index, i] = node.get_area(scenario_index)
-        return 0
-NumpyArrayAreaRecorder.register()
-
-
-cdef class NumpyArrayParameterRecorder(ParameterRecorder):
-    """Recorder for timeseries information from a `Parameter`.
-
-    This class stores the value from a specific `Parameter` for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.Parameter`
-        Parameter instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    def __init__(self, model, Parameter param, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
-        super(NumpyArrayParameterRecorder, self).__init__(model, param, **kwargs)
-
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        cdef int nts = len(self.model.timestepper)
-        self._data = np.zeros((nts, ncomb))
-
-    cpdef reset(self):
-        self._data[:, :] = 0.0
-
-    cpdef after(self):
-        cdef int i
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        self._data[ts.index, :] = self._param.get_all_values()
-        return 0
-
-    property data:
-        def __get__(self, ):
-            return np.array(self._data)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-NumpyArrayParameterRecorder.register()
-
-
-cdef class NumpyArrayDailyProfileParameterRecorder(ParameterRecorder):
-    """Recorder for an annual profile from a `Parameter`.
-
-    This recorder stores a daily profile returned by a specific parameter. For each day of the year
-    it stores the value encountered for that day during a simulation. This results in the final profile
-    being the last value encountered on each day of the year during a simulation. This recorder is useful
-    for returning the daily profile that may result from the combination of one or more parameters. For
-    example, during optimisation of new profiles non-daily parameters (e.g. `RbfProfileParameter`) and/or
-    aggregations of several parameters might be used. With this recorder the daily profile used in the simulation
-    can be easily saved.
-
-    The data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.Parameter`
-        Parameter instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    def __init__(self, model, Parameter param, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
-        super().__init__(model, param, **kwargs)
-
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        self._data = np.zeros((366, ncomb))
-
-    cpdef reset(self):
-        self._data[:, :] = 0.0
-
-    cpdef after(self):
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef int i = ts.dayofyear_index
-        self._data[i, :] = self._param.get_all_values()
-        return 0
-
-    property data:
-        def __get__(self, ):
-            return np.array(self._data)
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = np.arange(1, 367)
-        sc_index = self.model.scenarios.multiindex
-        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-NumpyArrayDailyProfileParameterRecorder.register()
-
-
-cdef class NumpyArrayIndexParameterRecorder(IndexParameterRecorder):
-    """Recorder for timeseries information from an `IndexParameter`.
-
-    This class stores the value from a specific `IndexParameter` for each time-step of a simulation. The
-    data is saved internally using a memory view. The data can be accessed through the `data`
-    attribute or `to_dataframe()` method.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.IndexParameter`
-        Parameter instance to record.
-    temporal_agg_func : str or callable (default="mean")
-        Aggregation function used over time when computing a value per scenario. This can be used
-        to return, for example, the median flow over a simulation. For aggregation over scenarios
-        see the `agg_func` keyword argument.
-    """
-    def __init__(self, model, IndexParameter param, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
-        super(NumpyArrayIndexParameterRecorder, self).__init__(model, param, **kwargs)
-
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        cdef int nts = len(self.model.timestepper)
-        self._data = np.zeros((nts, ncomb), dtype=np.int32)
-
-    cpdef reset(self):
-        self._data[:, :] = 0
-
-    cpdef after(self):
-        cdef int i
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        self._data[ts.index, :] = self._param.get_all_indices()
-        return 0
-
-    property data:
-        def __get__(self, ):
-            return np.array(self._data)
-
-    def to_dataframe(self):
-        """ Return a `pandas.DataFrame` of the recorder data
-        This DataFrame contains a MultiIndex for the columns with the recorder name
-        as the first level and scenario combination names as the second level. This
-        allows for easy combination with multiple recorder's DataFrames
-        """
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-
-        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
-NumpyArrayIndexParameterRecorder.register()
-
-
-cdef class RollingWindowParameterRecorder(ParameterRecorder):
-    """Records the mean value of a Parameter for the last N timesteps.
-    """
-    def __init__(self, model, Parameter param, int window, *args, **kwargs):
-
-        if "agg_func" in kwargs and "temporal_agg_func" not in kwargs:
-            # Support previous behaviour
-            warnings.warn('The "agg_func" key is deprecated for defining the temporal '
-                          'aggregation in {}. Please "temporal_agg_func" instead.'
-                          .format(self.__class__.__name__))
-            temporal_agg_func = kwargs.get("agg_func")
-        else:
-            temporal_agg_func = kwargs.pop("temporal_agg_func", "mean")
-
-        super(RollingWindowParameterRecorder, self).__init__(model, param, *args, **kwargs)
-        self.window = window
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        cdef int nts = len(self.model.timestepper)
-        self._data = np.zeros((nts, ncomb,), np.float64)
-        self._memory = np.empty((nts, ncomb,), np.float64)
-        self.position = 0
-
-    cpdef reset(self):
-        self._data[...] = 0
-        self.position = 0
-
-    cpdef after(self):
-        cdef int i, n
-        cdef double[:] value
-        cdef ScenarioIndex scenario_index
-        cdef Timestep timestep = self.model.timestepper.current
-
-        for i, scenario_index in enumerate(self.model.scenarios.combinations):
-            self._memory[self.position, i] = self._param.get_value(scenario_index)
-
-        if timestep.index < self.window:
-            n = timestep.index + 1
-        else:
-            n = self.window
-
-        value = self._temporal_aggregator.aggregate_2d(self._memory[0:n, :], axis=0)
-        self._data[timestep.index, :] = value
-
-        self.position += 1
-        if self.position >= self.window:
-            self.position = 0
-
-    property data:
-        def __get__(self):
-            return np.array(self._data, dtype=np.float64)
-
-    def to_dataframe(self):
-        index = self.model.timestepper.datetime_index
-        sc_index = self.model.scenarios.multiindex
-        return pd.DataFrame(data=self.data, index=index, columns=sc_index)
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.parameters import load_parameter
-        parameter = load_parameter(model, data.pop("parameter"))
-        window = int(data.pop("window"))
-        return cls(model, parameter, window, **data)
-
-RollingWindowParameterRecorder.register()
-
-cdef class RollingMeanFlowNodeRecorder(NodeRecorder):
-    """Records the mean flow of a Node for the previous N timesteps
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    node : `pywr.core.Node`
-        The node to record
-    timesteps : int
-        The number of timesteps to calculate the mean flow for
-    name : str (optional)
-        The name of the recorder
-
-    """
-    def __init__(self, model, node, timesteps=None, days=None, name=None, **kwargs):
-        super(RollingMeanFlowNodeRecorder, self).__init__(model, node, name=name, **kwargs)
-        self.model = model
-        if not timesteps and not days:
-            raise ValueError("Either `timesteps` or `days` must be specified.")
-        if timesteps:
-            self.timesteps = int(timesteps)
-        else:
-            self.timesteps = 0
-        if days:
-            self.days = int(days)
-        else:
-            self.days = 0
-        self._data = None
-
-    cpdef setup(self):
-        super(RollingMeanFlowNodeRecorder, self).setup()
-        self.position = 0
-        self._data = np.empty([len(self.model.timestepper), len(self.model.scenarios.combinations)])
-        if self.days > 0:
-            try:
-                self.timesteps = self.days // self.model.timestepper.delta
-            except TypeError:
-                raise TypeError('A rolling window defined as a number of days is only valid with daily time-steps.')
-        if self.timesteps == 0:
-            raise ValueError("Timesteps property of MeanFlowRecorder is less than 1.")
-        self._memory = np.zeros([len(self.model.scenarios.combinations), self.timesteps])
-
-    cpdef after(self):
-        cdef Timestep timestep
-        cdef int i, n
-        cdef double[:] mean_flow
-        # save today's flow
-        for i in range(0, self._memory.shape[0]):
-            self._memory[i, self.position] = self._node._flow[i]
-        # calculate the mean flow
-        timestep = self.model.timestepper.current
-        if timestep.index < self.timesteps:
-            n = timestep.index + 1
-        else:
-            n = self.timesteps
-        # save the mean flow
-        mean_flow = np.mean(self._memory[:, 0:n], axis=1)
-        self._data[<int>(timestep.index), :] = mean_flow
-        # prepare for the next timestep
-        self.position += 1
-        if self.position >= self.timesteps:
-            self.position = 0
-
-    property data:
-        def __get__(self):
-            return np.array(self._data, dtype=np.float64)
-
-    @classmethod
-    def load(cls, model, data):
-        name = data.get("name")
-        node = model._get_node_from_ref(model, data["node"])
-        if "timesteps" in data:
-            timesteps = int(data["timesteps"])
-        else:
-            timesteps = None
-        if "days" in data:
-            days = int(data["days"])
-        else:
-            days = None
-        return cls(model, node, timesteps=timesteps, days=days, name=name)
-
-RollingMeanFlowNodeRecorder.register()
-
-cdef class BaseConstantNodeRecorder(NodeRecorder):
-    """
-    Base class for NodeRecorder classes with a single value for each scenario combination
-    """
-
-    cpdef setup(self):
-        self._values = np.zeros(len(self.model.scenarios.combinations))
-
-    cpdef reset(self):
-        self._values[...] = 0.0
-
-    cpdef after(self):
-        raise NotImplementedError()
-
-    cpdef double[:] values(self) except *:
-        return self._values
-
-
-cdef class TotalDeficitNodeRecorder(BaseConstantNodeRecorder):
-    """
-    Recorder to total the difference between modelled flow and max_flow for a Node
-    """
-    cpdef after(self):
-        cdef double max_flow
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef double days = self.model.timestepper.current.days
-        cdef AbstractNode node = self._node
-        for scenario_index in self.model.scenarios.combinations:
-            max_flow = node.get_max_flow(scenario_index)
-            self._values[scenario_index.global_id] += (max_flow - node._flow[scenario_index.global_id])*days
-
-        return 0
-TotalDeficitNodeRecorder.register()
-
-
-cdef class TotalFlowNodeRecorder(BaseConstantNodeRecorder):
-    """
-    Recorder to total the flow for a Node.
-
-    A factor can be provided to scale the total flow (e.g. for calculating operational costs).
-    """
-    def __init__(self, *args, **kwargs):
-        self.factor = kwargs.pop('factor', 1.0)
-        super(TotalFlowNodeRecorder, self).__init__(*args, **kwargs)
-
-    cpdef after(self):
-        cdef ScenarioIndex scenario_index
-        cdef int i
-        cdef double days = self.model.timestepper.current.days
-        for scenario_index in self.model.scenarios.combinations:
-            i = scenario_index.global_id
-            self._values[i] += self._node._flow[i]*self.factor*days
-        return 0
-TotalFlowNodeRecorder.register()
-
-
-cdef class MeanFlowNodeRecorder(BaseConstantNodeRecorder):
-    """
-    Record the mean flow for a Node.
-
-    A factor can be provided to scale the total flow (e.g. for calculating operational costs).
-    """
-    def __init__(self, *args, **kwargs):
-        self.factor = kwargs.pop('factor', 1.0)
-        super(MeanFlowNodeRecorder, self).__init__(*args, **kwargs)
-
-    cpdef after(self):
-        cdef ScenarioIndex scenario_index
-        cdef int i
-        for scenario_index in self.model.scenarios.combinations:
-            i = scenario_index.global_id
-            self._values[i] += self._node._flow[i]*self.factor
-        return 0
-
-    cpdef finish(self):
-        cdef int i
-        cdef int nt = self.model.timestepper.current.index
-        for i in range(self._values.shape[0]):
-            self._values[i] /= nt
-MeanFlowNodeRecorder.register()
-
-
-cdef class DeficitFrequencyNodeRecorder(BaseConstantNodeRecorder):
-    """Recorder to return the frequency of timesteps with a failure to meet max_flow.
-    """
-    cpdef after(self):
-        cdef double max_flow
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-        cdef AbstractNode node = self._node
-        for scenario_index in self.model.scenarios.combinations:
-            max_flow = node.get_max_flow(scenario_index)
-            if abs(node._flow[scenario_index.global_id] - max_flow) > 1e-6:
-                self._values[scenario_index.global_id] += 1.0
-
-    cpdef finish(self):
-        cdef int i
-        cdef int nt = self.model.timestepper.current.index
-        for i in range(self._values.shape[0]):
-            self._values[i] /= nt
-DeficitFrequencyNodeRecorder.register()
-
-cdef class BaseConstantStorageRecorder(StorageRecorder):
-    """
-    Base class for StorageRecorder classes with a single value for each scenario combination
-    """
-
-    cpdef setup(self):
-        self._values = np.zeros(len(self.model.scenarios.combinations))
-
-    cpdef reset(self):
-        self._values[...] = 0.0
-
-    cpdef after(self):
-        raise NotImplementedError()
-
-    cpdef double[:] values(self) except *:
-        return self._values
-BaseConstantStorageRecorder.register()
-
-cdef class MinimumVolumeStorageRecorder(BaseConstantStorageRecorder):
-    """Record the minimum volume in a `Storage` node during a simulation."""
-    cpdef reset(self):
-        self._values[...] = np.inf
-
-    cpdef after(self):
-        cdef int i
-        for i in range(self._values.shape[0]):
-            self._values[i] = np.min([self._node._volume[i], self._values[i]])
-        return 0
-MinimumVolumeStorageRecorder.register()
-
-cdef class MinimumThresholdVolumeStorageRecorder(BaseConstantStorageRecorder):
-    """Record whether a `Storage` node falls below a particular volume threshold during a simulation.
-
-    This recorder will return a value of `1.0` for scenarios where the volume `Storage` is less
-    than or equal to the threshold at any time-step during the simulation. Otherwise it will return zero.
-    """
-    def __init__(self, model, node, threshold, *args, **kwargs):
-        self.threshold = threshold
-        super(MinimumThresholdVolumeStorageRecorder, self).__init__(model, node, *args, **kwargs)
-
-    cpdef reset(self):
-        self._values[...] = 0.0
-
-    cpdef after(self):
-        cdef int i
-        for i in range(self._values.shape[0]):
-            if self._node._volume[i] <= self.threshold:
-                self._values[i] = 1.0
-        return 0
-MinimumThresholdVolumeStorageRecorder.register()
-
-
-cdef class TimestepCountIndexParameterRecorder(IndexParameterRecorder):
-    """Record the number of times an index parameter exceeds a threshold for each scenario.
-
-    This recorder will count the number of timesteps so will be a daily count when running on a
-    daily timestep.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    parameter : `pywr.core.IndexParameter`
-        The parameter to record
-    threshold : int
-        The threshold to compare the parameter to
-    """
-    def __init__(self, model, IndexParameter parameter, int threshold, *args, **kwargs):
-        super().__init__(model, parameter, *args, **kwargs)
-        self.threshold = threshold
-
-    cpdef setup(self):
-        self._count = np.zeros(len(self.model.scenarios.combinations), np.int32)
-
-    cpdef reset(self):
-        self._count[...] = 0
-
-    cpdef after(self):
-        cdef Timestep ts = self.model.timestepper.current
-        cdef int value
-        cdef ScenarioIndex scenario_index
-
-        for scenario_index in self.model.scenarios.combinations:
-            value = self._param.get_index(scenario_index)
-            if value >= self.threshold:
-                # threshold achieved, increment count
-                self._count[scenario_index.global_id] += 1
-
-    cpdef double[:] values(self) except *:
-        return np.asarray(self._count).astype(np.float64)
-TimestepCountIndexParameterRecorder.register()
-
-
-cdef class AnnualCountIndexThresholdRecorder(Recorder):
-    """
-    For each scenario, count the number of times a list of parameters exceeds a threshold in each year.
-    If multiple parameters exceed in one timestep then it is only counted once.
-
-    Output from data property has shape: (years, scenario combinations)
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    parameters : list
-        List of `pywr.core.IndexParameter` to record against
-    name : str
-        The name of the recorder
-    threshold : int
-        Threshold to compare parameters against
-    """
-    def __init__(self, model, list parameters, str name, int threshold, *args, **kwargs):
-        # Optional different method for aggregating across time.
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'sum')
-        super().__init__(model, name=name, *args, **kwargs)
-        self.parameters = parameters
-        self.threshold = threshold
-        for parameter in self.parameters:
-            self.children.add(parameter)
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    cpdef setup(self):
-        super(AnnualCountIndexThresholdRecorder, self).setup()
-        self._num_years = self.model.timestepper.end.year - self.model.timestepper.start.year + 1
-        self._ncomb = len(self.model.scenarios.combinations)
-        self._data = np.empty([self._num_years, self._ncomb])
-        self._data_this_year = np.zeros([len(self.parameters), self._ncomb])
-
-    cpdef reset(self):
-        self._data[...] = 0
-        self._current_year = -1
-        self._start_year = self.model.timestepper.start.year
-
-    cpdef after(self):
-        cdef Timestep ts = self.model.timestepper.current
-        cdef int idx = self._current_year - self._start_year
-        cdef int p
-        cdef Py_ssize_t i
-        cdef int value
-        cdef ScenarioIndex scenario_index
-        cdef IndexParameter parameter
-
-        if ts.year != self._current_year:
-            # A new year
-            if self._current_year != -1:
-                # As long as at least one year has been run
-                # then save data for previous year
-                for i in range(self._ncomb):
-                    self._data[idx, i] = np.sum(self._data_this_year[:, i])
-
-            self._data_this_year[...] = 0
-            self._current_year = ts.year
-
-        for scenario_index in self.model.scenarios.combinations:
-            for p, parameter in enumerate(self.parameters):
-                value = parameter.get_index(scenario_index)
-                if value >= self.threshold:
-                    self._data_this_year[p, scenario_index.global_id] += 1
-                    break  # if multiple parameters exceed, only count once
-
-    cpdef finish(self):
-        cdef int idx = self._current_year - self._start_year
-        cdef Py_ssize_t i
-        for i in range(self._ncomb):
-            self._data[idx, i] = np.sum(self._data_this_year[:, i])
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    property data:
-        def __get__(self):
-            return np.array(self._data, dtype=np.int16)
-
-    @classmethod
-    def load(cls, model, data):
-        from pywr.parameters import load_parameter
-        parameters = [load_parameter(model, p) for p in data.pop("parameters")]
-        return cls(model, parameters=parameters, **data)
-AnnualCountIndexThresholdRecorder.register()
-
-
-cdef class AnnualTotalFlowRecorder(Recorder):
-    """
-    For each scenario, record the total flow in each year across a list of nodes.
-    Output from data property has shape: (years, scenario combinations)
-
-    A list of factors can be provided to scale the total flow (e.g. for calculating operational costs).
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    name : str
-        The name of the recorder
-    nodes : list
-        List of `pywr.core.Node` instances to record
-    factors : list, optional
-        List of factors to apply to each node
-    """
-    def __init__(self, model, str name, list nodes, *args, **kwargs):
-        temporal_agg_func = kwargs.pop('temporal_agg_func', 'sum')
-        factors = kwargs.pop('factors', None)
-        super().__init__(model, name=name, *args, **kwargs)
-        self.nodes = nodes
-        self.factors = factors
-        self._temporal_aggregator = Aggregator(temporal_agg_func)
-
-    property temporal_agg_func:
-        def __set__(self, agg_func):
-            self._temporal_aggregator.func = agg_func
-
-    property factors:
-        # Property provides np.array style access to the internal memoryview.
-        def __get__(self):
-            return np.array(self._factors)
-        def __set__(self, factors):
-            if factors is None:
-                factors = np.array([1.0 for n in self.nodes])
-            self._factors = np.array(factors)
-
-    cpdef setup(self):
-        super(AnnualTotalFlowRecorder, self).setup()
-        self._num_years = self.model.timestepper.end.year - self.model.timestepper.start.year + 1
-        self._ncomb = len(self.model.scenarios.combinations)
-        self._data = np.empty([self._num_years, self._ncomb])
-
-    cpdef reset(self):
-        self._data[...] = 0
-        self._current_year = -1
-        self._start_year = self.model.timestepper.start.year
-
-    cpdef after(self):
-        cdef int i, j
-        cdef Timestep ts = self.model.timestepper.current
-        cdef int idx = ts.year - self._start_year
-        cdef AbstractNode node
-        cdef double[:] flow = np.zeros(self._ncomb, np.float64)
-
-        for i in range(self._ncomb):
-            for j, node in enumerate(self.nodes):
-                self._data[idx, i] += node._flow[i] * self._factors[j]
-
-    cpdef double[:] values(self) except *:
-        """Compute a value for each scenario using `temporal_agg_func`.
-        """
-        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
-
-    property data:
-        def __get__(self):
-            return np.array(self._data, dtype=np.float64)
-
-    @classmethod
-    def load(cls, model, data):
-        nodes = [model._get_node_from_ref(model, n) for n in data.pop("nodes")]
-        return cls(model, nodes=nodes, **data)
-AnnualTotalFlowRecorder.register()
-
-
-cdef class AnnualCountIndexParameterRecorder(IndexParameterRecorder):
-    """ Record the number of years where an IndexParameter is greater than or equal to a threshold """
-    def __init__(self, model, IndexParameter param, int threshold, *args, **kwargs):
-        super(AnnualCountIndexParameterRecorder, self).__init__(model, param, *args, **kwargs)
-        self.threshold = threshold
-
-    cpdef setup(self):
-        self._count = np.zeros(len(self.model.scenarios.combinations), np.int32)
-        self._current_max = np.zeros_like(self._count)
-
-    cpdef reset(self):
-        self._count[...] = 0
-        self._current_max[...] = 0
-        self._current_year = -1
-
-    cpdef after(self):
-        cdef int i, ncomb, value
-        cdef ScenarioIndex scenario_index
-        cdef Timestep ts = self.model.timestepper.current
-
-        ncomb = len(self.model.scenarios.combinations)
-
-        if ts.year != self._current_year:
-            # A new year
-            if self._current_year != -1:
-                # As long as at least one year has been run
-                # then update the count if threshold equal to or exceeded
-                for i in range(ncomb):
-                    if self._current_max[i] >= self.threshold:
-                        self._count[i] += 1
-
-            # Finally reset current maximum and update current year
-            self._current_max[...] = 0
-            self._current_year = ts.year
-
-        for scenario_index in self.model.scenarios.combinations:
-            # Get current parameter value
-            value = self._param.get_index(scenario_index)
-
-            # Update annual max if a new maximum is found
-            if value > self._current_max[scenario_index.global_id]:
-                self._current_max[scenario_index.global_id] = value
-
-        return 0
-
-    cpdef finish(self):
-        cdef int i
-        cdef int ncomb = len(self.model.scenarios.combinations)
-        # Complete the current year by updating the count if threshold equal to or exceeded
-        for i in range(ncomb):
-            if self._current_max[i] >= self.threshold:
-                self._count[i] += 1
-
-    cpdef double[:] values(self) except *:
-        return np.asarray(self._count).astype(np.float64)
-AnnualCountIndexParameterRecorder.register()
-
-
-def load_recorder(model, data, recorder_name=None):
-    recorder = None
-
-    if isinstance(data, str):
-        recorder_name = data
-
-    # check if recorder has already been loaded
-    for rec in model.recorders:
-        if rec.name == recorder_name:
-            recorder = rec
-            break
-
-    if recorder is None and isinstance(data, str):
-        # recorder was requested by name, but hasn't been loaded yet
-        if hasattr(model, "_recorders_to_load"):
-            # we're still in the process of loading data from JSON and
-            # the parameter requested hasn't been loaded yet - do it now
-            try:
-                data = model._recorders_to_load.pop(recorder_name)
-            except KeyError:
-                raise KeyError("Unknown recorder: '{}'".format(data))
-            recorder = load_recorder(model, data)
-        else:
-            raise KeyError("Unknown recorder: '{}'".format(data))
-
-    if recorder is None:
-        recorder_type = data['type']
-
-        name = recorder_type.lower()
-        try:
-            cls = recorder_registry[name]
-        except KeyError:
-            if name.endswith("recorder"):
-                name = name.replace("recorder", "")
-            else:
-                name += "recorder"
-            try:
-                cls = recorder_registry[name]
-            except KeyError:
-                raise NotImplementedError('Unrecognised recorder type "{}"'.format(recorder_type))
-
-        del(data["type"])
-        recorder = cls.load(model, data)
-
-    return recorder
-
-
-cdef class BaseConstantParameterRecorder(ParameterRecorder):
-    """Base class for `ParameterRecorder` classes with a single value for each scenario combination
-    """
-    cpdef setup(self):
-        self._values = np.zeros(len(self.model.scenarios.combinations))
-
-    cpdef reset(self):
-        self._values[...] = 0.0
-
-    cpdef after(self):
-        raise NotImplementedError()
-
-    cpdef double[:] values(self) except *:
-        return self._values
-
-
-cdef class TotalParameterRecorder(BaseConstantParameterRecorder):
-    """Record the total value of a `Parameter` during a simulation.
-
-    This recorder can be used to track the sum total of the values returned by a
-    `Parameter` during a models simulation. An optional factor can be provided to
-    apply a linear scaling of the values. If the parameter represents a flux
-    the `integrate` keyword argument can be used to multiply the values by the time-step
-    length in days.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.Parameter`
-        The parameter to record.
-    name : str (optional)
-        The name of the recorder
-    factor : float (default=1.0)
-        Scaling factor for the values of `param`.
-    integrate : bool (default=False)
-        Whether to multiply by the time-step length in days during summation.
-    """
-    def __init__(self, *args, **kwargs):
-        self.factor = kwargs.pop('factor', 1.0)
-        self.integrate = kwargs.pop('integrate', False)
-        super(TotalParameterRecorder, self).__init__(*args, **kwargs)
-
-    cpdef after(self):
-        cdef ScenarioIndex scenario_index
-        cdef int i
-        cdef double[:] values
-        cdef factor = self.factor
-
-        if self.integrate:
-            factor *= self.model.timestepper.current.days
-
-        values = self._param.get_all_values()
-        for scenario_index in self.model.scenarios.combinations:
-            i = scenario_index.global_id
-            self._values[i] += values[i]*factor
-        return 0
-TotalParameterRecorder.register()
-
-
-cdef class MeanParameterRecorder(BaseConstantParameterRecorder):
-    """Record the mean value of a `Parameter` during a simulation.
-
-    This recorder can be used to track the mean of the values returned by a
-    `Parameter` during a models simulation. An optional factor can be provided to
-    apply a linear scaling of the values.
-
-    Parameters
-    ----------
-    model : `pywr.core.Model`
-    param : `pywr.parameters.Parameter`
-        The parameter to record.
-    name : str (optional)
-        The name of the recorder
-    factor : float (default=1.0)
-        Scaling factor for the values of `param`.
-    """
-    def __init__(self, *args, **kwargs):
-        self.factor = kwargs.pop('factor', 1.0)
-        super(MeanParameterRecorder, self).__init__(*args, **kwargs)
-
-    cpdef after(self):
-        cdef ScenarioIndex scenario_index
-        cdef int i
-        cdef double[:] values
-        cdef factor = self.factor
-
-        values = self._param.get_all_values()
-        for scenario_index in self.model.scenarios.combinations:
-            i = scenario_index.global_id
-            self._values[i] += values[i]*factor
-        return 0
-
-    cpdef finish(self):
-        cdef int i
-        cdef int nt = self.model.timestepper.current.index
-        for i in range(self._values.shape[0]):
-            self._values[i] /= nt
-MeanParameterRecorder.register()
+import numpy as np
+cimport numpy as np
+from scipy.stats import percentileofscore
+import pandas as pd
+import warnings
+
+
+recorder_registry = {}
+
+cdef enum AggFuncs:
+    SUM = 0
+    MIN = 1
+    MAX = 2
+    MEAN = 3
+    MEDIAN = 4
+    PRODUCT = 5
+    CUSTOM = 6
+    PERCENTILE = 7
+    PERCENTILEOFSCORE = 8
+    COUNT_NONZERO = 9
+_agg_func_lookup = {
+    "sum": AggFuncs.SUM,
+    "min": AggFuncs.MIN,
+    "max": AggFuncs.MAX,
+    "mean": AggFuncs.MEAN,
+    "median": AggFuncs.MEDIAN,
+    "product": AggFuncs.PRODUCT,
+    "custom": AggFuncs.CUSTOM,
+    "percentile": AggFuncs.PERCENTILE,
+    "percentileofscore": AggFuncs.PERCENTILEOFSCORE,
+    "count_nonzero": AggFuncs.COUNT_NONZERO
+}
+_agg_func_lookup_reverse = {v: k for k, v in _agg_func_lookup.items()}
+
+cdef enum ObjDirection:
+    NONE = 0
+    MAXIMISE = 1
+    MINIMISE = 2
+_obj_direction_lookup = {
+    "maximize": ObjDirection.MAXIMISE,
+    "maximise": ObjDirection.MAXIMISE,
+    "max": ObjDirection.MAXIMISE,
+    "minimise": ObjDirection.MINIMISE,
+    "minimize": ObjDirection.MINIMISE,
+    "min": ObjDirection.MINIMISE,
+}
+
+cdef class Aggregator:
+    """Utility class for computing aggregate values.
+
+    Users are unlikely to use this class directly. Instead `Recorder` sub-classes will use this functionality
+    to aggregate their results across different dimensions (e.g. time, scenarios, etc.).
+
+    Parameters
+    ==========
+    func : str, dict or callable
+        The aggregation function to use. Can be a string or dict defining aggregation functions, or a callable
+        custom function that performs aggregation.
+
+        When a string it can be one of: "sum", "min", "max", "mean", "median", "product", or "count_nonzero". These
+        strings map to and cause the aggregator to use the corresponding `numpy` functions.
+
+        A dict can be provided containing a "func" key, and optional "args" and "kwargs" keys. The value of "func"
+        should be a string corresponding to the aforementioned numpy function names with the additional options of
+        "percentile" and "percentileofscore". These latter two functions require additional arguments (the percentile
+        and score) to function and must be provided as the values in either the "args" or "kwargs" keys of the
+        dictionary. Please refer to the corresponding numpy (or scipy) function definitions for documentation on these
+        arguments.
+
+        Finally, a callable function can be given. This function must accept either a 1D or 2D numpy array as the
+        first argument, and support the "axis" keyword as integer value that determines which axis over which the
+        function should apply aggregation. The axis keyword is only supplied when a 2D array is given. Therefore,`
+        the callable function should behave in a similar fashion to the numpy functions.
+
+    Examples
+    ========
+    >>> Aggregator("sum")
+    >>> Aggregator({"func": "percentile", "args": [95],"kwargs": {}})
+    >>> Aggregator({"func": "percentileofscore", "kwargs": {"score": 0.5, "kind": "rank"}})
+
+    """
+    def __init__(self, func):
+        self.func = func
+
+    property func:
+        def __get__(self):
+            if self._func == AggFuncs.CUSTOM:
+                return self._user_func
+            return _agg_func_lookup_reverse[self._func]
+        def __set__(self, func):
+            self._user_func = None
+            func_args = []
+            func_kwargs = {}
+            if isinstance(func, str):
+                func_type = _agg_func_lookup[func.lower()]
+            elif isinstance(func, dict):
+                func_type = _agg_func_lookup[func['func']]
+                func_args = func.get('args', [])
+                func_kwargs = func.get('kwargs', {})
+            elif callable(func):
+                self._user_func = func
+                func_type = AggFuncs.CUSTOM
+            else:
+                raise ValueError("Unrecognised aggregation function: \"{}\".".format(func))
+            self._func = func_type
+            self.func_args = func_args
+            self.func_kwargs = func_kwargs
+
+    cpdef double aggregate_1d(self, double[:] data, ignore_nan=False) except *:
+        """Compute an aggregated value across 1D array.
+        """
+        cdef double[:] values = data
+
+        if ignore_nan:
+            values = np.array(values)[~np.isnan(values)]
+
+        if self._func == AggFuncs.PRODUCT:
+            return np.product(values)
+        elif self._func == AggFuncs.SUM:
+            return np.sum(values)
+        elif self._func == AggFuncs.MAX:
+            return np.max(values)
+        elif self._func == AggFuncs.MIN:
+            return np.min(values)
+        elif self._func == AggFuncs.MEAN:
+            return np.mean(values)
+        elif self._func == AggFuncs.MEDIAN:
+            return np.median(values)
+        elif self._func == AggFuncs.CUSTOM:
+            return self._user_func(np.array(values))
+        elif self._func == AggFuncs.PERCENTILE:
+            return np.percentile(values, *self.func_args, **self.func_kwargs)
+        elif self._func == AggFuncs.PERCENTILEOFSCORE:
+            return percentileofscore(values, *self.func_args, **self.func_kwargs)
+        elif self._func == AggFuncs.COUNT_NONZERO:
+            return np.count_nonzero(values)
+        else:
+            raise ValueError('Aggregation function code "{}" not recognised.'.format(self._func))
+
+    cpdef double[:] aggregate_2d(self, double[:, :] data, axis=0, ignore_nan=False) except *:
+        """Compute an aggregated value along an axis of a 2D array.
+        """
+        cdef double[:, :] values = data
+        cdef Py_ssize_t i
+
+        if ignore_nan:
+            values = np.array(values)[~np.isnan(values)]
+
+        if self._func == AggFuncs.PRODUCT:
+            return np.product(values, axis=axis)
+        elif self._func == AggFuncs.SUM:
+            return np.sum(values, axis=axis)
+        elif self._func == AggFuncs.MAX:
+            return np.max(values, axis=axis)
+        elif self._func == AggFuncs.MIN:
+            return np.min(values, axis=axis)
+        elif self._func == AggFuncs.MEAN:
+            return np.mean(values, axis=axis)
+        elif self._func == AggFuncs.MEDIAN:
+            return np.median(values, axis=axis)
+        elif self._func == AggFuncs.CUSTOM:
+            return self._user_func(np.array(values), axis=axis)
+        elif self._func == AggFuncs.PERCENTILE:
+            return np.percentile(values, *self.func_args, axis=axis, **self.func_kwargs)
+        elif self._func == AggFuncs.PERCENTILEOFSCORE:
+            # percentileofscore doesn't support the axis argument
+            # we must therefore iterate over the array
+            if axis == 0:
+                out = np.empty(data.shape[1])
+                for i in range(data.shape[1]):
+                    out[i] = percentileofscore(values[:, i], *self.func_args, **self.func_kwargs)
+            elif axis == 1:
+                out = np.empty(data.shape[0])
+                for i in range(data.shape[0]):
+                    out[i] = percentileofscore(values[i, :], *self.func_args, **self.func_kwargs)
+            else:
+                raise ValueError('Axis "{}" not recognised for percentileofscore function.'.format(axis))
+            return out
+        elif self._func == AggFuncs.COUNT_NONZERO:
+            return np.count_nonzero(values, axis=axis).astype(np.float64)
+        else:
+            raise ValueError('Aggregation function code "{}" not recognised.'.format(self._func))
+
+
+cdef class Recorder(Component):
+    """Base class for recording information from a `pywr.model.Model`.
+
+    Recorder components are used to calculate, aggregate and save data from a simulation. This
+    base class provides the basic functionality for all recorders.
+
+    Parameters
+    ==========
+    model : `pywr.core.Model`
+    agg_func : str or callable (default="mean")
+        Scenario aggregation function to use when `aggregated_value` is called.
+    name : str (default=None)
+        Name of the recorder.
+    comment : str (default=None)
+        Comment or description of the recorder.
+    ignore_nan : bool (default=False)
+        Flag to ignore NaN values when calling `aggregated_value`.
+    is_objective : {None, 'maximize', 'maximise', 'max', 'minimize', 'minimise', 'min'}
+        Flag to denote the direction, if any, of optimisation undertaken with this recorder.
+    epsilon : float (default=1.0)
+        Epsilon distance used by some optimisation algorithms.
+    constraint_lower_bounds, constraint_upper_bounds : double (default=None)
+        The value(s) to use for lower and upper bound definitions. These values determine whether the recorder
+        instance is marked as a constraint. Either bound can be `None` (the default) to disable the respective
+        bound. If both bounds are `None` then the `is_constraint` property will return `False`. The lower bound must
+        be strictly less than the upper bound. An equality constraint can be created by setting both bounds to the
+        same value.
+
+        The constraint bounds are not used during model simulation. Instead they are intended for use by optimisation
+        wrappers (or other external tools) to define constrained optimisation problems.
+    """
+    def __init__(self, model, agg_func="mean", ignore_nan=False, is_objective=None, epsilon=1.0,
+                 name=None, constraint_lower_bounds=None, constraint_upper_bounds=None, **kwargs):
+        # Default the constraints internal values to be +/- inf.
+        # This ensures the bounds checking works later in the init.
+        self._constraint_lower_bounds = -np.inf
+        self._constraint_upper_bounds = np.inf
+        if name is None:
+            name = self.__class__.__name__.lower()
+        super(Recorder, self).__init__(model, name=name, **kwargs)
+        self.ignore_nan = ignore_nan
+        self.is_objective = is_objective
+        self.epsilon = epsilon
+        self.constraint_lower_bounds = constraint_lower_bounds
+        self.constraint_upper_bounds = constraint_upper_bounds
+        # Create the aggregator for scenarios
+        self._scenario_aggregator = Aggregator(agg_func)
+
+    property agg_func:
+        def __get__(self):
+            return self._scenario_aggregator.func
+        def __set__(self, agg_func):
+            self._scenario_aggregator.func = agg_func
+
+    property is_objective:
+        def __set__(self, value):
+            if value is None:
+                self._is_objective = ObjDirection.NONE
+            else:
+                self._is_objective = _obj_direction_lookup[value]
+        def __get__(self):
+            if self._is_objective == ObjDirection.NONE:
+                return None
+            elif self._is_objective == ObjDirection.MAXIMISE:
+                return 'maximise'
+            elif self._is_objective == ObjDirection.MINIMISE:
+                return 'minimise'
+            else:
+                raise ValueError("Objective direction type not recognised.")
+
+    property constraint_lower_bounds:
+        def __set__(self, value):
+            if value is None:
+                self._constraint_lower_bounds = -np.inf
+            else:
+                if self.constraint_upper_bounds is not None and value > self.constraint_upper_bounds:
+                    raise ValueError('Lower bounds can not be larger than the upper bounds.')
+                self._constraint_lower_bounds = value
+        def __get__(self):
+            if np.isneginf(self._constraint_lower_bounds):
+                return None
+            else:
+                return self._constraint_lower_bounds
+
+    property constraint_upper_bounds:
+        def __set__(self, value):
+            if value is None:
+                self._constraint_upper_bounds = np.inf
+            else:
+                if self.constraint_lower_bounds is not None and value < self.constraint_lower_bounds:
+                    raise ValueError('Upper bounds can not be smaller than the lower bounds.')
+                self._constraint_upper_bounds = value
+        def __get__(self):
+            if np.isinf(self._constraint_upper_bounds):
+                return None
+            else:
+                return self._constraint_upper_bounds
+
+    @property
+    def is_equality_constraint(self):
+        """Returns true if upper and lower constraint bounds are both defined and equal to one another."""
+        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is not None and \
+               self.constraint_lower_bounds == self.constraint_upper_bounds
+
+    @property
+    def is_double_bounded_constraint(self):
+        """Returns true if upper and lower constraint bounds are both defined and not-equal to one another."""
+        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is not None and \
+               self.constraint_lower_bounds != self.constraint_upper_bounds
+
+    @property
+    def is_lower_bounded_constraint(self):
+        """Returns true if lower constraint bounds is defined and upper constraint bounds is not."""
+        return self.constraint_upper_bounds is None and self.constraint_lower_bounds is not None
+
+    @property
+    def is_upper_bounded_constraint(self):
+        """Returns true if upper constraint bounds is defined and lower constraint bounds is not."""
+        return self.constraint_upper_bounds is not None and self.constraint_lower_bounds is None
+
+    @property
+    def is_constraint(self):
+        """Returns true if either upper or lower constraint bounds is defined."""
+        return self.constraint_upper_bounds is not None or self.constraint_lower_bounds is not None
+
+    def is_constraint_violated(self):
+        """Returns true if the value from this Recorder violates its constraint bounds.
+
+        If no constraint bounds are defined (i.e. self.is_constraint == False) then a ValueError is raised.
+        """
+        value = self.aggregated_value()
+        if self.is_equality_constraint:
+            feasible = value == self.constraint_lower_bounds
+        elif self.is_double_bounded_constraint:
+            feasible = self.constraint_lower_bounds <= value <= self.constraint_upper_bounds
+        elif self.is_lower_bounded_constraint:
+            feasible = self.constraint_lower_bounds <= value
+        elif self.is_upper_bounded_constraint:
+            feasible = value <= self.constraint_upper_bounds
+        else:
+            raise ValueError(f'Recorder "{self.name}" has no constraint bounds defined.')
+        return not feasible
+
+    def __repr__(self):
+        return '<{} "{}">'.format(self.__class__.__name__, self.name)
+
+    cpdef double aggregated_value(self) except *:
+        cdef double[:] values = self.values()
+        return self._scenario_aggregator.aggregate_1d(values)
+
+    cpdef double[:] values(self) except *:
+        raise NotImplementedError()
+
+    @classmethod
+    def load(cls, model, data):
+        try:
+            node_name = data["node"]
+        except KeyError:
+            pass
+        else:
+            data["node"] = model._get_node_from_ref(model, node_name)
+        return cls(model, **data)
+
+    @classmethod
+    def register(cls):
+        recorder_registry[cls.__name__.lower()] = cls
+
+    @classmethod
+    def unregister(cls):
+        del(recorder_registry[cls.__name__.lower()])
+
+cdef class AggregatedRecorder(Recorder):
+    """
+    This Recorder is used to aggregate across multiple other Recorder objects.
+
+    The class provides a method to produce a complex aggregated recorder by taking
+    the results of other records. The `.values()` method first collects unaggregated values
+    from the provided recorders. These are then aggregated on a per scenario basis and returned
+    by this classes `.values()` method. This method allows `AggregatedRecorder` to be used as
+    a recorder for in other `AggregatedRecorder` instances.
+
+    By default the same `agg_func` function is used for both steps, but an optional
+    `recorder_agg_func` can undertake a different aggregation across scenarios. For
+    example summing recorders per scenario, and then taking a mean of the sum totals.
+
+    Parameters
+    ==========
+    model : `pywr.core.Model`
+    recorders: iterable of `Recorder` objects.
+        The other `Recorder` instances to perform aggregation over.
+    agg_func : str or callable, optional
+        Scenario aggregation function to use when `aggregated_value` is called (default="mean").
+    recorder_agg_func : str or callable, optional
+        Recorder aggregation function to use when `aggregated_value` is called (default=`agg_func`).
+    """
+    def __init__(self, model, recorders, **kwargs):
+        # Optional different method for aggregating across self.recorders scenarios
+        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
+
+        if isinstance(agg_func, str):
+            agg_func = _agg_func_lookup[agg_func.lower()]
+        elif callable(agg_func):
+            self.recorder_agg_func = agg_func
+            agg_func = AggFuncs.CUSTOM
+        else:
+            raise ValueError("Unrecognised recorder aggregation function: \"{}\".".format(agg_func))
+        self._recorder_agg_func = agg_func
+
+        super(AggregatedRecorder, self).__init__(model, **kwargs)
+        self.recorders = list(recorders)
+
+        for rec in self.recorders:
+            self.children.add(rec)
+
+    cpdef double[:] values(self) except *:
+        cdef Recorder recorder
+        cdef double[:] value, value2
+        assert(len(self.recorders))
+        cdef int n = len(self.model.scenarios.combinations)
+        cdef int i
+
+        if self._recorder_agg_func == AggFuncs.PRODUCT:
+            value = np.ones(n, np.float64)
+            for recorder in self.recorders:
+                value2 = recorder.values()
+                for i in range(n):
+                    value[i] *= value2[i]
+        elif self._recorder_agg_func == AggFuncs.SUM:
+            value = np.zeros(n, np.float64)
+            for recorder in self.recorders:
+                value2 = recorder.values()
+                for i in range(n):
+                    value[i] += value2[i]
+        elif self._recorder_agg_func == AggFuncs.MAX:
+            value = np.empty(n)
+            value[:] = np.NINF
+            for recorder in self.recorders:
+                value2 = recorder.values()
+                for i in range(n):
+                    if value2[i] > value[i]:
+                        value[i] = value2[i]
+        elif self._recorder_agg_func == AggFuncs.MIN:
+            value = np.empty(n)
+            value[:] = np.PINF
+            for recorder in self.recorders:
+                value2 = recorder.values()
+                for i in range(n):
+                    if value2[i] < value[i]:
+                        value[i] = value2[i]
+        elif self._recorder_agg_func == AggFuncs.MEAN:
+            value = np.zeros(n, np.float64)
+            for recorder in self.recorders:
+                value2 = recorder.values()
+                for i in range(n):
+                    value[i] += value2[i]
+            for i in range(n):
+                value[i] /= len(self.recorders)
+        else:
+            value = self.recorder_agg_func([recorder.values() for recorder in self.recorders], axis=0)
+        return value
+
+    @classmethod
+    def load(cls, model, data):
+        recorder_names = data.pop("recorders")
+        recorders = [load_recorder(model, name) for name in recorder_names]
+        rec = cls(model, recorders, **data)
+        return rec
+
+AggregatedRecorder.register()
+
+
+cdef class NodeRecorder(Recorder):
+    def __init__(self, model, AbstractNode node, name=None, **kwargs):
+        if name is None:
+            name = "{}.{}".format(self.__class__.__name__.lower(), node.name)
+        super(NodeRecorder, self).__init__(model, name=name, **kwargs)
+        self._node = node
+        node._recorders.append(self)
+
+    cpdef double[:] values(self) except *:
+        return self._node._flow
+
+    property node:
+        def __get__(self):
+            return self._node
+
+    def __repr__(self):
+        return '<{} on {} "{}">'.format(self.__class__.__name__, self.node, self.name)
+
+NodeRecorder.register()
+
+
+cdef class StorageRecorder(Recorder):
+    def __init__(self, model, AbstractStorage node, name=None, **kwargs):
+        if name is None:
+            name = "{}.{}".format(self.__class__.__name__.lower(), node.name)
+        super(StorageRecorder, self).__init__(model, name=name, **kwargs)
+        self._node = node
+        node._recorders.append(self)
+
+    cpdef double[:] values(self) except *:
+        return self._node._volume
+
+    property node:
+        def __get__(self):
+            return self._node
+
+    def __repr__(self):
+        return '<{} on {} "{}">'.format(self.__class__.__name__, self.node, self.name)
+
+StorageRecorder.register()
+
+
+cdef class ParameterRecorder(Recorder):
+    """Base class for recorders that track `Parameter` values.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.Parameter`
+        The parameter to record.
+    name : str (optional)
+        The name of the recorder
+    """
+    def __init__(self, model, Parameter param, name=None, **kwargs):
+        if name is None:
+            name = "{}.{}".format(self.__class__.__name__.lower(), param.name)
+        super(ParameterRecorder, self).__init__(model, name=name, **kwargs)
+        self._param = param
+        param.parents.add(self)
+
+    property parameter:
+        def __get__(self):
+            return self._param
+
+    def __repr__(self):
+        return '<{} on {} "{}" ({})>'.format(self.__class__.__name__, repr(self.parameter), self.name, hex(id(self)))
+
+    def __str__(self):
+        return '<{} on {} "{}">'.format(self.__class__.__name__, self.parameter, self.name)
+
+    @classmethod
+    def load(cls, model, data):
+        # when the parameter being recorder is defined inline (i.e. not in the
+        # parameters section, but within the node) we need to make sure the
+        # node has been loaded first
+        try:
+            node_name = data["node"]
+        except KeyError:
+            node = None
+        else:
+            del(data["node"])
+            node = model._get_node_from_ref(model, node_name)
+        from pywr.parameters import load_parameter
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+
+ParameterRecorder.register()
+
+
+cdef class IndexParameterRecorder(Recorder):
+    def __init__(self, model, IndexParameter param, name=None, **kwargs):
+        if name is None:
+            name = "{}.{}".format(self.__class__.__name__.lower(), param.name)
+        super(IndexParameterRecorder, self).__init__(model, name=name, **kwargs)
+        self._param = param
+        param.parents.add(self)
+
+    property parameter:
+        def __get__(self):
+            return self._param
+
+    def __repr__(self):
+        return '<{} on {} "{}" ({})>'.format(self.__class__.__name__, repr(self.parameter), self.name, hex(id(self)))
+
+    def __str__(self):
+        return '<{} on {} "{}">'.format(self.__class__.__name__, self.parameter, self.name)
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.parameters import load_parameter
+        parameter = load_parameter(model, data.pop("parameter"))
+        return cls(model, parameter, **data)
+
+IndexParameterRecorder.register()
+
+
+cdef class NumpyArrayNodeRecorder(NodeRecorder):
+    """Recorder for timeseries information from a `Node`.
+
+    This class stores flow from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    factor: float (default=1.0)
+        A factor can be provided to scale the total flow (e.g. for calculating operational costs).
+
+    See also
+    --------
+    NumpyArrayNodeDeficitRecorder
+    NumpyArrayNodeSuppliedRatioRecorder
+    NumpyArrayNodeCurtailmentRatioRecorder
+    """
+    def __init__(self, model, AbstractNode node, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
+        factor = kwargs.pop('factor', 1.0)
+        super(NumpyArrayNodeRecorder, self).__init__(model, node, **kwargs)
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+        self.factor = factor
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        cdef int nts = len(self.model.timestepper)
+        self._data = np.zeros((nts, ncomb))
+
+    cpdef reset(self):
+        self._data[:, :] = 0.0
+
+    cpdef after(self):
+        cdef int i
+        cdef Timestep ts = self.model.timestepper.current
+        for i in range(self._data.shape[1]):
+            self._data[ts.index, i] = self._node._flow[i]*self.factor
+        return 0
+
+    property data:
+        def __get__(self, ):
+            return np.array(self._data)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+
+NumpyArrayNodeRecorder.register()
+
+
+cdef class NumpyArrayNodeDeficitRecorder(NumpyArrayNodeRecorder):
+    """Recorder for timeseries of deficit from a `Node`.
+
+    This class stores deficit from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+
+    Notes
+    -----
+    Deficit is calculated as the difference between `max_flow` and `self.node.flow` (i.e. the actual
+    flow allocated during the time-step)::
+
+        deficit = max_flow - actual_flow
+
+    See also
+    --------
+    NumpyArrayNodeRecorder
+    NumpyArrayNodeSuppliedRatioRecorder
+    NumpyArrayNodeCurtailmentRatioRecorder
+    """
+    cpdef after(self):
+        cdef double max_flow
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef Node node = self._node
+        for scenario_index in self.model.scenarios.combinations:
+            max_flow = node.get_max_flow(scenario_index)
+            self._data[ts.index,scenario_index.global_id] = max_flow - node._flow[scenario_index.global_id]
+        return 0
+NumpyArrayNodeDeficitRecorder.register()
+
+
+cdef class NumpyArrayNodeSuppliedRatioRecorder(NumpyArrayNodeRecorder):
+    """Recorder for timeseries of ratio of supplied flow from a `Node`.
+
+    This class stores supply ratio from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+
+    Notes
+    -----
+    Supply ratio is calculated calculated as the ratio of `self.node.flow` to `self.node.max_flow`
+    for each time-step::
+
+        supply_ratio = actual_flow / max_flow
+
+    See also
+    --------
+    NumpyArrayNodeRecorder
+    NumpyArrayNodeDeficitRecorder
+    NumpyArrayNodeCurtailmentRatioRecorder
+    """
+    cpdef after(self):
+        cdef double max_flow
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef Node node = self._node
+        for scenario_index in self.model.scenarios.combinations:
+            max_flow = node.get_max_flow(scenario_index)
+            self._data[ts.index,scenario_index.global_id] = node._flow[scenario_index.global_id] / max_flow
+        return 0
+NumpyArrayNodeSuppliedRatioRecorder.register()
+
+
+cdef class NumpyArrayNodeCurtailmentRatioRecorder(NumpyArrayNodeRecorder):
+    """Recorder for timeseries of curtailment ratio from a `Node`.
+
+    This class stores curtailment ratio from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+
+    Notes
+    -----
+    Curtailment ratio is calculated calculated as one minues the ratio of `self.node.flow` to
+    `self.node.max_flow` for each time-step::
+
+        curtailment_ratio = 1 - actual_flow / max_flow
+
+    See also
+    --------
+    NumpyArrayNodeRecorder
+    NumpyArrayNodeDeficitRecorder
+    NumpyArrayNodeSuppliedRatioRecorder
+    """
+    cpdef after(self):
+        cdef double max_flow
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef Node node = self._node
+        for scenario_index in self.model.scenarios.combinations:
+            max_flow = node.get_max_flow(scenario_index)
+            self._data[ts.index,scenario_index.global_id] = 1 - node._flow[scenario_index.global_id] / max_flow
+NumpyArrayNodeCurtailmentRatioRecorder.register()
+
+
+cdef class FlowDurationCurveRecorder(NumpyArrayNodeRecorder):
+    """
+    This recorder calculates a flow duration curve for each scenario.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        The node to record
+    percentiles : array
+        The percentiles to use in the calculation of the flow duration curve.
+        Values must be in the range 0-100.
+    agg_func: str, optional
+        function used for aggregating the FDC across percentiles.
+        Numpy style functions that support an axis argument are supported.
+    fdc_agg_func: str, optional
+        optional different function for aggregating across scenarios.
+    """
+    def __init__(self, model, AbstractNode node, percentiles, **kwargs):
+
+        # Optional different method for aggregating across percentiles
+        if 'fdc_agg_func' in kwargs:
+            # Support previous behaviour
+            warnings.warn('The "fdc_agg_func" key is deprecated for defining the temporal '
+                          'aggregation in {}. Please "temporal_agg_func" instead.'
+                          .format(self.__class__.__name__))
+            if "temporal_agg_func" in kwargs:
+                raise ValueError('Both "fdc_agg_func" and "temporal_agg_func" keywords given.'
+                                 'This is ambiguous. Please use "temporal_agg_func" only.')
+            kwargs["temporal_agg_func"] = kwargs.pop("fdc_agg_func")
+
+        super(FlowDurationCurveRecorder, self).__init__(model, node, **kwargs)
+        self._percentiles = np.asarray(percentiles, dtype=np.float64)
+
+    cpdef finish(self):
+        self._fdc = np.percentile(np.asarray(self._data), np.asarray(self._percentiles), axis=0)
+
+    property fdc:
+        def __get__(self, ):
+            return np.array(self._fdc)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._fdc, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self._percentiles
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=np.array(self.fdc), index=index, columns=sc_index)
+
+FlowDurationCurveRecorder.register()
+
+
+cdef class SeasonalFlowDurationCurveRecorder(FlowDurationCurveRecorder):
+    """
+    This recorder calculates a flow duration curve for each scenario for a given season
+    specified in months.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        The node to record
+    percentiles : array
+        The percentiles to use in the calculation of the flow duration curve.
+        Values must be in the range 0-100.
+    agg_func: str, optional
+        function used for aggregating the FDC across percentiles.
+        Numpy style functions that support an axis argument are supported.
+    fdc_agg_func: str, optional
+        optional different function for aggregating across scenarios.
+    months: array
+        The numeric values of the months the flow duration curve should be calculated for.
+    """
+
+    def __init__(self, model, AbstractNode node, percentiles, months, **kwargs):
+        super(SeasonalFlowDurationCurveRecorder, self).__init__(model, node, percentiles, **kwargs)
+        self._months = set(months)
+
+    cpdef finish(self):
+        # this is a def method rather than cpdef because closures inside cpdef functions are not supported yet.
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+
+        df = pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+        mask = np.asarray(df.index.map(self.is_season))
+        self._fdc = np.percentile(df.loc[mask, :], np.asarray(self._percentiles), axis=0)
+
+    def is_season(self, x):
+        return x.month in self._months
+
+SeasonalFlowDurationCurveRecorder.register()
+
+cdef class FlowDurationCurveDeviationRecorder(FlowDurationCurveRecorder):
+    """
+    This recorder calculates a Flow Duration Curve (FDC) for each scenario and then
+    calculates their deviation from upper and lower target FDCs. The 2nd dimension of the target
+    duration curves and percentiles list must be of the same length and have the same
+    order (high to low values or low to high values).
+
+    Deviation is calculated as positive if actual FDC is above the upper target or below the lower
+    target. If actual FDC falls between the upper and lower targets zero deviation is returned.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        The node to record
+    percentiles : array
+        The percentiles to use in the calculation of the flow duration curve.
+        Values must be in the range 0-100.
+    lower_target_fdc : array
+        The lower FDC against which the scenario FDCs are compared
+    upper_target_fdc : array
+        The upper FDC against which the scenario FDCs are compared
+    agg_func: str, optional
+        Function used for aggregating the FDC deviations across percentiles.
+        Numpy style functions that support an axis argument are supported.
+    fdc_agg_func: str, optional
+        Optional different function for aggregating across scenarios.
+
+    """
+    def __init__(self, model, AbstractNode node, percentiles, lower_target_fdc, upper_target_fdc, scenario=None, **kwargs):
+        super(FlowDurationCurveDeviationRecorder, self).__init__(model, node, percentiles, **kwargs)
+
+        lower_target = np.array(lower_target_fdc, dtype=np.float64)
+        if lower_target.ndim < 2:
+            lower_target = lower_target[:, np.newaxis]
+
+        upper_target = np.array(upper_target_fdc, dtype=np.float64)
+        if upper_target.ndim < 2:
+            upper_target = upper_target[:, np.newaxis]
+
+        self._lower_target_fdc = lower_target
+        self._upper_target_fdc = upper_target
+        self.scenario = scenario
+        if len(self._percentiles) != self._lower_target_fdc.shape[0]:
+            raise ValueError("The lengths of the lower target FDC and the percentiles list do not match")
+        if len(self._percentiles) != self._upper_target_fdc.shape[0]:
+            raise ValueError("The lengths of the upper target FDC and the percentiles list do not match")
+
+    cpdef setup(self):
+        super(FlowDurationCurveDeviationRecorder, self).setup()
+        # Check target FDC is the correct size; this is done in setup rather than __init__
+        # because the scenarios might change after the Recorder is created.
+        if self.scenario is not None:
+            if self._lower_target_fdc.shape[1] != self.scenario.size:
+                raise ValueError('The number of lower target FDCs does not match the size ({}) of scenario "{}"'.format(self.scenario.size, self.scenario.name))
+            if self._upper_target_fdc.shape[1] != self.scenario.size:
+                raise ValueError('The number of upper target FDCs does not match the size ({}) of scenario "{}"'.format(self.scenario.size, self.scenario.name))
+        else:
+            if self._lower_target_fdc.shape[1] > 1 and \
+                    self._lower_target_fdc.shape[1] != len(self.model.scenarios.combinations):
+                raise ValueError("The number of lower target FDCs does not match the number of scenarios")
+            if self._upper_target_fdc.shape[1] > 1 and \
+                    self._upper_target_fdc.shape[1] != len(self.model.scenarios.combinations):
+                raise ValueError("The number of upper target FDCs does not match the number of scenarios")
+
+    cpdef finish(self):
+        super(FlowDurationCurveDeviationRecorder, self).finish()
+
+        cdef int i, j, jl, ju, k, sc_index
+        cdef ScenarioIndex scenario_index
+        cdef double[:] utrgt_fdc, ltrgt_fdc
+        cdef double udev, ldev
+
+        # We have to do this the slow way by iterating through all scenario combinations
+        if self.scenario is not None:
+            sc_index = self.model.scenarios.get_scenario_index(self.scenario)
+
+        self._fdc_deviations = np.empty((self._lower_target_fdc.shape[0], len(self.model.scenarios.combinations)), dtype=np.float64)
+        for i, scenario_index in enumerate(self.model.scenarios.combinations):
+
+            if self.scenario is not None:
+                # Get the scenario specific ensemble id for this combination
+                j = scenario_index._indices[sc_index]
+            else:
+                j = scenario_index.global_id
+
+            if self._lower_target_fdc.shape[1] == 1:
+                jl = 0
+            else:
+                jl = j
+
+            if self._upper_target_fdc.shape[1] == 1:
+                ju = 0
+            else:
+                ju = j
+
+            # Cache the target FDC to use in this combination
+            ltrgt_fdc = self._lower_target_fdc[:, jl]
+            utrgt_fdc = self._upper_target_fdc[:, ju]
+            # Finally calculate deviation
+            for k in range(ltrgt_fdc.shape[0]):
+                try:
+                    # upper deviation (+ve when flow higher than upper target)
+                    udev = (self._fdc[k, i] - utrgt_fdc[k])  / utrgt_fdc[k]
+                    # lower deviation (+ve when flow less than lower target)
+                    ldev = (ltrgt_fdc[k] - self._fdc[k, i])  / ltrgt_fdc[k]
+                    # Overall deviation is the worst of upper and lower, but if both
+                    # are negative (i.e. FDC is between upper and lower) there is zero deviation
+                    self._fdc_deviations[k, i] = max(udev, ldev, 0.0)
+                except ZeroDivisionError:
+                    self._fdc_deviations[k, i] = np.nan
+
+    property fdc_deviations:
+        def __get__(self, ):
+            return np.array(self._fdc_deviations)
+
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._fdc_deviations, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self, return_fdc=False):
+        """ Return a `pandas.DataFrame` of the deviations from the target FDCs
+
+        Parameters
+        ----------
+        return_fdc : bool (default=False)
+            If true returns a tuple of two dataframes. The first is the deviations, the second
+            is the actual FDC.
+        """
+        index = self._percentiles
+        sc_index = self.model.scenarios.multiindex
+
+        df = pd.DataFrame(data=np.array(self._fdc_deviations), index=index, columns=sc_index)
+        if return_fdc:
+            return df, super(FlowDurationCurveDeviationRecorder, self).to_dataframe()
+        else:
+            return df
+
+FlowDurationCurveDeviationRecorder.register()
+
+
+cdef class NumpyArrayAbstractStorageRecorder(StorageRecorder):
+    def __init__(self, model, AbstractStorage node, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
+        super().__init__(model, node, **kwargs)
+
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        cdef int nts = len(self.model.timestepper)
+        self._data = np.zeros((nts, ncomb))
+
+    cpdef reset(self):
+        self._data[:, :] = 0.0
+
+    cpdef after(self):
+        raise NotImplementedError()
+
+    property data:
+        def __get__(self, ):
+            return np.array(self._data)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+
+
+cdef class NumpyArrayStorageRecorder(NumpyArrayAbstractStorageRecorder):
+    """Recorder for timeseries information from a `Storage` node.
+
+    This class stores volume from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    proportional : bool
+        Whether to record proportional [0, 1.0] or absolute storage volumes (default=False).
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    def __init__(self, *args, **kwargs):
+        # Optional different method for aggregating across time.
+        self.proportional = kwargs.pop('proportional', False)
+        super().__init__(*args, **kwargs)
+
+    cpdef after(self):
+        cdef int i
+        cdef Timestep ts = self.model.timestepper.current
+        for i in range(self._data.shape[1]):
+            if self.proportional:
+                self._data[ts.index, i] = self._node._current_pc[i]
+            else:
+                self._data[ts.index, i] = self._node._volume[i]
+        return 0
+NumpyArrayStorageRecorder.register()
+
+
+cdef class StorageDurationCurveRecorder(NumpyArrayStorageRecorder):
+    """
+    This recorder calculates a storage duration curve for each scenario.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.AbstractStorage`
+        The node to record
+    percentiles : array
+        The percentiles to use in the calculation of the flow duration curve.
+        Values must be in the range 0-100.
+    agg_func: str, optional
+        function used for aggregating the FDC across percentiles.
+        Numpy style functions that support an axis argument are supported.
+    sdc_agg_func: str, optional
+        optional different function for aggregating across scenarios.
+
+    """
+
+    def __init__(self, model, AbstractStorage node, percentiles, **kwargs):
+
+        if "sdc_agg_func" in kwargs:
+            # Support previous behaviour
+            warnings.warn('The "sdc_agg_func" key is deprecated for defining the temporal '
+                          'aggregation in {}. Please "temporal_agg_func" instead.'
+                          .format(self.__class__.__name__))
+            if "temporal_agg_func" in kwargs:
+                raise ValueError('Both "sdc_agg_func" and "temporal_agg_func" keywords given.'
+                                 'This is ambiguous. Please use "temporal_agg_func" only.')
+            kwargs["temporal_agg_func"] = kwargs.pop("sdc_agg_func")
+
+        super(StorageDurationCurveRecorder, self).__init__(model, node, **kwargs)
+        self._percentiles = np.asarray(percentiles, dtype=np.float64)
+
+
+    cpdef finish(self):
+        self._sdc = np.percentile(np.asarray(self._data), np.asarray(self._percentiles), axis=0)
+
+    property sdc:
+        def __get__(self, ):
+            return np.array(self._sdc)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._sdc, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self._percentiles
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=self.sdc, index=index, columns=sc_index)
+
+StorageDurationCurveRecorder.register()
+
+cdef class NumpyArrayLevelRecorder(NumpyArrayAbstractStorageRecorder):
+    """Recorder for level timeseries from a `Storage` node.
+
+    This class stores level from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    cpdef after(self):
+        cdef int i
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef Storage node = self._node
+        for i, scenario_index in enumerate(self.model.scenarios.combinations):
+            self._data[ts.index, i] = node.get_level(scenario_index)
+        return 0
+NumpyArrayLevelRecorder.register()
+
+
+cdef class NumpyArrayAreaRecorder(NumpyArrayAbstractStorageRecorder):
+    """Recorder for area timeseries from a `Storage` node.
+
+    This class stores area from a specific node for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        Node instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    cpdef after(self):
+        cdef int i
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef Storage node = self._node
+        for i, scenario_index in enumerate(self.model.scenarios.combinations):
+            self._data[ts.index, i] = node.get_area(scenario_index)
+        return 0
+NumpyArrayAreaRecorder.register()
+
+
+cdef class NumpyArrayParameterRecorder(ParameterRecorder):
+    """Recorder for timeseries information from a `Parameter`.
+
+    This class stores the value from a specific `Parameter` for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.Parameter`
+        Parameter instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    def __init__(self, model, Parameter param, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
+        super(NumpyArrayParameterRecorder, self).__init__(model, param, **kwargs)
+
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        cdef int nts = len(self.model.timestepper)
+        self._data = np.zeros((nts, ncomb))
+
+    cpdef reset(self):
+        self._data[:, :] = 0.0
+
+    cpdef after(self):
+        cdef int i
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        self._data[ts.index, :] = self._param.get_all_values()
+        return 0
+
+    property data:
+        def __get__(self, ):
+            return np.array(self._data)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+NumpyArrayParameterRecorder.register()
+
+
+cdef class NumpyArrayDailyProfileParameterRecorder(ParameterRecorder):
+    """Recorder for an annual profile from a `Parameter`.
+
+    This recorder stores a daily profile returned by a specific parameter. For each day of the year
+    it stores the value encountered for that day during a simulation. This results in the final profile
+    being the last value encountered on each day of the year during a simulation. This recorder is useful
+    for returning the daily profile that may result from the combination of one or more parameters. For
+    example, during optimisation of new profiles non-daily parameters (e.g. `RbfProfileParameter`) and/or
+    aggregations of several parameters might be used. With this recorder the daily profile used in the simulation
+    can be easily saved.
+
+    The data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.Parameter`
+        Parameter instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    def __init__(self, model, Parameter param, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
+        super().__init__(model, param, **kwargs)
+
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        self._data = np.zeros((366, ncomb))
+
+    cpdef reset(self):
+        self._data[:, :] = 0.0
+
+    cpdef after(self):
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef int i = ts.dayofyear_index
+        self._data[i, :] = self._param.get_all_values()
+        return 0
+
+    property data:
+        def __get__(self, ):
+            return np.array(self._data)
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = np.arange(1, 367)
+        sc_index = self.model.scenarios.multiindex
+        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+NumpyArrayDailyProfileParameterRecorder.register()
+
+
+cdef class NumpyArrayIndexParameterRecorder(IndexParameterRecorder):
+    """Recorder for timeseries information from an `IndexParameter`.
+
+    This class stores the value from a specific `IndexParameter` for each time-step of a simulation. The
+    data is saved internally using a memory view. The data can be accessed through the `data`
+    attribute or `to_dataframe()` method.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.IndexParameter`
+        Parameter instance to record.
+    temporal_agg_func : str or callable (default="mean")
+        Aggregation function used over time when computing a value per scenario. This can be used
+        to return, for example, the median flow over a simulation. For aggregation over scenarios
+        see the `agg_func` keyword argument.
+    """
+    def __init__(self, model, IndexParameter param, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'mean')
+        super(NumpyArrayIndexParameterRecorder, self).__init__(model, param, **kwargs)
+
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        cdef int nts = len(self.model.timestepper)
+        self._data = np.zeros((nts, ncomb), dtype=np.int32)
+
+    cpdef reset(self):
+        self._data[:, :] = 0
+
+    cpdef after(self):
+        cdef int i
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        self._data[ts.index, :] = self._param.get_all_indices()
+        return 0
+
+    property data:
+        def __get__(self, ):
+            return np.array(self._data)
+
+    def to_dataframe(self):
+        """ Return a `pandas.DataFrame` of the recorder data
+        This DataFrame contains a MultiIndex for the columns with the recorder name
+        as the first level and scenario combination names as the second level. This
+        allows for easy combination with multiple recorder's DataFrames
+        """
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+
+        return pd.DataFrame(data=np.array(self._data), index=index, columns=sc_index)
+NumpyArrayIndexParameterRecorder.register()
+
+
+cdef class RollingWindowParameterRecorder(ParameterRecorder):
+    """Records the mean value of a Parameter for the last N timesteps.
+    """
+    def __init__(self, model, Parameter param, int window, *args, **kwargs):
+
+        if "agg_func" in kwargs and "temporal_agg_func" not in kwargs:
+            # Support previous behaviour
+            warnings.warn('The "agg_func" key is deprecated for defining the temporal '
+                          'aggregation in {}. Please "temporal_agg_func" instead.'
+                          .format(self.__class__.__name__))
+            temporal_agg_func = kwargs.get("agg_func")
+        else:
+            temporal_agg_func = kwargs.pop("temporal_agg_func", "mean")
+
+        super(RollingWindowParameterRecorder, self).__init__(model, param, *args, **kwargs)
+        self.window = window
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        cdef int nts = len(self.model.timestepper)
+        self._data = np.zeros((nts, ncomb,), np.float64)
+        self._memory = np.empty((nts, ncomb,), np.float64)
+        self.position = 0
+
+    cpdef reset(self):
+        self._data[...] = 0
+        self.position = 0
+
+    cpdef after(self):
+        cdef int i, n
+        cdef double[:] value
+        cdef ScenarioIndex scenario_index
+        cdef Timestep timestep = self.model.timestepper.current
+
+        for i, scenario_index in enumerate(self.model.scenarios.combinations):
+            self._memory[self.position, i] = self._param.get_value(scenario_index)
+
+        if timestep.index < self.window:
+            n = timestep.index + 1
+        else:
+            n = self.window
+
+        value = self._temporal_aggregator.aggregate_2d(self._memory[0:n, :], axis=0)
+        self._data[timestep.index, :] = value
+
+        self.position += 1
+        if self.position >= self.window:
+            self.position = 0
+
+    property data:
+        def __get__(self):
+            return np.array(self._data, dtype=np.float64)
+
+    def to_dataframe(self):
+        index = self.model.timestepper.datetime_index
+        sc_index = self.model.scenarios.multiindex
+        return pd.DataFrame(data=self.data, index=index, columns=sc_index)
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.parameters import load_parameter
+        parameter = load_parameter(model, data.pop("parameter"))
+        window = int(data.pop("window"))
+        return cls(model, parameter, window, **data)
+
+RollingWindowParameterRecorder.register()
+
+cdef class RollingMeanFlowNodeRecorder(NodeRecorder):
+    """Records the mean flow of a Node for the previous N timesteps
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    node : `pywr.core.Node`
+        The node to record
+    timesteps : int
+        The number of timesteps to calculate the mean flow for
+    name : str (optional)
+        The name of the recorder
+
+    """
+    def __init__(self, model, node, timesteps=None, days=None, name=None, **kwargs):
+        super(RollingMeanFlowNodeRecorder, self).__init__(model, node, name=name, **kwargs)
+        self.model = model
+        if not timesteps and not days:
+            raise ValueError("Either `timesteps` or `days` must be specified.")
+        if timesteps:
+            self.timesteps = int(timesteps)
+        else:
+            self.timesteps = 0
+        if days:
+            self.days = int(days)
+        else:
+            self.days = 0
+        self._data = None
+        self.position = 0
+
+    cpdef setup(self):
+        super(RollingMeanFlowNodeRecorder, self).setup()
+        self._data = np.empty([len(self.model.timestepper), len(self.model.scenarios.combinations)])
+        if self.days > 0:
+            try:
+                self.timesteps = self.days // self.model.timestepper.delta
+            except TypeError:
+                raise TypeError('A rolling window defined as a number of days is only valid with daily time-steps.')
+        if self.timesteps == 0:
+            raise ValueError("Timesteps property of MeanFlowRecorder is less than 1.")
+        self._memory = np.zeros([len(self.model.scenarios.combinations), self.timesteps])
+
+    cpdef reset(self):
+        super(RollingMeanFlowNodeRecorder, self).reset()
+        self.position = 0
+
+    cpdef after(self):
+        cdef Timestep timestep
+        cdef int i, n
+        cdef double[:] mean_flow
+        # save today's flow
+        for i in range(0, self._memory.shape[0]):
+            self._memory[i, self.position] = self._node._flow[i]
+        # calculate the mean flow
+        timestep = self.model.timestepper.current
+        if timestep.index < self.timesteps:
+            n = timestep.index + 1
+        else:
+            n = self.timesteps
+        # save the mean flow
+        mean_flow = np.mean(self._memory[:, 0:n], axis=1)
+        self._data[<int>(timestep.index), :] = mean_flow
+        # prepare for the next timestep
+        self.position += 1
+        if self.position >= self.timesteps:
+            self.position = 0
+
+    property data:
+        def __get__(self):
+            return np.array(self._data, dtype=np.float64)
+
+    @classmethod
+    def load(cls, model, data):
+        name = data.get("name")
+        node = model._get_node_from_ref(model, data["node"])
+        if "timesteps" in data:
+            timesteps = int(data["timesteps"])
+        else:
+            timesteps = None
+        if "days" in data:
+            days = int(data["days"])
+        else:
+            days = None
+        return cls(model, node, timesteps=timesteps, days=days, name=name)
+
+RollingMeanFlowNodeRecorder.register()
+
+cdef class BaseConstantNodeRecorder(NodeRecorder):
+    """
+    Base class for NodeRecorder classes with a single value for each scenario combination
+    """
+
+    cpdef setup(self):
+        self._values = np.zeros(len(self.model.scenarios.combinations))
+
+    cpdef reset(self):
+        self._values[...] = 0.0
+
+    cpdef after(self):
+        raise NotImplementedError()
+
+    cpdef double[:] values(self) except *:
+        return self._values
+
+
+cdef class TotalDeficitNodeRecorder(BaseConstantNodeRecorder):
+    """
+    Recorder to total the difference between modelled flow and max_flow for a Node
+    """
+    cpdef after(self):
+        cdef double max_flow
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef double days = self.model.timestepper.current.days
+        cdef AbstractNode node = self._node
+        for scenario_index in self.model.scenarios.combinations:
+            max_flow = node.get_max_flow(scenario_index)
+            self._values[scenario_index.global_id] += (max_flow - node._flow[scenario_index.global_id])*days
+
+        return 0
+TotalDeficitNodeRecorder.register()
+
+
+cdef class TotalFlowNodeRecorder(BaseConstantNodeRecorder):
+    """
+    Recorder to total the flow for a Node.
+
+    A factor can be provided to scale the total flow (e.g. for calculating operational costs).
+    """
+    def __init__(self, *args, **kwargs):
+        self.factor = kwargs.pop('factor', 1.0)
+        super(TotalFlowNodeRecorder, self).__init__(*args, **kwargs)
+
+    cpdef after(self):
+        cdef ScenarioIndex scenario_index
+        cdef int i
+        cdef double days = self.model.timestepper.current.days
+        for scenario_index in self.model.scenarios.combinations:
+            i = scenario_index.global_id
+            self._values[i] += self._node._flow[i]*self.factor*days
+        return 0
+TotalFlowNodeRecorder.register()
+
+
+cdef class MeanFlowNodeRecorder(BaseConstantNodeRecorder):
+    """
+    Record the mean flow for a Node.
+
+    A factor can be provided to scale the total flow (e.g. for calculating operational costs).
+    """
+    def __init__(self, *args, **kwargs):
+        self.factor = kwargs.pop('factor', 1.0)
+        super(MeanFlowNodeRecorder, self).__init__(*args, **kwargs)
+
+    cpdef after(self):
+        cdef ScenarioIndex scenario_index
+        cdef int i
+        for scenario_index in self.model.scenarios.combinations:
+            i = scenario_index.global_id
+            self._values[i] += self._node._flow[i]*self.factor
+        return 0
+
+    cpdef finish(self):
+        cdef int i
+        cdef int nt = self.model.timestepper.current.index
+        for i in range(self._values.shape[0]):
+            self._values[i] /= nt
+MeanFlowNodeRecorder.register()
+
+
+cdef class DeficitFrequencyNodeRecorder(BaseConstantNodeRecorder):
+    """Recorder to return the frequency of timesteps with a failure to meet max_flow.
+    """
+    cpdef after(self):
+        cdef double max_flow
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+        cdef AbstractNode node = self._node
+        for scenario_index in self.model.scenarios.combinations:
+            max_flow = node.get_max_flow(scenario_index)
+            if abs(node._flow[scenario_index.global_id] - max_flow) > 1e-6:
+                self._values[scenario_index.global_id] += 1.0
+
+    cpdef finish(self):
+        cdef int i
+        cdef int nt = self.model.timestepper.current.index
+        for i in range(self._values.shape[0]):
+            self._values[i] /= nt
+DeficitFrequencyNodeRecorder.register()
+
+cdef class BaseConstantStorageRecorder(StorageRecorder):
+    """
+    Base class for StorageRecorder classes with a single value for each scenario combination
+    """
+
+    cpdef setup(self):
+        self._values = np.zeros(len(self.model.scenarios.combinations))
+
+    cpdef reset(self):
+        self._values[...] = 0.0
+
+    cpdef after(self):
+        raise NotImplementedError()
+
+    cpdef double[:] values(self) except *:
+        return self._values
+BaseConstantStorageRecorder.register()
+
+cdef class MinimumVolumeStorageRecorder(BaseConstantStorageRecorder):
+    """Record the minimum volume in a `Storage` node during a simulation."""
+    cpdef reset(self):
+        self._values[...] = np.inf
+
+    cpdef after(self):
+        cdef int i
+        for i in range(self._values.shape[0]):
+            self._values[i] = np.min([self._node._volume[i], self._values[i]])
+        return 0
+MinimumVolumeStorageRecorder.register()
+
+cdef class MinimumThresholdVolumeStorageRecorder(BaseConstantStorageRecorder):
+    """Record whether a `Storage` node falls below a particular volume threshold during a simulation.
+
+    This recorder will return a value of `1.0` for scenarios where the volume `Storage` is less
+    than or equal to the threshold at any time-step during the simulation. Otherwise it will return zero.
+    """
+    def __init__(self, model, node, threshold, *args, **kwargs):
+        self.threshold = threshold
+        super(MinimumThresholdVolumeStorageRecorder, self).__init__(model, node, *args, **kwargs)
+
+    cpdef reset(self):
+        self._values[...] = 0.0
+
+    cpdef after(self):
+        cdef int i
+        for i in range(self._values.shape[0]):
+            if self._node._volume[i] <= self.threshold:
+                self._values[i] = 1.0
+        return 0
+MinimumThresholdVolumeStorageRecorder.register()
+
+
+cdef class TimestepCountIndexParameterRecorder(IndexParameterRecorder):
+    """Record the number of times an index parameter exceeds a threshold for each scenario.
+
+    This recorder will count the number of timesteps so will be a daily count when running on a
+    daily timestep.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    parameter : `pywr.core.IndexParameter`
+        The parameter to record
+    threshold : int
+        The threshold to compare the parameter to
+    """
+    def __init__(self, model, IndexParameter parameter, int threshold, *args, **kwargs):
+        super().__init__(model, parameter, *args, **kwargs)
+        self.threshold = threshold
+
+    cpdef setup(self):
+        self._count = np.zeros(len(self.model.scenarios.combinations), np.int32)
+
+    cpdef reset(self):
+        self._count[...] = 0
+
+    cpdef after(self):
+        cdef Timestep ts = self.model.timestepper.current
+        cdef int value
+        cdef ScenarioIndex scenario_index
+
+        for scenario_index in self.model.scenarios.combinations:
+            value = self._param.get_index(scenario_index)
+            if value >= self.threshold:
+                # threshold achieved, increment count
+                self._count[scenario_index.global_id] += 1
+
+    cpdef double[:] values(self) except *:
+        return np.asarray(self._count).astype(np.float64)
+TimestepCountIndexParameterRecorder.register()
+
+
+cdef class AnnualCountIndexThresholdRecorder(Recorder):
+    """
+    For each scenario, count the number of times a list of parameters exceeds a threshold in each year.
+    If multiple parameters exceed in one timestep then it is only counted once.
+
+    Output from data property has shape: (years, scenario combinations)
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    parameters : list
+        List of `pywr.core.IndexParameter` to record against
+    name : str
+        The name of the recorder
+    threshold : int
+        Threshold to compare parameters against
+    """
+    def __init__(self, model, list parameters, str name, int threshold, *args, **kwargs):
+        # Optional different method for aggregating across time.
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'sum')
+        super().__init__(model, name=name, *args, **kwargs)
+        self.parameters = parameters
+        self.threshold = threshold
+        for parameter in self.parameters:
+            self.children.add(parameter)
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    cpdef setup(self):
+        super(AnnualCountIndexThresholdRecorder, self).setup()
+        self._num_years = self.model.timestepper.end.year - self.model.timestepper.start.year + 1
+        self._ncomb = len(self.model.scenarios.combinations)
+        self._data = np.empty([self._num_years, self._ncomb])
+        self._data_this_year = np.zeros([len(self.parameters), self._ncomb])
+
+    cpdef reset(self):
+        self._data[...] = 0
+        self._current_year = -1
+        self._start_year = self.model.timestepper.start.year
+
+    cpdef after(self):
+        cdef Timestep ts = self.model.timestepper.current
+        cdef int idx = self._current_year - self._start_year
+        cdef int p
+        cdef Py_ssize_t i
+        cdef int value
+        cdef ScenarioIndex scenario_index
+        cdef IndexParameter parameter
+
+        if ts.year != self._current_year:
+            # A new year
+            if self._current_year != -1:
+                # As long as at least one year has been run
+                # then save data for previous year
+                for i in range(self._ncomb):
+                    self._data[idx, i] = np.sum(self._data_this_year[:, i])
+
+            self._data_this_year[...] = 0
+            self._current_year = ts.year
+
+        for scenario_index in self.model.scenarios.combinations:
+            for p, parameter in enumerate(self.parameters):
+                value = parameter.get_index(scenario_index)
+                if value >= self.threshold:
+                    self._data_this_year[p, scenario_index.global_id] += 1
+                    break  # if multiple parameters exceed, only count once
+
+    cpdef finish(self):
+        cdef int idx = self._current_year - self._start_year
+        cdef Py_ssize_t i
+        for i in range(self._ncomb):
+            self._data[idx, i] = np.sum(self._data_this_year[:, i])
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    property data:
+        def __get__(self):
+            return np.array(self._data, dtype=np.int16)
+
+    @classmethod
+    def load(cls, model, data):
+        from pywr.parameters import load_parameter
+        parameters = [load_parameter(model, p) for p in data.pop("parameters")]
+        return cls(model, parameters=parameters, **data)
+AnnualCountIndexThresholdRecorder.register()
+
+
+cdef class AnnualTotalFlowRecorder(Recorder):
+    """
+    For each scenario, record the total flow in each year across a list of nodes.
+    Output from data property has shape: (years, scenario combinations)
+
+    A list of factors can be provided to scale the total flow (e.g. for calculating operational costs).
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    name : str
+        The name of the recorder
+    nodes : list
+        List of `pywr.core.Node` instances to record
+    factors : list, optional
+        List of factors to apply to each node
+    """
+    def __init__(self, model, str name, list nodes, *args, **kwargs):
+        temporal_agg_func = kwargs.pop('temporal_agg_func', 'sum')
+        factors = kwargs.pop('factors', None)
+        super().__init__(model, name=name, *args, **kwargs)
+        self.nodes = nodes
+        self.factors = factors
+        self._temporal_aggregator = Aggregator(temporal_agg_func)
+
+    property temporal_agg_func:
+        def __set__(self, agg_func):
+            self._temporal_aggregator.func = agg_func
+
+    property factors:
+        # Property provides np.array style access to the internal memoryview.
+        def __get__(self):
+            return np.array(self._factors)
+        def __set__(self, factors):
+            if factors is None:
+                factors = np.array([1.0 for n in self.nodes])
+            self._factors = np.array(factors)
+
+    cpdef setup(self):
+        super(AnnualTotalFlowRecorder, self).setup()
+        self._num_years = self.model.timestepper.end.year - self.model.timestepper.start.year + 1
+        self._ncomb = len(self.model.scenarios.combinations)
+        self._data = np.empty([self._num_years, self._ncomb])
+
+    cpdef reset(self):
+        self._data[...] = 0
+        self._current_year = -1
+        self._start_year = self.model.timestepper.start.year
+
+    cpdef after(self):
+        cdef int i, j
+        cdef Timestep ts = self.model.timestepper.current
+        cdef int idx = ts.year - self._start_year
+        cdef AbstractNode node
+        cdef double[:] flow = np.zeros(self._ncomb, np.float64)
+
+        for i in range(self._ncomb):
+            for j, node in enumerate(self.nodes):
+                self._data[idx, i] += node._flow[i] * self._factors[j]
+
+    cpdef double[:] values(self) except *:
+        """Compute a value for each scenario using `temporal_agg_func`.
+        """
+        return self._temporal_aggregator.aggregate_2d(self._data, axis=0, ignore_nan=self.ignore_nan)
+
+    property data:
+        def __get__(self):
+            return np.array(self._data, dtype=np.float64)
+
+    @classmethod
+    def load(cls, model, data):
+        nodes = [model._get_node_from_ref(model, n) for n in data.pop("nodes")]
+        return cls(model, nodes=nodes, **data)
+AnnualTotalFlowRecorder.register()
+
+
+cdef class AnnualCountIndexParameterRecorder(IndexParameterRecorder):
+    """ Record the number of years where an IndexParameter is greater than or equal to a threshold """
+    def __init__(self, model, IndexParameter param, int threshold, *args, **kwargs):
+        super(AnnualCountIndexParameterRecorder, self).__init__(model, param, *args, **kwargs)
+        self.threshold = threshold
+
+    cpdef setup(self):
+        self._count = np.zeros(len(self.model.scenarios.combinations), np.int32)
+        self._current_max = np.zeros_like(self._count)
+
+    cpdef reset(self):
+        self._count[...] = 0
+        self._current_max[...] = 0
+        self._current_year = -1
+
+    cpdef after(self):
+        cdef int i, ncomb, value
+        cdef ScenarioIndex scenario_index
+        cdef Timestep ts = self.model.timestepper.current
+
+        ncomb = len(self.model.scenarios.combinations)
+
+        if ts.year != self._current_year:
+            # A new year
+            if self._current_year != -1:
+                # As long as at least one year has been run
+                # then update the count if threshold equal to or exceeded
+                for i in range(ncomb):
+                    if self._current_max[i] >= self.threshold:
+                        self._count[i] += 1
+
+            # Finally reset current maximum and update current year
+            self._current_max[...] = 0
+            self._current_year = ts.year
+
+        for scenario_index in self.model.scenarios.combinations:
+            # Get current parameter value
+            value = self._param.get_index(scenario_index)
+
+            # Update annual max if a new maximum is found
+            if value > self._current_max[scenario_index.global_id]:
+                self._current_max[scenario_index.global_id] = value
+
+        return 0
+
+    cpdef finish(self):
+        cdef int i
+        cdef int ncomb = len(self.model.scenarios.combinations)
+        # Complete the current year by updating the count if threshold equal to or exceeded
+        for i in range(ncomb):
+            if self._current_max[i] >= self.threshold:
+                self._count[i] += 1
+
+    cpdef double[:] values(self) except *:
+        return np.asarray(self._count).astype(np.float64)
+AnnualCountIndexParameterRecorder.register()
+
+
+def load_recorder(model, data, recorder_name=None):
+    recorder = None
+
+    if isinstance(data, str):
+        recorder_name = data
+
+    # check if recorder has already been loaded
+    for rec in model.recorders:
+        if rec.name == recorder_name:
+            recorder = rec
+            break
+
+    if recorder is None and isinstance(data, str):
+        # recorder was requested by name, but hasn't been loaded yet
+        if hasattr(model, "_recorders_to_load"):
+            # we're still in the process of loading data from JSON and
+            # the parameter requested hasn't been loaded yet - do it now
+            try:
+                data = model._recorders_to_load.pop(recorder_name)
+            except KeyError:
+                raise KeyError("Unknown recorder: '{}'".format(data))
+            recorder = load_recorder(model, data)
+        else:
+            raise KeyError("Unknown recorder: '{}'".format(data))
+
+    if recorder is None:
+        recorder_type = data['type']
+
+        name = recorder_type.lower()
+        try:
+            cls = recorder_registry[name]
+        except KeyError:
+            if name.endswith("recorder"):
+                name = name.replace("recorder", "")
+            else:
+                name += "recorder"
+            try:
+                cls = recorder_registry[name]
+            except KeyError:
+                raise NotImplementedError('Unrecognised recorder type "{}"'.format(recorder_type))
+
+        del(data["type"])
+        recorder = cls.load(model, data)
+
+    return recorder
+
+
+cdef class BaseConstantParameterRecorder(ParameterRecorder):
+    """Base class for `ParameterRecorder` classes with a single value for each scenario combination
+    """
+    cpdef setup(self):
+        self._values = np.zeros(len(self.model.scenarios.combinations))
+
+    cpdef reset(self):
+        self._values[...] = 0.0
+
+    cpdef after(self):
+        raise NotImplementedError()
+
+    cpdef double[:] values(self) except *:
+        return self._values
+
+
+cdef class TotalParameterRecorder(BaseConstantParameterRecorder):
+    """Record the total value of a `Parameter` during a simulation.
+
+    This recorder can be used to track the sum total of the values returned by a
+    `Parameter` during a models simulation. An optional factor can be provided to
+    apply a linear scaling of the values. If the parameter represents a flux
+    the `integrate` keyword argument can be used to multiply the values by the time-step
+    length in days.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.Parameter`
+        The parameter to record.
+    name : str (optional)
+        The name of the recorder
+    factor : float (default=1.0)
+        Scaling factor for the values of `param`.
+    integrate : bool (default=False)
+        Whether to multiply by the time-step length in days during summation.
+    """
+    def __init__(self, *args, **kwargs):
+        self.factor = kwargs.pop('factor', 1.0)
+        self.integrate = kwargs.pop('integrate', False)
+        super(TotalParameterRecorder, self).__init__(*args, **kwargs)
+
+    cpdef after(self):
+        cdef ScenarioIndex scenario_index
+        cdef int i
+        cdef double[:] values
+        cdef factor = self.factor
+
+        if self.integrate:
+            factor *= self.model.timestepper.current.days
+
+        values = self._param.get_all_values()
+        for scenario_index in self.model.scenarios.combinations:
+            i = scenario_index.global_id
+            self._values[i] += values[i]*factor
+        return 0
+TotalParameterRecorder.register()
+
+
+cdef class MeanParameterRecorder(BaseConstantParameterRecorder):
+    """Record the mean value of a `Parameter` during a simulation.
+
+    This recorder can be used to track the mean of the values returned by a
+    `Parameter` during a models simulation. An optional factor can be provided to
+    apply a linear scaling of the values.
+
+    Parameters
+    ----------
+    model : `pywr.core.Model`
+    param : `pywr.parameters.Parameter`
+        The parameter to record.
+    name : str (optional)
+        The name of the recorder
+    factor : float (default=1.0)
+        Scaling factor for the values of `param`.
+    """
+    def __init__(self, *args, **kwargs):
+        self.factor = kwargs.pop('factor', 1.0)
+        super(MeanParameterRecorder, self).__init__(*args, **kwargs)
+
+    cpdef after(self):
+        cdef ScenarioIndex scenario_index
+        cdef int i
+        cdef double[:] values
+        cdef factor = self.factor
+
+        values = self._param.get_all_values()
+        for scenario_index in self.model.scenarios.combinations:
+            i = scenario_index.global_id
+            self._values[i] += values[i]*factor
+        return 0
+
+    cpdef finish(self):
+        cdef int i
+        cdef int nt = self.model.timestepper.current.index
+        for i in range(self._values.shape[0]):
+            self._values[i] /= nt
+MeanParameterRecorder.register()
```

### Comparing `pywr-1.8.0/pywr/recorders/_thresholds.pyx` & `pywr-1.9.0/pywr/recorders/_thresholds.pyx`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,125 +1,125 @@
-cimport numpy as np
-import numpy as np
-
-
-cdef enum Predicates:
-    LT = 0
-    GT = 1
-    EQ = 2
-    LE = 3
-    GE = 4
-_predicate_lookup = {
-    "LT": Predicates.LT, "<": Predicates.LT,
-    "GT": Predicates.GT, ">": Predicates.GT,
-    "EQ": Predicates.EQ, "=": Predicates.EQ,
-    "LE": Predicates.LE, "<=": Predicates.LE,
-    "GE": Predicates.GE, ">=": Predicates.GE,
-}
-
-cdef int compare(double x, double threshold, int predicate) except? -1:
-    """Returns 1 if the predicate evalutes True, else 0"""
-
-    if predicate == Predicates.LT:
-        ind = x < threshold
-    elif predicate == Predicates.GT:
-        ind = x > threshold
-    elif predicate == Predicates.LE:
-        ind = x <= threshold
-    elif predicate == Predicates.GE:
-        ind = x >= threshold
-    else:
-        ind = x == threshold
-    return ind
-
-
-cdef class StorageThresholdRecorder(StorageRecorder):
-    """ Recorder for tracking state of Storage volume against a threshold.
-
-    Parameters
-    ----------
-    node : Storage
-        Storage instance to compare with the threshold.
-    threshold : double
-        Threshold to compare the value of the recorder to
-    values : iterable of doubles
-        If the predicate evaluates False the zeroth value is returned,
-        otherwise the first value is returned.
-    predicate : string
-        One of {"LT", "GT", "EQ", "LE", "GE"}.
-
-
-    """
-    def __init__(self, model, Storage node, threshold, *args,  predicate=None, **kwargs):
-        super(StorageThresholdRecorder, self).__init__(model, node, *args, **kwargs)
-        self.threshold = threshold
-
-        if predicate is None:
-            predicate = Predicates.LT
-        elif isinstance(predicate, str):
-            predicate = _predicate_lookup[predicate.upper()]
-        self.predicate = predicate
-
-    cpdef setup(self):
-        self._state = np.zeros(len(self.model.scenarios.combinations), dtype=np.int32)
-
-    cpdef reset(self):
-        self._state[...] = 0
-
-    cpdef double[:] values(self) except *:
-        return np.array(self._state, dtype=np.float)
-
-    cpdef after(self):
-        cdef double volume
-        cdef ScenarioIndex scenario_index
-        for scenario_index in self.model.scenarios.combinations:
-            volume = self._node._volume[scenario_index.global_id]
-            self._state[scenario_index.global_id] = compare(volume, self.threshold, self.predicate)
-        return 0
-
-
-cdef class NodeThresholdRecorder(NodeRecorder):
-    """ Recorder for tracking state of Node flow against a threshold.
-
-    Parameters
-    ----------
-    node : Node
-        Node instance to compare with the threshold.
-    threshold : double
-        Threshold to compare the value of the recorder to
-    values : iterable of doubles
-        If the predicate evaluates False the zeroth value is returned,
-        otherwise the first value is returned.
-    predicate : string
-        One of {"LT", "GT", "EQ", "LE", "GE"}.
-
-
-    """
-    def __init__(self, model, AbstractNode node, threshold, *args,  predicate=None, **kwargs):
-        super(NodeThresholdRecorder, self).__init__(model, node, *args, **kwargs)
-        self.threshold = threshold
-
-        if predicate is None:
-            predicate = Predicates.LT
-        elif isinstance(predicate, str):
-            predicate = _predicate_lookup[predicate.upper()]
-        self.predicate = predicate
-
-    cpdef setup(self):
-        self._state = np.zeros(len(self.model.scenarios.combinations), dtype=np.int32)
-
-    cpdef reset(self):
-        self._state[...] = 0
-
-    cpdef double[:] values(self) except *:
-        return np.array(self._state, dtype=np.float)
-
-    cpdef after(self):
-        cdef double flow
-        cdef ScenarioIndex scenario_index
-        for scenario_index in self.model.scenarios.combinations:
-            flow = self._node._flow[scenario_index.global_id]
-            self._state[scenario_index.global_id] = compare(flow, self.threshold, self.predicate)
-        return 0
-
-
-
+cimport numpy as np
+import numpy as np
+
+
+cdef enum Predicates:
+    LT = 0
+    GT = 1
+    EQ = 2
+    LE = 3
+    GE = 4
+_predicate_lookup = {
+    "LT": Predicates.LT, "<": Predicates.LT,
+    "GT": Predicates.GT, ">": Predicates.GT,
+    "EQ": Predicates.EQ, "=": Predicates.EQ,
+    "LE": Predicates.LE, "<=": Predicates.LE,
+    "GE": Predicates.GE, ">=": Predicates.GE,
+}
+
+cdef int compare(double x, double threshold, int predicate) except? -1:
+    """Returns 1 if the predicate evalutes True, else 0"""
+
+    if predicate == Predicates.LT:
+        ind = x < threshold
+    elif predicate == Predicates.GT:
+        ind = x > threshold
+    elif predicate == Predicates.LE:
+        ind = x <= threshold
+    elif predicate == Predicates.GE:
+        ind = x >= threshold
+    else:
+        ind = x == threshold
+    return ind
+
+
+cdef class StorageThresholdRecorder(StorageRecorder):
+    """ Recorder for tracking state of Storage volume against a threshold.
+
+    Parameters
+    ----------
+    node : Storage
+        Storage instance to compare with the threshold.
+    threshold : double
+        Threshold to compare the value of the recorder to
+    values : iterable of doubles
+        If the predicate evaluates False the zeroth value is returned,
+        otherwise the first value is returned.
+    predicate : string
+        One of {"LT", "GT", "EQ", "LE", "GE"}.
+
+
+    """
+    def __init__(self, model, Storage node, threshold, *args,  predicate=None, **kwargs):
+        super(StorageThresholdRecorder, self).__init__(model, node, *args, **kwargs)
+        self.threshold = threshold
+
+        if predicate is None:
+            predicate = Predicates.LT
+        elif isinstance(predicate, str):
+            predicate = _predicate_lookup[predicate.upper()]
+        self.predicate = predicate
+
+    cpdef setup(self):
+        self._state = np.zeros(len(self.model.scenarios.combinations), dtype=np.int32)
+
+    cpdef reset(self):
+        self._state[...] = 0
+
+    cpdef double[:] values(self) except *:
+        return np.array(self._state, dtype=np.float)
+
+    cpdef after(self):
+        cdef double volume
+        cdef ScenarioIndex scenario_index
+        for scenario_index in self.model.scenarios.combinations:
+            volume = self._node._volume[scenario_index.global_id]
+            self._state[scenario_index.global_id] = compare(volume, self.threshold, self.predicate)
+        return 0
+
+
+cdef class NodeThresholdRecorder(NodeRecorder):
+    """ Recorder for tracking state of Node flow against a threshold.
+
+    Parameters
+    ----------
+    node : Node
+        Node instance to compare with the threshold.
+    threshold : double
+        Threshold to compare the value of the recorder to
+    values : iterable of doubles
+        If the predicate evaluates False the zeroth value is returned,
+        otherwise the first value is returned.
+    predicate : string
+        One of {"LT", "GT", "EQ", "LE", "GE"}.
+
+
+    """
+    def __init__(self, model, AbstractNode node, threshold, *args,  predicate=None, **kwargs):
+        super(NodeThresholdRecorder, self).__init__(model, node, *args, **kwargs)
+        self.threshold = threshold
+
+        if predicate is None:
+            predicate = Predicates.LT
+        elif isinstance(predicate, str):
+            predicate = _predicate_lookup[predicate.upper()]
+        self.predicate = predicate
+
+    cpdef setup(self):
+        self._state = np.zeros(len(self.model.scenarios.combinations), dtype=np.int32)
+
+    cpdef reset(self):
+        self._state[...] = 0
+
+    cpdef double[:] values(self) except *:
+        return np.array(self._state, dtype=np.float)
+
+    cpdef after(self):
+        cdef double flow
+        cdef ScenarioIndex scenario_index
+        for scenario_index in self.model.scenarios.combinations:
+            flow = self._node._flow[scenario_index.global_id]
+            self._state[scenario_index.global_id] = compare(flow, self.threshold, self.predicate)
+        return 0
+
+
+
```

### Comparing `pywr-1.8.0/pywr/recorders/calibration.py` & `pywr-1.9.0/pywr/recorders/calibration.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-"""
-This module contains `Recorder` subclasses useful in the evaluation of model
- performance and calibration.
-
-Several different evaluation metrics are implemented in this module. Many are
- well known to modellers. However an important reference for this implementation
- is `[1]_` which reviews the available metrics for watershed models.
-
-..  [1] Moriasi, D.N., et al. (2007) Model Evaluation Guidelines for Systematic
-    Quantification of Accuracy in Watershed Simulations. Transactions of the ASABE, 50, 885-900.
-    http://dx.doi.org/10.13031/2013.23153
-
-
-"""
-from .recorders import NumpyArrayNodeRecorder
-import numpy as np
-
-
-class AbstractComparisonNodeRecorder(NumpyArrayNodeRecorder):
-    """ Base class for all Recorders performing timeseries comparison of `Node` flows
-
-
-    """
-    def __init__(self, model, node, observed, **kwargs):
-        super(AbstractComparisonNodeRecorder, self).__init__(model, node, **kwargs)
-        self.observed = observed
-        self._aligned_observed = None
-
-    def setup(self):
-        super(AbstractComparisonNodeRecorder, self).setup()
-        # Align the observed data to the model
-        from pywr.parameters import align_and_resample_dataframe
-        self._aligned_observed = align_and_resample_dataframe(self.observed, self.model.timestepper.datetime_index)
-
-
-class RootMeanSquaredErrorNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the RMSE between model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        return np.sqrt(np.mean((obs-mod)**2, axis=0))
-
-
-class MeanAbsoluteErrorNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the MAE between model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        return np.mean(np.abs(obs-mod), axis=0)
-
-
-class MeanSquareErrorNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the MSE between model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        return np.mean((obs-mod)**2, axis=0)
-
-
-class PercentBiasNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the percent bias between model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        return np.sum(obs-mod, axis=0)*100/np.sum(obs, axis=0)
-
-
-class RMSEStandardDeviationRatioNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the RMSE-observations standard deviation ratio between model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        return np.sqrt(np.mean((obs-mod)**2, axis=0))/np.std(obs, axis=0)
-
-
-class NashSutcliffeEfficiencyNodeRecorder(AbstractComparisonNodeRecorder):
-    """ Recorder evaluates the Nash-Sutcliffe efficiency model and observed """
-    def values(self):
-        mod = self.data
-        obs = self._aligned_observed
-        obs_mean = np.mean(obs, axis=0)
-        return 1.0 - np.sum((obs-mod)**2, axis=0)/np.sum((obs-obs_mean)**2, axis=0)
-
-
+"""
+This module contains `Recorder` subclasses useful in the evaluation of model
+ performance and calibration.
+
+Several different evaluation metrics are implemented in this module. Many are
+ well known to modellers. However an important reference for this implementation
+ is `[1]_` which reviews the available metrics for watershed models.
+
+..  [1] Moriasi, D.N., et al. (2007) Model Evaluation Guidelines for Systematic
+    Quantification of Accuracy in Watershed Simulations. Transactions of the ASABE, 50, 885-900.
+    http://dx.doi.org/10.13031/2013.23153
+
+
+"""
+from .recorders import NumpyArrayNodeRecorder
+import numpy as np
+
+
+class AbstractComparisonNodeRecorder(NumpyArrayNodeRecorder):
+    """ Base class for all Recorders performing timeseries comparison of `Node` flows
+
+
+    """
+    def __init__(self, model, node, observed, **kwargs):
+        super(AbstractComparisonNodeRecorder, self).__init__(model, node, **kwargs)
+        self.observed = observed
+        self._aligned_observed = None
+
+    def setup(self):
+        super(AbstractComparisonNodeRecorder, self).setup()
+        # Align the observed data to the model
+        from pywr.parameters import align_and_resample_dataframe
+        self._aligned_observed = align_and_resample_dataframe(self.observed, self.model.timestepper.datetime_index)
+
+
+class RootMeanSquaredErrorNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the RMSE between model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        return np.sqrt(np.mean((obs-mod)**2, axis=0))
+
+
+class MeanAbsoluteErrorNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the MAE between model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        return np.mean(np.abs(obs-mod), axis=0)
+
+
+class MeanSquareErrorNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the MSE between model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        return np.mean((obs-mod)**2, axis=0)
+
+
+class PercentBiasNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the percent bias between model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        return np.sum(obs-mod, axis=0)*100/np.sum(obs, axis=0)
+
+
+class RMSEStandardDeviationRatioNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the RMSE-observations standard deviation ratio between model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        return np.sqrt(np.mean((obs-mod)**2, axis=0))/np.std(obs, axis=0)
+
+
+class NashSutcliffeEfficiencyNodeRecorder(AbstractComparisonNodeRecorder):
+    """ Recorder evaluates the Nash-Sutcliffe efficiency model and observed """
+    def values(self):
+        mod = self.data
+        obs = self._aligned_observed
+        obs_mean = np.mean(obs, axis=0)
+        return 1.0 - np.sum((obs-mod)**2, axis=0)/np.sum((obs-obs_mean)**2, axis=0)
+
+
```

### Comparing `pywr-1.8.0/pywr/recorders/events.py` & `pywr-1.9.0/pywr/recorders/events.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,312 +1,312 @@
-from ._recorders import Recorder
-import numpy as np
-import pandas
-
-
-class Event(object):
-    """ Container for event information """
-    def __init__(self, start, scenario_index):
-        self.start = start
-        self.scenario_index = scenario_index
-        self.end = None
-        self.values = None  # to record any  tracked values
-
-    @property
-    def duration(self):
-        td = self.end.datetime - self.start.datetime
-        return td.days
-
-
-class EventRecorder(Recorder):
-    """Track discrete events using a Parameter or Recorder
-
-    The recorder works with an `IndexParameter`, `Parameter` or `Recorder`. An
-    event is considered active while the value of the threshold is non-zero.
-
-    The events are stored in a flat list across all scenarios. Each
-    event is stored as a separate `Event` object. Events can be accessed as a
-    dataframe using the `to_dataframe` method.
-
-    Parameters
-    ----------
-    threshold - IndexParameter, Parameter or Recorder
-       The object that defines the start and end of an event.
-    minimum_event_length - int (default=1)
-        The minimum number of time-steps that an event must last for
-        to be recorded. This is useful to not record events that are
-        caused by model hysteresis. The default will cause all events
-        to be recorded.
-    agg_func - string, callable
-        Function used for aggregating across the recorders. Numpy style functions that
-        support an axis argument are supported.
-    event_agg_func - string, callable
-        Optional different function for aggregating the `tracked_parameter` across events.
-        If given this aggregation will be added as a `value` column in the `to_dataframe` method.
-    tracked_parameter - `Parameter`
-        The parameter to track across each event. The values from this parameter are appended each
-        time-step to each event. These can then be used with other event recorders for statistical
-        aggregation, or with `event_agg_func`.
-
-     See also
-     --------
-     `pywr.parameters._thresholds`
-
-
-     """
-    def __init__(self, model, threshold, minimum_event_length=1, tracked_parameter=None, **kwargs):
-        self.event_agg_func = kwargs.pop('event_agg_func', kwargs.get('agg_func'))
-        super(EventRecorder, self).__init__(model, **kwargs)
-        self.threshold = threshold
-        self.threshold.parents.add(self)
-        if minimum_event_length < 1:
-            raise ValueError('Keyword "minimum_event_length" must be >= 1')
-        self.minimum_event_length = minimum_event_length
-        self.events = None
-        self._current_events = None
-        # TODO make this more generic to track components or  nodes (e.g. storage volume)
-        self.tracked_parameter = tracked_parameter
-        if self.tracked_parameter is not None:
-            self.tracked_parameter.parents.add(self)
-
-    def setup(self):
-        pass
-
-    def reset(self):
-        self.events = []
-        # This list stores if an event is current active in each scenario.
-        self._current_events = [None for si in self.model.scenarios.combinations]
-
-    def after(self):
-        # Current timestep
-        ts = self.model.timestepper.current
-
-        from pywr.parameters import Parameter, IndexParameter
-
-        if isinstance(self.threshold, Recorder):
-            all_triggered = np.array(self.threshold.values(), dtype=np.int)
-        elif isinstance(self.threshold, IndexParameter):
-            all_triggered = self.threshold.get_all_indices()
-        elif isinstance(self.threshold, Parameter):
-            all_triggered = np.array(self.threshold.get_all_values(), dtype=np.int)
-        else:
-            raise TypeError("Threshold must be either a Recorder or Parameter instance.")
-
-        for si in self.model.scenarios.combinations:
-            # Determine if an event is active this time-step/scenario combination
-            triggered = all_triggered[si.global_id]
-
-            # Get the current event
-            current_event = self._current_events[si.global_id]
-            if current_event is not None:
-                # A current event is active
-                if triggered:
-                    # Current event continues
-                    # Update the timeseries of event data
-                    if self.tracked_parameter is not None:
-                        value = self.tracked_parameter.get_value(si)
-                        current_event.values.append(value)
-                else:
-                    # Update the end of the current event.
-                    current_event.end = ts
-                    current_event.values = np.array(current_event.values)  # Convert list to nparray
-                    current_length = ts.index - current_event.start.index
-
-                    if current_length >= self.minimum_event_length:
-                        # Current event ends
-                        self.events.append(current_event)
-                        # Event has ended; no further updates
-                        current_event = None
-                    else:
-                        # Event wasn't long enough; don't append
-                        current_event = None
-            else:
-                # No current event
-                if triggered:
-                    # Start of a new event
-                    current_event = Event(ts, si)
-                    # Start the timeseries of event data
-                    if self.tracked_parameter is not None:
-                        value = self.tracked_parameter.get_value(si)
-                        current_event.values = [value, ]
-                else:
-                    # No event active and one hasn't started
-                    # Therefore do nothing.
-                    pass
-
-            # Update list of current events
-            self._current_events[si.global_id] = current_event
-
-    def finish(self):
-        ts = self.model.timestepper.current
-        # Complete any unfinished events
-        for si in self.model.scenarios.combinations:
-            # Get the current event
-            current_event = self._current_events[si.global_id]
-            if current_event is not None:
-                # Unfinished event
-                current_event.end = ts
-                self.events.append(current_event)
-                self._current_events[si.global_id] = None
-
-    def to_dataframe(self):
-        """ Returns a `pandas.DataFrame` containing all of the events.
-
-        If `event_agg_func` is a valid aggregation function and `tracked_parameter`
-         is given then a "value" column is added to the dataframe containing the
-         result of the aggregation.
-
-        """
-        # Return empty dataframe if no events are found.
-        if len(self.events) == 0:
-            return pandas.DataFrame(columns=['scenario_id', 'start', 'end'])
-
-        scen_id = np.empty(len(self.events), dtype=np.int)
-        start = np.empty_like(scen_id, dtype=object)
-        end = np.empty_like(scen_id, dtype=object)
-        values = np.empty_like(scen_id, dtype=float)
-
-        for i, evt in enumerate(self.events):
-            scen_id[i] = evt.scenario_index.global_id
-            start[i] = evt.start.datetime
-            end[i] = evt.end.datetime
-            if self.tracked_parameter is not None and self.event_agg_func is not None:
-                values[i] = pandas.Series(evt.values).aggregate(self.event_agg_func)
-
-        df_dict = {'scenario_id': scen_id, 'start': start, 'end': end}
-        if self.tracked_parameter is not None and self.event_agg_func is not None:
-            df_dict["value"] = values
-
-        return pandas.DataFrame(df_dict)
-
-
-class EventDurationRecorder(Recorder):
-    """ Recorder for the duration of events found by an EventRecorder
-
-    This Recorder uses the results of an EventRecorder to calculate the duration
-    of those events in each scenario. Aggregation by scenario is done via
-    the pandas.DataFrame.groupby() method.
-
-    Any scenario which has no events will contain a NaN value.
-
-    Parameters
-    ----------
-    event_recorder : EventRecorder
-        EventRecorder instance to calculate the events.
-    agg_func - string, callable
-        Function used for aggregating across the recorders. Numpy style functions that
-        support an axis argument are supported.
-    recorder_agg_func - string, callable
-        Optional aggregating function for all events in each scenario. The function
-        must be supported by the `DataFrame.group_by` method.
-
-    """
-    def __init__(self, model, event_recorder, **kwargs):
-        # Optional different method for aggregating across self.recorders scenarios
-        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
-        self.recorder_agg_func = agg_func
-
-        super(EventDurationRecorder, self).__init__(model, **kwargs)
-        self.event_recorder = event_recorder
-        self.event_recorder.parents.add(self)
-
-    def setup(self):
-        self._values = np.empty(len(self.model.scenarios.combinations))
-
-    def reset(self):
-        self._values[...] = 0.0
-
-    def values(self):
-        return self._values
-
-    def finish(self):
-        df = self.event_recorder.to_dataframe()
-
-        self._values[...] = 0.0
-        # No events found
-        if len(df) == 0:
-            return
-
-        # Calculate duration
-        df['duration'] = df['end'] - df['start']
-        # Convert to int of days
-        df['duration'] = df['duration'].dt.days
-        # Drop other columns
-        df = df[['scenario_id', 'duration']]
-
-        # Group by scenario ...
-        grouped = df.groupby('scenario_id').agg(self.recorder_agg_func)
-        # ... and update the internal values
-        for index, row in grouped.iterrows():
-            self._values[index] = row['duration']
-
-
-class EventStatisticRecorder(Recorder):
-    """ Recorder for the duration of events found by an EventRecorder
-
-    This Recorder uses the results of an EventRecorder to calculate aggregated statistics
-    of those events in each scenario. This requires the EventRecorder to be given a `tracked_parameter`
-    in order to save an array of values during each event. This recorder uses `event_agg_func` to aggregate
-    those saved values in each event before applying `recorder_agg_func` to those values in each scenario.
-    Aggregation by scenario is done via the pandas.DataFrame.groupby() method.
-
-    Any scenario which has no events will contain a NaN value regardless of the aggregation function defined.
-
-    Parameters
-    ----------
-    model : pywr.model.Model
-    event_recorder : EventRecorder
-        EventRecorder instance to calculate the events.
-    agg_func - string, callable
-        Function used for aggregating across the recorders. Numpy style functions that
-        support an axis argument are supported.
-    recorder_agg_func - string, callable
-        Optional aggregating function for all events in each scenario. The function
-        must be supported by the `DataFrame.group_by` method.
-    event_agg_func - string, callable
-        Optional different function for aggregating the `tracked_parameter` across events.
-        If given this aggregation will be added as a `value` column in the `to_dataframe` method.
-    """
-    def __init__(self, model, event_recorder, **kwargs):
-        # Optional different method for aggregating across self.recorders scenarios
-        agg_func = kwargs.pop('event_agg_func', kwargs.get('agg_func'))
-        self.event_agg_func = agg_func
-        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
-        self.recorder_agg_func = agg_func
-
-        super(EventStatisticRecorder, self).__init__(model, **kwargs)
-        self.event_recorder = event_recorder
-        self.event_recorder.parents.add(self)
-
-    def setup(self):
-        self._values = np.empty(len(self.model.scenarios.combinations))
-
-        if self.event_recorder.tracked_parameter is None:
-            raise ValueError('To calculate event statistics requires the parent `EventRecorder` to have a `tracked_parameter`.')
-
-    def reset(self):
-        self._values[...] = np.nan
-
-    def values(self):
-        return self._values
-
-    def finish(self):
-        """ Compute the aggregated value in each scenario based on the parent `EventRecorder` events """
-        events = self.event_recorder.events
-        # Return NaN if no events found
-        if len(events) == 0:
-            return
-
-        scen_id = np.empty(len(events), dtype=np.int)
-        values = np.empty_like(scen_id, dtype=np.float64)
-
-        for i, evt in enumerate(events):
-            scen_id[i] = evt.scenario_index.global_id
-            values[i] = pandas.Series(evt.values).aggregate(self.event_agg_func)
-
-        df = pandas.DataFrame({'scenario_id': scen_id, 'value': values})
-
-        # Group by scenario ...
-        grouped = df.groupby('scenario_id').agg(self.recorder_agg_func)
-        # ... and update the internal values
-        for index, row in grouped.iterrows():
-            self._values[index] = row['value']
+from ._recorders import Recorder
+import numpy as np
+import pandas
+
+
+class Event(object):
+    """ Container for event information """
+    def __init__(self, start, scenario_index):
+        self.start = start
+        self.scenario_index = scenario_index
+        self.end = None
+        self.values = None  # to record any  tracked values
+
+    @property
+    def duration(self):
+        td = self.end.datetime - self.start.datetime
+        return td.days
+
+
+class EventRecorder(Recorder):
+    """Track discrete events using a Parameter or Recorder
+
+    The recorder works with an `IndexParameter`, `Parameter` or `Recorder`. An
+    event is considered active while the value of the threshold is non-zero.
+
+    The events are stored in a flat list across all scenarios. Each
+    event is stored as a separate `Event` object. Events can be accessed as a
+    dataframe using the `to_dataframe` method.
+
+    Parameters
+    ----------
+    threshold - IndexParameter, Parameter or Recorder
+       The object that defines the start and end of an event.
+    minimum_event_length - int (default=1)
+        The minimum number of time-steps that an event must last for
+        to be recorded. This is useful to not record events that are
+        caused by model hysteresis. The default will cause all events
+        to be recorded.
+    agg_func - string, callable
+        Function used for aggregating across the recorders. Numpy style functions that
+        support an axis argument are supported.
+    event_agg_func - string, callable
+        Optional different function for aggregating the `tracked_parameter` across events.
+        If given this aggregation will be added as a `value` column in the `to_dataframe` method.
+    tracked_parameter - `Parameter`
+        The parameter to track across each event. The values from this parameter are appended each
+        time-step to each event. These can then be used with other event recorders for statistical
+        aggregation, or with `event_agg_func`.
+
+     See also
+     --------
+     `pywr.parameters._thresholds`
+
+
+     """
+    def __init__(self, model, threshold, minimum_event_length=1, tracked_parameter=None, **kwargs):
+        self.event_agg_func = kwargs.pop('event_agg_func', kwargs.get('agg_func'))
+        super(EventRecorder, self).__init__(model, **kwargs)
+        self.threshold = threshold
+        self.threshold.parents.add(self)
+        if minimum_event_length < 1:
+            raise ValueError('Keyword "minimum_event_length" must be >= 1')
+        self.minimum_event_length = minimum_event_length
+        self.events = None
+        self._current_events = None
+        # TODO make this more generic to track components or  nodes (e.g. storage volume)
+        self.tracked_parameter = tracked_parameter
+        if self.tracked_parameter is not None:
+            self.tracked_parameter.parents.add(self)
+
+    def setup(self):
+        pass
+
+    def reset(self):
+        self.events = []
+        # This list stores if an event is current active in each scenario.
+        self._current_events = [None for si in self.model.scenarios.combinations]
+
+    def after(self):
+        # Current timestep
+        ts = self.model.timestepper.current
+
+        from pywr.parameters import Parameter, IndexParameter
+
+        if isinstance(self.threshold, Recorder):
+            all_triggered = np.array(self.threshold.values(), dtype=np.int)
+        elif isinstance(self.threshold, IndexParameter):
+            all_triggered = self.threshold.get_all_indices()
+        elif isinstance(self.threshold, Parameter):
+            all_triggered = np.array(self.threshold.get_all_values(), dtype=np.int)
+        else:
+            raise TypeError("Threshold must be either a Recorder or Parameter instance.")
+
+        for si in self.model.scenarios.combinations:
+            # Determine if an event is active this time-step/scenario combination
+            triggered = all_triggered[si.global_id]
+
+            # Get the current event
+            current_event = self._current_events[si.global_id]
+            if current_event is not None:
+                # A current event is active
+                if triggered:
+                    # Current event continues
+                    # Update the timeseries of event data
+                    if self.tracked_parameter is not None:
+                        value = self.tracked_parameter.get_value(si)
+                        current_event.values.append(value)
+                else:
+                    # Update the end of the current event.
+                    current_event.end = ts
+                    current_event.values = np.array(current_event.values)  # Convert list to nparray
+                    current_length = ts.index - current_event.start.index
+
+                    if current_length >= self.minimum_event_length:
+                        # Current event ends
+                        self.events.append(current_event)
+                        # Event has ended; no further updates
+                        current_event = None
+                    else:
+                        # Event wasn't long enough; don't append
+                        current_event = None
+            else:
+                # No current event
+                if triggered:
+                    # Start of a new event
+                    current_event = Event(ts, si)
+                    # Start the timeseries of event data
+                    if self.tracked_parameter is not None:
+                        value = self.tracked_parameter.get_value(si)
+                        current_event.values = [value, ]
+                else:
+                    # No event active and one hasn't started
+                    # Therefore do nothing.
+                    pass
+
+            # Update list of current events
+            self._current_events[si.global_id] = current_event
+
+    def finish(self):
+        ts = self.model.timestepper.current
+        # Complete any unfinished events
+        for si in self.model.scenarios.combinations:
+            # Get the current event
+            current_event = self._current_events[si.global_id]
+            if current_event is not None:
+                # Unfinished event
+                current_event.end = ts
+                self.events.append(current_event)
+                self._current_events[si.global_id] = None
+
+    def to_dataframe(self):
+        """ Returns a `pandas.DataFrame` containing all of the events.
+
+        If `event_agg_func` is a valid aggregation function and `tracked_parameter`
+         is given then a "value" column is added to the dataframe containing the
+         result of the aggregation.
+
+        """
+        # Return empty dataframe if no events are found.
+        if len(self.events) == 0:
+            return pandas.DataFrame(columns=['scenario_id', 'start', 'end'])
+
+        scen_id = np.empty(len(self.events), dtype=np.int)
+        start = np.empty_like(scen_id, dtype=object)
+        end = np.empty_like(scen_id, dtype=object)
+        values = np.empty_like(scen_id, dtype=float)
+
+        for i, evt in enumerate(self.events):
+            scen_id[i] = evt.scenario_index.global_id
+            start[i] = evt.start.datetime
+            end[i] = evt.end.datetime
+            if self.tracked_parameter is not None and self.event_agg_func is not None:
+                values[i] = pandas.Series(evt.values).aggregate(self.event_agg_func)
+
+        df_dict = {'scenario_id': scen_id, 'start': start, 'end': end}
+        if self.tracked_parameter is not None and self.event_agg_func is not None:
+            df_dict["value"] = values
+
+        return pandas.DataFrame(df_dict)
+
+
+class EventDurationRecorder(Recorder):
+    """ Recorder for the duration of events found by an EventRecorder
+
+    This Recorder uses the results of an EventRecorder to calculate the duration
+    of those events in each scenario. Aggregation by scenario is done via
+    the pandas.DataFrame.groupby() method.
+
+    Any scenario which has no events will contain a NaN value.
+
+    Parameters
+    ----------
+    event_recorder : EventRecorder
+        EventRecorder instance to calculate the events.
+    agg_func - string, callable
+        Function used for aggregating across the recorders. Numpy style functions that
+        support an axis argument are supported.
+    recorder_agg_func - string, callable
+        Optional aggregating function for all events in each scenario. The function
+        must be supported by the `DataFrame.group_by` method.
+
+    """
+    def __init__(self, model, event_recorder, **kwargs):
+        # Optional different method for aggregating across self.recorders scenarios
+        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
+        self.recorder_agg_func = agg_func
+
+        super(EventDurationRecorder, self).__init__(model, **kwargs)
+        self.event_recorder = event_recorder
+        self.event_recorder.parents.add(self)
+
+    def setup(self):
+        self._values = np.empty(len(self.model.scenarios.combinations))
+
+    def reset(self):
+        self._values[...] = 0.0
+
+    def values(self):
+        return self._values
+
+    def finish(self):
+        df = self.event_recorder.to_dataframe()
+
+        self._values[...] = 0.0
+        # No events found
+        if len(df) == 0:
+            return
+
+        # Calculate duration
+        df['duration'] = df['end'] - df['start']
+        # Convert to int of days
+        df['duration'] = df['duration'].dt.days
+        # Drop other columns
+        df = df[['scenario_id', 'duration']]
+
+        # Group by scenario ...
+        grouped = df.groupby('scenario_id').agg(self.recorder_agg_func)
+        # ... and update the internal values
+        for index, row in grouped.iterrows():
+            self._values[index] = row['duration']
+
+
+class EventStatisticRecorder(Recorder):
+    """ Recorder for the duration of events found by an EventRecorder
+
+    This Recorder uses the results of an EventRecorder to calculate aggregated statistics
+    of those events in each scenario. This requires the EventRecorder to be given a `tracked_parameter`
+    in order to save an array of values during each event. This recorder uses `event_agg_func` to aggregate
+    those saved values in each event before applying `recorder_agg_func` to those values in each scenario.
+    Aggregation by scenario is done via the pandas.DataFrame.groupby() method.
+
+    Any scenario which has no events will contain a NaN value regardless of the aggregation function defined.
+
+    Parameters
+    ----------
+    model : pywr.model.Model
+    event_recorder : EventRecorder
+        EventRecorder instance to calculate the events.
+    agg_func - string, callable
+        Function used for aggregating across the recorders. Numpy style functions that
+        support an axis argument are supported.
+    recorder_agg_func - string, callable
+        Optional aggregating function for all events in each scenario. The function
+        must be supported by the `DataFrame.group_by` method.
+    event_agg_func - string, callable
+        Optional different function for aggregating the `tracked_parameter` across events.
+        If given this aggregation will be added as a `value` column in the `to_dataframe` method.
+    """
+    def __init__(self, model, event_recorder, **kwargs):
+        # Optional different method for aggregating across self.recorders scenarios
+        agg_func = kwargs.pop('event_agg_func', kwargs.get('agg_func'))
+        self.event_agg_func = agg_func
+        agg_func = kwargs.pop('recorder_agg_func', kwargs.get('agg_func'))
+        self.recorder_agg_func = agg_func
+
+        super(EventStatisticRecorder, self).__init__(model, **kwargs)
+        self.event_recorder = event_recorder
+        self.event_recorder.parents.add(self)
+
+    def setup(self):
+        self._values = np.empty(len(self.model.scenarios.combinations))
+
+        if self.event_recorder.tracked_parameter is None:
+            raise ValueError('To calculate event statistics requires the parent `EventRecorder` to have a `tracked_parameter`.')
+
+    def reset(self):
+        self._values[...] = np.nan
+
+    def values(self):
+        return self._values
+
+    def finish(self):
+        """ Compute the aggregated value in each scenario based on the parent `EventRecorder` events """
+        events = self.event_recorder.events
+        # Return NaN if no events found
+        if len(events) == 0:
+            return
+
+        scen_id = np.empty(len(events), dtype=np.int)
+        values = np.empty_like(scen_id, dtype=np.float64)
+
+        for i, evt in enumerate(events):
+            scen_id[i] = evt.scenario_index.global_id
+            values[i] = pandas.Series(evt.values).aggregate(self.event_agg_func)
+
+        df = pandas.DataFrame({'scenario_id': scen_id, 'value': values})
+
+        # Group by scenario ...
+        grouped = df.groupby('scenario_id').agg(self.recorder_agg_func)
+        # ... and update the internal values
+        for index, row in grouped.iterrows():
+            self._values[index] = row['value']
```

### Comparing `pywr-1.8.0/pywr/recorders/progress.py` & `pywr-1.9.0/pywr/recorders/progress.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-from pywr.recorders import Recorder
-import time
-import logging
-logger = logging.getLogger(__name__)
-
-
-class ProgressRecorder(Recorder):
-    """Simple text-based progress notifications
-
-    Parameters
-    ----------
-    print_func : callable or None
-        The function to call when updating progress. The function is given a single str argument that
-        contains the progress message. Defaults to `logger.info`.
-    """
-    def __init__(self, *args, **kwargs):
-        print_func = kwargs.pop('print_func', None)
-        super(ProgressRecorder, self).__init__(*args, **kwargs)
-        if print_func is None:
-            print_func = logger.info
-        self.print_func = print_func
-        self.last_progress = None
-        self.last_timestep = None
-        self.t0 = None
-        self.combinations = None
-
-    def reset(self):
-        self.last_progress = -1
-        self.last_timestep = 0
-        self.t0 = time.time()
-        self.combinations = len(self.model.scenarios.combinations)
-
-    def after(self):
-        total_timesteps = len(self.model.timestepper)
-        timestep = self.model.timestepper.current.index
-        progress = int((timestep+1) / total_timesteps * 100)
-        if progress > self.last_progress:
-            self.last_progress = progress
-            if progress >= 1:
-                time_taken = time.time() - self.t0
-                try:
-                    speed = ((timestep-self.last_timestep)*self.combinations) / time_taken
-                except ZeroDivisionError:
-                    speed = float("inf")
-                self.last_timestep = timestep
-                self.update_progress(progress, speed)
-            else:
-                self.last_timestep = timestep
-                self.update_progress(progress, None)
-            self.t0 = time.time()
-            self.last_timestep = timestep
-
-    def update_progress(self, progress, speed=None):
-        if speed is not None:
-            self.print_func("Completed {}%, {:.0f} steps/second".format(progress, speed))
-        else:
-            self.print_func("Completed {}%".format(progress))
-
-class JupyterProgressRecorder(ProgressRecorder):
-    """Graphical progress bar for use in Jupyter notebooks"""
-    def __init__(self, *args, **kwargs):
-        super(JupyterProgressRecorder, self).__init__(*args, **kwargs)
-
-    def reset(self):
-        from ipywidgets import FloatProgress, HBox, Label, Layout
-        from IPython.display import display
-        super(JupyterProgressRecorder, self).reset()
-        self.progress_bar = FloatProgress(min=0, max=100, description='Running:')
-        self.label = Label("", layout=Layout(width='100%'))
-        self.box = HBox([self.progress_bar, self.label])
-        display(self.box)
-
-    def update_progress(self, progress, speed):
-        self.progress_bar.value = progress
-        if speed is not None:
-            self.label.value = "{:.0f} steps/second".format(speed)
-        else:
-            self.label.value = ""
-
-    def finish(self):
-        super(JupyterProgressRecorder, self).finish()
-        if self.progress_bar.value >= 100.0:
-            self.progress_bar.bar_style = "success"
-        else:
-            self.progress_bar.bar_style = "danger"
-
+from pywr.recorders import Recorder
+import time
+import logging
+logger = logging.getLogger(__name__)
+
+
+class ProgressRecorder(Recorder):
+    """Simple text-based progress notifications
+
+    Parameters
+    ----------
+    print_func : callable or None
+        The function to call when updating progress. The function is given a single str argument that
+        contains the progress message. Defaults to `logger.info`.
+    """
+    def __init__(self, *args, **kwargs):
+        print_func = kwargs.pop('print_func', None)
+        super(ProgressRecorder, self).__init__(*args, **kwargs)
+        if print_func is None:
+            print_func = logger.info
+        self.print_func = print_func
+        self.last_progress = None
+        self.last_timestep = None
+        self.t0 = None
+        self.combinations = None
+
+    def reset(self):
+        self.last_progress = -1
+        self.last_timestep = 0
+        self.t0 = time.time()
+        self.combinations = len(self.model.scenarios.combinations)
+
+    def after(self):
+        total_timesteps = len(self.model.timestepper)
+        timestep = self.model.timestepper.current.index
+        progress = int((timestep+1) / total_timesteps * 100)
+        if progress > self.last_progress:
+            self.last_progress = progress
+            if progress >= 1:
+                time_taken = time.time() - self.t0
+                try:
+                    speed = ((timestep-self.last_timestep)*self.combinations) / time_taken
+                except ZeroDivisionError:
+                    speed = float("inf")
+                self.last_timestep = timestep
+                self.update_progress(progress, speed)
+            else:
+                self.last_timestep = timestep
+                self.update_progress(progress, None)
+            self.t0 = time.time()
+            self.last_timestep = timestep
+
+    def update_progress(self, progress, speed=None):
+        if speed is not None:
+            self.print_func("Completed {}%, {:.0f} steps/second".format(progress, speed))
+        else:
+            self.print_func("Completed {}%".format(progress))
+
+class JupyterProgressRecorder(ProgressRecorder):
+    """Graphical progress bar for use in Jupyter notebooks"""
+    def __init__(self, *args, **kwargs):
+        super(JupyterProgressRecorder, self).__init__(*args, **kwargs)
+
+    def reset(self):
+        from ipywidgets import FloatProgress, HBox, Label, Layout
+        from IPython.display import display
+        super(JupyterProgressRecorder, self).reset()
+        self.progress_bar = FloatProgress(min=0, max=100, description='Running:')
+        self.label = Label("", layout=Layout(width='100%'))
+        self.box = HBox([self.progress_bar, self.label])
+        display(self.box)
+
+    def update_progress(self, progress, speed):
+        self.progress_bar.value = progress
+        if speed is not None:
+            self.label.value = "{:.0f} steps/second".format(speed)
+        else:
+            self.label.value = ""
+
+    def finish(self):
+        super(JupyterProgressRecorder, self).finish()
+        if self.progress_bar.value >= 100.0:
+            self.progress_bar.bar_style = "success"
+        else:
+            self.progress_bar.bar_style = "danger"
+
```

### Comparing `pywr-1.8.0/pywr/recorders/recorders.py` & `pywr-1.9.0/pywr/recorders/recorders.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,560 +1,560 @@
-import sys
-import pandas
-import numpy as np
-from functools import wraps
-from pywr._core import AbstractNode, AbstractStorage
-from ._recorders import *
-from ._thresholds import *
-from ._hydropower import *
-from .events import *
-from .calibration import *
-from pywr.h5tools import H5Store
-from ..parameter_property import parameter_property
-import warnings
-
-class ParameterNameWarning(UserWarning):
-    pass
-
-def assert_rec(model, parameter, name=None, get_index=False):
-    """Decorator for creating AssertionRecorder objects
-
-    Example
-    -------
-    @assert_rec(model, parameter)
-    def expected_func(timestep, scenario_index):
-        return timestep.dayofyear * 2.0
-    """
-    def assert_rec_(f):
-        rec = AssertionRecorder(model, parameter, expected_func=f, name=name, get_index=get_index)
-        return f
-    return assert_rec_
-
-class AssertionRecorder(Recorder):
-    """A recorder that asserts the value of a parameter for testing purposes"""
-    def __init__(self, model, parameter, expected_data=None, expected_func=None, get_index=False, **kwargs):
-        """
-        Parameters
-        ----------
-        model : pywr.model.Model
-        parameter : pywr.parameters.Parameter
-        expected_data : np.ndarray[timestep, scenario] (optional)
-        expected_func : function
-        get_index : bool
-
-        See also
-        --------
-        pywr.recorders.assert_rec
-        """
-        super(AssertionRecorder, self).__init__(model, **kwargs)
-        self._parameter = None
-        self.parameter = parameter
-        self.expected_data = expected_data
-        self.expected_func = expected_func
-        self.get_index = get_index
-
-    parameter = parameter_property("_parameter")
-
-    def setup(self):
-        super(AssertionRecorder, self).setup()
-        self.count = 0
-
-    def after(self):
-        timestep = self.model.timestep
-        self.count += 1
-        for scenario_index in self.model.scenarios.combinations:
-            if self.expected_func:
-                expected_value = self.expected_func(timestep, scenario_index)
-            elif self.expected_data is not None:
-                expected_value = self.expected_data[timestep.index, scenario_index.global_id]
-            if self.get_index:
-                value = self._parameter.get_index(scenario_index)
-            else:
-                value = self._parameter.get_value(scenario_index)
-            try:
-                np.testing.assert_allclose(value, expected_value)
-            except AssertionError:
-                raise AssertionError("Expected {}, got {} from \"{}\" [timestep={}, scenario={}]".format(expected_value, value, self._parameter.name, timestep.index, scenario_index.global_id))
-
-    def finish(self):
-        super(AssertionRecorder, self).finish()
-        if sys.exc_info():
-            # exception was raised before we had a chance! (e.g. ModelStructureError)
-            pass
-        elif self.count == 0:
-            # this still requires model.run() to have been called...
-            raise RuntimeError("AssertionRecorder was never called!")
-
-
-class CSVRecorder(Recorder):
-    """
-    A Recorder that saves Node values to a CSV file.
-
-    This class uses the csv package from the Python standard library
-
-    Parameters
-    ----------
-
-    model : `pywr.model.Model`
-        The model to record nodes from.
-    csvfile : str
-        The path to the CSV file.
-    scenario_index : int
-        The scenario index of the model to save.
-    nodes : iterable (default=None)
-        An iterable of nodes to save data. It defaults to None which is all nodes in the model
-    kwargs : Additional keyword arguments to pass to the `csv.writer` object
-
-    """
-    def __init__(self, model, csvfile, scenario_index=0, nodes=None, complib=None, complevel=9, **kwargs):
-        super(CSVRecorder, self).__init__(model, **kwargs)
-        self.csvfile = csvfile
-        self.scenario_index = scenario_index
-        self.nodes = nodes
-        self.csv_kwargs = kwargs.pop('csv_kwargs', {})
-        self._node_names = None
-        self._fh = None
-        self._writer = None
-        self.complib = complib
-        self.complevel = complevel
-
-    @classmethod
-    def load(cls, model, data):
-        import os
-        url = data.pop("url")
-        if not os.path.isabs(url) and model.path is not None:
-            url = os.path.join(model.path, url)
-        return cls(model, url, **data)
-
-    def setup(self):
-        """
-        Setup the CSV file recorder.
-        """
-
-        if self.nodes is None:
-            self._node_names = sorted(self.model.nodes.keys())
-        else:
-            node_names = []
-            for node_ in self.nodes:
-                # test if the node name is provided
-                if isinstance(node_, str):
-                    # lookup node by name
-                    node_names.append(node_)
-                else:
-                    node_names.append((node_.name))
-            self._node_names = node_names
-
-    def reset(self):
-        import csv
-        kwargs = {"newline": "", "encoding": "utf-8"}
-        mode = "wt"
-
-        if self.complib == "gzip":
-            import gzip
-            self._fh = gzip.open(self.csvfile, mode, self.complevel, **kwargs)
-        elif self.complib in ("bz2", "bzip2"):
-            import bz2
-            self._fh = bz2.open(self.csvfile, mode, self.complevel, **kwargs)
-        elif self.complib is None:
-            self._fh = open(self.csvfile, mode, **kwargs)
-        else:
-            raise KeyError("Unexpected compression library: {}".format(self.complib))
-        self._writer = csv.writer(self._fh, **self.csv_kwargs)
-        # Write header data
-        row = ["Datetime"] + [name for name in self._node_names]
-        self._writer.writerow(row)
-
-    def after(self):
-        """
-        Write the node values to the CSV file
-        """
-        values = [self.model.timestepper.current.datetime.isoformat()]
-        for node_name in self._node_names:
-            node = self.model.nodes[node_name]
-            if isinstance(node, AbstractStorage):
-                values.append(node.volume[self.scenario_index])
-            elif isinstance(node, AbstractNode):
-                values.append(node.flow[self.scenario_index])
-            else:
-                raise ValueError("Unrecognised Node type '{}' for CSV writer".format(type(node)))
-
-        self._writer.writerow(values)
-
-    def finish(self):
-        if self._fh:
-            self._fh.close()
-CSVRecorder.register()
-
-
-class TablesRecorder(Recorder):
-    """
-    A recorder that saves to PyTables CArray
-
-    This Recorder creates a CArray for every node passed to the constructor.
-    Each CArray stores the data for all scenarios on the specific node. This
-    is useful for analysis of Node statistics across multiple scenarios.
-    """
-    def __init__(self, model, h5file, nodes=None, parameters=None, where='/', time='/time',
-                 routes_flows=None, routes='/routes', scenarios='/scenarios', **kwargs):
-        """
-
-        Parameters
-        ----------
-        model : `pywr.model.Model`
-            The model to record nodes from.
-        h5file : tables.File or filename
-            The tables file handle or filename to attach the CArray objects to. If a
-            filename is given the object will open and close the file handles.
-        nodes : iterable or None
-            Nodes to save in the tables database. Can be an iterable of Node objects or
-            node names. It can also be a iterable of tuples with a node specific where
-            keyword as the first item and a Node object or name as the second item. If
-            an iterable of tuples is provided then the node specific where keyword is
-            used in preference to the where keyword (see below).
-        parameters : iterable or None
-            Parameters to save. Similar to the nodes keyword, except refers to Parameter
-            objects or names thereof.
-        where : string
-            Default path to create the CArrays inside the database.
-        time : string
-            Default full node path to save a time tables.Table. If None no table is created.
-        scenarios : string
-            Default full node path to save a scenarios tables.Table. If None no table is created.
-        routes_flows : string
-            Relative (to `where`) node path to save the routes flow CArray. If None (default) no array is created.
-        routes : string
-            Full node path to save the routes tables.Table. If None not table is created.
-        filter_kwds : dict
-            Filter keywords to pass to tables.open_file when opening a file.
-        mode : string
-            Model argument to pass to tables.open_file. Defaults to 'w'
-        metadata : dict
-            Dict of user defined attributes to save on the root node (`root._v_attrs`)
-        create_directories : bool
-            If a file path is given and create_directories is True then attempt to make the intermediate
-            directories. This uses os.makedirs() underneath.
-        """
-        self.filter_kwds = kwargs.pop('filter_kwds', {})
-        self.mode = kwargs.pop('mode', 'w')
-        self.metadata = kwargs.pop('metadata', {})
-        self.create_directories = kwargs.pop('create_directories', False)
-
-        title = kwargs.pop('title', None)
-        if title is None:
-            try:
-                title = model.metadata['title']
-            except KeyError:
-                title = ''
-        self.title = title
-        super(TablesRecorder, self).__init__(model, **kwargs)
-
-        self.h5file = h5file
-        self.h5store = None
-        self._arrays = {}
-        self.nodes = nodes
-        self.where = where
-        self.time = time
-        self.scenarios = scenarios
-        self.routes = routes
-        self.routes_flows = routes_flows
-
-        # Enable saving routes in the solver.
-        if routes_flows:
-            self.model.solver.save_routes_flows = True
-
-        self.parameters = []
-        if parameters:
-            for parameter in parameters:
-                self._add_parameter(parameter)
-
-        self._arrays = None
-        self._time_table = None
-        self._routes_flow_array = None
-
-    def _add_parameter(self, parameter):
-        try:
-            where, param = parameter
-        except (TypeError, ValueError):
-            where = None
-            param = parameter
-        if isinstance(param, str):
-            from ..parameters import load_parameter
-            param = load_parameter(self.model, param)
-        if not param.name:
-            raise ValueError("Can only record named Parameter objects")
-        if where is None:
-            name = param.name.replace("/", "_")
-            if name != param.name:
-                warnings.warn(
-                    "Recorded parameter has \"/\" in name, replaced with \"_\" to avoid creation of subgroup: {}".format(param.name),
-                    ParameterNameWarning
-                )
-            where = self.where + "/" + name
-        where = where.replace("//", "/")
-        self.children.add(param)
-        self.parameters.append((where, param))
-
-    def _remove_parameter(self, parameter):
-        if isinstance(parameter, str):
-            parameter = self.model.parameters[parameter]
-        index = None
-        for n, (where, param) in enumerate(self.parameters):
-            if param is parameter:
-                index = n
-        if index is None:
-            raise KeyError("Parameter is not in TablesRecorder: {}".format(parameter))
-        self.parameters.pop(index)
-        self.children.remove(parameter)
-
-    @classmethod
-    def load(cls, model, data):
-        import os
-        url = data.pop("url")
-        if not os.path.isabs(url) and model.path is not None:
-            url = os.path.join(model.path, url)
-        return cls(model, url, **data)
-
-    def setup(self):
-        """
-        Setup the tables
-        """
-        from pywr.parameters import IndexParameter
-        import tables
-
-        # The first dimension is the number of timesteps.
-        # The following dimensions are sized per scenario
-        scenario_shape = list(self.model.scenarios.shape)
-        shape = [len(self.model.timestepper)] + scenario_shape
-
-        self.h5store = H5Store(self.h5file, self.filter_kwds, self.mode, title=self.title, metadata=self.metadata,
-                               create_directories=self.create_directories)
-
-        # Create a CArray for each node
-        self._arrays = {}
-
-        # Default to all nodes if None given.
-        if self.nodes is None:
-            nodes = [((self.where + "/" + n.name).replace("//", "/"), n) for n in self.model.nodes.values()]
-        else:
-            nodes = []
-            for n in self.nodes:
-
-                try:
-                    where, node = n
-                except (TypeError, ValueError):
-                    node = n
-                    where = self.where + "/" + node
-
-                # Accept a str, and lookup node by name instead.
-                if isinstance(node, str):
-                    node = self.model.nodes[node]
-                # Otherwise assume it is a node object anyway
-
-                where = where.replace("//", "/")
-                nodes.append((where, node))
-
-        if self.parameters is not None:
-            nodes.extend(self.parameters)
-
-        self._nodes = nodes
-
-        for where, node in self._nodes:
-            if isinstance(node, IndexParameter):
-                atom = tables.Int32Atom()
-            else:
-                atom = tables.Float64Atom()
-            group_name, node_name = where.rsplit("/", 1)
-            if group_name == "":
-                group_name = "/"
-            self.h5store.file.create_carray(group_name, node_name, atom, shape, createparents=True)
-
-        # Create scenario tables
-        if self.scenarios is not None:
-            group_name, node_name = self.scenarios.rsplit('/', 1)
-            if group_name == "":
-                group_name = "/"
-            description = {
-                # TODO make string length configurable
-                'name': tables.StringCol(1024),
-                'size': tables.Int64Col()
-            }
-            tbl = self.h5store.file.create_table(group_name, node_name, description=description, createparents=True)
-            # Now add the scenarios
-            entry = tbl.row
-            for scenario in self.model.scenarios.scenarios:
-                entry['name'] = scenario.name.encode('utf-8')
-                entry['size'] = scenario.size
-                entry.append()
-            tbl.flush()
-
-            if self.model.scenarios.user_combinations is not None:
-                description = {s.name: tables.Int64Col() for s in self.model.scenarios.scenarios}
-                tbl = self.h5store.file.create_table(group_name, 'scenario_combinations', description=description)
-                entry = tbl.row
-                for comb in self.model.scenarios.user_combinations:
-                    for s, i in zip(self.model.scenarios.scenarios, comb):
-                        entry[s.name] = i
-                    entry.append()
-                tbl.flush()
-
-        self.h5store = None
-
-    def reset(self):
-        import tables
-
-        mode = "r+"  # always need to append, as file already created in setup
-        self.h5store = H5Store(self.h5file, self.filter_kwds, mode)
-        self._arrays = {}
-        for where, node in self._nodes:
-            self._arrays[node] = self.h5store.file.get_node(where)
-
-        self._time_table = None
-        # Create time table
-        # This is created in reset so that the table is always recreated
-        if self.time is not None:
-            group_name, node_name = self.time.rsplit('/', 1)
-            if group_name == "":
-                group_name = "/"
-            description = {c: tables.Int64Col() for c in ('year', 'month', 'day', 'index')}
-
-            try:
-                self.h5store.file.remove_node(group_name, node_name)
-            except tables.NoSuchNodeError:
-                pass
-            finally:
-                self._time_table = self.h5store.file.create_table(group_name, node_name,
-                                                                  description=description,
-                                                                  createparents=True)
-
-        self._routes_flow_array = None
-        if self.routes_flows is not None:
-            # Create a CArray for the flows
-            # The first dimension is the number of timesteps.
-            # The second dimension is the number of routes
-            # The following dimensions are sized per scenario
-            scenario_shape = list(self.model.scenarios.shape)
-            shape = [len(self.model.timestepper), len(self.model.solver.routes)] + scenario_shape
-            atom = tables.Float64Atom()
-
-            try:
-                self.h5store.file.remove_node(self.where, self.routes_flows)
-            except tables.NoSuchNodeError:
-                pass
-            finally:
-                self._routes_flow_array = self.h5store.file.create_carray(self.where, self.routes_flows, atom, shape, createparents=True)
-
-            # Create routes table. This must be done in reset
-            if self.routes is not None:
-                group_name, node_name = self.routes.rsplit('/', 1)
-                if group_name == "":
-                    group_name = "/"
-
-                description = {
-                    # TODO make string length configurable
-                    'start': tables.StringCol(1024),
-                    'end': tables.StringCol(1024),
-                }
-                try:
-                    self.h5store.file.remove_node(group_name, node_name)
-                except tables.NoSuchNodeError:
-                    pass
-                finally:
-                    tbl = self.h5store.file.create_table(group_name, node_name, description=description, createparents=True)
-
-                entry = tbl.row
-                for route in self.model.solver.routes:
-                    node_first = route[0]
-                    node_last = route[-1]
-
-                    if node_first.parent is not None:
-                        node_first = node_first.parent
-                    if node_last.parent is not None:
-                        node_last = node_last.parent
-
-                    entry['start'] = node_first.name.encode('utf-8')
-                    entry['end'] = node_last.name.encode('utf-8')
-                    entry.append()
-
-                tbl.flush()
-
-    def after(self):
-        """
-        Save data to the tables
-        """
-        from pywr._core import AbstractNode, AbstractStorage
-        from pywr.parameters import Parameter, IndexParameter
-        scenario_shape = list(self.model.scenarios.shape)
-        ts = self.model.timestepper.current
-        idx = ts.index
-
-        if self._time_table is not None:
-            entry = self._time_table.row
-            entry['year'] = ts.year
-            entry['month'] = ts.month
-            entry['day'] = ts.day
-            entry['index'] = idx
-            entry.append()
-
-        for node, ca in self._arrays.items():
-            if isinstance(node, AbstractStorage):
-                ca[idx, :] = np.reshape(node.volume, scenario_shape)
-            elif isinstance(node, AbstractNode):
-                ca[idx, :] = np.reshape(node.flow, scenario_shape)
-            elif isinstance(node, IndexParameter):
-                a = node.get_all_indices()
-                ca[idx, :] = np.reshape(a, scenario_shape)
-            elif isinstance(node, Parameter):
-                a = node.get_all_values()
-                ca[idx, :] = np.reshape(a, scenario_shape)
-            else:
-                raise ValueError("Unrecognised Node type '{}' for TablesRecorder".format(type(node)))
-
-        if self._routes_flow_array is not None:
-            routes_shape = [len(self.model.solver.routes), ] + scenario_shape
-            self._routes_flow_array[idx, ...] = np.reshape(self.model.solver.routes_flows_array.T, routes_shape)
-
-    def finish(self):
-        if self._time_table is not None:
-            self._time_table.flush()
-        self.h5store = None
-        self._arrays = {}
-        self._routes_flow_array = None
-
-    @staticmethod
-    def generate_dataframes(h5file, time='/time', scenarios='/scenarios'):
-        """Helper function to generate pandas dataframes from `TablesRecorder` data.
-
-        Parameters
-        h5file : str
-            A path to a H5 file created by `TablesRecorder`.
-        time : str
-            The internal table that contains the time information (default "/time")
-        scenarios : str
-            The internal table that contains the scenario information (default "/scenarios")
-        """
-        store = H5Store(h5file, mode='r')
-
-        # Get the time information
-        if time:
-            time_table = store.file.get_node(time)
-            index = pandas.to_datetime({k: time_table.col(k) for k in ('year', 'month', 'day')})
-        else:
-            index = None
-
-        # Get the scenario information
-        if scenarios:
-            scenarios_table = store.file.get_node(scenarios)
-            scenarios = pandas.DataFrame({k: scenarios_table.col(k) for k in ('name', 'size')})
-            columns = pandas.MultiIndex.from_product(
-                [range(row['size']) for _, row in scenarios.iterrows()],
-                names=[row['name'].decode() for _, row in scenarios.iterrows()]
-            )
-        else:
-            columns = None
-
-        for node in store.file.walk_nodes('/', 'CArray'):
-            data = node.read()
-            data = data.reshape((data.shape[0], -1))
-            df = pandas.DataFrame(data, index=index, columns=columns)
-            yield node._v_name, df
-
-TablesRecorder.register()
+import sys
+import pandas
+import numpy as np
+from functools import wraps
+from pywr._core import AbstractNode, AbstractStorage
+from ._recorders import *
+from ._thresholds import *
+from ._hydropower import *
+from .events import *
+from .calibration import *
+from pywr.h5tools import H5Store
+from ..parameter_property import parameter_property
+import warnings
+
+class ParameterNameWarning(UserWarning):
+    pass
+
+def assert_rec(model, parameter, name=None, get_index=False):
+    """Decorator for creating AssertionRecorder objects
+
+    Example
+    -------
+    @assert_rec(model, parameter)
+    def expected_func(timestep, scenario_index):
+        return timestep.dayofyear * 2.0
+    """
+    def assert_rec_(f):
+        rec = AssertionRecorder(model, parameter, expected_func=f, name=name, get_index=get_index)
+        return f
+    return assert_rec_
+
+class AssertionRecorder(Recorder):
+    """A recorder that asserts the value of a parameter for testing purposes"""
+    def __init__(self, model, parameter, expected_data=None, expected_func=None, get_index=False, **kwargs):
+        """
+        Parameters
+        ----------
+        model : pywr.model.Model
+        parameter : pywr.parameters.Parameter
+        expected_data : np.ndarray[timestep, scenario] (optional)
+        expected_func : function
+        get_index : bool
+
+        See also
+        --------
+        pywr.recorders.assert_rec
+        """
+        super(AssertionRecorder, self).__init__(model, **kwargs)
+        self._parameter = None
+        self.parameter = parameter
+        self.expected_data = expected_data
+        self.expected_func = expected_func
+        self.get_index = get_index
+
+    parameter = parameter_property("_parameter")
+
+    def setup(self):
+        super(AssertionRecorder, self).setup()
+        self.count = 0
+
+    def after(self):
+        timestep = self.model.timestep
+        self.count += 1
+        for scenario_index in self.model.scenarios.combinations:
+            if self.expected_func:
+                expected_value = self.expected_func(timestep, scenario_index)
+            elif self.expected_data is not None:
+                expected_value = self.expected_data[timestep.index, scenario_index.global_id]
+            if self.get_index:
+                value = self._parameter.get_index(scenario_index)
+            else:
+                value = self._parameter.get_value(scenario_index)
+            try:
+                np.testing.assert_allclose(value, expected_value)
+            except AssertionError:
+                raise AssertionError("Expected {}, got {} from \"{}\" [timestep={}, scenario={}]".format(expected_value, value, self._parameter.name, timestep.index, scenario_index.global_id))
+
+    def finish(self):
+        super(AssertionRecorder, self).finish()
+        if sys.exc_info():
+            # exception was raised before we had a chance! (e.g. ModelStructureError)
+            pass
+        elif self.count == 0:
+            # this still requires model.run() to have been called...
+            raise RuntimeError("AssertionRecorder was never called!")
+
+
+class CSVRecorder(Recorder):
+    """
+    A Recorder that saves Node values to a CSV file.
+
+    This class uses the csv package from the Python standard library
+
+    Parameters
+    ----------
+
+    model : `pywr.model.Model`
+        The model to record nodes from.
+    csvfile : str
+        The path to the CSV file.
+    scenario_index : int
+        The scenario index of the model to save.
+    nodes : iterable (default=None)
+        An iterable of nodes to save data. It defaults to None which is all nodes in the model
+    kwargs : Additional keyword arguments to pass to the `csv.writer` object
+
+    """
+    def __init__(self, model, csvfile, scenario_index=0, nodes=None, complib=None, complevel=9, **kwargs):
+        super(CSVRecorder, self).__init__(model, **kwargs)
+        self.csvfile = csvfile
+        self.scenario_index = scenario_index
+        self.nodes = nodes
+        self.csv_kwargs = kwargs.pop('csv_kwargs', {})
+        self._node_names = None
+        self._fh = None
+        self._writer = None
+        self.complib = complib
+        self.complevel = complevel
+
+    @classmethod
+    def load(cls, model, data):
+        import os
+        url = data.pop("url")
+        if not os.path.isabs(url) and model.path is not None:
+            url = os.path.join(model.path, url)
+        return cls(model, url, **data)
+
+    def setup(self):
+        """
+        Setup the CSV file recorder.
+        """
+
+        if self.nodes is None:
+            self._node_names = sorted(self.model.nodes.keys())
+        else:
+            node_names = []
+            for node_ in self.nodes:
+                # test if the node name is provided
+                if isinstance(node_, str):
+                    # lookup node by name
+                    node_names.append(node_)
+                else:
+                    node_names.append((node_.name))
+            self._node_names = node_names
+
+    def reset(self):
+        import csv
+        kwargs = {"newline": "", "encoding": "utf-8"}
+        mode = "wt"
+
+        if self.complib == "gzip":
+            import gzip
+            self._fh = gzip.open(self.csvfile, mode, self.complevel, **kwargs)
+        elif self.complib in ("bz2", "bzip2"):
+            import bz2
+            self._fh = bz2.open(self.csvfile, mode, self.complevel, **kwargs)
+        elif self.complib is None:
+            self._fh = open(self.csvfile, mode, **kwargs)
+        else:
+            raise KeyError("Unexpected compression library: {}".format(self.complib))
+        self._writer = csv.writer(self._fh, **self.csv_kwargs)
+        # Write header data
+        row = ["Datetime"] + [name for name in self._node_names]
+        self._writer.writerow(row)
+
+    def after(self):
+        """
+        Write the node values to the CSV file
+        """
+        values = [self.model.timestepper.current.datetime.isoformat()]
+        for node_name in self._node_names:
+            node = self.model.nodes[node_name]
+            if isinstance(node, AbstractStorage):
+                values.append(node.volume[self.scenario_index])
+            elif isinstance(node, AbstractNode):
+                values.append(node.flow[self.scenario_index])
+            else:
+                raise ValueError("Unrecognised Node type '{}' for CSV writer".format(type(node)))
+
+        self._writer.writerow(values)
+
+    def finish(self):
+        if self._fh:
+            self._fh.close()
+CSVRecorder.register()
+
+
+class TablesRecorder(Recorder):
+    """
+    A recorder that saves to PyTables CArray
+
+    This Recorder creates a CArray for every node passed to the constructor.
+    Each CArray stores the data for all scenarios on the specific node. This
+    is useful for analysis of Node statistics across multiple scenarios.
+    """
+    def __init__(self, model, h5file, nodes=None, parameters=None, where='/', time='/time',
+                 routes_flows=None, routes='/routes', scenarios='/scenarios', **kwargs):
+        """
+
+        Parameters
+        ----------
+        model : `pywr.model.Model`
+            The model to record nodes from.
+        h5file : tables.File or filename
+            The tables file handle or filename to attach the CArray objects to. If a
+            filename is given the object will open and close the file handles.
+        nodes : iterable or None
+            Nodes to save in the tables database. Can be an iterable of Node objects or
+            node names. It can also be a iterable of tuples with a node specific where
+            keyword as the first item and a Node object or name as the second item. If
+            an iterable of tuples is provided then the node specific where keyword is
+            used in preference to the where keyword (see below).
+        parameters : iterable or None
+            Parameters to save. Similar to the nodes keyword, except refers to Parameter
+            objects or names thereof.
+        where : string
+            Default path to create the CArrays inside the database.
+        time : string
+            Default full node path to save a time tables.Table. If None no table is created.
+        scenarios : string
+            Default full node path to save a scenarios tables.Table. If None no table is created.
+        routes_flows : string
+            Relative (to `where`) node path to save the routes flow CArray. If None (default) no array is created.
+        routes : string
+            Full node path to save the routes tables.Table. If None not table is created.
+        filter_kwds : dict
+            Filter keywords to pass to tables.open_file when opening a file.
+        mode : string
+            Model argument to pass to tables.open_file. Defaults to 'w'
+        metadata : dict
+            Dict of user defined attributes to save on the root node (`root._v_attrs`)
+        create_directories : bool
+            If a file path is given and create_directories is True then attempt to make the intermediate
+            directories. This uses os.makedirs() underneath.
+        """
+        self.filter_kwds = kwargs.pop('filter_kwds', {})
+        self.mode = kwargs.pop('mode', 'w')
+        self.metadata = kwargs.pop('metadata', {})
+        self.create_directories = kwargs.pop('create_directories', False)
+
+        title = kwargs.pop('title', None)
+        if title is None:
+            try:
+                title = model.metadata['title']
+            except KeyError:
+                title = ''
+        self.title = title
+        super(TablesRecorder, self).__init__(model, **kwargs)
+
+        self.h5file = h5file
+        self.h5store = None
+        self._arrays = {}
+        self.nodes = nodes
+        self.where = where
+        self.time = time
+        self.scenarios = scenarios
+        self.routes = routes
+        self.routes_flows = routes_flows
+
+        # Enable saving routes in the solver.
+        if routes_flows:
+            self.model.solver.save_routes_flows = True
+
+        self.parameters = []
+        if parameters:
+            for parameter in parameters:
+                self._add_parameter(parameter)
+
+        self._arrays = None
+        self._time_table = None
+        self._routes_flow_array = None
+
+    def _add_parameter(self, parameter):
+        try:
+            where, param = parameter
+        except (TypeError, ValueError):
+            where = None
+            param = parameter
+        if isinstance(param, str):
+            from ..parameters import load_parameter
+            param = load_parameter(self.model, param)
+        if not param.name:
+            raise ValueError("Can only record named Parameter objects")
+        if where is None:
+            name = param.name.replace("/", "_")
+            if name != param.name:
+                warnings.warn(
+                    "Recorded parameter has \"/\" in name, replaced with \"_\" to avoid creation of subgroup: {}".format(param.name),
+                    ParameterNameWarning
+                )
+            where = self.where + "/" + name
+        where = where.replace("//", "/")
+        self.children.add(param)
+        self.parameters.append((where, param))
+
+    def _remove_parameter(self, parameter):
+        if isinstance(parameter, str):
+            parameter = self.model.parameters[parameter]
+        index = None
+        for n, (where, param) in enumerate(self.parameters):
+            if param is parameter:
+                index = n
+        if index is None:
+            raise KeyError("Parameter is not in TablesRecorder: {}".format(parameter))
+        self.parameters.pop(index)
+        self.children.remove(parameter)
+
+    @classmethod
+    def load(cls, model, data):
+        import os
+        url = data.pop("url")
+        if not os.path.isabs(url) and model.path is not None:
+            url = os.path.join(model.path, url)
+        return cls(model, url, **data)
+
+    def setup(self):
+        """
+        Setup the tables
+        """
+        from pywr.parameters import IndexParameter
+        import tables
+
+        # The first dimension is the number of timesteps.
+        # The following dimensions are sized per scenario
+        scenario_shape = list(self.model.scenarios.shape)
+        shape = [len(self.model.timestepper)] + scenario_shape
+
+        self.h5store = H5Store(self.h5file, self.filter_kwds, self.mode, title=self.title, metadata=self.metadata,
+                               create_directories=self.create_directories)
+
+        # Create a CArray for each node
+        self._arrays = {}
+
+        # Default to all nodes if None given.
+        if self.nodes is None:
+            nodes = [((self.where + "/" + n.name).replace("//", "/"), n) for n in self.model.nodes.values()]
+        else:
+            nodes = []
+            for n in self.nodes:
+
+                try:
+                    where, node = n
+                except (TypeError, ValueError):
+                    node = n
+                    where = self.where + "/" + node
+
+                # Accept a str, and lookup node by name instead.
+                if isinstance(node, str):
+                    node = self.model.nodes[node]
+                # Otherwise assume it is a node object anyway
+
+                where = where.replace("//", "/")
+                nodes.append((where, node))
+
+        if self.parameters is not None:
+            nodes.extend(self.parameters)
+
+        self._nodes = nodes
+
+        for where, node in self._nodes:
+            if isinstance(node, IndexParameter):
+                atom = tables.Int32Atom()
+            else:
+                atom = tables.Float64Atom()
+            group_name, node_name = where.rsplit("/", 1)
+            if group_name == "":
+                group_name = "/"
+            self.h5store.file.create_carray(group_name, node_name, atom, shape, createparents=True)
+
+        # Create scenario tables
+        if self.scenarios is not None:
+            group_name, node_name = self.scenarios.rsplit('/', 1)
+            if group_name == "":
+                group_name = "/"
+            description = {
+                # TODO make string length configurable
+                'name': tables.StringCol(1024),
+                'size': tables.Int64Col()
+            }
+            tbl = self.h5store.file.create_table(group_name, node_name, description=description, createparents=True)
+            # Now add the scenarios
+            entry = tbl.row
+            for scenario in self.model.scenarios.scenarios:
+                entry['name'] = scenario.name.encode('utf-8')
+                entry['size'] = scenario.size
+                entry.append()
+            tbl.flush()
+
+            if self.model.scenarios.user_combinations is not None:
+                description = {s.name: tables.Int64Col() for s in self.model.scenarios.scenarios}
+                tbl = self.h5store.file.create_table(group_name, 'scenario_combinations', description=description)
+                entry = tbl.row
+                for comb in self.model.scenarios.user_combinations:
+                    for s, i in zip(self.model.scenarios.scenarios, comb):
+                        entry[s.name] = i
+                    entry.append()
+                tbl.flush()
+
+        self.h5store = None
+
+    def reset(self):
+        import tables
+
+        mode = "r+"  # always need to append, as file already created in setup
+        self.h5store = H5Store(self.h5file, self.filter_kwds, mode)
+        self._arrays = {}
+        for where, node in self._nodes:
+            self._arrays[node] = self.h5store.file.get_node(where)
+
+        self._time_table = None
+        # Create time table
+        # This is created in reset so that the table is always recreated
+        if self.time is not None:
+            group_name, node_name = self.time.rsplit('/', 1)
+            if group_name == "":
+                group_name = "/"
+            description = {c: tables.Int64Col() for c in ('year', 'month', 'day', 'index')}
+
+            try:
+                self.h5store.file.remove_node(group_name, node_name)
+            except tables.NoSuchNodeError:
+                pass
+            finally:
+                self._time_table = self.h5store.file.create_table(group_name, node_name,
+                                                                  description=description,
+                                                                  createparents=True)
+
+        self._routes_flow_array = None
+        if self.routes_flows is not None:
+            # Create a CArray for the flows
+            # The first dimension is the number of timesteps.
+            # The second dimension is the number of routes
+            # The following dimensions are sized per scenario
+            scenario_shape = list(self.model.scenarios.shape)
+            shape = [len(self.model.timestepper), len(self.model.solver.routes)] + scenario_shape
+            atom = tables.Float64Atom()
+
+            try:
+                self.h5store.file.remove_node(self.where, self.routes_flows)
+            except tables.NoSuchNodeError:
+                pass
+            finally:
+                self._routes_flow_array = self.h5store.file.create_carray(self.where, self.routes_flows, atom, shape, createparents=True)
+
+            # Create routes table. This must be done in reset
+            if self.routes is not None:
+                group_name, node_name = self.routes.rsplit('/', 1)
+                if group_name == "":
+                    group_name = "/"
+
+                description = {
+                    # TODO make string length configurable
+                    'start': tables.StringCol(1024),
+                    'end': tables.StringCol(1024),
+                }
+                try:
+                    self.h5store.file.remove_node(group_name, node_name)
+                except tables.NoSuchNodeError:
+                    pass
+                finally:
+                    tbl = self.h5store.file.create_table(group_name, node_name, description=description, createparents=True)
+
+                entry = tbl.row
+                for route in self.model.solver.routes:
+                    node_first = route[0]
+                    node_last = route[-1]
+
+                    if node_first.parent is not None:
+                        node_first = node_first.parent
+                    if node_last.parent is not None:
+                        node_last = node_last.parent
+
+                    entry['start'] = node_first.name.encode('utf-8')
+                    entry['end'] = node_last.name.encode('utf-8')
+                    entry.append()
+
+                tbl.flush()
+
+    def after(self):
+        """
+        Save data to the tables
+        """
+        from pywr._core import AbstractNode, AbstractStorage
+        from pywr.parameters import Parameter, IndexParameter
+        scenario_shape = list(self.model.scenarios.shape)
+        ts = self.model.timestepper.current
+        idx = ts.index
+
+        if self._time_table is not None:
+            entry = self._time_table.row
+            entry['year'] = ts.year
+            entry['month'] = ts.month
+            entry['day'] = ts.day
+            entry['index'] = idx
+            entry.append()
+
+        for node, ca in self._arrays.items():
+            if isinstance(node, AbstractStorage):
+                ca[idx, :] = np.reshape(node.volume, scenario_shape)
+            elif isinstance(node, AbstractNode):
+                ca[idx, :] = np.reshape(node.flow, scenario_shape)
+            elif isinstance(node, IndexParameter):
+                a = node.get_all_indices()
+                ca[idx, :] = np.reshape(a, scenario_shape)
+            elif isinstance(node, Parameter):
+                a = node.get_all_values()
+                ca[idx, :] = np.reshape(a, scenario_shape)
+            else:
+                raise ValueError("Unrecognised Node type '{}' for TablesRecorder".format(type(node)))
+
+        if self._routes_flow_array is not None:
+            routes_shape = [len(self.model.solver.routes), ] + scenario_shape
+            self._routes_flow_array[idx, ...] = np.reshape(self.model.solver.routes_flows_array.T, routes_shape)
+
+    def finish(self):
+        if self._time_table is not None:
+            self._time_table.flush()
+        self.h5store = None
+        self._arrays = {}
+        self._routes_flow_array = None
+
+    @staticmethod
+    def generate_dataframes(h5file, time='/time', scenarios='/scenarios'):
+        """Helper function to generate pandas dataframes from `TablesRecorder` data.
+
+        Parameters
+        h5file : str
+            A path to a H5 file created by `TablesRecorder`.
+        time : str
+            The internal table that contains the time information (default "/time")
+        scenarios : str
+            The internal table that contains the scenario information (default "/scenarios")
+        """
+        store = H5Store(h5file, mode='r')
+
+        # Get the time information
+        if time:
+            time_table = store.file.get_node(time)
+            index = pandas.to_datetime({k: time_table.col(k) for k in ('year', 'month', 'day')})
+        else:
+            index = None
+
+        # Get the scenario information
+        if scenarios:
+            scenarios_table = store.file.get_node(scenarios)
+            scenarios = pandas.DataFrame({k: scenarios_table.col(k) for k in ('name', 'size')})
+            columns = pandas.MultiIndex.from_product(
+                [range(row['size']) for _, row in scenarios.iterrows()],
+                names=[row['name'].decode() for _, row in scenarios.iterrows()]
+            )
+        else:
+            columns = None
+
+        for node in store.file.walk_nodes('/', 'CArray'):
+            data = node.read()
+            data = data.reshape((data.shape[0], -1))
+            df = pandas.DataFrame(data, index=index, columns=columns)
+            yield node._v_name, df
+
+TablesRecorder.register()
```

### Comparing `pywr-1.8.0/pywr/solvers/__init__.py` & `pywr-1.9.0/pywr/solvers/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,199 +1,217 @@
-"""
-This module contains a Solver baseclass and several implemented subclasses.
-
-Solvers are used to with pywr.core.Model classes to solve the network
-allocation problem every time step.
-
-Currently there are only linear programme based solvers using,
-    - GLPK
-    - LPSolve55
-"""
-
-solver_registry = []
-
-
-class Solver(object):
-    """Solver base class from which all solvers should inherit"""
-    name = 'default'
-
-    def __init__(self, *args, **kwargs):
-        pass
-
-    def setup(self, model):
-        raise NotImplementedError('Solver should be subclassed to provide setup()')
-
-    def solve(self, model):
-        raise NotImplementedError('Solver should be subclassed to provide solve()')
-
-    def reset(self):
-        raise NotImplementedError('Solver should be subclassed to provide reset()')
-
-    @property
-    def stats(self):
-        return {}
-
-
-# Attempt to import solvers. These will only be successful if they are built correctly.
-try:
-    from .cython_glpk import CythonGLPKSolver as cy_CythonGLPKSolver
-except ImportError:
-    pass
-else:
-    class CythonGLPKSolver(Solver):
-        """Python wrapper of Cython GLPK solver.
-
-        This is required to subclass Solver and get the metaclass magic.
-        """
-        name = 'glpk'
-
-        def __init__(self, *args, **kwargs):
-            super(CythonGLPKSolver, self).__init__(*args, **kwargs)
-            self._cy_solver = cy_CythonGLPKSolver(**kwargs)
-
-        def setup(self, model):
-            return self._cy_solver.setup(model)
-
-        def solve(self, model):
-            return self._cy_solver.solve(model)
-
-        def reset(self):
-            return self._cy_solver.reset()
-
-        def dump_mps(self, filename):
-            return self._cy_solver.dump_mps(filename)
-
-        def dump_lp(self, filename):
-            return self._cy_solver.dump_lp(filename)
-
-        def dump_glpk(self, filename):
-            return self._cy_solver.dump_glpk(filename)
-
-        def retry_solve():
-            def fget(self):
-                return self._cy_solver.retry_solve
-
-            def fset(self, value):
-                self._cy_solver.retry_solve = value
-
-            return locals()
-        retry_solve = property(**retry_solve())
-
-        def save_routes_flows():
-            def fget(self):
-                return self._cy_solver.save_routes_flows
-
-            def fset(self, value):
-                self._cy_solver.save_routes_flows = value
-
-            return locals()
-        save_routes_flows = property(**save_routes_flows())
-
-        @property
-        def routes(self):
-            return self._cy_solver.routes
-
-        @property
-        def routes_flows_array(self):
-            return self._cy_solver.route_flows_arr
-
-        @property
-        def stats(self):
-            return self._cy_solver.stats
-    solver_registry.append(CythonGLPKSolver)
-
-
-try:
-    from .cython_glpk import CythonGLPKEdgeSolver as cy_CythonGLPKEdgeSolver
-except ImportError:
-    pass
-else:
-    class CythonGLPKEdgeSolver(Solver):
-        """Python wrapper of Cython GLPK solver.
-
-        This is required to subclass Solver and get the metaclass magic.
-        """
-        name = 'glpk-edge'
-
-        def __init__(self, *args, **kwargs):
-            super(CythonGLPKEdgeSolver, self).__init__(*args, **kwargs)
-            self._cy_solver = cy_CythonGLPKEdgeSolver(**kwargs)
-
-        def setup(self, model):
-            return self._cy_solver.setup(model)
-
-        def solve(self, model):
-            return self._cy_solver.solve(model)
-
-        def reset(self):
-            return self._cy_solver.reset()
-
-        def dump_mps(self, filename):
-            return self._cy_solver.dump_mps(filename)
-
-        def dump_lp(self, filename):
-            return self._cy_solver.dump_lp(filename)
-
-        def dump_glpk(self, filename):
-            return self._cy_solver.dump_glpk(filename)
-
-        def retry_solve():
-            def fget(self):
-                return self._cy_solver.retry_solve
-
-            def fset(self, value):
-                self._cy_solver.retry_solve = value
-
-            return locals()
-        retry_solve = property(**retry_solve())
-
-        @property
-        def stats(self):
-            return self._cy_solver.stats
-    solver_registry.append(CythonGLPKEdgeSolver)
-
-
-try:
-    from .cython_lpsolve import CythonLPSolveSolver as cy_CythonLPSolveSolver
-except ImportError:
-    pass
-else:
-    class CythonLPSolveSolver(Solver):
-        """Python wrapper of Cython LPSolve55 solver.
-
-        This is required to subclass Solver and get the metaclass magic.
-        """
-        name = 'lpsolve'
-
-        def __init__(self, *args, **kwargs):
-            super(CythonLPSolveSolver, self).__init__(*args, **kwargs)
-            self._cy_solver = cy_CythonLPSolveSolver(**kwargs)
-
-        def setup(self, model):
-            return self._cy_solver.setup(model)
-
-        def solve(self, model):
-            return self._cy_solver.solve(model)
-
-        def reset(self):
-            pass
-
-        @property
-        def save_routes_flows(self):
-            return self._cy_solver.save_routes_flows
-
-        @save_routes_flows.setter
-        def save_routes_flows(self, value):
-            self._cy_solver.save_routes_flows = value
-
-        @property
-        def routes(self):
-            return self._cy_solver.routes
-
-        @property
-        def routes_flows_array(self):
-            return self._cy_solver.route_flows_arr
-
-        @property
-        def stats(self):
-            return self._cy_solver.stats
-    solver_registry.append(CythonLPSolveSolver)
+"""
+This module contains a Solver baseclass and several implemented subclasses.
+
+Solvers are used to with pywr.core.Model classes to solve the network
+allocation problem every time step.
+
+Currently there are only linear programme based solvers using,
+    - GLPK
+    - LPSolve55
+"""
+
+solver_registry = []
+
+
+class Solver(object):
+    """Solver base class from which all solvers should inherit"""
+    name = 'default'
+
+    def __init__(self, *args, **kwargs):
+        pass
+
+    def setup(self, model):
+        raise NotImplementedError('Solver should be subclassed to provide setup()')
+
+    def solve(self, model):
+        raise NotImplementedError('Solver should be subclassed to provide solve()')
+
+    def reset(self):
+        raise NotImplementedError('Solver should be subclassed to provide reset()')
+
+    @property
+    def stats(self):
+        return {}
+
+
+# Attempt to import solvers. These will only be successful if they are built correctly.
+try:
+    from .cython_glpk import CythonGLPKSolver as cy_CythonGLPKSolver
+except ImportError:
+    pass
+else:
+    class CythonGLPKSolver(Solver):
+        """Python wrapper of Cython GLPK solver.
+
+        This is required to subclass Solver and get the metaclass magic.
+        """
+        name = 'glpk'
+
+        def __init__(self, *args, **kwargs):
+            super(CythonGLPKSolver, self).__init__(*args, **kwargs)
+            self._cy_solver = cy_CythonGLPKSolver(**kwargs)
+
+        def setup(self, model):
+            return self._cy_solver.setup(model)
+
+        def solve(self, model):
+            return self._cy_solver.solve(model)
+
+        def reset(self):
+            return self._cy_solver.reset()
+
+        def dump_mps(self, filename):
+            return self._cy_solver.dump_mps(filename)
+
+        def dump_lp(self, filename):
+            return self._cy_solver.dump_lp(filename)
+
+        def dump_glpk(self, filename):
+            return self._cy_solver.dump_glpk(filename)
+
+        def retry_solve():
+            def fget(self):
+                return self._cy_solver.retry_solve
+
+            def fset(self, value):
+                self._cy_solver.retry_solve = value
+
+            return locals()
+        retry_solve = property(**retry_solve())
+
+        def save_routes_flows():
+            def fget(self):
+                return self._cy_solver.save_routes_flows
+
+            def fset(self, value):
+                self._cy_solver.save_routes_flows = value
+
+            return locals()
+        save_routes_flows = property(**save_routes_flows())
+
+        @property
+        def routes(self):
+            return self._cy_solver.routes
+
+        @property
+        def routes_flows_array(self):
+            return self._cy_solver.route_flows_arr
+
+        @property
+        def stats(self):
+            return self._cy_solver.stats
+    solver_registry.append(CythonGLPKSolver)
+
+
+try:
+    from .cython_glpk import CythonGLPKEdgeSolver as cy_CythonGLPKEdgeSolver
+except ImportError:
+    pass
+else:
+    class CythonGLPKEdgeSolver(Solver):
+        """Python wrapper of Cython GLPK solver.
+
+        This is required to subclass Solver and get the metaclass magic.
+        """
+        name = 'glpk-edge'
+
+        def __init__(self, *args, **kwargs):
+            super(CythonGLPKEdgeSolver, self).__init__(*args, **kwargs)
+            self._cy_solver = cy_CythonGLPKEdgeSolver(**kwargs)
+
+        def setup(self, model):
+            return self._cy_solver.setup(model)
+
+        def solve(self, model):
+            return self._cy_solver.solve(model)
+
+        def reset(self):
+            return self._cy_solver.reset()
+
+        def dump_mps(self, filename):
+            return self._cy_solver.dump_mps(filename)
+
+        def dump_lp(self, filename):
+            return self._cy_solver.dump_lp(filename)
+
+        def dump_glpk(self, filename):
+            return self._cy_solver.dump_glpk(filename)
+
+        def retry_solve():
+            def fget(self):
+                return self._cy_solver.retry_solve
+
+            def fset(self, value):
+                self._cy_solver.retry_solve = value
+
+            return locals()
+        retry_solve = property(**retry_solve())
+
+        @property
+        def stats(self):
+            return self._cy_solver.stats
+    solver_registry.append(CythonGLPKEdgeSolver)
+
+
+try:
+    from .cython_lpsolve import CythonLPSolveSolver as cy_CythonLPSolveSolver
+except ImportError:
+    pass
+else:
+    class CythonLPSolveSolver(Solver):
+        """Python wrapper of Cython LPSolve55 solver.
+
+        This is required to subclass Solver and get the metaclass magic.
+        """
+        name = 'lpsolve'
+
+        def __init__(self, *args, **kwargs):
+            super(CythonLPSolveSolver, self).__init__(*args, **kwargs)
+            self._cy_solver = cy_CythonLPSolveSolver(**kwargs)
+
+        def setup(self, model):
+            return self._cy_solver.setup(model)
+
+        def solve(self, model):
+            return self._cy_solver.solve(model)
+
+        def reset(self):
+            pass
+
+        @property
+        def save_routes_flows(self):
+            return self._cy_solver.save_routes_flows
+
+        @save_routes_flows.setter
+        def save_routes_flows(self, value):
+            self._cy_solver.save_routes_flows = value
+
+        @property
+        def routes(self):
+            return self._cy_solver.routes
+
+        @property
+        def routes_flows_array(self):
+            return self._cy_solver.route_flows_arr
+
+        @property
+        def stats(self):
+            return self._cy_solver.stats
+    solver_registry.append(CythonLPSolveSolver)
+
+
+class NullSolver(Solver):
+    """A null solver that performs no allocation routine.
+
+    This solver does no allocation and is mainly useful for debugging purposes.
+    """
+    name = 'null'
+
+    def setup(self, model):
+        pass
+
+    def solve(self, model):
+        pass
+
+    def reset(self):
+        pass
+solver_registry.append(NullSolver)
```

### Comparing `pywr-1.8.0/pywr/solvers/cython_glpk.pyx` & `pywr-1.9.0/pywr/solvers/cython_glpk.pyx`

 * *Files 26% similar despite different names*

```diff
@@ -1,1405 +1,1480 @@
-from libc.stdlib cimport malloc, free
-from libc.math cimport abs
-from libc.float cimport DBL_MAX
-from cython.view cimport array as cvarray
-import numpy as np
-cimport numpy as np
-cimport cython
-
-from pywr._core import BaseInput, BaseOutput, BaseLink
-from pywr._core cimport *
-from pywr.core import ModelStructureError
-import time
-
-from .libglpk cimport *
-
-# Constants and message strings
-# =============================
-status_string = [
-    None,
-    'solution is undefined',
-    'solution is feasible',
-    'solution is infeasible',
-    'no feasible solution exists',
-    'solution is optimal',
-    'solution is unbounded',
-]
-
-simplex_status_string = [
-    None,
-    'invalid basis',
-    'singular matrix',
-    'ill-conditioned matrix',
-    'invalid bounds',
-    'solver failed',
-    'objective lower limit reached',
-    'objective upper limit reached',
-    'iteration limit exceeded',
-    'time limit exceeded',
-    'no primal feasible solution',
-    'no dual feasible solution',
-    'root LP optimum not provided',
-    'search terminated by application',
-    'relative mip gap tolerance reached',
-    'no primal/dual feasible solution',
-    'no convergence',
-    'numerical instability',
-    'invalid data',
-    'result out of range',
-]
-
-message_levels = {
-    'off': GLP_MSG_OFF,
-    'error': GLP_MSG_ERR,
-    'normal': GLP_MSG_ON,
-    'all': GLP_MSG_ALL,
-    'debug': GLP_MSG_DBG,
-}
-
-# Inline helper functions
-# =======================
-cdef inline int constraint_type(double a, double b):
-    if a == b:
-        return GLP_FX
-    elif b == DBL_MAX:
-        if a == -DBL_MAX:
-            return GLP_FR
-        else:
-            return GLP_LO
-    elif a == -DBL_MAX:
-        return GLP_UP
-    else:
-        return GLP_DB
-
-
-cdef double inf = float('inf')
-
-
-cdef inline double dbl_max_to_inf(double a):
-    if a == DBL_MAX:
-        return inf
-    elif a == -DBL_MAX:
-        return -inf
-    return a
-
-cdef inline double inf_to_dbl_max(double a):
-    if a == inf:
-        return DBL_MAX
-    elif a == -inf:
-        return -DBL_MAX
-    return a
-
-
-cdef class AbstractNodeData:
-    """Helper class for caching node data for the solver."""
-    cdef public int id
-    cdef public bint is_link
-    cdef public list in_edges, out_edges
-
-
-cdef class GLPKSolver:
-    cdef glp_prob* prob
-    cdef glp_smcp smcp
-
-    cdef public bint use_presolve
-    cdef bint has_presolved
-
-    def __cinit__(self):
-        self.prob = glp_create_prob()
-
-    def __dealloc__(self):
-        glp_delete_prob(self.prob)
-
-    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level="error"):
-        self.use_presolve = use_presolve
-        self.set_solver_options(time_limit, iteration_limit, message_level)
-        glp_term_hook(term_hook, NULL)
-        self.has_presolved = False
-
-    def set_solver_options(self, time_limit, iteration_limit, message_level):
-        glp_init_smcp(&self.smcp)
-        self.smcp.msg_lev = message_levels[message_level]
-        if time_limit is not None:
-            self.smcp.tm_lim = time_limit
-        if iteration_limit is not None:
-            self.smcp.it_lim = iteration_limit
-
-    def setup(self, model):
-        self.has_presolved = False
-
-    def reset(self):
-        pass
-
-    cpdef object solve(self, model):
-        if self.use_presolve and not self.has_presolved:
-            self.smcp.presolve = GLP_ON
-            self.has_presolved = True
-        else:
-            self.smcp.presolve = GLP_OFF
-
-    cpdef dump_mps(self, filename):
-        glp_write_mps(self.prob, GLP_MPS_FILE, NULL, filename)
-
-    cpdef dump_lp(self, filename):
-        glp_write_lp(self.prob, NULL, filename)
-
-    cpdef dump_glpk(self, filename):
-        glp_write_prob(self.prob, 0, filename)
-
-
-cdef class BasisManager:
-    cdef int[:, :] row_stat
-    cdef int[:, :] col_stat
-
-    cdef init_basis(self, glp_prob* prob, int nscenarios):
-        """Initialise the arrays used for storing the LP basis by scenario"""
-        cdef int nrows = glp_get_num_rows(prob)
-        cdef int ncols = glp_get_num_cols(prob)
-        self.row_stat = np.empty((nscenarios, nrows), dtype=np.int32)
-        self.col_stat = np.empty((nscenarios, ncols), dtype=np.int32)
-
-    cdef save_basis(self, glp_prob* prob, int global_id):
-        """Save the current basis for scenario associated with global_id"""
-        cdef int i
-        cdef int nrows = glp_get_num_rows(prob)
-        cdef int ncols = glp_get_num_cols(prob)
-        for i in range(nrows):
-            self.row_stat[global_id, i] = glp_get_row_stat(prob, i + 1)
-        for i in range(ncols):
-            self.col_stat[global_id, i] = glp_get_col_stat(prob, i + 1)
-
-    cdef set_basis(self, glp_prob* prob, bint is_first_solve, int global_id):
-        """Set the current basis for scenario associated with global_id"""
-        cdef int i, nrows, ncols
-
-        if is_first_solve:
-            # First time solving we use the default advanced basis
-            glp_adv_basis(prob, 0)
-        else:
-            # Otherwise we restore basis from previous solve of this scenario
-            nrows = glp_get_num_rows(prob)
-            ncols = glp_get_num_cols(prob)
-            for i in range(nrows):
-                glp_set_row_stat(prob, i + 1, self.row_stat[global_id, i])
-            for i in range(ncols):
-                glp_set_col_stat(prob, i + 1, self.col_stat[global_id, i])
-
-
-cdef int term_hook(void *info, const char *s):
-    """ Callback function to print GLPK messages through Python's print function """
-    # TODO make this use logging.
-    message = s.strip().decode('UTF-8')
-    if message.startswith("Constructing initial basis"):
-        pass
-    elif message.startswith("Size of triangular part is"):
-        pass
-    else:
-        print(message)
-    return 1
-
-
-cdef int simplex(glp_prob *P, glp_smcp parm):
-    return glp_simplex(P, &parm)
-
-
-cdef set_obj_coef(glp_prob *P, int j, double coef):
-    IF SOLVER_DEBUG:
-        assert np.isfinite(coef)
-        if abs(coef) < 1e-9:
-            if abs(coef) != 0.0:
-                print(j, coef)
-                assert False
-    glp_set_obj_coef(P, j, coef)
-
-
-cdef set_row_bnds(glp_prob *P, int i, int type, double lb, double ub):
-    IF SOLVER_DEBUG:
-        assert np.isfinite(lb)
-        assert np.isfinite(ub)
-        assert lb <= ub
-        if abs(lb) < 1e-9:
-            if abs(lb) != 0.0:
-                print(i, type, lb, ub)
-                assert False
-        if abs(ub) < 1e-9:
-            if abs(ub) != 0.0:
-                print(i, type, lb, ub)
-                assert False
-    glp_set_row_bnds(P, i, type, lb, ub)
-
-
-cdef set_col_bnds(glp_prob *P, int i, int type, double lb, double ub):
-    IF SOLVER_DEBUG:
-        assert np.isfinite(lb)
-        assert np.isfinite(ub)
-        assert lb <= ub
-    glp_set_col_bnds(P, i, type, lb, ub)
-
-
-cdef set_mat_row(glp_prob *P, int i, int len, int* ind, double* val):
-    IF SOLVER_DEBUG:
-        cdef int j
-        for j in range(len):
-            assert np.isfinite(val[j+1])
-            assert abs(val[j+1]) > 1e-6
-            assert ind[j+1] > 0
-    glp_set_mat_row(P, i, len, ind, val)
-
-
-cdef class CythonGLPKSolver(GLPKSolver):
-    cdef int idx_col_routes
-    cdef int idx_row_non_storages
-    cdef int idx_row_cross_domain
-    cdef int idx_row_storages
-    cdef int idx_row_virtual_storages
-    cdef int idx_row_aggregated
-    cdef int idx_row_aggregated_min_max
-
-    cdef public list routes
-    cdef list non_storages
-    cdef list storages
-    cdef list virtual_storages
-    cdef list aggregated
-
-    cdef int[:] routes_cost
-    cdef int[:] routes_cost_indptr
-
-    cdef list all_nodes
-    cdef int num_nodes
-    cdef int num_routes
-    cdef int num_storages
-    cdef int num_scenarios
-    cdef cvarray node_costs_arr
-    cdef cvarray node_flows_arr
-    cdef public cvarray route_flows_arr
-    cdef public object stats
-
-    # Internal representation of the basis for each scenario
-    cdef BasisManager basis_manager
-    cdef bint is_first_solve
-    cdef public bint save_routes_flows
-    cdef public bint retry_solve
-
-    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level='error',
-                 save_routes_flows=False, retry_solve=False):
-        super().__init__(use_presolve, time_limit, iteration_limit, message_level)
-        self.stats = None
-        self.is_first_solve = True
-        self.save_routes_flows = save_routes_flows
-        self.retry_solve = retry_solve
-        self.basis_manager = BasisManager()
-
-    def setup(self, model):
-        super().setup(model)
-        cdef Node input
-        cdef Node output
-        cdef AbstractNode some_node
-        cdef AbstractNode _node
-        cdef AggregatedNode agg_node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double avail_volume
-        cdef int col, row
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status
-        cdef cross_domain_row
-        cdef int n, num
-
-        self.all_nodes = list(sorted(model.graph.nodes(), key=lambda n: n.fully_qualified_name))
-        if not self.all_nodes:
-            raise ModelStructureError("Model is empty")
-
-        for n, _node in enumerate(self.all_nodes):
-            _node.__data = AbstractNodeData()
-            _node.__data.id = n
-            if isinstance(_node, BaseLink):
-                _node.__data.is_link = True
-
-        self.num_nodes = len(self.all_nodes)
-
-        self.node_costs_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
-        self.node_flows_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
-
-        routes = model.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
-        # Find cross-domain routes
-        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
-
-        non_storages = []
-        storages = []
-        virtual_storages = []
-        aggregated_with_factors = []
-        aggregated = []
-
-        for some_node in self.all_nodes:
-            if isinstance(some_node, (BaseInput, BaseLink, BaseOutput)):
-                non_storages.append(some_node)
-            elif isinstance(some_node, VirtualStorage):
-                virtual_storages.append(some_node)
-            elif isinstance(some_node, Storage):
-                storages.append(some_node)
-            elif isinstance(some_node, AggregatedNode):
-                if some_node.factors is not None:
-                    aggregated_with_factors.append(some_node)
-                aggregated.append(some_node)
-
-        if len(routes) == 0:
-            raise ModelStructureError("Model has no valid routes")
-        if len(non_storages) == 0:
-            raise ModelStructureError("Model has no non-storage nodes")
-
-        self.num_routes = len(routes)
-        self.num_scenarios = len(model.scenarios.combinations)
-
-        if self.save_routes_flows:
-            # If saving flows this array needs to be 2D (one for each scenario)
-            self.route_flows_arr = cvarray(shape=(self.num_scenarios, self.num_routes),
-                                           itemsize=sizeof(double), format="d")
-        else:
-            # Otherwise the array can just be used to store a single solve to save some memory
-            self.route_flows_arr = cvarray(shape=(self.num_routes, ), itemsize=sizeof(double), format="d")
-
-        # clear the previous problem
-        glp_erase_prob(self.prob)
-        glp_set_obj_dir(self.prob, GLP_MIN)
-        # add a column for each route
-        self.idx_col_routes = glp_add_cols(self.prob, <int>(len(routes)))
-
-        # create a lookup for the cross-domain routes.
-        cross_domain_cols = {}
-        for cross_domain_route in cross_domain_routes:
-            # These routes are only 2 nodes. From output to input
-            output, input = cross_domain_route
-            # note that the conversion factor is not time varying
-            conv_factor = input.get_conversion_factor()
-            input_cols = [(n, conv_factor) for n, route in enumerate(routes) if route[0] is input]
-            # create easy lookup for the route columns this output might
-            # provide cross-domain connection to
-            if output in cross_domain_cols:
-                cross_domain_cols[output].extend(input_cols)
-            else:
-                cross_domain_cols[output] = input_cols
-
-        # explicitly set bounds on route and demand columns
-        for col, route in enumerate(routes):
-            set_col_bnds(self.prob, self.idx_col_routes+col, GLP_LO, 0.0, DBL_MAX)
-
-        # constrain supply minimum and maximum flow
-        self.idx_row_non_storages = glp_add_rows(self.prob, len(non_storages))
-        # Add rows for the cross-domain routes.
-        if len(cross_domain_cols) > 0:
-            self.idx_row_cross_domain = glp_add_rows(self.prob, len(cross_domain_cols))
-
-        cross_domain_row = 0
-        for row, some_node in enumerate(non_storages):
-            # Differentiate betwen the node type.
-            # Input & Output only apply their flow constraints when they
-            # are the first and last node on the route respectively.
-            if isinstance(some_node, BaseInput):
-                cols = [n for n, route in enumerate(routes) if route[0] is some_node]
-            elif isinstance(some_node, BaseOutput):
-                cols = [n for n, route in enumerate(routes) if route[-1] is some_node]
-            else:
-                # Other nodes apply their flow constraints to all routes passing through them
-                cols = [n for n, route in enumerate(routes) if some_node in route]
-            ind = <int*>malloc((1+len(cols)) * sizeof(int))
-            val = <double*>malloc((1+len(cols)) * sizeof(double))
-            for n, c in enumerate(cols):
-                ind[1+n] = 1+c
-                val[1+n] = 1
-            set_mat_row(self.prob, self.idx_row_non_storages+row, len(cols), ind, val)
-            set_row_bnds(self.prob, self.idx_row_non_storages+row, GLP_FX, 0.0, 0.0)
-            # glp_set_row_name(self.prob, self.idx_row_non_storages+row,
-            #                  b'ns.'+some_node.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-            # Add constraint for cross-domain routes
-            # i.e. those from a demand to a supply
-            if some_node in cross_domain_cols:
-                col_vals = cross_domain_cols[some_node]
-                ind = <int*>malloc((1+len(col_vals)+len(cols)) * sizeof(int))
-                val = <double*>malloc((1+len(col_vals)+len(cols)) * sizeof(double))
-                for n, c in enumerate(cols):
-                    ind[1+n] = 1+c
-                    val[1+n] = -1
-                for n, (c, v) in enumerate(col_vals):
-                    ind[1+n+len(cols)] = 1+c
-                    val[1+n+len(cols)] = 1./v
-                set_mat_row(self.prob, self.idx_row_cross_domain+cross_domain_row, len(col_vals)+len(cols), ind, val)
-                set_row_bnds(self.prob, self.idx_row_cross_domain+cross_domain_row, GLP_FX, 0.0, 0.0)
-                # glp_set_row_name(self.prob, self.idx_row_cross_domain+cross_domain_row,
-                #                  b'cd.'+some_node.fully_qualified_name.encode('utf-8'))
-                free(ind)
-                free(val)
-                cross_domain_row += 1
-
-        # storage
-        if len(storages):
-            self.idx_row_storages = glp_add_rows(self.prob, len(storages))
-        for row, storage in enumerate(storages):
-            cols_output = [n for n, route in enumerate(routes)
-                           if route[-1] in storage.outputs and route[0] not in storage.inputs]
-            cols_input = [n for n, route in enumerate(routes)
-                          if route[0] in storage.inputs and route[-1] not in storage.outputs]
-            ind = <int*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(int))
-            val = <double*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(double))
-            for n, c in enumerate(cols_output):
-                ind[1+n] = self.idx_col_routes+c
-                val[1+n] = 1
-            for n, c in enumerate(cols_input):
-                ind[1+len(cols_output)+n] = self.idx_col_routes+c
-                val[1+len(cols_output)+n] = -1
-            set_mat_row(self.prob, self.idx_row_storages+row, len(cols_output)+len(cols_input), ind, val)
-            # glp_set_row_name(self.prob, self.idx_row_storages+row,
-            #                  b's.'+storage.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        # virtual storage
-        if len(virtual_storages):
-            self.idx_row_virtual_storages = glp_add_rows(self.prob, len(virtual_storages))
-        for row, storage in enumerate(virtual_storages):
-            # We need to handle the same route appearing twice here.
-            cols = {}
-            for n, route in enumerate(routes):
-                for some_node in route:
-                    try:
-                        i = storage.nodes.index(some_node)
-                    except ValueError:
-                        pass
-                    else:
-                        try:
-                            cols[n] += storage.factors[i]
-                        except KeyError:
-                            cols[n] = storage.factors[i]
-
-            ind = <int*>malloc((1+len(cols)) * sizeof(int))
-            val = <double*>malloc((1+len(cols)) * sizeof(double))
-            for n, (c, f) in enumerate(cols.items()):
-                ind[1+n] = self.idx_col_routes+c
-                val[1+n] = -f
-
-            set_mat_row(self.prob, self.idx_row_virtual_storages+row, len(cols), ind, val)
-            # glp_set_row_name(self.prob, self.idx_row_virtual_storages+row,
-            #                  b'vs.'+storage.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        # aggregated node flow ratio constraints
-        if len(aggregated_with_factors):
-            self.idx_row_aggregated = self.idx_row_virtual_storages + len(virtual_storages)
-        for agg_node in aggregated_with_factors:
-            nodes = agg_node.nodes
-            factors = agg_node.factors
-            assert(len(nodes) == len(factors))
-
-            row = glp_add_rows(self.prob, len(agg_node.nodes)-1)
-
-            cols = []
-            for node in nodes:
-                cols.append([n for n, route in enumerate(routes) if node in route])
-
-            # normalise factors
-            f0 = factors[0]
-            factors_norm = [f0/f for f in factors]
-
-            # update matrix
-            for n in range(len(nodes)-1):
-                length = len(cols[0])+len(cols[n+1])
-                ind = <int*>malloc(1+length * sizeof(int))
-                val = <double*>malloc(1+length * sizeof(double))
-                for i, c in enumerate(cols[0]):
-                    ind[1+i] = 1+c
-                    val[1+i] = 1.0
-                for i, c in enumerate(cols[n+1]):
-                    ind[1+len(cols[0])+i] = 1+c
-                    val[1+len(cols[0])+i] = -factors_norm[n+1]
-                set_mat_row(self.prob, row+n, length, ind, val)
-                free(ind)
-                free(val)
-
-                set_row_bnds(self.prob, row+n, GLP_FX, 0.0, 0.0)
-                # glp_set_row_name(self.prob, row+n,
-                #                  'ag.f{}.{}'.format(n, agg_node.fully_qualified_name).encode('utf-8'))
-
-        # aggregated node min/max flow constraints
-        if aggregated:
-            self.idx_row_aggregated_min_max = glp_add_rows(self.prob, len(aggregated))
-        for row, agg_node in enumerate(aggregated):
-            row = self.idx_row_aggregated_min_max + row
-            nodes = agg_node.nodes
-
-            weights = agg_node.flow_weights
-            if weights is None:
-                weights = [1.0]*len(nodes)
-
-            matrix = {}
-            for some_node, w in zip(nodes, weights):
-                for n, route in enumerate(routes):
-                    if some_node in route:
-                        matrix[n] = w
-            length = len(matrix)
-            ind = <int*>malloc(1+length * sizeof(int))
-            val = <double*>malloc(1+length * sizeof(double))
-            for i, col in enumerate(sorted(matrix)):
-                ind[1+i] = 1+col
-                val[1+i] = matrix[col]
-            set_mat_row(self.prob, row, length, ind, val)
-            set_row_bnds(self.prob, row, GLP_FX, 0.0, 0.0)
-            # glp_set_row_name(self.prob, row, b'ag.'+agg_node.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        # update route properties
-        routes_cost = []
-        routes_cost_indptr = [0, ]
-        for col, route in enumerate(routes):
-            route_cost = []
-            route_cost.append(route[0].__data.id)
-            for some_node in route[1:-1]:
-                if isinstance(some_node, BaseLink):
-                    route_cost.append(some_node.__data.id)
-            route_cost.append(route[-1].__data.id)
-            routes_cost.extend(route_cost)
-            routes_cost_indptr.append(len(routes_cost))
-
-        assert(len(routes_cost_indptr) == len(routes) + 1)
-
-        self.routes_cost_indptr = np.array(routes_cost_indptr, dtype=np.int32)
-        self.routes_cost = np.array(routes_cost, dtype=np.int32)
-
-        self.routes = routes
-        self.non_storages = non_storages
-        self.storages = storages
-        self.virtual_storages = virtual_storages
-        self.aggregated = aggregated
-
-        self.basis_manager.init_basis(self.prob, len(model.scenarios.combinations))
-        self.is_first_solve = True
-
-        # reset stats
-        self.stats = {
-            'total': 0.0,
-            'lp_solve': 0.0,
-            'result_update': 0.0,
-            'bounds_update_nonstorage': 0.0,
-            'bounds_update_storage': 0.0,
-            'bounds_update_nonstorage': 0.0,
-            'bounds_update_storage': 0.0,
-            'objective_update': 0.0,
-            'number_of_rows': glp_get_num_rows(self.prob),
-            'number_of_cols': glp_get_num_cols(self.prob),
-            'number_of_nonzero': glp_get_num_nz(self.prob),
-            'number_of_routes': len(routes),
-            'number_of_nodes': len(self.all_nodes)
-        }
-
-    def reset(self):
-        # Resetting this triggers a crashing of a new basis in each scenario
-        self.is_first_solve = True
-
-    cpdef object solve(self, model):
-        GLPKSolver.solve(self, model)
-        t0 = time.perf_counter()
-        cdef int[:] scenario_combination
-        cdef int scenario_id
-        cdef ScenarioIndex scenario_index
-        for scenario_index in model.scenarios.combinations:
-            self._solve_scenario(model, scenario_index)
-        self.stats['total'] += time.perf_counter() - t0
-        # After solving this is always false
-        self.is_first_solve = False
-
-    @cython.boundscheck(False)
-    @cython.initializedcheck(False)
-    @cython.cdivision(True)
-    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
-        cdef Node node
-        cdef Storage storage
-        cdef AbstractNode _node
-        cdef AbstractNodeData data
-        cdef AggregatedNode agg_node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double max_volume
-        cdef double min_volume
-        cdef double avail_volume
-        cdef double t0
-        cdef int col, row
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status, simplex_ret
-        cdef cross_domain_col
-        cdef list route
-        cdef int node_id, indptr, nroutes
-        cdef double flow
-        cdef int n, m
-        cdef Py_ssize_t length
-
-        timestep = model.timestep
-        cdef list routes = self.routes
-        nroutes = len(routes)
-        cdef list non_storages = self.non_storages
-        cdef list storages = self.storages
-        cdef list virtual_storages = self.virtual_storages
-        cdef list aggregated = self.aggregated
-
-        # update route cost
-
-        t0 = time.perf_counter()
-
-        # update the cost of each node in the model
-        cdef double[:] node_costs = self.node_costs_arr
-        for _node in self.all_nodes:
-            data = _node.__data
-            node_costs[data.id] = _node.get_cost(scenario_index)
-
-        # calculate the total cost of each route
-        for col in range(nroutes):
-            cost = 0.0
-            for indptr in range(self.routes_cost_indptr[col], self.routes_cost_indptr[col+1]):
-                node_id = self.routes_cost[indptr]
-                cost += node_costs[node_id]
-
-            if abs(cost) < 1e-8:
-                cost = 0.0
-            set_obj_coef(self.prob, self.idx_col_routes+col, cost)
-
-        self.stats['objective_update'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update non-storage properties
-        for row, node in enumerate(non_storages):
-            min_flow = inf_to_dbl_max(node.get_min_flow(scenario_index))
-            if abs(min_flow) < 1e-8:
-                min_flow = 0.0
-            max_flow = inf_to_dbl_max(node.get_max_flow(scenario_index))
-            if abs(max_flow) < 1e-8:
-                max_flow = 0.0
-            set_row_bnds(self.prob, self.idx_row_non_storages+row, constraint_type(min_flow, max_flow),
-                         min_flow, max_flow)
-
-        for row, agg_node in enumerate(aggregated):
-            min_flow = inf_to_dbl_max(agg_node.get_min_flow(scenario_index))
-            if abs(min_flow) < 1e-8:
-                min_flow = 0.0
-            max_flow = inf_to_dbl_max(agg_node.get_max_flow(scenario_index))
-            if abs(max_flow) < 1e-8:
-                max_flow = 0.0
-            set_row_bnds(self.prob, self.idx_row_aggregated_min_max + row, constraint_type(min_flow, max_flow),
-                         min_flow, max_flow)
-
-        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update storage node constraint
-        for row, storage in enumerate(storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            min_volume = storage.get_min_volume(scenario_index)
-
-            if max_volume == min_volume:
-                set_row_bnds(self.prob, self.idx_row_storages+row, GLP_FX, 0.0, 0.0)
-            else:
-                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
-                # change in storage cannot be more than the current volume or
-                # result in maximum volume being exceeded
-                lb = -avail_volume/timestep.days
-                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
-
-                if abs(lb) < 1e-8:
-                    lb = 0.0
-                if abs(ub) < 1e-8:
-                    ub = 0.0
-                set_row_bnds(self.prob, self.idx_row_storages+row, constraint_type(lb, ub), lb, ub)
-
-        # update virtual storage node constraint
-        for row, storage in enumerate(virtual_storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            min_volume = storage.get_min_volume(scenario_index)
-
-            if max_volume == min_volume:
-                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FX, 0.0, 0.0)
-            else:
-                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
-                # change in storage cannot be more than the current volume or
-                # result in maximum volume being exceeded
-                lb = -avail_volume/timestep.days
-                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
-
-                if abs(lb) < 1e-8:
-                    lb = 0.0
-                if abs(ub) < 1e-8:
-                    ub = 0.0
-                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, constraint_type(lb, ub), lb, ub)
-
-        self.stats['bounds_update_storage'] += time.perf_counter() - t0
-
-        t0 = time.perf_counter()
-
-        # Set the basis for this scenario
-        self.basis_manager.set_basis(self.prob, self.is_first_solve, scenario_index.global_id)
-        # attempt to solve the linear programme
-        simplex_ret = simplex(self.prob, self.smcp)
-        status = glp_get_status(self.prob)
-        if (status != GLP_OPT or simplex_ret != 0) and self.retry_solve:
-            # try creating a new basis and resolving
-            print('Retrying solve with new basis.')
-            glp_std_basis(self.prob)
-            simplex_ret = simplex(self.prob, self.smcp)
-            status = glp_get_status(self.prob)
-
-        if status != GLP_OPT or simplex_ret != 0:
-            # If problem is not solved. Print some debugging information and error.
-            print("Simplex solve returned: {} ({})".format(simplex_status_string[simplex_ret], simplex_ret))
-            print("Simplex status: {} ({})".format(status_string[status], status))
-            print("Scenario ID: {}".format(scenario_index.global_id))
-            print("Timestep index: {}".format(timestep.index))
-            self.dump_mps(b'pywr_glpk_debug.mps')
-            self.dump_lp(b'pywr_glpk_debug.lp')
-
-            self.smcp.msg_lev = GLP_MSG_DBG
-            # Retry solve with debug messages
-            simplex_ret = simplex(self.prob, self.smcp)
-            status = glp_get_status(self.prob)
-            raise RuntimeError('Simplex solver failed with message: "{}", status: "{}".'.format(
-                simplex_status_string[simplex_ret], status_string[status]))
-        # Now save the basis
-        self.basis_manager.save_basis(self.prob, scenario_index.global_id)
-
-        self.stats['lp_solve'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        cdef double[:] route_flows
-        if self.save_routes_flows:
-            route_flows = self.route_flows_arr[scenario_index.global_id, :]
-        else:
-            route_flows = self.route_flows_arr
-
-        for col in range(0, self.num_routes):
-            route_flows[col] = glp_get_col_prim(self.prob, col+1)
-
-        # collect the total flow via each node
-        cdef double[:] node_flows = self.node_flows_arr
-        node_flows[:] = 0.0
-        for n, route in enumerate(routes):
-            flow = route_flows[n]
-            if flow == 0:
-                continue
-            length = len(route)
-            for m, _node in enumerate(route):
-                data = _node.__data
-                if (m == 0) or (m == length-1) or data.is_link:
-                    node_flows[data.id] += flow
-
-        # commit the total flows
-        for n in range(0, self.num_nodes):
-            _node = self.all_nodes[n]
-            _node.commit(scenario_index.global_id, node_flows[n])
-
-        self.stats['result_update'] += time.perf_counter() - t0
-
-
-cdef class CythonGLPKEdgeSolver(GLPKSolver):
-    cdef int idx_col_edges
-    cdef int idx_row_non_storages
-    cdef int idx_row_link_mass_bal
-    cdef int idx_row_cross_domain
-    cdef int idx_row_storages
-    cdef int idx_row_virtual_storages
-    cdef int idx_row_aggregated
-    cdef int idx_row_aggregated_min_max
-
-    cdef list non_storages
-    cdef list storages
-    cdef list virtual_storages
-    cdef list aggregated
-
-    cdef list all_nodes
-    cdef list all_edges
-
-    cdef int num_nodes
-    cdef int num_edges
-    cdef int num_storages
-    cdef int num_scenarios
-    cdef cvarray edge_cost_arr
-    cdef cvarray edge_flows_arr
-    cdef cvarray node_flows_arr
-    cdef public cvarray route_flows_arr
-    cdef public object stats
-
-    # Internal representation of the basis for each scenario
-    cdef BasisManager basis_manager
-    cdef bint is_first_solve
-    cdef public bint save_routes_flows
-    cdef public bint retry_solve
-
-    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level='error',
-                 save_routes_flows=False, retry_solve=False):
-        super().__init__(use_presolve, time_limit, iteration_limit, message_level)
-        self.stats = None
-        self.is_first_solve = True
-        self.save_routes_flows = save_routes_flows
-        self.retry_solve = retry_solve
-        self.basis_manager = BasisManager()
-
-    def setup(self, model):
-        super().setup(model)
-
-        cdef Node input
-        cdef Node output
-        cdef AbstractNode some_node
-        cdef AbstractNode _node
-        cdef AggregatedNode agg_node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double avail_volume
-        cdef int col, row
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status
-        cdef cross_domain_row
-        cdef int n, num
-
-        self.all_nodes = list(sorted(model.graph.nodes(), key=lambda n: n.fully_qualified_name))
-        self.all_edges = edges = list(model.graph.edges())
-        if not self.all_nodes or not self.all_edges:
-            raise ModelStructureError("Model is empty")
-
-        for n, _node in enumerate(self.all_nodes):
-            _node.__data = AbstractNodeData()
-            _node.__data.id = n
-            _node.__data.in_edges = []
-            _node.__data.out_edges = []
-            if isinstance(_node, BaseLink):
-                _node.__data.is_link = True
-
-        self.num_nodes = len(self.all_nodes)
-
-        self.edge_cost_arr = cvarray(shape=(len(self.all_edges),), itemsize=sizeof(double), format="d")
-        self.edge_flows_arr = cvarray(shape=(len(self.all_edges),), itemsize=sizeof(double), format="d")
-        self.node_flows_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
-
-        # Find cross-domain routes
-        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
-
-        link_nodes = []
-        non_storages = []
-        storages = []
-        virtual_storages = []
-        aggregated_with_factors = []
-        aggregated = []
-
-        for some_node in self.all_nodes:
-            if isinstance(some_node, (BaseInput, BaseLink, BaseOutput)):
-                non_storages.append(some_node)
-                if isinstance(some_node, BaseLink):
-                    link_nodes.append(some_node)
-            elif isinstance(some_node, VirtualStorage):
-                virtual_storages.append(some_node)
-            elif isinstance(some_node, Storage):
-                storages.append(some_node)
-            elif isinstance(some_node, AggregatedNode):
-                if some_node.factors is not None:
-                    aggregated_with_factors.append(some_node)
-                aggregated.append(some_node)
-
-        if len(non_storages) == 0:
-            raise ModelStructureError("Model has no non-storage nodes")
-
-        self.num_edges = len(edges)
-        self.num_scenarios = len(model.scenarios.combinations)
-        self.num_storages = len(storages)
-
-        # clear the previous problem
-        glp_erase_prob(self.prob)
-        glp_set_obj_dir(self.prob, GLP_MIN)
-        # add a column for each edge
-        self.idx_col_edges = glp_add_cols(self.prob, self.num_edges)
-
-        # create a lookup for edges associated with each node (ignoring cross domain edges)
-        for row, (start_node, end_node) in enumerate(self.all_edges):
-            if start_node.domain != end_node.domain:
-                continue
-            start_node.__data.out_edges.append(row)
-            end_node.__data.in_edges.append(row)
-
-        # create a lookup for the cross-domain routes.
-        cross_domain_cols = {}
-        for cross_domain_route in cross_domain_routes:
-            # These routes are only 2 nodes. From output to input
-            output, input = cross_domain_route
-            # note that the conversion factor is not time varying
-            conv_factor = input.get_conversion_factor()
-            input_cols = [(n, conv_factor) for n in input.__data.out_edges]
-            # create easy lookup for the route columns this output might
-            # provide cross-domain connection to
-            if output in cross_domain_cols:
-                cross_domain_cols[output].extend(input_cols)
-            else:
-                cross_domain_cols[output] = input_cols
-
-        # explicitly set bounds on route and demand columns
-        for row, edge in enumerate(edges):
-            set_col_bnds(self.prob, self.idx_col_edges+row, GLP_LO, 0.0, DBL_MAX)
-
-        # Apply nodal flow constraints
-        self.idx_row_non_storages = glp_add_rows(self.prob, len(non_storages))
-        # # Add rows for the cross-domain routes.
-        if len(cross_domain_cols) > 0:
-            self.idx_row_cross_domain = glp_add_rows(self.prob, len(cross_domain_cols))
-
-        cross_domain_row = 0
-        for row, some_node in enumerate(non_storages):
-            # Differentiate betwen the node type.
-            # Input and other nodes use the outgoing edge flows to apply the flow constraint on
-            # This requires the mass balance constraints to ensure the inflow and outflow are equal
-            # The Output nodes, in contrast, apply the constraint to the incoming flow (because there is no out going flow)
-            if isinstance(some_node, BaseInput):
-                cols = some_node.__data.out_edges
-                if len(some_node.__data.in_edges) != 0:
-                    raise ModelStructureError(f'Input node "{some_node.name}" should not have any upstream '
-                                              f'connections.')
-            elif isinstance(some_node, BaseOutput):
-                cols = some_node.__data.in_edges
-                if len(some_node.__data.out_edges) != 0:
-                    raise ModelStructureError(f'Output node "{some_node.name}" should not have any downstream '
-                                              f'connections.')
-            else:
-                # Other nodes apply their flow constraints to all routes passing through them
-                cols = some_node.__data.out_edges
-
-            ind = <int*>malloc((1+len(cols)) * sizeof(int))
-            val = <double*>malloc((1+len(cols)) * sizeof(double))
-            for n, c in enumerate(cols):
-                ind[1+n] = 1+c
-                val[1+n] = 1
-            set_mat_row(self.prob, self.idx_row_non_storages+row, len(cols), ind, val)
-            set_row_bnds(self.prob, self.idx_row_non_storages+row, GLP_FX, 0.0, 0.0)
-
-            free(ind)
-            free(val)
-
-            # Add constraint for cross-domain routes
-            # i.e. those from a demand to a supply
-            if some_node in cross_domain_cols:
-                col_vals = cross_domain_cols[some_node]
-                ind = <int*>malloc((1+len(col_vals)+len(cols)) * sizeof(int))
-                val = <double*>malloc((1+len(col_vals)+len(cols)) * sizeof(double))
-                for n, c in enumerate(cols):
-                    ind[1+n] = 1+c
-                    val[1+n] = -1
-                for n, (c, v) in enumerate(col_vals):
-                    ind[1+n+len(cols)] = 1+c
-                    val[1+n+len(cols)] = 1./v
-                set_mat_row(self.prob, self.idx_row_cross_domain+cross_domain_row, len(col_vals)+len(cols), ind, val)
-                set_row_bnds(self.prob, self.idx_row_cross_domain+cross_domain_row, GLP_FX, 0.0, 0.0)
-                # glp_set_row_name(self.prob, self.idx_row_cross_domain+cross_domain_row,
-                #                  b'cd.'+some_node.fully_qualified_name.encode('utf-8'))
-                free(ind)
-                free(val)
-                cross_domain_row += 1
-
-        # Add mass balance constraints
-        if len(link_nodes) > 0:
-            self.idx_row_link_mass_bal = glp_add_rows(self.prob, len(link_nodes))
-        for row, some_node in enumerate(link_nodes):
-
-            in_cols = some_node.__data.in_edges
-            out_cols = some_node.__data.out_edges
-            ind = <int*>malloc((1+len(in_cols)+len(out_cols)) * sizeof(int))
-            val = <double*>malloc((1+len(in_cols)+len(out_cols)) * sizeof(double))
-            for n, c in enumerate(in_cols):
-                ind[1+n] = 1+c
-                val[1+n] = 1
-            for n, c in enumerate(out_cols):
-                ind[1+len(in_cols)+n] = 1+c
-                val[1+len(in_cols)+n] = -1
-            set_mat_row(self.prob, self.idx_row_link_mass_bal+row, len(in_cols)+len(out_cols), ind, val)
-            set_row_bnds(self.prob, self.idx_row_link_mass_bal+row, GLP_FX, 0.0, 0.0)
-
-            free(ind)
-            free(val)
-
-        # storage
-        if len(storages):
-            self.idx_row_storages = glp_add_rows(self.prob, len(storages))
-        for row, storage in enumerate(storages):
-
-            cols_output = []
-            for output in storage.outputs:
-                cols_output.extend(output.__data.in_edges)
-            cols_input = []
-            for input in storage.inputs:
-                cols_input.extend(input.__data.out_edges)
-
-            ind = <int*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(int))
-            val = <double*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(double))
-            for n, c in enumerate(cols_output):
-                ind[1+n] = self.idx_col_edges+c
-                val[1+n] = 1
-            for n, c in enumerate(cols_input):
-                ind[1+len(cols_output)+n] = self.idx_col_edges+c
-                val[1+len(cols_output)+n] = -1
-
-            set_mat_row(self.prob, self.idx_row_storages+row, len(cols_output)+len(cols_input), ind, val)
-            # glp_set_row_name(self.prob, self.idx_row_storages+row,
-            #                  b's.'+storage.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        # virtual storage
-        if len(virtual_storages):
-            self.idx_row_virtual_storages = glp_add_rows(self.prob, len(virtual_storages))
-        for row, storage in enumerate(virtual_storages):
-            # We need to handle the same route appearing twice here.
-            cols = {}
-
-            for i, some_node in enumerate(storage.nodes):
-                if isinstance(some_node, BaseOutput):
-                    node_cols = some_node.__data.in_edges
-                else:
-                    node_cols = some_node.__data.out_edges
-
-                for n in node_cols:
-                    try:
-                        cols[n] += storage.factors[i]
-                    except KeyError:
-                        cols[n] = storage.factors[i]
-
-            ind = <int*>malloc((1+len(cols)) * sizeof(int))
-            val = <double*>malloc((1+len(cols)) * sizeof(double))
-            for n, (c, f) in enumerate(cols.items()):
-                ind[1+n] = self.idx_col_edges+c
-                val[1+n] = -f
-
-            set_mat_row(self.prob, self.idx_row_virtual_storages+row, len(cols), ind, val)
-            # glp_set_row_name(self.prob, self.idx_row_virtual_storages+row,
-            #                  b'vs.'+storage.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        # aggregated node flow ratio constraints
-        if len(aggregated_with_factors):
-            self.idx_row_aggregated = self.idx_row_virtual_storages + len(virtual_storages)
-        for agg_node in aggregated_with_factors:
-            nodes = agg_node.nodes
-            factors = agg_node.factors
-            assert(len(nodes) == len(factors))
-
-            row = glp_add_rows(self.prob, len(agg_node.nodes)-1)
-
-            cols = []
-            for node in nodes:
-                if isinstance(node, BaseOutput):
-                    cols.append([c for c in node.__data.in_edges])
-                else:
-                    cols.append([c for c in node.__data.out_edges])
-
-            # normalise factors
-            f0 = factors[0]
-            factors_norm = [f0/f for f in factors]
-
-            # update matrix
-            for n in range(len(nodes)-1):
-                length = len(cols[0])+len(cols[n+1])
-                ind = <int*>malloc(1+length * sizeof(int))
-                val = <double*>malloc(1+length * sizeof(double))
-                for i, c in enumerate(cols[0]):
-                    ind[1+i] = 1+c
-                    val[1+i] = 1.0
-                for i, c in enumerate(cols[n+1]):
-                    ind[1+len(cols[0])+i] = 1+c
-                    val[1+len(cols[0])+i] = -factors_norm[n+1]
-                set_mat_row(self.prob, row+n, length, ind, val)
-                free(ind)
-                free(val)
-
-                set_row_bnds(self.prob, row+n, GLP_FX, 0.0, 0.0)
-                # glp_set_row_name(self.prob, row+n,
-                #                  'ag.f{}.{}'.format(n, agg_node.fully_qualified_name).encode('utf-8'))
-
-        # aggregated node min/max flow constraints
-        if aggregated:
-            self.idx_row_aggregated_min_max = glp_add_rows(self.prob, len(aggregated))
-        for row, agg_node in enumerate(aggregated):
-            row = self.idx_row_aggregated_min_max + row
-            nodes = agg_node.nodes
-
-            weights = agg_node.flow_weights
-            if weights is None:
-                weights = [1.0]*len(nodes)
-
-            matrix = {}
-            for some_node, w in zip(nodes, weights):
-                if isinstance(some_node, BaseOutput):
-                    node_cols = some_node.__data.in_edges
-                else:
-                    node_cols = some_node.__data.out_edges
-
-                for n in node_cols:
-                    matrix[n] = w
-
-            length = len(matrix)
-            ind = <int*>malloc(1+length * sizeof(int))
-            val = <double*>malloc(1+length * sizeof(double))
-            for i, col in enumerate(sorted(matrix)):
-                ind[1+i] = 1+col
-                val[1+i] = matrix[col]
-            set_mat_row(self.prob, row, length, ind, val)
-            set_row_bnds(self.prob, row, GLP_FX, 0.0, 0.0)
-            # glp_set_row_name(self.prob, row, b'ag.'+agg_node.fully_qualified_name.encode('utf-8'))
-            free(ind)
-            free(val)
-
-        self.non_storages = non_storages
-        self.storages = storages
-        self.virtual_storages = virtual_storages
-        self.aggregated = aggregated
-
-        self.basis_manager.init_basis(self.prob, len(model.scenarios.combinations))
-        self.is_first_solve = True
-
-        # reset stats
-        self.stats = {
-            'total': 0.0,
-            'lp_solve': 0.0,
-            'result_update': 0.0,
-            'bounds_update_nonstorage': 0.0,
-            'bounds_update_storage': 0.0,
-            'objective_update': 0.0,
-            'number_of_rows': glp_get_num_rows(self.prob),
-            'number_of_cols': glp_get_num_cols(self.prob),
-            'number_of_nonzero': glp_get_num_nz(self.prob),
-            'number_of_edges': len(self.all_edges),
-            'number_of_nodes': len(self.all_nodes)
-        }
-
-    def reset(self):
-        # Resetting this triggers a crashing of a new basis in each scenario
-        self.is_first_solve = True
-
-    cpdef object solve(self, model):
-        GLPKSolver.solve(self, model)
-        t0 = time.perf_counter()
-        cdef int[:] scenario_combination
-        cdef int scenario_id
-        cdef ScenarioIndex scenario_index
-        for scenario_index in model.scenarios.combinations:
-            self._solve_scenario(model, scenario_index)
-        self.stats['total'] += time.perf_counter() - t0
-        # After solving this is always false
-        self.is_first_solve = False
-
-    @cython.boundscheck(False)
-    @cython.initializedcheck(False)
-    @cython.cdivision(True)
-    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
-        cdef Node node
-        cdef Storage storage
-        cdef AbstractNode _node
-        cdef AbstractNodeData data
-        cdef AggregatedNode agg_node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double max_volume
-        cdef double min_volume
-        cdef double avail_volume
-        cdef double t0
-        cdef int col, row
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status, simplex_ret
-        cdef cross_domain_col
-        cdef list route
-        cdef int node_id, indptr, nedges
-        cdef double flow
-        cdef int n, m
-        cdef Py_ssize_t length
-
-        timestep = model.timestep
-        cdef list edges = self.all_edges
-        nedges = self.num_edges
-        cdef list non_storages = self.non_storages
-        cdef list storages = self.storages
-        cdef list virtual_storages = self.virtual_storages
-        cdef list aggregated = self.aggregated
-
-        # update route cost
-
-        t0 = time.perf_counter()
-
-        # Initialise the cost on each edge to zero
-        cdef double[:] edge_costs = self.edge_cost_arr
-        for col in range(nedges):
-            edge_costs[col] = 0.0
-
-        # update the cost of each node in the model
-        for _node in self.all_nodes:
-            cost = _node.get_cost(scenario_index)
-            data = _node.__data
-
-            # Link nodes have edges connected upstream & downstream. We apply
-            # half the cost assigned to the node to all the connected edges.
-            # The edge costs are then the mean of the node costs at either end.
-            if data.is_link:
-                cost /= 2
-
-            for col in data.in_edges:
-                edge_costs[col] += cost
-            for col in data.out_edges:
-                edge_costs[col] += cost
-
-        # calculate the total cost of each route
-        for col in range(nedges):
-            set_obj_coef(self.prob, self.idx_col_edges+col, edge_costs[col])
-
-        self.stats['objective_update'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update non-storage properties
-        for row, node in enumerate(non_storages):
-            min_flow = inf_to_dbl_max(node.get_min_flow(scenario_index))
-            if abs(min_flow) < 1e-8:
-                min_flow = 0.0
-            max_flow = inf_to_dbl_max(node.get_max_flow(scenario_index))
-            if abs(max_flow) < 1e-8:
-                max_flow = 0.0
-
-            set_row_bnds(self.prob, self.idx_row_non_storages+row, constraint_type(min_flow, max_flow),
-                         min_flow, max_flow)
-
-        for row, agg_node in enumerate(aggregated):
-            min_flow = inf_to_dbl_max(agg_node.get_min_flow(scenario_index))
-            if abs(min_flow) < 1e-8:
-                min_flow = 0.0
-            max_flow = inf_to_dbl_max(agg_node.get_max_flow(scenario_index))
-            if abs(max_flow) < 1e-8:
-                max_flow = 0.0
-            set_row_bnds(self.prob, self.idx_row_aggregated_min_max + row, constraint_type(min_flow, max_flow),
-                         min_flow, max_flow)
-
-        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update storage node constraint
-        for row, storage in enumerate(storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            min_volume = storage.get_min_volume(scenario_index)
-
-            if max_volume == min_volume:
-                set_row_bnds(self.prob, self.idx_row_storages+row, GLP_FX, 0.0, 0.0)
-            else:
-                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
-                # change in storage cannot be more than the current volume or
-                # result in maximum volume being exceeded
-                lb = -avail_volume/timestep.days
-                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
-
-                if abs(lb) < 1e-8:
-                    lb = 0.0
-                if abs(ub) < 1e-8:
-                    ub = 0.0
-                set_row_bnds(self.prob, self.idx_row_storages+row, constraint_type(lb, ub), lb, ub)
-
-        # update virtual storage node constraint
-        for row, storage in enumerate(virtual_storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            min_volume = storage.get_min_volume(scenario_index)
-
-            if max_volume == min_volume:
-                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FX, 0.0, 0.0)
-            else:
-                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
-                # change in storage cannot be more than the current volume or
-                # result in maximum volume being exceeded
-                lb = -avail_volume/timestep.days
-                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
-
-                if abs(lb) < 1e-8:
-                    lb = 0.0
-                if abs(ub) < 1e-8:
-                    ub = 0.0
-                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, constraint_type(lb, ub), lb, ub)
-
-        self.stats['bounds_update_storage'] += time.perf_counter() - t0
-
-        t0 = time.perf_counter()
-
-        # Set the basis for this scenario
-        self.basis_manager.set_basis(self.prob, self.is_first_solve, scenario_index.global_id)
-        # attempt to solve the linear programme
-        simplex_ret = simplex(self.prob, self.smcp)
-        status = glp_get_status(self.prob)
-        if (status != GLP_OPT or simplex_ret != 0) and self.retry_solve:
-            # try creating a new basis and resolving
-            print('Retrying solve with new basis.')
-            glp_std_basis(self.prob)
-            simplex_ret = simplex(self.prob, self.smcp)
-            status = glp_get_status(self.prob)
-
-        if status != GLP_OPT or simplex_ret != 0:
-            # If problem is not solved. Print some debugging information and error.
-            print("Simplex solve returned: {} ({})".format(simplex_status_string[simplex_ret], simplex_ret))
-            print("Simplex status: {} ({})".format(status_string[status], status))
-            print("Scenario ID: {}".format(scenario_index.global_id))
-            print("Timestep index: {}".format(timestep.index))
-            self.dump_mps(b'pywr_glpk_debug.mps')
-            self.dump_lp(b'pywr_glpk_debug.lp')
-
-            self.smcp.msg_lev = GLP_MSG_DBG
-            # Retry solve with debug messages
-            simplex_ret = simplex(self.prob, self.smcp)
-            status = glp_get_status(self.prob)
-            raise RuntimeError('Simplex solver failed with message: "{}", status: "{}".'.format(
-                simplex_status_string[simplex_ret], status_string[status]))
-        # Now save the basis
-        self.basis_manager.save_basis(self.prob, scenario_index.global_id)
-
-        self.stats['lp_solve'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        cdef double[:] edge_flows = self.edge_flows_arr
-
-        for col in range(self.num_edges):
-            edge_flows[col] = glp_get_col_prim(self.prob, col+1)
-
-        # collect the total flow via each node
-        cdef double[:] node_flows = self.node_flows_arr
-        node_flows[:] = 0.0
-        for n, edge in enumerate(edges):
-            flow = edge_flows[n]
-            if flow == 0:
-                continue
-
-            for _node in edge:
-                data = _node.__data
-                if data.is_link:
-                    # Link nodes are connected upstream & downstream so
-                    # we take half of flow from each edge.
-                    node_flows[data.id] += flow / 2
-                else:
-                    node_flows[data.id] += flow
-
-        # commit the total flows
-        for n in range(0, self.num_nodes):
-            _node = self.all_nodes[n]
-            _node.commit(scenario_index.global_id, node_flows[n])
-
-        self.stats['result_update'] += time.perf_counter() - t0
+from cpython cimport array
+import array
+from libc.stdlib cimport malloc, free
+from libc.math cimport abs
+from libc.float cimport DBL_MAX
+from cython.view cimport array as cvarray
+import numpy as np
+cimport numpy as np
+cimport cython
+
+from pywr._core import BaseInput, BaseOutput, BaseLink
+from pywr._core cimport *
+from pywr.core import ModelStructureError
+import time
+
+from .libglpk cimport *
+
+# Constants and message strings
+# =============================
+status_string = [
+    None,
+    'solution is undefined',
+    'solution is feasible',
+    'solution is infeasible',
+    'no feasible solution exists',
+    'solution is optimal',
+    'solution is unbounded',
+]
+
+simplex_status_string = [
+    None,
+    'invalid basis',
+    'singular matrix',
+    'ill-conditioned matrix',
+    'invalid bounds',
+    'solver failed',
+    'objective lower limit reached',
+    'objective upper limit reached',
+    'iteration limit exceeded',
+    'time limit exceeded',
+    'no primal feasible solution',
+    'no dual feasible solution',
+    'root LP optimum not provided',
+    'search terminated by application',
+    'relative mip gap tolerance reached',
+    'no primal/dual feasible solution',
+    'no convergence',
+    'numerical instability',
+    'invalid data',
+    'result out of range',
+]
+
+message_levels = {
+    'off': GLP_MSG_OFF,
+    'error': GLP_MSG_ERR,
+    'normal': GLP_MSG_ON,
+    'all': GLP_MSG_ALL,
+    'debug': GLP_MSG_DBG,
+}
+
+# Inline helper functions
+# =======================
+cdef inline int constraint_type(double a, double b):
+    if abs(a - b) < 1e-8:
+        return GLP_FX
+    elif b == DBL_MAX:
+        if a == -DBL_MAX:
+            return GLP_FR
+        else:
+            return GLP_LO
+    elif a == -DBL_MAX:
+        return GLP_UP
+    else:
+        return GLP_DB
+
+
+cdef double inf = float('inf')
+
+
+cdef inline double dbl_max_to_inf(double a):
+    if a == DBL_MAX:
+        return inf
+    elif a == -DBL_MAX:
+        return -inf
+    return a
+
+cdef inline double inf_to_dbl_max(double a):
+    if a == inf:
+        return DBL_MAX
+    elif a == -inf:
+        return -DBL_MAX
+    return a
+
+
+cdef class AbstractNodeData:
+    """Helper class for caching node data for the solver."""
+    cdef public int id
+    cdef public bint is_link
+    cdef public list in_edges, out_edges
+
+
+cdef class AggNodeFactorData:
+    """Helper class for caching data for Aggregated Nodes that have factors defined by parameters."""
+    cdef public int row
+    cdef public int node_ind
+    cdef public cvarray ind_ptr
+    cdef public cvarray inds
+    cdef public cvarray vals
+
+
+cdef class GLPKSolver:
+    cdef glp_prob* prob
+    cdef glp_smcp smcp
+
+    cdef public bint use_presolve
+    cdef bint has_presolved
+
+    def __cinit__(self):
+        self.prob = glp_create_prob()
+
+    def __dealloc__(self):
+        glp_delete_prob(self.prob)
+
+    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level="error"):
+        self.use_presolve = use_presolve
+        self.set_solver_options(time_limit, iteration_limit, message_level)
+        glp_term_hook(term_hook, NULL)
+        self.has_presolved = False
+
+    def set_solver_options(self, time_limit, iteration_limit, message_level):
+        glp_init_smcp(&self.smcp)
+        self.smcp.msg_lev = message_levels[message_level]
+        if time_limit is not None:
+            self.smcp.tm_lim = time_limit
+        if iteration_limit is not None:
+            self.smcp.it_lim = iteration_limit
+
+    def setup(self, model):
+        self.has_presolved = False
+
+    def reset(self):
+        pass
+
+    cpdef object solve(self, model):
+        if self.use_presolve and not self.has_presolved:
+            self.smcp.presolve = GLP_ON
+            self.has_presolved = True
+        else:
+            self.smcp.presolve = GLP_OFF
+
+    cpdef dump_mps(self, filename):
+        glp_write_mps(self.prob, GLP_MPS_FILE, NULL, filename)
+
+    cpdef dump_lp(self, filename):
+        glp_write_lp(self.prob, NULL, filename)
+
+    cpdef dump_glpk(self, filename):
+        glp_write_prob(self.prob, 0, filename)
+
+
+cdef class BasisManager:
+    cdef int[:, :] row_stat
+    cdef int[:, :] col_stat
+
+    cdef init_basis(self, glp_prob* prob, int nscenarios):
+        """Initialise the arrays used for storing the LP basis by scenario"""
+        cdef int nrows = glp_get_num_rows(prob)
+        cdef int ncols = glp_get_num_cols(prob)
+        self.row_stat = np.empty((nscenarios, nrows), dtype=np.int32)
+        self.col_stat = np.empty((nscenarios, ncols), dtype=np.int32)
+
+    cdef save_basis(self, glp_prob* prob, int global_id):
+        """Save the current basis for scenario associated with global_id"""
+        cdef int i
+        cdef int nrows = glp_get_num_rows(prob)
+        cdef int ncols = glp_get_num_cols(prob)
+        for i in range(nrows):
+            self.row_stat[global_id, i] = glp_get_row_stat(prob, i + 1)
+        for i in range(ncols):
+            self.col_stat[global_id, i] = glp_get_col_stat(prob, i + 1)
+
+    cdef set_basis(self, glp_prob* prob, bint is_first_solve, int global_id):
+        """Set the current basis for scenario associated with global_id"""
+        cdef int i, nrows, ncols
+
+        if is_first_solve:
+            # First time solving we use the default advanced basis
+            glp_adv_basis(prob, 0)
+        else:
+            # Otherwise we restore basis from previous solve of this scenario
+            nrows = glp_get_num_rows(prob)
+            ncols = glp_get_num_cols(prob)
+            for i in range(nrows):
+                glp_set_row_stat(prob, i + 1, self.row_stat[global_id, i])
+            for i in range(ncols):
+                glp_set_col_stat(prob, i + 1, self.col_stat[global_id, i])
+
+
+cdef int term_hook(void *info, const char *s):
+    """ Callback function to print GLPK messages through Python's print function """
+    # TODO make this use logging.
+    message = s.strip().decode('UTF-8')
+    if message.startswith("Constructing initial basis"):
+        pass
+    elif message.startswith("Size of triangular part is"):
+        pass
+    else:
+        print(message)
+    return 1
+
+
+cdef int simplex(glp_prob *P, glp_smcp parm):
+    return glp_simplex(P, &parm)
+
+
+cdef set_obj_coef(glp_prob *P, int j, double coef):
+    IF SOLVER_DEBUG:
+        assert np.isfinite(coef)
+        if abs(coef) < 1e-9:
+            if abs(coef) != 0.0:
+                print(j, coef)
+                assert False
+    glp_set_obj_coef(P, j, coef)
+
+
+cdef set_row_bnds(glp_prob *P, int i, int type, double lb, double ub):
+    IF SOLVER_DEBUG:
+        assert np.isfinite(lb)
+        assert np.isfinite(ub)
+        assert lb <= ub
+        if abs(lb) < 1e-9:
+            if abs(lb) != 0.0:
+                print(i, type, lb, ub)
+                assert False
+        if abs(ub) < 1e-9:
+            if abs(ub) != 0.0:
+                print(i, type, lb, ub)
+                assert False
+    glp_set_row_bnds(P, i, type, lb, ub)
+
+
+cdef set_col_bnds(glp_prob *P, int i, int type, double lb, double ub):
+    IF SOLVER_DEBUG:
+        assert np.isfinite(lb)
+        assert np.isfinite(ub)
+        assert lb <= ub
+    glp_set_col_bnds(P, i, type, lb, ub)
+
+
+cdef set_mat_row(glp_prob *P, int i, int len, int* ind, double* val):
+    IF SOLVER_DEBUG:
+        cdef int j
+        for j in range(len):
+            assert np.isfinite(val[j+1])
+            assert abs(val[j+1]) > 1e-6
+            assert ind[j+1] > 0
+    glp_set_mat_row(P, i, len, ind, val)
+
+
+cdef class CythonGLPKSolver(GLPKSolver):
+    cdef int idx_col_routes
+    cdef int idx_row_non_storages
+    cdef int idx_row_cross_domain
+    cdef int idx_row_storages
+    cdef int idx_row_virtual_storages
+    cdef int idx_row_aggregated
+    cdef int idx_row_aggregated_with_factors
+    cdef int idx_row_aggregated_min_max
+
+    cdef public list routes
+    cdef list non_storages
+    cdef list storages
+    cdef list virtual_storages
+    cdef list aggregated
+    cdef list aggregated_with_factors
+
+    cdef int[:] routes_cost
+    cdef int[:] routes_cost_indptr
+
+    cdef list all_nodes
+    cdef int num_nodes
+    cdef int num_routes
+    cdef int num_storages
+    cdef int num_scenarios
+    cdef cvarray node_costs_arr
+    cdef cvarray node_flows_arr
+    cdef public cvarray route_flows_arr
+    cdef public object stats
+
+    # Internal representation of the basis for each scenario
+    cdef BasisManager basis_manager
+    cdef bint is_first_solve
+    cdef public bint save_routes_flows
+    cdef public bint retry_solve
+
+    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level='error',
+                 save_routes_flows=False, retry_solve=False):
+        super().__init__(use_presolve, time_limit, iteration_limit, message_level)
+        self.stats = None
+        self.is_first_solve = True
+        self.save_routes_flows = save_routes_flows
+        self.retry_solve = retry_solve
+        self.basis_manager = BasisManager()
+
+    def setup(self, model):
+        super().setup(model)
+        cdef Node input
+        cdef Node output
+        cdef AbstractNode some_node
+        cdef AbstractNode _node
+        cdef AggregatedNode agg_node
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double avail_volume
+        cdef int col, row
+        cdef int* ind
+        cdef double* val
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status
+        cdef cross_domain_row
+        cdef int n, num
+
+        self.all_nodes = list(sorted(model.graph.nodes(), key=lambda n: n.fully_qualified_name))
+        if not self.all_nodes:
+            raise ModelStructureError("Model is empty")
+
+        for n, _node in enumerate(self.all_nodes):
+            _node.__data = AbstractNodeData()
+            _node.__data.id = n
+            if isinstance(_node, BaseLink):
+                _node.__data.is_link = True
+
+        self.num_nodes = len(self.all_nodes)
+
+        self.node_costs_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
+        self.node_flows_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
+
+        routes = model.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
+        # Find cross-domain routes
+        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
+
+        non_storages = []
+        storages = []
+        virtual_storages = []
+        aggregated_with_factors = []
+        aggregated = []
+
+        for some_node in self.all_nodes:
+            if isinstance(some_node, (BaseInput, BaseLink, BaseOutput)):
+                non_storages.append(some_node)
+            elif isinstance(some_node, VirtualStorage):
+                virtual_storages.append(some_node)
+            elif isinstance(some_node, Storage):
+                storages.append(some_node)
+            elif isinstance(some_node, AggregatedNode):
+                if some_node.factors is not None:
+                    aggregated_with_factors.append(some_node)
+                    some_node.__agg_factor_data = AggNodeFactorData()
+                aggregated.append(some_node)
+
+        if len(routes) == 0:
+            raise ModelStructureError("Model has no valid routes")
+        if len(non_storages) == 0:
+            raise ModelStructureError("Model has no non-storage nodes")
+
+        self.num_routes = len(routes)
+        self.num_scenarios = len(model.scenarios.combinations)
+
+        if self.save_routes_flows:
+            # If saving flows this array needs to be 2D (one for each scenario)
+            self.route_flows_arr = cvarray(shape=(self.num_scenarios, self.num_routes),
+                                           itemsize=sizeof(double), format="d")
+        else:
+            # Otherwise the array can just be used to store a single solve to save some memory
+            self.route_flows_arr = cvarray(shape=(self.num_routes, ), itemsize=sizeof(double), format="d")
+
+        # clear the previous problem
+        glp_erase_prob(self.prob)
+        glp_set_obj_dir(self.prob, GLP_MIN)
+        # add a column for each route
+        self.idx_col_routes = glp_add_cols(self.prob, <int>(len(routes)))
+
+        # create a lookup for the cross-domain routes.
+        cross_domain_cols = {}
+        for cross_domain_route in cross_domain_routes:
+            # These routes are only 2 nodes. From output to input
+            output, input = cross_domain_route
+            # note that the conversion factor is not time varying
+            conv_factor = input.get_conversion_factor()
+            input_cols = [(n, conv_factor) for n, route in enumerate(routes) if route[0] is input]
+            # create easy lookup for the route columns this output might
+            # provide cross-domain connection to
+            if output in cross_domain_cols:
+                cross_domain_cols[output].extend(input_cols)
+            else:
+                cross_domain_cols[output] = input_cols
+
+        # explicitly set bounds on route and demand columns
+        for col, route in enumerate(routes):
+            set_col_bnds(self.prob, self.idx_col_routes+col, GLP_LO, 0.0, DBL_MAX)
+
+        # constrain supply minimum and maximum flow
+        self.idx_row_non_storages = glp_add_rows(self.prob, len(non_storages))
+        # Add rows for the cross-domain routes.
+        if len(cross_domain_cols) > 0:
+            self.idx_row_cross_domain = glp_add_rows(self.prob, len(cross_domain_cols))
+
+        cross_domain_row = 0
+        for row, some_node in enumerate(non_storages):
+            # Differentiate betwen the node type.
+            # Input & Output only apply their flow constraints when they
+            # are the first and last node on the route respectively.
+            if isinstance(some_node, BaseInput):
+                cols = [n for n, route in enumerate(routes) if route[0] is some_node]
+            elif isinstance(some_node, BaseOutput):
+                cols = [n for n, route in enumerate(routes) if route[-1] is some_node]
+            else:
+                # Other nodes apply their flow constraints to all routes passing through them
+                cols = [n for n, route in enumerate(routes) if some_node in route]
+            ind = <int*>malloc((1+len(cols)) * sizeof(int))
+            val = <double*>malloc((1+len(cols)) * sizeof(double))
+            for n, c in enumerate(cols):
+                ind[1+n] = 1+c
+                val[1+n] = 1
+            set_mat_row(self.prob, self.idx_row_non_storages+row, len(cols), ind, val)
+            set_row_bnds(self.prob, self.idx_row_non_storages+row, GLP_FX, 0.0, 0.0)
+            # glp_set_row_name(self.prob, self.idx_row_non_storages+row,
+            #                  b'ns.'+some_node.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+            # Add constraint for cross-domain routes
+            # i.e. those from a demand to a supply
+            if some_node in cross_domain_cols:
+                col_vals = cross_domain_cols[some_node]
+                ind = <int*>malloc((1+len(col_vals)+len(cols)) * sizeof(int))
+                val = <double*>malloc((1+len(col_vals)+len(cols)) * sizeof(double))
+                for n, c in enumerate(cols):
+                    ind[1+n] = 1+c
+                    val[1+n] = -1
+                for n, (c, v) in enumerate(col_vals):
+                    ind[1+n+len(cols)] = 1+c
+                    val[1+n+len(cols)] = 1./v
+                set_mat_row(self.prob, self.idx_row_cross_domain+cross_domain_row, len(col_vals)+len(cols), ind, val)
+                set_row_bnds(self.prob, self.idx_row_cross_domain+cross_domain_row, GLP_FX, 0.0, 0.0)
+                # glp_set_row_name(self.prob, self.idx_row_cross_domain+cross_domain_row,
+                #                  b'cd.'+some_node.fully_qualified_name.encode('utf-8'))
+                free(ind)
+                free(val)
+                cross_domain_row += 1
+
+        # storage
+        if len(storages):
+            self.idx_row_storages = glp_add_rows(self.prob, len(storages))
+        for row, storage in enumerate(storages):
+            cols_output = [n for n, route in enumerate(routes)
+                           if route[-1] in storage.outputs and route[0] not in storage.inputs]
+            cols_input = [n for n, route in enumerate(routes)
+                          if route[0] in storage.inputs and route[-1] not in storage.outputs]
+            ind = <int*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(int))
+            val = <double*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(double))
+            for n, c in enumerate(cols_output):
+                ind[1+n] = self.idx_col_routes+c
+                val[1+n] = 1
+            for n, c in enumerate(cols_input):
+                ind[1+len(cols_output)+n] = self.idx_col_routes+c
+                val[1+len(cols_output)+n] = -1
+            set_mat_row(self.prob, self.idx_row_storages+row, len(cols_output)+len(cols_input), ind, val)
+            # glp_set_row_name(self.prob, self.idx_row_storages+row,
+            #                  b's.'+storage.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        # virtual storage
+        if len(virtual_storages):
+            self.idx_row_virtual_storages = glp_add_rows(self.prob, len(virtual_storages))
+        for row, storage in enumerate(virtual_storages):
+            # We need to handle the same route appearing twice here.
+            cols = {}
+            for n, route in enumerate(routes):
+                for some_node in route:
+                    try:
+                        i = storage.nodes.index(some_node)
+                    except ValueError:
+                        pass
+                    else:
+                        try:
+                            cols[n] += storage.factors[i]
+                        except KeyError:
+                            cols[n] = storage.factors[i]
+
+            ind = <int*>malloc((1+len(cols)) * sizeof(int))
+            val = <double*>malloc((1+len(cols)) * sizeof(double))
+            for n, (c, f) in enumerate(cols.items()):
+                ind[1+n] = self.idx_col_routes+c
+                val[1+n] = -f
+
+            set_mat_row(self.prob, self.idx_row_virtual_storages+row, len(cols), ind, val)
+            # glp_set_row_name(self.prob, self.idx_row_virtual_storages+row,
+            #                  b'vs.'+storage.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        # Add constraint rows for aggregated nodes with dynamics factors and
+        # cache data so row values can be updated in solve
+        if len(aggregated_with_factors):
+            self.idx_row_aggregated = self.idx_row_virtual_storages + len(virtual_storages)
+        for agg_node in aggregated_with_factors:
+            nodes = agg_node.nodes
+
+            row = glp_add_rows(self.prob, len(agg_node.nodes)-1)
+            agg_node.__agg_factor_data.row = row
+
+            cols = []
+            ind_ptr = [0,]
+            first_node_cols = [0] + [n+1 for n, route in enumerate(routes) if nodes[0] in route]
+            agg_node.__agg_factor_data.node_ind = len(first_node_cols)
+            for i, node in enumerate(nodes[1:]):
+                cols.extend(first_node_cols + [n+1 for n, route in enumerate(routes) if node in route])
+                ind_ptr.append(len(cols))
+
+            agg_node.__agg_factor_data.ind_ptr = cvarray(shape=(len(ind_ptr),), itemsize=sizeof(int), format="i")
+            for i, v in enumerate(ind_ptr):
+                agg_node.__agg_factor_data.ind_ptr[i] = v
+
+            agg_node.__agg_factor_data.inds = cvarray(shape=(len(cols),), itemsize=sizeof(int), format="i")
+            agg_node.__agg_factor_data.vals = cvarray(shape=(len(cols),), itemsize=sizeof(double), format="d")
+            for i, v in enumerate(cols):
+                agg_node.__agg_factor_data.inds[i] = v
+                agg_node.__agg_factor_data.vals[i] = 1.0
+
+            for n in range(len(nodes)-1):
+                set_row_bnds(self.prob, row+n, GLP_FX, 0.0, 0.0)
+
+        # aggregated node min/max flow constraints
+        if aggregated:
+            self.idx_row_aggregated_min_max = glp_add_rows(self.prob, len(aggregated))
+        for row, agg_node in enumerate(aggregated):
+            row = self.idx_row_aggregated_min_max + row
+            nodes = agg_node.nodes
+
+            weights = agg_node.flow_weights
+            if weights is None:
+                weights = [1.0]*len(nodes)
+
+            matrix = {}
+            for some_node, w in zip(nodes, weights):
+                for n, route in enumerate(routes):
+                    if some_node in route:
+                        matrix[n] = w
+            length = len(matrix)
+            ind = <int*>malloc(1+length * sizeof(int))
+            val = <double*>malloc(1+length * sizeof(double))
+            for i, col in enumerate(sorted(matrix)):
+                ind[1+i] = 1+col
+                val[1+i] = matrix[col]
+            set_mat_row(self.prob, row, length, ind, val)
+            set_row_bnds(self.prob, row, GLP_FX, 0.0, 0.0)
+            # glp_set_row_name(self.prob, row, b'ag.'+agg_node.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        # update route properties
+        routes_cost = []
+        routes_cost_indptr = [0, ]
+        for col, route in enumerate(routes):
+            route_cost = []
+            route_cost.append(route[0].__data.id)
+            for some_node in route[1:-1]:
+                if isinstance(some_node, BaseLink):
+                    route_cost.append(some_node.__data.id)
+            route_cost.append(route[-1].__data.id)
+            routes_cost.extend(route_cost)
+            routes_cost_indptr.append(len(routes_cost))
+
+        assert(len(routes_cost_indptr) == len(routes) + 1)
+
+        self.routes_cost_indptr = np.array(routes_cost_indptr, dtype=np.int32)
+        self.routes_cost = np.array(routes_cost, dtype=np.int32)
+
+        self.routes = routes
+        self.non_storages = non_storages
+        self.storages = storages
+        self.virtual_storages = virtual_storages
+        self.aggregated = aggregated
+        self.aggregated_with_factors = aggregated_with_factors
+
+        self.basis_manager.init_basis(self.prob, len(model.scenarios.combinations))
+        self.is_first_solve = True
+
+        # reset stats
+        self.stats = {
+            'total': 0.0,
+            'lp_solve': 0.0,
+            'result_update': 0.0,
+            'constraint_update_factors': 0.0,
+            'bounds_update_nonstorage': 0.0,
+            'bounds_update_storage': 0.0,
+            'objective_update': 0.0,
+            'number_of_rows': glp_get_num_rows(self.prob),
+            'number_of_cols': glp_get_num_cols(self.prob),
+            'number_of_nonzero': glp_get_num_nz(self.prob),
+            'number_of_routes': len(routes),
+            'number_of_nodes': len(self.all_nodes)
+        }
+
+    def reset(self):
+        # Resetting this triggers a crashing of a new basis in each scenario
+        self.is_first_solve = True
+
+    cpdef object solve(self, model):
+        GLPKSolver.solve(self, model)
+        t0 = time.perf_counter()
+        cdef int[:] scenario_combination
+        cdef int scenario_id
+        cdef ScenarioIndex scenario_index
+        for scenario_index in model.scenarios.combinations:
+            self._solve_scenario(model, scenario_index)
+        self.stats['total'] += time.perf_counter() - t0
+        # After solving this is always false
+        self.is_first_solve = False
+
+    @cython.boundscheck(False)
+    @cython.initializedcheck(False)
+    @cython.cdivision(True)
+    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
+        cdef Node node
+        cdef Storage storage
+        cdef AbstractNode _node
+        cdef AbstractNodeData data
+        cdef AggNodeFactorData agg_data
+        cdef AggregatedNode agg_node
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double max_volume
+        cdef double min_volume
+        cdef double avail_volume
+        cdef double t0
+        cdef int col, row
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status, simplex_ret
+        cdef cross_domain_col
+        cdef list route
+        cdef int node_id, indptr, nroutes
+        cdef int[::1] inds
+        cdef double[::1] vals
+        cdef double flow
+        cdef int n, m, i, ptr
+        cdef Py_ssize_t length
+        cdef int[:] indptr_array
+        cdef double[:] factors_norm
+
+        timestep = model.timestep
+        cdef list routes = self.routes
+        nroutes = len(routes)
+        cdef list non_storages = self.non_storages
+        cdef list storages = self.storages
+        cdef list virtual_storages = self.virtual_storages
+        cdef list aggregated = self.aggregated
+
+        # update route cost
+
+        t0 = time.perf_counter()
+
+        # update the cost of each node in the model
+        cdef double[:] node_costs = self.node_costs_arr
+        for _node in self.all_nodes:
+            data = _node.__data
+            node_costs[data.id] = _node.get_cost(scenario_index)
+
+        # calculate the total cost of each route
+        for col in range(nroutes):
+            cost = 0.0
+            for indptr in range(self.routes_cost_indptr[col], self.routes_cost_indptr[col+1]):
+                node_id = self.routes_cost[indptr]
+                cost += node_costs[node_id]
+
+            if abs(cost) < 1e-8:
+                cost = 0.0
+            set_obj_coef(self.prob, self.idx_col_routes+col, cost)
+
+        self.stats['objective_update'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # Update constraint matrix values for aggregated nodes that have factors defined as parameters
+        for agg_node in self.aggregated_with_factors:
+
+            factors_norm = agg_node.get_factors_norm(scenario_index)
+
+            agg_data = agg_node.__agg_factor_data
+            inds = agg_data.inds
+            vals = agg_data.vals
+            indptr_array = agg_data.ind_ptr
+
+            for n in range(len(agg_node.nodes)-1):
+
+                ptr = indptr_array[n]
+                length = indptr_array[n+1] - ptr
+
+                # only update factors for second node of each row, factor values for first node are already 1.0
+                for i in range(ptr + agg_data.node_ind, ptr + length):
+                    vals[i] = -factors_norm[n+1]
+
+                # 'length - 1' is used here because the ind and val slices start with a padded zero value.
+                # This is required by 'set_mat_row'.
+                set_mat_row(self.prob, agg_data.row+n, length-1, &inds[ptr], &vals[ptr])
+
+        self.stats['constraint_update_factors'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update non-storage properties
+        for row, node in enumerate(non_storages):
+            min_flow = inf_to_dbl_max(node.get_min_flow(scenario_index))
+            if abs(min_flow) < 1e-8:
+                min_flow = 0.0
+            max_flow = inf_to_dbl_max(node.get_max_flow(scenario_index))
+            if abs(max_flow) < 1e-8:
+                max_flow = 0.0
+            set_row_bnds(self.prob, self.idx_row_non_storages+row, constraint_type(min_flow, max_flow),
+                         min_flow, max_flow)
+
+        for row, agg_node in enumerate(aggregated):
+            min_flow = inf_to_dbl_max(agg_node.get_min_flow(scenario_index))
+            if abs(min_flow) < 1e-8:
+                min_flow = 0.0
+            max_flow = inf_to_dbl_max(agg_node.get_max_flow(scenario_index))
+            if abs(max_flow) < 1e-8:
+                max_flow = 0.0
+            set_row_bnds(self.prob, self.idx_row_aggregated_min_max + row, constraint_type(min_flow, max_flow),
+                         min_flow, max_flow)
+
+        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update storage node constraint
+        for row, storage in enumerate(storages):
+            max_volume = storage.get_max_volume(scenario_index)
+            min_volume = storage.get_min_volume(scenario_index)
+
+            if max_volume == min_volume:
+                set_row_bnds(self.prob, self.idx_row_storages+row, GLP_FX, 0.0, 0.0)
+            else:
+                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
+                # change in storage cannot be more than the current volume or
+                # result in maximum volume being exceeded
+                lb = -avail_volume/timestep.days
+                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
+
+                if abs(lb) < 1e-8:
+                    lb = 0.0
+                if abs(ub) < 1e-8:
+                    ub = 0.0
+                set_row_bnds(self.prob, self.idx_row_storages+row, constraint_type(lb, ub), lb, ub)
+
+        # update virtual storage node constraint
+        for row, storage in enumerate(virtual_storages):
+            max_volume = storage.get_max_volume(scenario_index)
+            min_volume = storage.get_min_volume(scenario_index)
+
+            if max_volume == min_volume:
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FX, 0.0, 0.0)
+            elif not storage.active:
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FR, -DBL_MAX, DBL_MAX)
+            else:
+                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
+                # change in storage cannot be more than the current volume or
+                # result in maximum volume being exceeded
+                lb = -avail_volume/timestep.days
+                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
+
+                if abs(lb) < 1e-8:
+                    lb = 0.0
+                if abs(ub) < 1e-8:
+                    ub = 0.0
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, constraint_type(lb, ub), lb, ub)
+
+        self.stats['bounds_update_storage'] += time.perf_counter() - t0
+
+        t0 = time.perf_counter()
+
+        # Set the basis for this scenario
+        self.basis_manager.set_basis(self.prob, self.is_first_solve, scenario_index.global_id)
+        # attempt to solve the linear programme
+        simplex_ret = simplex(self.prob, self.smcp)
+        status = glp_get_status(self.prob)
+        if (status != GLP_OPT or simplex_ret != 0) and self.retry_solve:
+            # try creating a new basis and resolving
+            print('Retrying solve with new basis.')
+            glp_std_basis(self.prob)
+            simplex_ret = simplex(self.prob, self.smcp)
+            status = glp_get_status(self.prob)
+
+        if status != GLP_OPT or simplex_ret != 0:
+            # If problem is not solved. Print some debugging information and error.
+            print("Simplex solve returned: {} ({})".format(simplex_status_string[simplex_ret], simplex_ret))
+            print("Simplex status: {} ({})".format(status_string[status], status))
+            print("Scenario ID: {}".format(scenario_index.global_id))
+            print("Timestep index: {}".format(timestep.index))
+            self.dump_mps(b'pywr_glpk_debug.mps')
+            self.dump_lp(b'pywr_glpk_debug.lp')
+
+            self.smcp.msg_lev = GLP_MSG_DBG
+            # Retry solve with debug messages
+            simplex_ret = simplex(self.prob, self.smcp)
+            status = glp_get_status(self.prob)
+            raise RuntimeError('Simplex solver failed with message: "{}", status: "{}".'.format(
+                simplex_status_string[simplex_ret], status_string[status]))
+        # Now save the basis
+        self.basis_manager.save_basis(self.prob, scenario_index.global_id)
+
+        self.stats['lp_solve'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        cdef double[:] route_flows
+        if self.save_routes_flows:
+            route_flows = self.route_flows_arr[scenario_index.global_id, :]
+        else:
+            route_flows = self.route_flows_arr
+
+        for col in range(0, self.num_routes):
+            route_flows[col] = glp_get_col_prim(self.prob, col+1)
+
+        # collect the total flow via each node
+        cdef double[:] node_flows = self.node_flows_arr
+        node_flows[:] = 0.0
+        for n, route in enumerate(routes):
+            flow = route_flows[n]
+            if flow == 0:
+                continue
+            length = len(route)
+            for m, _node in enumerate(route):
+                data = _node.__data
+                if (m == 0) or (m == length-1) or data.is_link:
+                    node_flows[data.id] += flow
+
+        # commit the total flows
+        for n in range(0, self.num_nodes):
+            _node = self.all_nodes[n]
+            _node.commit(scenario_index.global_id, node_flows[n])
+
+        self.stats['result_update'] += time.perf_counter() - t0
+
+
+cdef class CythonGLPKEdgeSolver(GLPKSolver):
+    cdef int idx_col_edges
+    cdef int idx_row_non_storages
+    cdef int idx_row_link_mass_bal
+    cdef int idx_row_cross_domain
+    cdef int idx_row_storages
+    cdef int idx_row_virtual_storages
+    cdef int idx_row_aggregated
+    cdef int idx_row_aggregated_with_factors
+    cdef int idx_row_aggregated_min_max
+
+    cdef list non_storages
+    cdef list storages
+    cdef list virtual_storages
+    cdef list aggregated
+    cdef list aggregated_with_factors
+
+    cdef list all_nodes
+    cdef list all_edges
+
+    cdef int num_nodes
+    cdef int num_edges
+    cdef int num_storages
+    cdef int num_scenarios
+    cdef cvarray edge_cost_arr
+    cdef cvarray edge_flows_arr
+    cdef cvarray node_flows_arr
+    cdef public cvarray route_flows_arr
+    cdef public object stats
+
+    # Internal representation of the basis for each scenario
+    cdef BasisManager basis_manager
+    cdef bint is_first_solve
+    cdef public bint save_routes_flows
+    cdef public bint retry_solve
+
+    def __init__(self, use_presolve=False, time_limit=None, iteration_limit=None, message_level='error',
+                 save_routes_flows=False, retry_solve=False):
+        super().__init__(use_presolve, time_limit, iteration_limit, message_level)
+        self.stats = None
+        self.is_first_solve = True
+        self.save_routes_flows = save_routes_flows
+        self.retry_solve = retry_solve
+        self.basis_manager = BasisManager()
+
+    def setup(self, model):
+        super().setup(model)
+
+        cdef Node input
+        cdef Node output
+        cdef AbstractNode some_node
+        cdef AbstractNode _node
+        cdef AggregatedNode agg_node
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double avail_volume
+        cdef int col, row
+        cdef int* ind
+        cdef double* val
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status
+        cdef cross_domain_row
+        cdef int n, num
+
+        self.all_nodes = list(sorted(model.graph.nodes(), key=lambda n: n.fully_qualified_name))
+        self.all_edges = edges = list(model.graph.edges())
+        if not self.all_nodes or not self.all_edges:
+            raise ModelStructureError("Model is empty")
+
+        for n, _node in enumerate(self.all_nodes):
+            _node.__data = AbstractNodeData()
+            _node.__data.id = n
+            _node.__data.in_edges = []
+            _node.__data.out_edges = []
+            if isinstance(_node, BaseLink):
+                _node.__data.is_link = True
+
+        self.num_nodes = len(self.all_nodes)
+
+        self.edge_cost_arr = cvarray(shape=(len(self.all_edges),), itemsize=sizeof(double), format="d")
+        self.edge_flows_arr = cvarray(shape=(len(self.all_edges),), itemsize=sizeof(double), format="d")
+        self.node_flows_arr = cvarray(shape=(self.num_nodes,), itemsize=sizeof(double), format="d")
+
+        # Find cross-domain routes
+        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
+
+        link_nodes = []
+        non_storages = []
+        storages = []
+        virtual_storages = []
+        aggregated_with_factors = []
+        aggregated = []
+
+        for some_node in self.all_nodes:
+            if isinstance(some_node, (BaseInput, BaseLink, BaseOutput)):
+                non_storages.append(some_node)
+                if isinstance(some_node, BaseLink):
+                    link_nodes.append(some_node)
+            elif isinstance(some_node, VirtualStorage):
+                virtual_storages.append(some_node)
+            elif isinstance(some_node, Storage):
+                storages.append(some_node)
+            elif isinstance(some_node, AggregatedNode):
+                if some_node.factors is not None:
+                    aggregated_with_factors.append(some_node)
+                    some_node.__agg_factor_data = AggNodeFactorData()
+                aggregated.append(some_node)
+
+        if len(non_storages) == 0:
+            raise ModelStructureError("Model has no non-storage nodes")
+
+        self.num_edges = len(edges)
+        self.num_scenarios = len(model.scenarios.combinations)
+        self.num_storages = len(storages)
+
+        # clear the previous problem
+        glp_erase_prob(self.prob)
+        glp_set_obj_dir(self.prob, GLP_MIN)
+        # add a column for each edge
+        self.idx_col_edges = glp_add_cols(self.prob, self.num_edges)
+
+        # create a lookup for edges associated with each node (ignoring cross domain edges)
+        for row, (start_node, end_node) in enumerate(self.all_edges):
+            if start_node.domain != end_node.domain:
+                continue
+            start_node.__data.out_edges.append(row)
+            end_node.__data.in_edges.append(row)
+
+        # create a lookup for the cross-domain routes.
+        cross_domain_cols = {}
+        for cross_domain_route in cross_domain_routes:
+            # These routes are only 2 nodes. From output to input
+            output, input = cross_domain_route
+            # note that the conversion factor is not time varying
+            conv_factor = input.get_conversion_factor()
+            input_cols = [(n, conv_factor) for n in input.__data.out_edges]
+            # create easy lookup for the route columns this output might
+            # provide cross-domain connection to
+            if output in cross_domain_cols:
+                cross_domain_cols[output].extend(input_cols)
+            else:
+                cross_domain_cols[output] = input_cols
+
+        # explicitly set bounds on route and demand columns
+        for row, edge in enumerate(edges):
+            set_col_bnds(self.prob, self.idx_col_edges+row, GLP_LO, 0.0, DBL_MAX)
+
+        # Apply nodal flow constraints
+        self.idx_row_non_storages = glp_add_rows(self.prob, len(non_storages))
+        # # Add rows for the cross-domain routes.
+        if len(cross_domain_cols) > 0:
+            self.idx_row_cross_domain = glp_add_rows(self.prob, len(cross_domain_cols))
+
+        cross_domain_row = 0
+        for row, some_node in enumerate(non_storages):
+            # Differentiate betwen the node type.
+            # Input and other nodes use the outgoing edge flows to apply the flow constraint on
+            # This requires the mass balance constraints to ensure the inflow and outflow are equal
+            # The Output nodes, in contrast, apply the constraint to the incoming flow (because there is no out going flow)
+            if isinstance(some_node, BaseInput):
+                cols = some_node.__data.out_edges
+                if len(some_node.__data.in_edges) != 0:
+                    raise ModelStructureError(f'Input node "{some_node.name}" should not have any upstream '
+                                              f'connections.')
+            elif isinstance(some_node, BaseOutput):
+                cols = some_node.__data.in_edges
+                if len(some_node.__data.out_edges) != 0:
+                    raise ModelStructureError(f'Output node "{some_node.name}" should not have any downstream '
+                                              f'connections.')
+            else:
+                # Other nodes apply their flow constraints to all routes passing through them
+                cols = some_node.__data.out_edges
+
+            ind = <int*>malloc((1+len(cols)) * sizeof(int))
+            val = <double*>malloc((1+len(cols)) * sizeof(double))
+            for n, c in enumerate(cols):
+                ind[1+n] = 1+c
+                val[1+n] = 1
+            set_mat_row(self.prob, self.idx_row_non_storages+row, len(cols), ind, val)
+            set_row_bnds(self.prob, self.idx_row_non_storages+row, GLP_FX, 0.0, 0.0)
+
+            free(ind)
+            free(val)
+
+            # Add constraint for cross-domain routes
+            # i.e. those from a demand to a supply
+            if some_node in cross_domain_cols:
+                col_vals = cross_domain_cols[some_node]
+                ind = <int*>malloc((1+len(col_vals)+len(cols)) * sizeof(int))
+                val = <double*>malloc((1+len(col_vals)+len(cols)) * sizeof(double))
+                for n, c in enumerate(cols):
+                    ind[1+n] = 1+c
+                    val[1+n] = -1
+                for n, (c, v) in enumerate(col_vals):
+                    ind[1+n+len(cols)] = 1+c
+                    val[1+n+len(cols)] = 1./v
+                set_mat_row(self.prob, self.idx_row_cross_domain+cross_domain_row, len(col_vals)+len(cols), ind, val)
+                set_row_bnds(self.prob, self.idx_row_cross_domain+cross_domain_row, GLP_FX, 0.0, 0.0)
+                # glp_set_row_name(self.prob, self.idx_row_cross_domain+cross_domain_row,
+                #                  b'cd.'+some_node.fully_qualified_name.encode('utf-8'))
+                free(ind)
+                free(val)
+                cross_domain_row += 1
+
+        # Add mass balance constraints
+        if len(link_nodes) > 0:
+            self.idx_row_link_mass_bal = glp_add_rows(self.prob, len(link_nodes))
+        for row, some_node in enumerate(link_nodes):
+
+            in_cols = some_node.__data.in_edges
+            out_cols = some_node.__data.out_edges
+            ind = <int*>malloc((1+len(in_cols)+len(out_cols)) * sizeof(int))
+            val = <double*>malloc((1+len(in_cols)+len(out_cols)) * sizeof(double))
+            for n, c in enumerate(in_cols):
+                ind[1+n] = 1+c
+                val[1+n] = 1
+            for n, c in enumerate(out_cols):
+                ind[1+len(in_cols)+n] = 1+c
+                val[1+len(in_cols)+n] = -1
+            set_mat_row(self.prob, self.idx_row_link_mass_bal+row, len(in_cols)+len(out_cols), ind, val)
+            set_row_bnds(self.prob, self.idx_row_link_mass_bal+row, GLP_FX, 0.0, 0.0)
+
+            free(ind)
+            free(val)
+
+        # storage
+        if len(storages):
+            self.idx_row_storages = glp_add_rows(self.prob, len(storages))
+        for row, storage in enumerate(storages):
+
+            cols_output = []
+            for output in storage.outputs:
+                cols_output.extend(output.__data.in_edges)
+            cols_input = []
+            for input in storage.inputs:
+                cols_input.extend(input.__data.out_edges)
+
+            ind = <int*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(int))
+            val = <double*>malloc((1+len(cols_output)+len(cols_input)) * sizeof(double))
+            for n, c in enumerate(cols_output):
+                ind[1+n] = self.idx_col_edges+c
+                val[1+n] = 1
+            for n, c in enumerate(cols_input):
+                ind[1+len(cols_output)+n] = self.idx_col_edges+c
+                val[1+len(cols_output)+n] = -1
+
+            set_mat_row(self.prob, self.idx_row_storages+row, len(cols_output)+len(cols_input), ind, val)
+            # glp_set_row_name(self.prob, self.idx_row_storages+row,
+            #                  b's.'+storage.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        # virtual storage
+        if len(virtual_storages):
+            self.idx_row_virtual_storages = glp_add_rows(self.prob, len(virtual_storages))
+        for row, storage in enumerate(virtual_storages):
+            # We need to handle the same route appearing twice here.
+            cols = {}
+
+            for i, some_node in enumerate(storage.nodes):
+                if isinstance(some_node, BaseOutput):
+                    node_cols = some_node.__data.in_edges
+                else:
+                    node_cols = some_node.__data.out_edges
+
+                for n in node_cols:
+                    try:
+                        cols[n] += storage.factors[i]
+                    except KeyError:
+                        cols[n] = storage.factors[i]
+
+            ind = <int*>malloc((1+len(cols)) * sizeof(int))
+            val = <double*>malloc((1+len(cols)) * sizeof(double))
+            for n, (c, f) in enumerate(cols.items()):
+                ind[1+n] = self.idx_col_edges+c
+                val[1+n] = -f
+
+            set_mat_row(self.prob, self.idx_row_virtual_storages+row, len(cols), ind, val)
+            # glp_set_row_name(self.prob, self.idx_row_virtual_storages+row,
+            #                  b'vs.'+storage.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        # Add constraint rows for aggregated nodes with dynamics factors and
+        # cache data so row values can be updated in solve
+        if len(aggregated_with_factors):
+            self.idx_row_aggregated_with_factors = self.idx_row_virtual_storages + len(virtual_storages)
+        for agg_node in aggregated_with_factors:
+            nodes = agg_node.nodes
+
+            row = glp_add_rows(self.prob, len(agg_node.nodes)-1)
+            agg_node.__agg_factor_data.row = row
+
+            cols = []
+            ind_ptr = [0,]
+            if isinstance(nodes[0], BaseOutput):
+                first_node_cols = [0] + [c+1 for c in nodes[0].__data.in_edges]
+            else:
+                first_node_cols = [0] + [c+1 for c in nodes[0].__data.out_edges]
+
+            agg_node.__agg_factor_data.node_ind = len(first_node_cols)
+            for i, some_node in enumerate(nodes[1:]):
+                if isinstance(some_node, BaseOutput):
+                    cols.extend(first_node_cols + [c+1 for c in some_node.__data.in_edges])
+                else:
+                    cols.extend(first_node_cols + [c+1 for c in some_node.__data.out_edges])
+                ind_ptr.append(len(cols))
+
+            agg_node.__agg_factor_data.ind_ptr = cvarray(shape=(len(ind_ptr),), itemsize=sizeof(int), format="i")
+            for i, v in enumerate(ind_ptr):
+                agg_node.__agg_factor_data.ind_ptr[i] = v
+
+            agg_node.__agg_factor_data.inds = cvarray(shape=(len(cols),), itemsize=sizeof(int), format="i")
+            agg_node.__agg_factor_data.vals = cvarray(shape=(len(cols),), itemsize=sizeof(double), format="d")
+            for i, v in enumerate(cols):
+                agg_node.__agg_factor_data.inds[i] = v
+                agg_node.__agg_factor_data.vals[i] = 1.0
+
+            for n in range(len(nodes)-1):
+                set_row_bnds(self.prob, row+n, GLP_FX, 0.0, 0.0)
+
+        # aggregated node min/max flow constraints
+        if aggregated:
+            self.idx_row_aggregated_min_max = glp_add_rows(self.prob, len(aggregated))
+        for row, agg_node in enumerate(aggregated):
+            row = self.idx_row_aggregated_min_max + row
+            nodes = agg_node.nodes
+
+            weights = agg_node.flow_weights
+            if weights is None:
+                weights = [1.0]*len(nodes)
+
+            matrix = {}
+            for some_node, w in zip(nodes, weights):
+                if isinstance(some_node, BaseOutput):
+                    node_cols = some_node.__data.in_edges
+                else:
+                    node_cols = some_node.__data.out_edges
+
+                for n in node_cols:
+                    matrix[n] = w
+
+            length = len(matrix)
+            ind = <int*>malloc(1+length * sizeof(int))
+            val = <double*>malloc(1+length * sizeof(double))
+            for i, col in enumerate(sorted(matrix)):
+                ind[1+i] = 1+col
+                val[1+i] = matrix[col]
+            set_mat_row(self.prob, row, length, ind, val)
+            set_row_bnds(self.prob, row, GLP_FX, 0.0, 0.0)
+            # glp_set_row_name(self.prob, row, b'ag.'+agg_node.fully_qualified_name.encode('utf-8'))
+            free(ind)
+            free(val)
+
+        self.non_storages = non_storages
+        self.storages = storages
+        self.virtual_storages = virtual_storages
+        self.aggregated = aggregated
+        self.aggregated_with_factors = aggregated_with_factors
+
+        self.basis_manager.init_basis(self.prob, len(model.scenarios.combinations))
+        self.is_first_solve = True
+
+        # reset stats
+        self.stats = {
+            'total': 0.0,
+            'lp_solve': 0.0,
+            'result_update': 0.0,
+            'constraint_update_factors': 0.0,
+            'bounds_update_nonstorage': 0.0,
+            'bounds_update_storage': 0.0,
+            'objective_update': 0.0,
+            'number_of_rows': glp_get_num_rows(self.prob),
+            'number_of_cols': glp_get_num_cols(self.prob),
+            'number_of_nonzero': glp_get_num_nz(self.prob),
+            'number_of_edges': len(self.all_edges),
+            'number_of_nodes': len(self.all_nodes)
+        }
+
+    def reset(self):
+        # Resetting this triggers a crashing of a new basis in each scenario
+        self.is_first_solve = True
+
+    cpdef object solve(self, model):
+        GLPKSolver.solve(self, model)
+        t0 = time.perf_counter()
+        cdef int[:] scenario_combination
+        cdef int scenario_id
+        cdef ScenarioIndex scenario_index
+        for scenario_index in model.scenarios.combinations:
+            self._solve_scenario(model, scenario_index)
+        self.stats['total'] += time.perf_counter() - t0
+        # After solving this is always false
+        self.is_first_solve = False
+
+    @cython.boundscheck(False)
+    @cython.initializedcheck(False)
+    @cython.cdivision(True)
+    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
+        cdef Node node
+        cdef Storage storage
+        cdef AbstractNode _node
+        cdef AbstractNodeData data
+        cdef AggregatedNode agg_node
+        cdef AggNodeFactorData agg_data
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double max_volume
+        cdef double min_volume
+        cdef double avail_volume
+        cdef double t0
+        cdef int col, row
+        cdef int* ind
+        cdef double* val
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status, simplex_ret
+        cdef cross_domain_col
+        cdef list route
+        cdef int node_id, indptr, nedges
+        cdef int[::1] inds
+        cdef double[::1] vals
+        cdef double flow
+        cdef int n, m, i, ptr
+        cdef int[:] indptr_array
+        cdef double[:] factors_norm
+        cdef Py_ssize_t length
+
+        timestep = model.timestep
+        cdef list edges = self.all_edges
+        nedges = self.num_edges
+        cdef list non_storages = self.non_storages
+        cdef list storages = self.storages
+        cdef list virtual_storages = self.virtual_storages
+        cdef list aggregated = self.aggregated
+
+        # update route cost
+
+        t0 = time.perf_counter()
+
+        # Initialise the cost on each edge to zero
+        cdef double[:] edge_costs = self.edge_cost_arr
+        for col in range(nedges):
+            edge_costs[col] = 0.0
+
+        # update the cost of each node in the model
+        for _node in self.all_nodes:
+            cost = _node.get_cost(scenario_index)
+            data = _node.__data
+
+            # Link nodes have edges connected upstream & downstream. We apply
+            # half the cost assigned to the node to all the connected edges.
+            # The edge costs are then the mean of the node costs at either end.
+            if data.is_link:
+                cost /= 2
+
+            for col in data.in_edges:
+                edge_costs[col] += cost
+            for col in data.out_edges:
+                edge_costs[col] += cost
+
+        # calculate the total cost of each route
+        for col in range(nedges):
+            set_obj_coef(self.prob, self.idx_col_edges+col, edge_costs[col])
+
+        self.stats['objective_update'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # Update constraint matrix values for aggregated nodes that have factors defined as parameters
+        for agg_node in self.aggregated_with_factors:
+
+            factors_norm = agg_node.get_factors_norm(scenario_index)
+
+            agg_data = agg_node.__agg_factor_data
+            inds = agg_data.inds
+            vals = agg_data.vals
+            indptr_array = agg_data.ind_ptr
+
+            for n in range(len(agg_node.nodes)-1):
+
+                ptr = indptr_array[n]
+                length = indptr_array[n+1] - ptr
+
+                # only update factors for second node of each row, factor values for first node are already 1.0
+                for i in range(ptr + agg_data.node_ind, ptr + length):
+                    vals[i] = -factors_norm[n+1]
+
+                # 'length - 1' is used here because the ind and val slices start with a padded zero value.
+                # This is required by 'set_mat_row'.
+                set_mat_row(self.prob, agg_data.row+n, length-1, &inds[ptr], &vals[ptr])
+
+        self.stats['constraint_update_factors'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update non-storage properties
+        for row, node in enumerate(non_storages):
+            min_flow = inf_to_dbl_max(node.get_min_flow(scenario_index))
+            if abs(min_flow) < 1e-8:
+                min_flow = 0.0
+            max_flow = inf_to_dbl_max(node.get_max_flow(scenario_index))
+            if abs(max_flow) < 1e-8:
+                max_flow = 0.0
+
+            set_row_bnds(self.prob, self.idx_row_non_storages+row, constraint_type(min_flow, max_flow),
+                         min_flow, max_flow)
+
+        for row, agg_node in enumerate(aggregated):
+            min_flow = inf_to_dbl_max(agg_node.get_min_flow(scenario_index))
+            if abs(min_flow) < 1e-8:
+                min_flow = 0.0
+            max_flow = inf_to_dbl_max(agg_node.get_max_flow(scenario_index))
+            if abs(max_flow) < 1e-8:
+                max_flow = 0.0
+            set_row_bnds(self.prob, self.idx_row_aggregated_min_max + row, constraint_type(min_flow, max_flow),
+                         min_flow, max_flow)
+
+        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update storage node constraint
+        for row, storage in enumerate(storages):
+            max_volume = storage.get_max_volume(scenario_index)
+            min_volume = storage.get_min_volume(scenario_index)
+
+            if max_volume == min_volume:
+                set_row_bnds(self.prob, self.idx_row_storages+row, GLP_FX, 0.0, 0.0)
+            else:
+                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
+                # change in storage cannot be more than the current volume or
+                # result in maximum volume being exceeded
+                lb = -avail_volume/timestep.days
+                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
+
+                if abs(lb) < 1e-8:
+                    lb = 0.0
+                if abs(ub) < 1e-8:
+                    ub = 0.0
+                set_row_bnds(self.prob, self.idx_row_storages+row, constraint_type(lb, ub), lb, ub)
+
+        # update virtual storage node constraint
+        for row, storage in enumerate(virtual_storages):
+            max_volume = storage.get_max_volume(scenario_index)
+            min_volume = storage.get_min_volume(scenario_index)
+
+            if max_volume == min_volume:
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FX, 0.0, 0.0)
+            elif not storage.active:
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, GLP_FR, -DBL_MAX, DBL_MAX)
+            else:
+                avail_volume = max(storage._volume[scenario_index.global_id] - min_volume, 0.0)
+                # change in storage cannot be more than the current volume or
+                # result in maximum volume being exceeded
+                lb = -avail_volume/timestep.days
+                ub = max(max_volume - storage._volume[scenario_index.global_id], 0.0) / timestep.days
+
+                if abs(lb) < 1e-8:
+                    lb = 0.0
+                if abs(ub) < 1e-8:
+                    ub = 0.0
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+row, constraint_type(lb, ub), lb, ub)
+
+        self.stats['bounds_update_storage'] += time.perf_counter() - t0
+
+        t0 = time.perf_counter()
+
+        # Set the basis for this scenario
+        self.basis_manager.set_basis(self.prob, self.is_first_solve, scenario_index.global_id)
+        # attempt to solve the linear programme
+        simplex_ret = simplex(self.prob, self.smcp)
+        status = glp_get_status(self.prob)
+        if (status != GLP_OPT or simplex_ret != 0) and self.retry_solve:
+            # try creating a new basis and resolving
+            print('Retrying solve with new basis.')
+            glp_std_basis(self.prob)
+            simplex_ret = simplex(self.prob, self.smcp)
+            status = glp_get_status(self.prob)
+
+        if status != GLP_OPT or simplex_ret != 0:
+            # If problem is not solved. Print some debugging information and error.
+            print("Simplex solve returned: {} ({})".format(simplex_status_string[simplex_ret], simplex_ret))
+            print("Simplex status: {} ({})".format(status_string[status], status))
+            print("Scenario ID: {}".format(scenario_index.global_id))
+            print("Timestep index: {}".format(timestep.index))
+            self.dump_mps(b'pywr_glpk_debug.mps')
+            self.dump_lp(b'pywr_glpk_debug.lp')
+
+            self.smcp.msg_lev = GLP_MSG_DBG
+            # Retry solve with debug messages
+            simplex_ret = simplex(self.prob, self.smcp)
+            status = glp_get_status(self.prob)
+            raise RuntimeError('Simplex solver failed with message: "{}", status: "{}".'.format(
+                simplex_status_string[simplex_ret], status_string[status]))
+        # Now save the basis
+        self.basis_manager.save_basis(self.prob, scenario_index.global_id)
+
+        self.stats['lp_solve'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        cdef double[:] edge_flows = self.edge_flows_arr
+
+        for col in range(self.num_edges):
+            edge_flows[col] = glp_get_col_prim(self.prob, col+1)
+
+        # collect the total flow via each node
+        cdef double[:] node_flows = self.node_flows_arr
+        node_flows[:] = 0.0
+        for n, edge in enumerate(edges):
+            flow = edge_flows[n]
+            if flow == 0:
+                continue
+
+            for _node in edge:
+                data = _node.__data
+                if data.is_link:
+                    # Link nodes are connected upstream & downstream so
+                    # we take half of flow from each edge.
+                    node_flows[data.id] += flow / 2
+                else:
+                    node_flows[data.id] += flow
+
+        # commit the total flows
+        for n in range(0, self.num_nodes):
+            _node = self.all_nodes[n]
+            _node.commit(scenario_index.global_id, node_flows[n])
+
+        self.stats['result_update'] += time.perf_counter() - t0
```

### Comparing `pywr-1.8.0/pywr/solvers/cython_lpsolve.pyx` & `pywr-1.9.0/pywr/solvers/cython_lpsolve.pyx`

 * *Files 12% similar despite different names*

```diff
@@ -1,581 +1,591 @@
-from libc.float cimport DBL_MAX
-from libc.stdlib cimport malloc, free
-from cython.view cimport array as cvarray
-from pywr._core import BaseInput, BaseOutput, BaseLink
-from pywr._core cimport *
-from pywr.core import ModelStructureError
-import time
-
-
-cdef extern from "lpsolve/lp_lib.h":
-    cdef struct _lprec:
-        pass
-    ctypedef _lprec lprec
-    ctypedef unsigned char MYBOOL
-    cdef unsigned char FALSE
-    cdef unsigned char TRUE
-    cdef unsigned char FR
-    cdef unsigned char LE
-    cdef unsigned char GE
-    cdef unsigned char EQ
-    cdef int OPTIMAL
-    ctypedef double REAL
-    cdef int CRITICAL
-    cdef int FULL
-    cdef int PRESOLVE_ROWS
-    cdef int PRESOLVE_COLS
-    cdef int PRESOLVE_LINDEP
-
-    lprec* make_lp(int rows, int columns)
-    MYBOOL resize_lp(lprec *lp, int rows, int columns)
-    char* get_statustext(lprec *lp, int statuscode)
-    MYBOOL get_variables(lprec *lp, REAL *var)
-    MYBOOL get_ptr_variables(lprec *lp, REAL **var)
-
-    MYBOOL add_constraint(lprec *lp, REAL *row, int constr_type, REAL rh)
-    MYBOOL add_constraintex(lprec *lp, int count, REAL *row, int *colno, int constr_type, REAL rh)
-    MYBOOL set_add_rowmode(lprec *lp, MYBOOL turnon)
-    void delete_lp(lprec *lp)
-    void free_lp(lprec **plp)
-    void set_maxim(lprec *lp)
-    void set_minim(lprec *lp)
-
-    MYBOOL add_column(lprec *lp, REAL *column)
-    MYBOOL add_columnex(lprec *lp, int count, REAL *column, int *rowno)
-
-    MYBOOL set_obj(lprec *lp, int colnr, REAL value)
-    MYBOOL set_bounds(lprec *lp, int colnr, REAL lower, REAL upper)
-    MYBOOL set_constr_type(lprec *lp, int rownr, int con_type)
-    MYBOOL set_bounds(lprec *lp, int colnr, REAL lower, REAL upper)
-    MYBOOL set_lowbo(lprec *lp, int colnr, REAL value)
-    MYBOOL set_row(lprec *lp, int rownr, REAL *row)
-    MYBOOL set_rowex(lprec *lp, int rownr, int count, REAL *row, int *colno)
-    MYBOOL set_rh(lprec *lp, int rownr, REAL value)
-    MYBOOL set_rh_range(lprec *lp, int rownr, REAL deltavalue)
-    int get_Norig_rows(lprec *lp)
-    int get_Nrows(lprec *lp)
-    int get_Lrows(lprec *lp)
-    int get_nonzeros(lprec *lp)
-    int get_Norig_columns(lprec *lp)
-    int get_Ncolumns(lprec *lp)
-    int solve(lprec *lp)
-    void print_lp(lprec *lp)
-    void set_verbose(lprec *lp, int verbose)
-    void set_presolve(lprec *lp, int presolvemode, int maxloops)
-    int get_presolve(lprec *lp)
-    int get_presolveloops(lprec *lp)
-
-cdef inline int set_row_bnds(lprec *prob, int row, double a, double b):
-    if a == b:
-        set_constr_type(prob, row, EQ)
-        set_rh(prob, row, a)
-        # set_rh_range(prob, row, 0.0)
-    elif b == DBL_MAX:
-        if a == -DBL_MAX:
-            set_constr_type(prob, row, FR)
-            set_rh(prob, row, 0.0)
-            # set_rh_range(prob, row, 0.0)
-        else:
-            set_constr_type(prob, row, GE)
-            set_rh(prob, row, a)
-            # set_rh_range(prob, row, 0.0)
-    elif a == -DBL_MAX:
-        set_constr_type(prob, row, LE)
-        set_rh(prob, row, b)
-        # set_rh_range(prob, row, 0.0)
-    else:
-        set_constr_type(prob, row, LE)
-        set_rh(prob, row, b)
-        set_rh_range(prob, row, b-a)
-
-cdef double inf = float('inf')
-
-cdef inline double dbl_max_to_inf(double a):
-    if a == DBL_MAX:
-        return inf
-    elif a == -DBL_MAX:
-        return -inf
-    return a
-
-cdef inline double inf_to_dbl_max(double a):
-    if a == inf:
-        return DBL_MAX
-    elif a == -inf:
-        return -DBL_MAX
-    return a
-
-cdef class CythonLPSolveSolver:
-    cdef lprec *prob
-    cdef int idx_col_routes
-    cdef int idx_col_demands
-    cdef int idx_row_supplys
-    cdef int idx_row_demands
-    cdef int idx_row_cross_domain
-    cdef int idx_row_storages
-    cdef int idx_row_virtual_storages
-    cdef int idx_row_aggregated
-    cdef int idx_row_aggregated_min_max
-
-    cdef public list routes
-    cdef list supplys
-    cdef list demands
-    cdef list storages
-    cdef list virtual_storages
-    cdef list aggregated
-    cdef public object stats
-    cdef int num_routes
-    cdef int num_scenarios
-    cdef cvarray node_flows_arr
-    cdef public cvarray route_flows_arr
-    cdef public bint save_routes_flows
-
-    def __cinit__(self):
-        # create a new problem
-        self.prob = make_lp(0, 0)
-        if self.prob is NULL:
-            raise MemoryError()
-        set_verbose(self.prob, CRITICAL)
-
-    def __dealloc__(self):
-        if self.prob is not NULL:
-            # free the problem
-            delete_lp(self.prob)
-
-    def __init__(self, save_routes_flows=False):
-        self.stats = None
-        self.save_routes_flows = save_routes_flows
-
-    cpdef object setup(self, model):
-        cdef Node supply
-        cdef Node demand
-        cdef Node node
-        cdef AggregatedNode agg_node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double avail_volume
-        cdef int col
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status
-        cdef cross_domain_col
-        cdef MYBOOL ret
-        cdef REAL *ptr_var
-
-        if not model.graph.nodes():
-            raise ModelStructureError("Model is empty")
-
-        routes = model.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
-        # Find cross-domain routes
-        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
-
-        supplys = []
-        demands = []
-        storages = []
-        virtual_storages = []
-        aggregated = []
-        aggregated_min_max = []
-        for some_node in model.graph.nodes():
-            if isinstance(some_node, (BaseInput, BaseLink)):
-                supplys.append(some_node)
-            if isinstance(some_node, BaseOutput):
-                demands.append(some_node)
-            if isinstance(some_node, VirtualStorage):
-                virtual_storages.append(some_node)
-            elif isinstance(some_node, Storage):
-                storages.append(some_node)
-            elif isinstance(some_node, AggregatedNode):
-                if some_node.factors is not None:
-                    aggregated.append(some_node)
-                if some_node.min_flow > -inf or \
-                   some_node.max_flow < inf:
-                    aggregated_min_max.append(some_node)
-
-        if len(routes) == 0:
-            raise ModelStructureError("Model has no valid routes")
-        if (len(supplys) + len(demands)) == 0:
-            raise ModelStructureError("Model has no non-storage nodes")
-
-        # clear the previous problem
-        ret = resize_lp(self.prob, 0, 0)
-        if ret == FALSE:
-            return -1
-        set_minim(self.prob)
-
-        self.num_routes = len(routes)
-        self.num_scenarios = len(model.scenarios.combinations)
-
-        if self.save_routes_flows:
-            # If saving flows this array needs to be 2D (one for each scenario)
-            self.route_flows_arr = cvarray(shape=(self.num_scenarios, self.num_routes),
-                                           itemsize=sizeof(double), format="d")
-        else:
-            # Otherwise the array can just be used to store a single solve to save some memory
-            self.route_flows_arr = cvarray(shape=(self.num_routes, ), itemsize=sizeof(double), format="d")
-
-        # add a column for each route and demand
-        self.idx_col_routes = get_Norig_columns(self.prob)+1
-        self.idx_col_demands = self.idx_col_routes + len(routes)
-        ret = resize_lp(self.prob, 0, len(routes)+len(demands))
-        ret = set_add_rowmode(self.prob, TRUE)
-
-        # create a lookup for the cross-domain routes.
-        cross_domain_cols = {}
-        for cross_domain_route in cross_domain_routes:
-            # These routes are only 2 nodes. From demand to supply
-            demand, supply = cross_domain_route
-            # TODO make this time varying.
-            conv_factor = supply.get_conversion_factor()
-            supply_cols = [(n, conv_factor) for n, route in enumerate(routes) if route[0] is supply]
-            # create easy lookup for the route columns this demand might
-            # provide cross-domain connection to
-            if demand in cross_domain_cols:
-                cross_domain_cols[demand].extend(supply_cols)
-            else:
-                cross_domain_cols[demand] = supply_cols
-
-        # explicitly set bounds on route and demand columns
-        for col, route in enumerate(routes):
-            ret = add_columnex(self.prob, 0, NULL, NULL)
-            ret = set_lowbo(self.prob, self.idx_col_routes+col, 0.0)
-        for col, demand in enumerate(demands):
-            ret = add_columnex(self.prob, 0, NULL, NULL)
-            ret = set_lowbo(self.prob, self.idx_col_demands+col, 0.0)
-
-        # constrain supply minimum and maximum flow
-        self.idx_row_supplys = get_Norig_rows(self.prob)+1
-        ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(supplys), get_Norig_columns(self.prob))
-        for col, supply in enumerate(supplys):
-            # TODO is this a bit hackish??
-            if isinstance(supply, BaseInput):
-                cols = [n for n, route in enumerate(routes) if route[0] is supply]
-            else:
-                cols = [n for n, route in enumerate(routes) if supply in route]
-
-            ind = <int*>malloc(len(cols) * sizeof(int))
-            val = <double*>malloc(len(cols) * sizeof(double))
-            for n, c in enumerate(cols):
-                ind[n] = 1+c
-                val[n] = 1
-            ret = add_constraintex(self.prob, len(cols), val, ind, GE, 0.0)
-
-            # set_rowex(self.prob, self.idx_row_supplys+col, len(cols)+1, val, ind)
-            # set_row_bnds(self.prob, self.idx_row_supplys+col, 0.0, 0.0)
-
-            free(ind)
-            free(val)
-
-        # link supply and demand variables
-        self.idx_row_demands = get_Norig_rows(self.prob)+1
-        ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(demands), get_Norig_columns(self.prob))
-        if len(cross_domain_cols) > 0:
-            self.idx_row_cross_domain = get_Norig_rows(self.prob)+1
-            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(cross_domain_cols), get_Norig_columns(self.prob))
-        cross_domain_col = 0
-        for col, demand in enumerate(demands):
-            cols = [n for n, route in enumerate(routes) if route[-1] is demand]
-            ind = <int*>malloc((1+len(cols)) * sizeof(int))
-            val = <double*>malloc((1+len(cols)) * sizeof(double))
-            for n, c in enumerate(cols):
-                ind[n] = 1+c
-                val[n] = 1
-            ind[len(cols)] = self.idx_col_demands+col
-            val[len(cols)] = -1
-            ret = add_constraintex(self.prob, len(cols)+1, val, ind, EQ, 0.0)
-            free(ind)
-            free(val)
-
-        for col, demand in enumerate(demands):
-            # Add constraint for cross-domain routes
-            # i.e. those from a demand to a supply
-            if demand in cross_domain_cols:
-                col_vals = cross_domain_cols[demand]
-                ind = <int*>malloc((1+len(col_vals)) * sizeof(int))
-                val = <double*>malloc((1+len(col_vals)) * sizeof(double))
-                for n, (c, v) in enumerate(col_vals):
-                    ind[n] = 1+c
-                    val[n] = 1./v
-                ind[len(col_vals)] = self.idx_col_demands+col
-                val[len(col_vals)] = -1
-                add_constraintex(self.prob, len(col_vals)+1, val, ind, EQ, 0.0)
-                cross_domain_col += 1
-                free(ind)
-                free(val)
-
-        # storage
-        if len(storages):
-            self.idx_row_storages = get_Norig_rows(self.prob)+1
-            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(storages), get_Norig_columns(self.prob))
-        for col, storage in enumerate(storages):
-            cols_output = [n for n, demand in enumerate(demands) if demand in storage.outputs]
-            cols_input = [n for n, route in enumerate(routes) if route[0] in storage.inputs]
-            ind = <int*>malloc((len(cols_output)+len(cols_input)) * sizeof(int))
-            val = <double*>malloc((len(cols_output)+len(cols_input)) * sizeof(double))
-            for n, c in enumerate(cols_output):
-                ind[n] = self.idx_col_demands+c
-                val[n] = 1
-            for n, c in enumerate(cols_input):
-                ind[len(cols_output)+n] = self.idx_col_routes+c
-                val[len(cols_output)+n] = -1
-            add_constraintex(self.prob, len(cols_output)+len(cols_input), val, ind, EQ, 0.0)
-            free(ind)
-            free(val)
-
-        if len(virtual_storages):
-            self.idx_row_virtual_storages = get_Norig_rows(self.prob) + 1
-            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(virtual_storages), get_Norig_columns(self.prob))
-        for col, storage in enumerate(virtual_storages):
-            # We need to handle the same route appearing twice here.
-            cols = {}
-            for n, route in enumerate(routes):
-                for some_node in route:
-                    try:
-                        i = storage.nodes.index(some_node)
-                    except ValueError:
-                        pass
-                    else:
-                        try:
-                            cols[n] += storage.factors[i]
-                        except KeyError:
-                            cols[n] = storage.factors[i]
-
-            ind = <int*>malloc((len(cols)) * sizeof(int))
-            val = <double*>malloc((len(cols)) * sizeof(double))
-            for n, (c, f) in enumerate(cols.items()):
-                ind[n] = self.idx_col_routes+c
-                val[n] = -f
-
-            add_constraintex(self.prob, len(cols), val, ind, EQ, 0.0)
-
-            free(ind)
-            free(val)
-
-        # aggregated node flow ratio constraints
-        if len(aggregated):
-            self.idx_row_aggregated = get_Norig_rows(self.prob)+1
-
-        for agg_node in aggregated:
-            nodes = agg_node.nodes
-            factors = agg_node.factors
-            assert(len(nodes) == len(factors))
-
-            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(agg_node.nodes)-1, get_Norig_columns(self.prob))
-            row = get_Norig_rows(self.prob)+1
-
-            cols = []
-            for node in nodes:
-                cols.append([n for n, route in enumerate(routes) if node in route])
-
-            # normalise factors
-            f0 = factors[0]
-            factors_norm = [f0/f for f in factors]
-
-            # update matrix
-            for n in range(len(nodes)-1):
-                length = len(cols[0])+len(cols[n+1])
-                ind = <int*>malloc(length * sizeof(int))
-                val = <double*>malloc(length * sizeof(double))
-                for i, c in enumerate(cols[0]):
-                    ind[i] = 1+c
-                    val[i] = 1.0
-                for i, c in enumerate(cols[n+1]):
-                    ind[len(cols[0])+i] = 1+c
-                    val[len(cols[0])+i] = -factors_norm[n+1]
-
-                add_constraintex(self.prob, length, val, ind, EQ, 0.0)
-
-                free(ind)
-                free(val)
-
-        # aggregated node min/max flow constraints
-        if aggregated_min_max:
-            self.idx_row_aggregated_min_max = get_Norig_rows(self.prob)+1
-            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(aggregated_min_max), get_Norig_columns(self.prob))
-
-        for row, agg_node in enumerate(aggregated_min_max):
-            row = self.idx_row_aggregated_min_max + row
-            nodes = agg_node.nodes
-            min_flow = agg_node.min_flow
-            max_flow = agg_node.max_flow
-            if min_flow is None:
-                min_flow = -inf
-            if max_flow is None:
-                max_flow = inf
-            min_flow = inf_to_dbl_max(min_flow)
-            max_flow = inf_to_dbl_max(max_flow)
-
-            weights = agg_node.flow_weights
-            if weights is None:
-                weights = [1.0]*len(nodes)
-
-            matrix = {}
-            for node, w in zip(nodes, weights):
-                for n, route in enumerate(routes):
-                    if node in route:
-                        matrix[n] = w
-
-            length = len(matrix)
-            ind = <int*>malloc(length * sizeof(int))
-            val = <double*>malloc(length * sizeof(double))
-            for i, col in enumerate(matrix):
-                ind[i] = 1+col
-                val[i] = matrix[col]
-            add_constraintex(self.prob, length, val, ind, EQ, 0.0)
-            set_row_bnds(self.prob, row, min_flow, max_flow)
-            free(ind)
-            free(val)
-
-        # reset stats
-        self.stats = {
-            'total': 0.0,
-            'lp_solve': 0.0,
-            'result_update': 0.0,
-            'bounds_update_routes': 0.0,
-            'bounds_update_nonstorage': 0.0,
-            'bounds_update_storage': 0.0,
-            'number_of_rows': get_Nrows(self.prob),
-            'number_of_cols': get_Ncolumns(self.prob),
-            'number_of_nonzero': get_nonzeros(self.prob),
-            'number_of_routes': len(routes),
-            'number_of_nodes': len(model.nodes)
-        }
-
-        ret = set_add_rowmode(self.prob, FALSE)
-        self.routes = routes
-        self.supplys = supplys
-        self.demands = demands
-        self.storages = storages
-        self.virtual_storages = virtual_storages
-        self.aggregated = aggregated
-
-    cpdef object solve(self, model):
-        cdef int[:] scenario_combination
-        cdef int scenario_id
-        cdef ScenarioIndex scenario_index
-
-        for scenario_index in model.scenarios.combinations:
-            self._solve_scenario(model, scenario_index)
-
-    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
-        cdef Node supply
-        cdef Node demand
-        cdef Node node
-        cdef double min_flow
-        cdef double max_flow
-        cdef double cost
-        cdef double avail_volume
-        cdef double t0
-        cdef int col
-        cdef int* ind
-        cdef double* val
-        cdef double lb
-        cdef double ub
-        cdef Timestep timestep
-        cdef int status
-        cdef cross_domain_col
-        cdef MYBOOL ret
-        cdef REAL *ptr_var
-
-        timestep = model.timestep
-
-        routes = self.routes
-        supplys = self.supplys
-        demands = self.demands
-        storages = self.storages
-        virtual_storages = self.virtual_storages
-
-        t0 = time.perf_counter()
-
-        # update route properties
-        for col, route in enumerate(routes):
-            cost = route[0].get_cost(scenario_index)
-            for node in route[1:-1]:
-                if isinstance(node, BaseLink):
-                    cost += node.get_cost(scenario_index)
-            set_obj(self.prob, self.idx_col_routes+col, cost)
-
-        self.stats['bounds_update_routes'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update supply properties
-        for col, supply in enumerate(supplys):
-            min_flow = inf_to_dbl_max(supply.get_min_flow(scenario_index))
-            max_flow = inf_to_dbl_max(supply.get_max_flow(scenario_index))
-            set_row_bnds(self.prob, self.idx_row_supplys+col, min_flow, max_flow)
-
-        # update demand properties
-        for col, demand in enumerate(demands):
-            min_flow = inf_to_dbl_max(demand.get_min_flow(scenario_index))
-            max_flow = inf_to_dbl_max(demand.get_max_flow(scenario_index))
-            cost = demand.get_cost(scenario_index)
-            set_bounds(self.prob, self.idx_col_demands+col, min_flow, max_flow)
-            set_obj(self.prob, self.idx_col_demands+col, cost)
-
-        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        # update storage node constraint
-        for col, storage in enumerate(storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            avail_volume = max(storage._volume[scenario_index.global_id] - storage.get_min_volume(scenario_index), 0.0)
-            # change in storage cannot be more than the current volume or
-            # result in maximum volume being exceeded
-            lb = -avail_volume/timestep.days
-            ub = (max_volume - storage._volume[scenario_index.global_id]) / timestep.days
-            set_row_bnds(self.prob, self.idx_row_storages+col, lb, ub)
-
-        # update virtual storage node constraint
-        for col, storage in enumerate(virtual_storages):
-            max_volume = storage.get_max_volume(scenario_index)
-            avail_volume = max(storage._volume[scenario_index.global_id] - storage.get_min_volume(scenario_index), 0.0)
-            # change in storage cannot be more than the current volume or
-            # result in maximum volume being exceeded
-            lb = -avail_volume/timestep.days
-            ub = (max_volume - storage._volume[scenario_index.global_id]) / timestep.days
-            set_row_bnds(self.prob, self.idx_row_virtual_storages+col, lb, ub)
-
-        self.stats['bounds_update_storage'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-        # attempt to solve the linear programme
-
-        # print_lp(self.prob)
-        # set_presolve(self.prob, PRESOLVE_ROWS | PRESOLVE_COLS, get_presolveloops(self.prob))
-        # attempt to solve the linear programme
-        status = solve(self.prob)
-
-        if status != OPTIMAL:
-            raise RuntimeError(get_statustext(self.prob, status))
-
-        self.stats['lp_solve'] += time.perf_counter() - t0
-        t0 = time.perf_counter()
-
-        get_ptr_variables(self.prob, &ptr_var)
-
-        cdef double[:] route_flows
-        if self.save_routes_flows:
-            route_flows = self.route_flows_arr[scenario_index.global_id, :]
-        else:
-            route_flows = self.route_flows_arr
-
-        for col in range(0, len(routes)):
-            route_flows[col] = ptr_var[col]
-
-        change_in_storage = []
-
-        result = {}
-
-        for col, route in enumerate(routes):
-            flow = route_flows[col]
-            # TODO make this cleaner.
-            route[0].commit(scenario_index.global_id, flow)
-            route[-1].commit(scenario_index.global_id, flow)
-            for node in route[1:-1]:
-                if isinstance(node, BaseLink):
-                    node.commit(scenario_index.global_id, flow)
-
-        self.stats['result_update'] += time.perf_counter() - t0
-
-        return route_flows, change_in_storage
+from libc.float cimport DBL_MAX
+from libc.stdlib cimport malloc, free
+from cython.view cimport array as cvarray
+from pywr._core import BaseInput, BaseOutput, BaseLink
+from pywr._core cimport *
+from pywr.core import ModelStructureError
+import time
+
+
+cdef extern from "lpsolve/lp_lib.h":
+    cdef struct _lprec:
+        pass
+    ctypedef _lprec lprec
+    ctypedef unsigned char MYBOOL
+    cdef unsigned char FALSE
+    cdef unsigned char TRUE
+    cdef unsigned char FR
+    cdef unsigned char LE
+    cdef unsigned char GE
+    cdef unsigned char EQ
+    cdef int OPTIMAL
+    ctypedef double REAL
+    cdef int CRITICAL
+    cdef int FULL
+    cdef int PRESOLVE_ROWS
+    cdef int PRESOLVE_COLS
+    cdef int PRESOLVE_LINDEP
+
+    lprec* make_lp(int rows, int columns)
+    MYBOOL resize_lp(lprec *lp, int rows, int columns)
+    char* get_statustext(lprec *lp, int statuscode)
+    MYBOOL get_variables(lprec *lp, REAL *var)
+    MYBOOL get_ptr_variables(lprec *lp, REAL **var)
+
+    MYBOOL add_constraint(lprec *lp, REAL *row, int constr_type, REAL rh)
+    MYBOOL add_constraintex(lprec *lp, int count, REAL *row, int *colno, int constr_type, REAL rh)
+    MYBOOL set_add_rowmode(lprec *lp, MYBOOL turnon)
+    void delete_lp(lprec *lp)
+    void free_lp(lprec **plp)
+    void set_maxim(lprec *lp)
+    void set_minim(lprec *lp)
+
+    MYBOOL add_column(lprec *lp, REAL *column)
+    MYBOOL add_columnex(lprec *lp, int count, REAL *column, int *rowno)
+
+    MYBOOL set_obj(lprec *lp, int colnr, REAL value)
+    MYBOOL set_bounds(lprec *lp, int colnr, REAL lower, REAL upper)
+    MYBOOL set_constr_type(lprec *lp, int rownr, int con_type)
+    MYBOOL set_bounds(lprec *lp, int colnr, REAL lower, REAL upper)
+    MYBOOL set_lowbo(lprec *lp, int colnr, REAL value)
+    MYBOOL set_row(lprec *lp, int rownr, REAL *row)
+    MYBOOL set_rowex(lprec *lp, int rownr, int count, REAL *row, int *colno)
+    MYBOOL set_rh(lprec *lp, int rownr, REAL value)
+    MYBOOL set_rh_range(lprec *lp, int rownr, REAL deltavalue)
+    int get_Norig_rows(lprec *lp)
+    int get_Nrows(lprec *lp)
+    int get_Lrows(lprec *lp)
+    int get_nonzeros(lprec *lp)
+    int get_Norig_columns(lprec *lp)
+    int get_Ncolumns(lprec *lp)
+    int solve(lprec *lp)
+    void print_lp(lprec *lp)
+    void set_verbose(lprec *lp, int verbose)
+    void set_presolve(lprec *lp, int presolvemode, int maxloops)
+    int get_presolve(lprec *lp)
+    int get_presolveloops(lprec *lp)
+
+cdef inline int set_row_bnds(lprec *prob, int row, double a, double b):
+    if a == b:
+        set_constr_type(prob, row, EQ)
+        set_rh(prob, row, a)
+        # set_rh_range(prob, row, 0.0)
+    elif b == DBL_MAX:
+        if a == -DBL_MAX:
+            set_constr_type(prob, row, FR)
+            set_rh(prob, row, 0.0)
+            # set_rh_range(prob, row, 0.0)
+        else:
+            set_constr_type(prob, row, GE)
+            set_rh(prob, row, a)
+            # set_rh_range(prob, row, 0.0)
+    elif a == -DBL_MAX:
+        set_constr_type(prob, row, LE)
+        set_rh(prob, row, b)
+        # set_rh_range(prob, row, 0.0)
+    else:
+        set_constr_type(prob, row, LE)
+        set_rh(prob, row, b)
+        set_rh_range(prob, row, b-a)
+
+cdef double inf = float('inf')
+
+cdef inline double dbl_max_to_inf(double a):
+    if a == DBL_MAX:
+        return inf
+    elif a == -DBL_MAX:
+        return -inf
+    return a
+
+cdef inline double inf_to_dbl_max(double a):
+    if a == inf:
+        return DBL_MAX
+    elif a == -inf:
+        return -DBL_MAX
+    return a
+
+cdef class CythonLPSolveSolver:
+    cdef lprec *prob
+    cdef int idx_col_routes
+    cdef int idx_col_demands
+    cdef int idx_row_supplys
+    cdef int idx_row_demands
+    cdef int idx_row_cross_domain
+    cdef int idx_row_storages
+    cdef int idx_row_virtual_storages
+    cdef int idx_row_aggregated
+    cdef int idx_row_aggregated_min_max
+
+    cdef public list routes
+    cdef list supplys
+    cdef list demands
+    cdef list storages
+    cdef list virtual_storages
+    cdef list aggregated
+    cdef public object stats
+    cdef int num_routes
+    cdef int num_scenarios
+    cdef cvarray node_flows_arr
+    cdef public cvarray route_flows_arr
+    cdef public bint save_routes_flows
+
+    def __cinit__(self):
+        # create a new problem
+        self.prob = make_lp(0, 0)
+        if self.prob is NULL:
+            raise MemoryError()
+        set_verbose(self.prob, CRITICAL)
+
+    def __dealloc__(self):
+        if self.prob is not NULL:
+            # free the problem
+            delete_lp(self.prob)
+
+    def __init__(self, save_routes_flows=False):
+        self.stats = None
+        self.save_routes_flows = save_routes_flows
+
+    cpdef object setup(self, model):
+        cdef Node supply
+        cdef Node demand
+        cdef Node node
+        cdef AggregatedNode agg_node
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double avail_volume
+        cdef int col
+        cdef int* ind
+        cdef double* val
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status
+        cdef cross_domain_col
+        cdef MYBOOL ret
+        cdef REAL *ptr_var
+
+        if not model.graph.nodes():
+            raise ModelStructureError("Model is empty")
+
+        routes = model.find_all_routes(BaseInput, BaseOutput, valid=(BaseLink, BaseInput, BaseOutput))
+        # Find cross-domain routes
+        cross_domain_routes = model.find_all_routes(BaseOutput, BaseInput, max_length=2, domain_match='different')
+
+        supplys = []
+        demands = []
+        storages = []
+        virtual_storages = []
+        aggregated = []
+        aggregated_min_max = []
+        for some_node in model.graph.nodes():
+            if isinstance(some_node, (BaseInput, BaseLink)):
+                supplys.append(some_node)
+            if isinstance(some_node, BaseOutput):
+                demands.append(some_node)
+            if isinstance(some_node, VirtualStorage):
+                virtual_storages.append(some_node)
+            elif isinstance(some_node, Storage):
+                storages.append(some_node)
+            elif isinstance(some_node, AggregatedNode):
+                if some_node.factors is not None:
+                    # See discussion: https://github.com/pywr/pywr/pull/919
+                    # Implementing fully dynamic factors in lpsolve is complicated.
+                    if not some_node.has_fixed_factors:
+                        raise ValueError("{} has one or more factors defined by a parameter. This is not allowed \
+                                        when using the lpsolve solver. Please use the glpk or glpk-edge solver \
+                                        instead".format(some_node.name))
+                    aggregated.append(some_node)
+                if some_node.min_flow > -inf or \
+                   some_node.max_flow < inf:
+                    aggregated_min_max.append(some_node)
+
+        if len(routes) == 0:
+            raise ModelStructureError("Model has no valid routes")
+        if (len(supplys) + len(demands)) == 0:
+            raise ModelStructureError("Model has no non-storage nodes")
+
+        # clear the previous problem
+        ret = resize_lp(self.prob, 0, 0)
+        if ret == FALSE:
+            return -1
+        set_minim(self.prob)
+
+        self.num_routes = len(routes)
+        self.num_scenarios = len(model.scenarios.combinations)
+
+        if self.save_routes_flows:
+            # If saving flows this array needs to be 2D (one for each scenario)
+            self.route_flows_arr = cvarray(shape=(self.num_scenarios, self.num_routes),
+                                           itemsize=sizeof(double), format="d")
+        else:
+            # Otherwise the array can just be used to store a single solve to save some memory
+            self.route_flows_arr = cvarray(shape=(self.num_routes, ), itemsize=sizeof(double), format="d")
+
+        # add a column for each route and demand
+        self.idx_col_routes = get_Norig_columns(self.prob)+1
+        self.idx_col_demands = self.idx_col_routes + len(routes)
+        ret = resize_lp(self.prob, 0, len(routes)+len(demands))
+        ret = set_add_rowmode(self.prob, TRUE)
+
+        # create a lookup for the cross-domain routes.
+        cross_domain_cols = {}
+        for cross_domain_route in cross_domain_routes:
+            # These routes are only 2 nodes. From demand to supply
+            demand, supply = cross_domain_route
+            # TODO make this time varying.
+            conv_factor = supply.get_conversion_factor()
+            supply_cols = [(n, conv_factor) for n, route in enumerate(routes) if route[0] is supply]
+            # create easy lookup for the route columns this demand might
+            # provide cross-domain connection to
+            if demand in cross_domain_cols:
+                cross_domain_cols[demand].extend(supply_cols)
+            else:
+                cross_domain_cols[demand] = supply_cols
+
+        # explicitly set bounds on route and demand columns
+        for col, route in enumerate(routes):
+            ret = add_columnex(self.prob, 0, NULL, NULL)
+            ret = set_lowbo(self.prob, self.idx_col_routes+col, 0.0)
+        for col, demand in enumerate(demands):
+            ret = add_columnex(self.prob, 0, NULL, NULL)
+            ret = set_lowbo(self.prob, self.idx_col_demands+col, 0.0)
+
+        # constrain supply minimum and maximum flow
+        self.idx_row_supplys = get_Norig_rows(self.prob)+1
+        ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(supplys), get_Norig_columns(self.prob))
+        for col, supply in enumerate(supplys):
+            # TODO is this a bit hackish??
+            if isinstance(supply, BaseInput):
+                cols = [n for n, route in enumerate(routes) if route[0] is supply]
+            else:
+                cols = [n for n, route in enumerate(routes) if supply in route]
+
+            ind = <int*>malloc(len(cols) * sizeof(int))
+            val = <double*>malloc(len(cols) * sizeof(double))
+            for n, c in enumerate(cols):
+                ind[n] = 1+c
+                val[n] = 1
+            ret = add_constraintex(self.prob, len(cols), val, ind, GE, 0.0)
+
+            # set_rowex(self.prob, self.idx_row_supplys+col, len(cols)+1, val, ind)
+            # set_row_bnds(self.prob, self.idx_row_supplys+col, 0.0, 0.0)
+
+            free(ind)
+            free(val)
+
+        # link supply and demand variables
+        self.idx_row_demands = get_Norig_rows(self.prob)+1
+        ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(demands), get_Norig_columns(self.prob))
+        if len(cross_domain_cols) > 0:
+            self.idx_row_cross_domain = get_Norig_rows(self.prob)+1
+            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(cross_domain_cols), get_Norig_columns(self.prob))
+        cross_domain_col = 0
+        for col, demand in enumerate(demands):
+            cols = [n for n, route in enumerate(routes) if route[-1] is demand]
+            ind = <int*>malloc((1+len(cols)) * sizeof(int))
+            val = <double*>malloc((1+len(cols)) * sizeof(double))
+            for n, c in enumerate(cols):
+                ind[n] = 1+c
+                val[n] = 1
+            ind[len(cols)] = self.idx_col_demands+col
+            val[len(cols)] = -1
+            ret = add_constraintex(self.prob, len(cols)+1, val, ind, EQ, 0.0)
+            free(ind)
+            free(val)
+
+        for col, demand in enumerate(demands):
+            # Add constraint for cross-domain routes
+            # i.e. those from a demand to a supply
+            if demand in cross_domain_cols:
+                col_vals = cross_domain_cols[demand]
+                ind = <int*>malloc((1+len(col_vals)) * sizeof(int))
+                val = <double*>malloc((1+len(col_vals)) * sizeof(double))
+                for n, (c, v) in enumerate(col_vals):
+                    ind[n] = 1+c
+                    val[n] = 1./v
+                ind[len(col_vals)] = self.idx_col_demands+col
+                val[len(col_vals)] = -1
+                add_constraintex(self.prob, len(col_vals)+1, val, ind, EQ, 0.0)
+                cross_domain_col += 1
+                free(ind)
+                free(val)
+
+        # storage
+        if len(storages):
+            self.idx_row_storages = get_Norig_rows(self.prob)+1
+            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(storages), get_Norig_columns(self.prob))
+        for col, storage in enumerate(storages):
+            cols_output = [n for n, demand in enumerate(demands) if demand in storage.outputs]
+            cols_input = [n for n, route in enumerate(routes) if route[0] in storage.inputs]
+            ind = <int*>malloc((len(cols_output)+len(cols_input)) * sizeof(int))
+            val = <double*>malloc((len(cols_output)+len(cols_input)) * sizeof(double))
+            for n, c in enumerate(cols_output):
+                ind[n] = self.idx_col_demands+c
+                val[n] = 1
+            for n, c in enumerate(cols_input):
+                ind[len(cols_output)+n] = self.idx_col_routes+c
+                val[len(cols_output)+n] = -1
+            add_constraintex(self.prob, len(cols_output)+len(cols_input), val, ind, EQ, 0.0)
+            free(ind)
+            free(val)
+
+        if len(virtual_storages):
+            self.idx_row_virtual_storages = get_Norig_rows(self.prob) + 1
+            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(virtual_storages), get_Norig_columns(self.prob))
+        for col, storage in enumerate(virtual_storages):
+            # We need to handle the same route appearing twice here.
+            cols = {}
+            for n, route in enumerate(routes):
+                for some_node in route:
+                    try:
+                        i = storage.nodes.index(some_node)
+                    except ValueError:
+                        pass
+                    else:
+                        try:
+                            cols[n] += storage.factors[i]
+                        except KeyError:
+                            cols[n] = storage.factors[i]
+
+            ind = <int*>malloc((len(cols)) * sizeof(int))
+            val = <double*>malloc((len(cols)) * sizeof(double))
+            for n, (c, f) in enumerate(cols.items()):
+                ind[n] = self.idx_col_routes+c
+                val[n] = -f
+
+            add_constraintex(self.prob, len(cols), val, ind, EQ, 0.0)
+
+            free(ind)
+            free(val)
+
+        # aggregated node flow ratio constraints
+        if len(aggregated):
+            self.idx_row_aggregated = get_Norig_rows(self.prob)+1
+
+        for agg_node in aggregated:
+            nodes = agg_node.nodes
+            # NB this only works because the solver supports only fixed (i.e. ConstantParameter) factors
+            factors = [f.get_double_variables()[0] for f in agg_node.factors]
+            assert(len(nodes) == len(factors))
+
+            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(agg_node.nodes)-1, get_Norig_columns(self.prob))
+            row = get_Norig_rows(self.prob)+1
+
+            cols = []
+            for node in nodes:
+                cols.append([n for n, route in enumerate(routes) if node in route])
+
+            # normalise factors
+            f0 = factors[0]
+            factors_norm = [f0/f for f in factors]
+
+            # update matrix
+            for n in range(len(nodes)-1):
+                length = len(cols[0])+len(cols[n+1])
+                ind = <int*>malloc(length * sizeof(int))
+                val = <double*>malloc(length * sizeof(double))
+                for i, c in enumerate(cols[0]):
+                    ind[i] = 1+c
+                    val[i] = 1.0
+                for i, c in enumerate(cols[n+1]):
+                    ind[len(cols[0])+i] = 1+c
+                    val[len(cols[0])+i] = -factors_norm[n+1]
+
+                add_constraintex(self.prob, length, val, ind, EQ, 0.0)
+
+                free(ind)
+                free(val)
+
+        # aggregated node min/max flow constraints
+        if aggregated_min_max:
+            self.idx_row_aggregated_min_max = get_Norig_rows(self.prob)+1
+            ret = resize_lp(self.prob, get_Norig_rows(self.prob)+len(aggregated_min_max), get_Norig_columns(self.prob))
+
+        for row, agg_node in enumerate(aggregated_min_max):
+            row = self.idx_row_aggregated_min_max + row
+            nodes = agg_node.nodes
+            min_flow = agg_node.min_flow
+            max_flow = agg_node.max_flow
+            if min_flow is None:
+                min_flow = -inf
+            if max_flow is None:
+                max_flow = inf
+            min_flow = inf_to_dbl_max(min_flow)
+            max_flow = inf_to_dbl_max(max_flow)
+
+            weights = agg_node.flow_weights
+            if weights is None:
+                weights = [1.0]*len(nodes)
+
+            matrix = {}
+            for node, w in zip(nodes, weights):
+                for n, route in enumerate(routes):
+                    if node in route:
+                        matrix[n] = w
+
+            length = len(matrix)
+            ind = <int*>malloc(length * sizeof(int))
+            val = <double*>malloc(length * sizeof(double))
+            for i, col in enumerate(matrix):
+                ind[i] = 1+col
+                val[i] = matrix[col]
+            add_constraintex(self.prob, length, val, ind, EQ, 0.0)
+            set_row_bnds(self.prob, row, min_flow, max_flow)
+            free(ind)
+            free(val)
+
+        # reset stats
+        self.stats = {
+            'total': 0.0,
+            'lp_solve': 0.0,
+            'result_update': 0.0,
+            'bounds_update_routes': 0.0,
+            'bounds_update_nonstorage': 0.0,
+            'bounds_update_storage': 0.0,
+            'number_of_rows': get_Nrows(self.prob),
+            'number_of_cols': get_Ncolumns(self.prob),
+            'number_of_nonzero': get_nonzeros(self.prob),
+            'number_of_routes': len(routes),
+            'number_of_nodes': len(model.nodes)
+        }
+
+        ret = set_add_rowmode(self.prob, FALSE)
+        self.routes = routes
+        self.supplys = supplys
+        self.demands = demands
+        self.storages = storages
+        self.virtual_storages = virtual_storages
+        self.aggregated = aggregated
+
+    cpdef object solve(self, model):
+        cdef int[:] scenario_combination
+        cdef int scenario_id
+        cdef ScenarioIndex scenario_index
+
+        for scenario_index in model.scenarios.combinations:
+            self._solve_scenario(model, scenario_index)
+
+    cdef object _solve_scenario(self, model, ScenarioIndex scenario_index):
+        cdef Node supply
+        cdef Node demand
+        cdef Node node
+        cdef double min_flow
+        cdef double max_flow
+        cdef double cost
+        cdef double avail_volume
+        cdef double t0
+        cdef int col
+        cdef int* ind
+        cdef double* val
+        cdef double lb
+        cdef double ub
+        cdef Timestep timestep
+        cdef int status
+        cdef cross_domain_col
+        cdef MYBOOL ret
+        cdef REAL *ptr_var
+
+        timestep = model.timestep
+
+        routes = self.routes
+        supplys = self.supplys
+        demands = self.demands
+        storages = self.storages
+        virtual_storages = self.virtual_storages
+
+        t0 = time.perf_counter()
+
+        # update route properties
+        for col, route in enumerate(routes):
+            cost = route[0].get_cost(scenario_index)
+            for node in route[1:-1]:
+                if isinstance(node, BaseLink):
+                    cost += node.get_cost(scenario_index)
+            set_obj(self.prob, self.idx_col_routes+col, cost)
+
+        self.stats['bounds_update_routes'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update supply properties
+        for col, supply in enumerate(supplys):
+            min_flow = inf_to_dbl_max(supply.get_min_flow(scenario_index))
+            max_flow = inf_to_dbl_max(supply.get_max_flow(scenario_index))
+            set_row_bnds(self.prob, self.idx_row_supplys+col, min_flow, max_flow)
+
+        # update demand properties
+        for col, demand in enumerate(demands):
+            min_flow = inf_to_dbl_max(demand.get_min_flow(scenario_index))
+            max_flow = inf_to_dbl_max(demand.get_max_flow(scenario_index))
+            cost = demand.get_cost(scenario_index)
+            set_bounds(self.prob, self.idx_col_demands+col, min_flow, max_flow)
+            set_obj(self.prob, self.idx_col_demands+col, cost)
+
+        self.stats['bounds_update_nonstorage'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        # update storage node constraint
+        for col, storage in enumerate(storages):
+            max_volume = storage.get_max_volume(scenario_index)
+            avail_volume = max(storage._volume[scenario_index.global_id] - storage.get_min_volume(scenario_index), 0.0)
+            # change in storage cannot be more than the current volume or
+            # result in maximum volume being exceeded
+            lb = -avail_volume/timestep.days
+            ub = (max_volume - storage._volume[scenario_index.global_id]) / timestep.days
+            set_row_bnds(self.prob, self.idx_row_storages+col, lb, ub)
+
+        # update virtual storage node constraint
+        for col, storage in enumerate(virtual_storages):
+            if not storage.active:
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+col, -DBL_MAX, DBL_MAX)
+            else:
+                max_volume = storage.get_max_volume(scenario_index)
+                avail_volume = max(storage._volume[scenario_index.global_id] - storage.get_min_volume(scenario_index), 0.0)
+                # change in storage cannot be more than the current volume or
+                # result in maximum volume being exceeded
+                lb = -avail_volume/timestep.days
+                ub = (max_volume - storage._volume[scenario_index.global_id]) / timestep.days
+                set_row_bnds(self.prob, self.idx_row_virtual_storages+col, lb, ub)
+
+        self.stats['bounds_update_storage'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+        # attempt to solve the linear programme
+
+        # print_lp(self.prob)
+        # set_presolve(self.prob, PRESOLVE_ROWS | PRESOLVE_COLS, get_presolveloops(self.prob))
+        # attempt to solve the linear programme
+        status = solve(self.prob)
+
+        if status != OPTIMAL:
+            raise RuntimeError(get_statustext(self.prob, status))
+
+        self.stats['lp_solve'] += time.perf_counter() - t0
+        t0 = time.perf_counter()
+
+        get_ptr_variables(self.prob, &ptr_var)
+
+        cdef double[:] route_flows
+        if self.save_routes_flows:
+            route_flows = self.route_flows_arr[scenario_index.global_id, :]
+        else:
+            route_flows = self.route_flows_arr
+
+        for col in range(0, len(routes)):
+            route_flows[col] = ptr_var[col]
+
+        change_in_storage = []
+
+        result = {}
+
+        for col, route in enumerate(routes):
+            flow = route_flows[col]
+            # TODO make this cleaner.
+            route[0].commit(scenario_index.global_id, flow)
+            route[-1].commit(scenario_index.global_id, flow)
+            for node in route[1:-1]:
+                if isinstance(node, BaseLink):
+                    node.commit(scenario_index.global_id, flow)
+
+        self.stats['result_update'] += time.perf_counter() - t0
+
+        return route_flows, change_in_storage
```

### Comparing `pywr-1.8.0/pywr/solvers/libglpk.pxd` & `pywr-1.9.0/pywr/solvers/libglpk.pxd`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-cdef extern from "glpk.h":
-    ctypedef struct glp_prob:
-        pass
-    ctypedef struct glp_smcp:
-        int msg_lev
-        int meth
-        double tol_bnd
-        double tol_dj
-        double tol_piv
-        double obj_ll
-        double obj_ul
-        int it_lim
-        int tm_lim
-        int presolve
-    ctypedef struct glp_mpscp:
-        pass
-
-    int GLP_MIN = 1  # minimization
-    int GLP_MAX = 2  # maximization
-
-    int GLP_FR = 1  # free (unbounded) variable
-    int GLP_LO = 2  # variable with lower bound
-    int GLP_UP = 3  # variable with upper bound
-    int GLP_DB = 4  # double-bounded variable
-    int GLP_FX = 5  # fixed variable
-
-    int GLP_UNDEF = 1  # solution is undefined
-    int GLP_FEAS = 2  # solution is feasible
-    int GLP_INFEAS = 3  # solution is infeasible
-    int GLP_NOFEAS = 4  # no feasible solution exists
-    int GLP_OPT = 5  # solution is optimal
-    int GLP_UNBND = 6  # solution is unbounded
-
-    int GLP_MSG_OFF = 0  # no output
-    int GLP_MSG_ERR = 1  # warning and error messages only
-    int GLP_MSG_ON = 2  # normal output
-    int GLP_MSG_ALL = 3  # full output
-    int GLP_MSG_DBG = 4  # debug output
-
-    int GLP_PRIMAL = 1  # use primal simplex
-    int GLP_DUALP = 2  # use dual; if it fails, use primal
-    int GLP_DUAL = 3  # use dual simplex
-
-    int GLP_MPS_DECK = 1  # fixed (ancient)
-    int GLP_MPS_FILE = 2  # free (modern)
-
-    int GLP_ON = 1
-    int GLP_OFF = 0
-
-    glp_prob* glp_create_prob()
-    void glp_init_smcp(glp_smcp *parm)
-    void glp_erase_prob(glp_prob *P)
-    void glp_delete_prob(glp_prob *P)
-    void glp_free(void *ptr)
-
-    int glp_add_rows(glp_prob *P, int nrs)
-    int glp_add_cols(glp_prob *P, int ncs)
-
-    void glp_set_mat_row(glp_prob *P, int i, int len, const int ind[], const double val[])
-    void glp_set_mat_col(glp_prob *P, int j, int len, const int ind[], const double val[])
-
-    void glp_set_row_bnds(glp_prob *P, int i, int type, double lb, double ub)
-    void glp_set_row_name(glp_prob *P, int i, const char *name)
-
-    void glp_set_col_bnds(glp_prob *P, int j, int type, double lb, double ub)
-    void glp_set_col_name(glp_prob *P, int j, const char *name)
-
-    void glp_set_obj_coef(glp_prob *P, int j, double coef)
-
-    void glp_set_obj_dir(glp_prob *P, int dir)
-
-    void glp_std_basis(glp_prob *P)
-    void glp_adv_basis(glp_prob *P, int flags)
-    int glp_simplex(glp_prob *P, const glp_smcp *parm)
-
-    int glp_get_status(glp_prob *P)
-    int glp_term_out(int flag)
-    void glp_term_hook(int (*func)(void *info, const char *s), void *info)
-
-    double glp_get_row_prim(glp_prob *P, int i)
-    double glp_get_col_prim(glp_prob *P, int j)
-
-    int glp_get_num_rows(glp_prob *P)
-    int glp_get_num_cols(glp_prob *P)
-    int glp_get_num_nz(glp_prob *P)
-
-    int glp_write_mps(glp_prob *P, int fmt, const glp_mpscp *parm, const char *fname)
-    int glp_write_lp(glp_prob *lp, const void *parm, const char *fname)
-    int glp_write_prob(glp_prob *P, int flags, const char *fname)
-
-    int glp_get_row_stat(glp_prob *P, int i)
-    int glp_get_col_stat(glp_prob *P, int i)
-    void glp_set_row_stat(glp_prob *P, int i, int state)
-    void glp_set_col_stat(glp_prob *P, int i, int state)
+cdef extern from "glpk.h":
+    ctypedef struct glp_prob:
+        pass
+    ctypedef struct glp_smcp:
+        int msg_lev
+        int meth
+        double tol_bnd
+        double tol_dj
+        double tol_piv
+        double obj_ll
+        double obj_ul
+        int it_lim
+        int tm_lim
+        int presolve
+    ctypedef struct glp_mpscp:
+        pass
+
+    int GLP_MIN = 1  # minimization
+    int GLP_MAX = 2  # maximization
+
+    int GLP_FR = 1  # free (unbounded) variable
+    int GLP_LO = 2  # variable with lower bound
+    int GLP_UP = 3  # variable with upper bound
+    int GLP_DB = 4  # double-bounded variable
+    int GLP_FX = 5  # fixed variable
+
+    int GLP_UNDEF = 1  # solution is undefined
+    int GLP_FEAS = 2  # solution is feasible
+    int GLP_INFEAS = 3  # solution is infeasible
+    int GLP_NOFEAS = 4  # no feasible solution exists
+    int GLP_OPT = 5  # solution is optimal
+    int GLP_UNBND = 6  # solution is unbounded
+
+    int GLP_MSG_OFF = 0  # no output
+    int GLP_MSG_ERR = 1  # warning and error messages only
+    int GLP_MSG_ON = 2  # normal output
+    int GLP_MSG_ALL = 3  # full output
+    int GLP_MSG_DBG = 4  # debug output
+
+    int GLP_PRIMAL = 1  # use primal simplex
+    int GLP_DUALP = 2  # use dual; if it fails, use primal
+    int GLP_DUAL = 3  # use dual simplex
+
+    int GLP_MPS_DECK = 1  # fixed (ancient)
+    int GLP_MPS_FILE = 2  # free (modern)
+
+    int GLP_ON = 1
+    int GLP_OFF = 0
+
+    glp_prob* glp_create_prob()
+    void glp_init_smcp(glp_smcp *parm)
+    void glp_erase_prob(glp_prob *P)
+    void glp_delete_prob(glp_prob *P)
+    void glp_free(void *ptr)
+
+    int glp_add_rows(glp_prob *P, int nrs)
+    int glp_add_cols(glp_prob *P, int ncs)
+
+    void glp_set_mat_row(glp_prob *P, int i, int len, const int ind[], const double val[])
+    void glp_set_mat_col(glp_prob *P, int j, int len, const int ind[], const double val[])
+
+    void glp_set_row_bnds(glp_prob *P, int i, int type, double lb, double ub)
+    void glp_set_row_name(glp_prob *P, int i, const char *name)
+
+    void glp_set_col_bnds(glp_prob *P, int j, int type, double lb, double ub)
+    void glp_set_col_name(glp_prob *P, int j, const char *name)
+
+    void glp_set_obj_coef(glp_prob *P, int j, double coef)
+
+    void glp_set_obj_dir(glp_prob *P, int dir)
+
+    void glp_std_basis(glp_prob *P)
+    void glp_adv_basis(glp_prob *P, int flags)
+    int glp_simplex(glp_prob *P, const glp_smcp *parm)
+
+    int glp_get_status(glp_prob *P)
+    int glp_term_out(int flag)
+    void glp_term_hook(int (*func)(void *info, const char *s), void *info)
+
+    double glp_get_row_prim(glp_prob *P, int i)
+    double glp_get_col_prim(glp_prob *P, int j)
+
+    int glp_get_num_rows(glp_prob *P)
+    int glp_get_num_cols(glp_prob *P)
+    int glp_get_num_nz(glp_prob *P)
+
+    int glp_write_mps(glp_prob *P, int fmt, const glp_mpscp *parm, const char *fname)
+    int glp_write_lp(glp_prob *lp, const void *parm, const char *fname)
+    int glp_write_prob(glp_prob *P, int flags, const char *fname)
+
+    int glp_get_row_stat(glp_prob *P, int i)
+    int glp_get_col_stat(glp_prob *P, int i)
+    void glp_set_row_stat(glp_prob *P, int i, int state)
+    void glp_set_col_stat(glp_prob *P, int i, int state)
```

### Comparing `pywr-1.8.0/pywr.egg-info/PKG-INFO` & `pywr-1.9.0/pywr.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,98 +1,98 @@
-Metadata-Version: 2.1
-Name: pywr
-Version: 1.8.0
-Summary: Python Water Resource model
-Home-page: https://github.com/pywr/pywr
-Author: Joshua Arnott
-Author-email: josh@snorfalorpagus.net
-License: UNKNOWN
-Description: ====
-        Pywr
-        ====
-        
-        Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
-        
-        .. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
-           :target: https://travis-ci.org/pywr/pywr
-        
-        .. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh?svg=true
-           :target: https://ci.appveyor.com/project/pywr-admin/pywr
-        
-        .. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
-           :target: https://gitter.im/pywr/pywr
-        
-        .. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
-          :target: https://codecov.io/gh/pywr/pywr
-        
-        Overview
-        ========
-        
-        Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
-        
-        Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
-        
-        .. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
-           :width: 250px
-           :height: 190px
-        
-        New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
-        
-        Design goals
-        ============
-        
-        Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywrâ€™s principle design goals are that it is:
-        
-        - Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
-        - Free to use without restriction â€“ licensed under the GNU General Public Licence;
-        - Extendable â€“ uses the Python programming language to define complex operational rules and control model runs.
-        
-        Installation
-        ============
-        
-        Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
-        
-        See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
-        
-        Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
-        
-        .. code-block:: console
-        
-            python setup.py install --with-glpk --with-lpsolve
-        
-        For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
-        
-        Citation
-        ========
-        
-        Please consider citing the following paper when using Pywr:
-        
-        
-            Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
-        
-        
-        License
-        =======
-        
-        Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
-        
-        
-        This program is free software; you can redistribute it and/or modify
-        it under the terms of the GNU General Public License as published by
-        the Free Software Foundation; either version 1, or (at your option)
-        any later version.
-        
-        This program is distributed in the hope that it will be useful,
-        but WITHOUT ANY WARRANTY; without even the implied warranty of
-        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-        GNU General Public License for more details.
-        
-        You should have received a copy of the GNU General Public License
-        along with this program; if not, write to the Free Software
-        Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
-        
-Platform: UNKNOWN
-Description-Content-Type: text/x-rst
-Provides-Extra: docs
-Provides-Extra: test
-Provides-Extra: dev
-Provides-Extra: optimisation
+Metadata-Version: 2.1
+Name: pywr
+Version: 1.9.0
+Summary: Python Water Resource model
+Home-page: https://github.com/pywr/pywr
+Author: Joshua Arnott
+Author-email: josh@snorfalorpagus.net
+License: UNKNOWN
+Description: ====
+        Pywr
+        ====
+        
+        Pywr is a generalised network resource allocation model written in Python. It aims to be fast, free, and extendable.
+        
+        .. image:: https://travis-ci.org/pywr/pywr.svg?branch=master
+           :target: https://travis-ci.org/pywr/pywr
+        
+        .. image:: https://ci.appveyor.com/api/projects/status/ik9u75bxfvracimh/branch/master?svg=true
+           :target: https://ci.appveyor.com/project/pywr-admin/pywr
+        
+        .. image:: https://img.shields.io/badge/chat-on%20gitter-blue.svg
+           :target: https://gitter.im/pywr/pywr
+        
+        .. image:: https://codecov.io/gh/pywr/pywr/branch/master/graph/badge.svg
+          :target: https://codecov.io/gh/pywr/pywr
+        
+        Overview
+        ========
+        
+        Pywr is a tool for solving network resource allocation problems at discrete timesteps using a linear programming approach. It's principal application is in resource allocation in water supply networks, although other uses are conceivable. A network is represented as a directional graph using `NetworkX <https://networkx.github.io/>`__. Nodes in the network can be given constraints (e.g. minimum/maximum flows) and costs, and can be connected as required. Parameters in the model can vary time according to boundary conditions (e.g. an inflow timeseries) or based on states in the model (e.g. the current volume of a reservoir).
+        
+        Models can be developed using the Python API, either in a script or interactively using `IPython <https://ipython.org/>`__/`Jupyter <https://jupyter.org/>`__. Alternatively, models can be defined in a rich `JSON-based document format <https://pywr.github.io/pywr-docs/master/json.html>`__.
+        
+        .. image:: https://raw.githubusercontent.com/pywr/pywr/master/docs/source/_static/pywr_d3.png
+           :width: 250px
+           :height: 190px
+        
+        New users are encouraged to read the `Pywr Tutorial <https://pywr.github.io/pywr-docs/master/tutorial.html>`__.
+        
+        Design goals
+        ============
+        
+        Pywr is a tool for solving network resource allocation problems. It has many similarities with other software packages such as WEAP, Wathnet, Aquator and MISER, but also has some significant differences. Pywr’s principle design goals are that it is:
+        
+        - Fast enough to handle large stochastic datasets and large numbers of scenarios and function evaluations required by advanced decision making methodologies;
+        - Free to use without restriction – licensed under the GNU General Public Licence;
+        - Extendable – uses the Python programming language to define complex operational rules and control model runs.
+        
+        Installation
+        ============
+        
+        Pywr should work on Python 3.6 (or later) on Windows, Linux or OS X.
+        
+        See the documentation for `detailed installation instructions <https://pywr.github.io/pywr-docs/master/install.html>`__.
+        
+        Provided that you have the required `dependencies <https://pywr.github.io/pywr-docs/master/install.html#dependencies>`__ already installed, it's as simple as:
+        
+        .. code-block:: console
+        
+            python setup.py install --with-glpk --with-lpsolve
+        
+        For most users it will be easier to install the `binary packages made available for the Anaconda Python distribution <https://anaconda.org/pywr/pywr>`__. See install docs for more information. Note that these packages may lag behind the development version.
+        
+        Citation
+        ========
+        
+        Please consider citing the following paper when using Pywr:
+        
+        
+            Tomlinson, J.E., Arnott, J.H. and Harou, J.J., 2020. A water resource simulator in Python. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2020.104635
+        
+        
+        License
+        =======
+        
+        Copyright (C) 2014-19 Joshua Arnott, James E. Tomlinson, Atkins, University of Manchester
+        
+        
+        This program is free software; you can redistribute it and/or modify
+        it under the terms of the GNU General Public License as published by
+        the Free Software Foundation; either version 1, or (at your option)
+        any later version.
+        
+        This program is distributed in the hope that it will be useful,
+        but WITHOUT ANY WARRANTY; without even the implied warranty of
+        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+        GNU General Public License for more details.
+        
+        You should have received a copy of the GNU General Public License
+        along with this program; if not, write to the Free Software
+        Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston MA  02110-1301 USA.
+        
+Platform: UNKNOWN
+Description-Content-Type: text/x-rst
+Provides-Extra: docs
+Provides-Extra: test
+Provides-Extra: dev
+Provides-Extra: optimisation
```

### Comparing `pywr-1.8.0/pywr.egg-info/SOURCES.txt` & `pywr-1.9.0/pywr.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 docs/source/api/pywr.nodes.rst
 docs/source/api/pywr.notebook.rst
 docs/source/api/pywr.optimisation.rst
 docs/source/api/pywr.parameters.rst
 docs/source/api/pywr.recorders.rst
 docs/source/api/pywr.rst
 docs/source/api/pywr.solvers.rst
+docs/source/api/pywr.utils.rst
 docs/source/cookbook/aggregated_node.rst
 docs/source/cookbook/aggregated_parameter.rst
 docs/source/cookbook/control_curves.rst
 docs/source/cookbook/cookbook.rst
 docs/source/cookbook/dataframes.rst
 docs/source/cookbook/demand_saving.rst
 docs/source/extending_pywr/extending_pywr_nodes.rst
@@ -110,23 +111,26 @@
 pywr/recorders/events.py
 pywr/recorders/progress.py
 pywr/recorders/recorders.py
 pywr/solvers/__init__.py
 pywr/solvers/cython_glpk.pyx
 pywr/solvers/cython_lpsolve.pyx
 pywr/solvers/libglpk.pxd
+pywr/utils/__init__.py
+pywr/utils/bisect.py
 tests/conftest.py
 tests/fixtures.py
 tests/helpers.py
 tests/notebook.ipynb
 tests/pytest.ini
 tests/test_agg_constraints.py
 tests/test_aggregated_nodes.py
 tests/test_aggregator.py
 tests/test_analytical.py
+tests/test_bisect.py
 tests/test_components.py
 tests/test_control_curves.py
 tests/test_core.py
 tests/test_delay.py
 tests/test_df_resampling.py
 tests/test_groundwater.py
 tests/test_hashes.py
@@ -193,15 +197,18 @@
 tests/models/river_discharge1.json
 tests/models/river_discharge2.json
 tests/models/river_mrf1.json
 tests/models/river_split_with_gauge1.json
 tests/models/scenario_monthly_profile.json
 tests/models/scenario_with_slices.json
 tests/models/scenario_with_user_combinations.json
+tests/models/seasonal_virtual_storage.json
 tests/models/simple1.json
+tests/models/simple1_bisect.json
+tests/models/simple1_infeasible_bisect.json
 tests/models/simple1_monthly.json
 tests/models/simple_data.csv
 tests/models/simple_df.json
 tests/models/simple_df_shared.json
 tests/models/simple_with_scenario.json
 tests/models/simple_with_scenario_wrapper.json
 tests/models/test_data1.xlsx
```

### Comparing `pywr-1.8.0/tests/fixtures.py` & `pywr-1.9.0/tests/fixtures.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,92 +1,92 @@
-# -*- coding: utf-8 -*-
-
-import pywr
-from pywr.core import Model, Input, Output, Link, Storage, AggregatedStorage, AggregatedNode
-
-import pandas
-import datetime
-
-import pytest
-
-@pytest.fixture()
-def model(request):
-    model = Model()
-    return model
-
-@pytest.fixture()
-def simple_linear_model(request):
-    """
-    Make a simple model with a single Input and Output.
-
-    Input -> Link -> Output
-
-    """
-    model = Model()
-    inpt = Input(model, name="Input")
-    lnk = Link(model, name="Link", cost=1.0)
-    inpt.connect(lnk)
-    otpt = Output(model, name="Output")
-    lnk.connect(otpt)
-
-    return model
-
-@pytest.fixture()
-def simple_storage_model(request):
-    """
-    Make a simple model with a single Input, Storage and Output.
-    
-    Input -> Storage -> Output
-    """
-
-    model = pywr.core.Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-05'),
-        timestep=datetime.timedelta(1),
-    )
-
-    inpt = Input(model, name="Input", max_flow=5.0, cost=-1)
-    res = Storage(model, name="Storage", num_outputs=1, num_inputs=1, max_volume=20, initial_volume=10)
-    otpt = Output(model, name="Output", max_flow=8, cost=-999)
-    
-    inpt.connect(res)
-    res.connect(otpt)
-    
-    return model
-
-
-@pytest.fixture()
-def three_storage_model(request):
-    """
-    Make a simple model with three input, storage and output nodes. Also adds
-    an `AggregatedStorage` and `AggregatedNode`.
-
-        Input 0 -> Storage 0 -> Output 0
-        Input 1 -> Storage 1 -> Output 1
-        Input 2 -> Storage 2 -> Output 2
-
-
-    """
-
-    model = pywr.core.Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-05'),
-        timestep=datetime.timedelta(1),
-    )
-
-    all_res = []
-    all_otpt = []
-
-    for num in range(3):
-        inpt = Input(model, name="Input {}".format(num), max_flow=5.0*num, cost=-1)
-        res = Storage(model, name="Storage {}".format(num), num_outputs=1, num_inputs=1, max_volume=20, initial_volume=10+num)
-        otpt = Output(model, name="Output {}".format(num), max_flow=8+num, cost=-999)
-
-        inpt.connect(res)
-        res.connect(otpt)
-
-        all_res.append(res)
-        all_otpt.append(otpt)
-
-    AggregatedStorage(model, name='Total Storage', storage_nodes=all_res)
-    AggregatedNode(model, name='Total Output', nodes=all_otpt)
+# -*- coding: utf-8 -*-
+
+import pywr
+from pywr.core import Model, Input, Output, Link, Storage, AggregatedStorage, AggregatedNode
+
+import pandas
+import datetime
+
+import pytest
+
+@pytest.fixture()
+def model(request):
+    model = Model()
+    return model
+
+@pytest.fixture()
+def simple_linear_model(request):
+    """
+    Make a simple model with a single Input and Output.
+
+    Input -> Link -> Output
+
+    """
+    model = Model()
+    inpt = Input(model, name="Input")
+    lnk = Link(model, name="Link", cost=1.0)
+    inpt.connect(lnk)
+    otpt = Output(model, name="Output")
+    lnk.connect(otpt)
+
+    return model
+
+@pytest.fixture()
+def simple_storage_model(request):
+    """
+    Make a simple model with a single Input, Storage and Output.
+    
+    Input -> Storage -> Output
+    """
+
+    model = pywr.core.Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-05'),
+        timestep=datetime.timedelta(1),
+    )
+
+    inpt = Input(model, name="Input", max_flow=5.0, cost=-1)
+    res = Storage(model, name="Storage", num_outputs=1, num_inputs=1, max_volume=20, initial_volume=10)
+    otpt = Output(model, name="Output", max_flow=8, cost=-999)
+    
+    inpt.connect(res)
+    res.connect(otpt)
+    
+    return model
+
+
+@pytest.fixture()
+def three_storage_model(request):
+    """
+    Make a simple model with three input, storage and output nodes. Also adds
+    an `AggregatedStorage` and `AggregatedNode`.
+
+        Input 0 -> Storage 0 -> Output 0
+        Input 1 -> Storage 1 -> Output 1
+        Input 2 -> Storage 2 -> Output 2
+
+
+    """
+
+    model = pywr.core.Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-05'),
+        timestep=datetime.timedelta(1),
+    )
+
+    all_res = []
+    all_otpt = []
+
+    for num in range(3):
+        inpt = Input(model, name="Input {}".format(num), max_flow=5.0*num, cost=-1)
+        res = Storage(model, name="Storage {}".format(num), num_outputs=1, num_inputs=1, max_volume=20, initial_volume=10+num)
+        otpt = Output(model, name="Output {}".format(num), max_flow=8+num, cost=-999)
+
+        inpt.connect(res)
+        res.connect(otpt)
+
+        all_res.append(res)
+        all_otpt.append(otpt)
+
+    AggregatedStorage(model, name='Total Storage', storage_nodes=all_res)
+    AggregatedNode(model, name='Total Output', nodes=all_otpt)
     return model
```

### Comparing `pywr-1.8.0/tests/models/control_curve.csv` & `pywr-1.9.0/tests/models/control_curve.csv`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/cost1.json` & `pywr-1.9.0/tests/models/cost1.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 22% similar despite different names*

```diff
@@ -1,51 +1,48 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 436f 7374 2031 222c  itle": "Cost 1",
-00000030: 0d0a 2020 2020 2020 2020 2264 6573 6372  ..        "descr
-00000040: 6970 7469 6f6e 223a 2022 4120 7665 7279  iption": "A very
-00000050: 2073 696d 706c 6520 6578 616d 706c 6520   simple example 
-00000060: 6f66 2073 7570 706c 7920 636f 7374 732e  of supply costs.
-00000070: 222c 0d0a 2020 2020 2020 2020 226d 696e  ",..        "min
-00000080: 696d 756d 5f76 6572 7369 6f6e 223a 2022  imum_version": "
-00000090: 302e 3122 0d0a 2020 2020 7d2c 0d0a 2020  0.1"..    },..  
-000000a0: 2020 2274 696d 6573 7465 7070 6572 223a    "timestepper":
-000000b0: 207b 0d0a 2020 2020 2020 2020 2273 7461   {..        "sta
-000000c0: 7274 223a 2022 3230 3135 2d30 312d 3031  rt": "2015-01-01
-000000d0: 222c 0d0a 2020 2020 2020 2020 2265 6e64  ",..        "end
-000000e0: 223a 2022 3230 3135 2d31 322d 3331 222c  ": "2015-12-31",
-000000f0: 0d0a 2020 2020 2020 2020 2274 696d 6573  ..        "times
-00000100: 7465 7022 3a20 310d 0a20 2020 207d 2c0d  tep": 1..    },.
-00000110: 0a20 2020 2022 6e6f 6465 7322 3a20 5b0d  .    "nodes": [.
-00000120: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000130: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000140: 2273 7570 706c 7931 222c 0d0a 2020 2020  "supply1",..    
-00000150: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000160: 2249 6e70 7574 222c 0d0a 2020 2020 2020  "Input",..      
-00000170: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
-00000180: 3a20 3135 2c0d 0a20 2020 2020 2020 2020  : 15,..         
-00000190: 2020 2022 636f 7374 223a 2031 0d0a 2020     "cost": 1..  
-000001a0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000001b0: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-000001c0: 2022 6e61 6d65 223a 2022 7375 7070 6c79   "name": "supply
-000001d0: 3222 2c0d 0a20 2020 2020 2020 2020 2020  2",..           
-000001e0: 2022 7479 7065 223a 2022 496e 7075 7422   "type": "Input"
-000001f0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000200: 6d61 785f 666c 6f77 223a 2031 352c 0d0a  max_flow": 15,..
-00000210: 2020 2020 2020 2020 2020 2020 2263 6f73              "cos
-00000220: 7422 3a20 320d 0a20 2020 2020 2020 207d  t": 2..        }
-00000230: 2c0d 0a20 2020 2020 2020 207b 0d0a 2020  ,..        {..  
-00000240: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
-00000250: 3a20 2264 656d 616e 6431 222c 0d0a 2020  : "demand1",..  
-00000260: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000270: 3a20 224f 7574 7075 7422 2c0d 0a20 2020  : "Output",..   
-00000280: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
-00000290: 6f77 223a 2031 302c 0d0a 2020 2020 2020  ow": 10,..      
-000002a0: 2020 2020 2020 2263 6f73 7422 3a20 2d31        "cost": -1
-000002b0: 300d 0a20 2020 2020 2020 207d 0d0a 2020  0..        }..  
-000002c0: 2020 5d2c 0d0a 2020 2020 2265 6467 6573    ],..    "edges
-000002d0: 223a 205b 0d0a 2020 2020 2020 2020 5b22  ": [..        ["
-000002e0: 7375 7070 6c79 3122 2c20 2264 656d 616e  supply1", "deman
-000002f0: 6431 225d 2c0d 0a20 2020 2020 2020 205b  d1"],..        [
-00000300: 2273 7570 706c 7932 222c 2022 6465 6d61  "supply2", "dema
-00000310: 6e64 3122 5d0d 0a20 2020 205d 0d0a 7d0d  nd1"]..    ]..}.
-00000320: 0a                                       .
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 436f 7374 2031 222c 0a20  le": "Cost 1",. 
+00000030: 2020 2020 2020 2022 6465 7363 7269 7074         "descript
+00000040: 696f 6e22 3a20 2241 2076 6572 7920 7369  ion": "A very si
+00000050: 6d70 6c65 2065 7861 6d70 6c65 206f 6620  mple example of 
+00000060: 7375 7070 6c79 2063 6f73 7473 2e22 2c0a  supply costs.",.
+00000070: 2020 2020 2020 2020 226d 696e 696d 756d          "minimum
+00000080: 5f76 6572 7369 6f6e 223a 2022 302e 3122  _version": "0.1"
+00000090: 0a20 2020 207d 2c0a 2020 2020 2274 696d  .    },.    "tim
+000000a0: 6573 7465 7070 6572 223a 207b 0a20 2020  estepper": {.   
+000000b0: 2020 2020 2022 7374 6172 7422 3a20 2232       "start": "2
+000000c0: 3031 352d 3031 2d30 3122 2c0a 2020 2020  015-01-01",.    
+000000d0: 2020 2020 2265 6e64 223a 2022 3230 3135      "end": "2015
+000000e0: 2d31 322d 3331 222c 0a20 2020 2020 2020  -12-31",.       
+000000f0: 2022 7469 6d65 7374 6570 223a 2031 0a20   "timestep": 1. 
+00000100: 2020 207d 2c0a 2020 2020 226e 6f64 6573     },.    "nodes
+00000110: 223a 205b 0a20 2020 2020 2020 207b 0a20  ": [.        {. 
+00000120: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
+00000130: 223a 2022 7375 7070 6c79 3122 2c0a 2020  ": "supply1",.  
+00000140: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000150: 3a20 2249 6e70 7574 222c 0a20 2020 2020  : "Input",.     
+00000160: 2020 2020 2020 2022 6d61 785f 666c 6f77         "max_flow
+00000170: 223a 2031 352c 0a20 2020 2020 2020 2020  ": 15,.         
+00000180: 2020 2022 636f 7374 223a 2031 0a20 2020     "cost": 1.   
+00000190: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000001a0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000001b0: 616d 6522 3a20 2273 7570 706c 7932 222c  ame": "supply2",
+000001c0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+000001d0: 7065 223a 2022 496e 7075 7422 2c0a 2020  pe": "Input",.  
+000001e0: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
+000001f0: 6c6f 7722 3a20 3135 2c0a 2020 2020 2020  low": 15,.      
+00000200: 2020 2020 2020 2263 6f73 7422 3a20 320a        "cost": 2.
+00000210: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000220: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000230: 2022 6e61 6d65 223a 2022 6465 6d61 6e64   "name": "demand
+00000240: 3122 2c0a 2020 2020 2020 2020 2020 2020  1",.            
+00000250: 2274 7970 6522 3a20 224f 7574 7075 7422  "type": "Output"
+00000260: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
+00000270: 6178 5f66 6c6f 7722 3a20 3130 2c0a 2020  ax_flow": 10,.  
+00000280: 2020 2020 2020 2020 2020 2263 6f73 7422            "cost"
+00000290: 3a20 2d31 300a 2020 2020 2020 2020 7d0a  : -10.        }.
+000002a0: 2020 2020 5d2c 0a20 2020 2022 6564 6765      ],.    "edge
+000002b0: 7322 3a20 5b0a 2020 2020 2020 2020 5b22  s": [.        ["
+000002c0: 7375 7070 6c79 3122 2c20 2264 656d 616e  supply1", "deman
+000002d0: 6431 225d 2c0a 2020 2020 2020 2020 5b22  d1"],.        ["
+000002e0: 7375 7070 6c79 3222 2c20 2264 656d 616e  supply2", "deman
+000002f0: 6431 225d 0a20 2020 205d 0a7d 0a         d1"].    ].}.
```

### Comparing `pywr-1.8.0/tests/models/data/thames_stochastic_flow.gz` & `pywr-1.9.0/tests/models/data/thames_stochastic_flow.gz`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/deficit.json` & `pywr-1.9.0/tests/models/deficit.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,92 +1,89 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4465 6669 6369 7422  itle": "Deficit"
-00000030: 2c0d 0a20 2020 2020 2020 2022 6465 7363  ,..        "desc
-00000040: 7269 7074 696f 6e22 3a20 2245 7861 6d70  ription": "Examp
-00000050: 6c65 206f 6620 6465 6669 6369 7420 7061  le of deficit pa
-00000060: 7261 6d65 7465 7222 2c0d 0a20 2020 2020  rameter",..     
-00000070: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
-00000080: 696f 6e22 3a20 2230 2e34 6465 7630 220d  ion": "0.4dev0".
-00000090: 0a20 2020 207d 2c0d 0a20 2020 2022 7469  .    },..    "ti
-000000a0: 6d65 7374 6570 7065 7222 3a20 7b0d 0a20  mestepper": {.. 
-000000b0: 2020 2020 2020 2022 7374 6172 7422 3a20         "start": 
-000000c0: 2232 3031 352d 3031 2d32 3022 2c0d 0a20  "2015-01-20",.. 
-000000d0: 2020 2020 2020 2022 656e 6422 3a20 2232         "end": "2
-000000e0: 3031 352d 3132 2d33 3122 2c0d 0a20 2020  015-12-31",..   
-000000f0: 2020 2020 2022 7469 6d65 7374 6570 223a       "timestep":
-00000100: 2033 310d 0a20 2020 207d 2c0d 0a20 2020   31..    },..   
-00000110: 2022 6e6f 6465 7322 3a20 5b0d 0a20 2020   "nodes": [..   
-00000120: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-00000130: 2020 2020 226e 616d 6522 3a20 2273 7570      "name": "sup
-00000140: 706c 7931 222c 0d0a 2020 2020 2020 2020  ply1",..        
-00000150: 2020 2020 2274 7970 6522 3a20 2249 6e70      "type": "Inp
-00000160: 7574 222c 0d0a 2020 2020 2020 2020 2020  ut",..          
-00000170: 2020 226d 6178 5f66 6c6f 7722 3a20 7b0d    "max_flow": {.
-00000180: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000190: 2022 7479 7065 223a 2022 6d6f 6e74 686c   "type": "monthl
-000001a0: 7970 726f 6669 6c65 222c 0d0a 2020 2020  yprofile",..    
-000001b0: 2020 2020 2020 2020 2020 2020 2276 616c              "val
-000001c0: 7565 7322 3a20 5b35 2c20 362c 2037 2c20  ues": [5, 6, 7, 
-000001d0: 382c 2039 2c20 3130 2c20 3131 2c20 3132  8, 9, 10, 11, 12
-000001e0: 2c20 3131 2c20 3130 2c20 392c 2038 5d0d  , 11, 10, 9, 8].
-000001f0: 0a20 2020 2020 2020 2020 2020 207d 0d0a  .            }..
-00000200: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000210: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-00000220: 2020 2022 6e61 6d65 223a 2022 6465 6d61     "name": "dema
-00000230: 6e64 3122 2c0d 0a20 2020 2020 2020 2020  nd1",..         
-00000240: 2020 2022 7479 7065 223a 2022 4f75 7470     "type": "Outp
-00000250: 7574 222c 0d0a 2020 2020 2020 2020 2020  ut",..          
-00000260: 2020 226d 6178 5f66 6c6f 7722 3a20 3130    "max_flow": 10
-00000270: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000280: 636f 7374 223a 202d 3130 0d0a 2020 2020  cost": -10..    
-00000290: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-000002a0: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-000002b0: 6e61 6d65 223a 2022 7965 7374 6572 6461  name": "yesterda
-000002c0: 795f 6465 6669 6369 7422 2c0d 0a20 2020  y_deficit",..   
-000002d0: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-000002e0: 2022 696e 7075 7422 2c0d 0a20 2020 2020   "input",..     
-000002f0: 2020 2020 2020 2022 6d61 785f 666c 6f77         "max_flow
-00000300: 223a 2022 6465 6669 6369 7422 0d0a 2020  ": "deficit"..  
-00000310: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000320: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-00000330: 2022 6e61 6d65 223a 2022 6475 6d6d 795f   "name": "dummy_
-00000340: 6f75 7470 7574 222c 0d0a 2020 2020 2020  output",..      
-00000350: 2020 2020 2020 2274 7970 6522 3a20 226f        "type": "o
-00000360: 7574 7075 7422 2c0d 0a20 2020 2020 2020  utput",..       
-00000370: 2020 2020 2022 6d61 785f 666c 6f77 223a       "max_flow":
-00000380: 206e 756c 6c2c 0d0a 2020 2020 2020 2020   null,..        
-00000390: 2020 2020 2263 6f73 7422 3a20 2d31 300d      "cost": -10.
-000003a0: 0a20 2020 2020 2020 207d 0d0a 2020 2020  .        }..    
-000003b0: 5d2c 0d0a 2020 2020 2265 6467 6573 223a  ],..    "edges":
-000003c0: 205b 0d0a 2020 2020 2020 2020 5b22 7375   [..        ["su
-000003d0: 7070 6c79 3122 2c20 2264 656d 616e 6431  pply1", "demand1
-000003e0: 225d 2c0d 0a20 2020 2020 2020 205b 2279  "],..        ["y
-000003f0: 6573 7465 7264 6179 5f64 6566 6963 6974  esterday_deficit
-00000400: 222c 2022 6475 6d6d 795f 6f75 7470 7574  ", "dummy_output
-00000410: 225d 0d0a 2020 2020 5d2c 0d0a 2020 2020  "]..    ],..    
-00000420: 2270 6172 616d 6574 6572 7322 3a20 7b0d  "parameters": {.
-00000430: 0a20 2020 2020 2020 2022 6465 6669 6369  .        "defici
-00000440: 7422 3a20 7b0d 0a20 2020 2020 2020 2020  t": {..         
-00000450: 2020 2022 7479 7065 223a 2022 6465 6669     "type": "defi
-00000460: 6369 7422 2c0d 0a20 2020 2020 2020 2020  cit",..         
-00000470: 2020 2022 6e6f 6465 223a 2022 6465 6d61     "node": "dema
-00000480: 6e64 3122 0d0a 2020 2020 2020 2020 7d0d  nd1"..        }.
-00000490: 0a20 2020 207d 2c0d 0a20 2020 2022 7265  .    },..    "re
-000004a0: 636f 7264 6572 7322 3a20 7b0d 0a20 2020  corders": {..   
-000004b0: 2020 2020 2022 6465 6669 6369 745f 7265       "deficit_re
-000004c0: 636f 7264 6572 223a 207b 0d0a 2020 2020  corder": {..    
-000004d0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-000004e0: 226e 756d 7079 6172 7261 7970 6172 616d  "numpyarrayparam
-000004f0: 6574 6572 7265 636f 7264 6572 222c 0d0a  eterrecorder",..
-00000500: 2020 2020 2020 2020 2020 2020 2270 6172              "par
-00000510: 616d 6574 6572 223a 2022 6465 6669 6369  ameter": "defici
-00000520: 7422 0d0a 2020 2020 2020 2020 7d2c 0d0a  t"..        },..
-00000530: 2020 2020 2020 2020 2279 6573 7465 7264          "yesterd
-00000540: 6179 5f72 6563 6f72 6465 7222 3a20 7b0d  ay_recorder": {.
-00000550: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
-00000560: 7065 223a 2022 6e75 6d70 7961 7272 6179  pe": "numpyarray
-00000570: 6e6f 6465 7265 636f 7264 6572 222c 0d0a  noderecorder",..
-00000580: 2020 2020 2020 2020 2020 2020 226e 6f64              "nod
-00000590: 6522 3a20 2279 6573 7465 7264 6179 5f64  e": "yesterday_d
-000005a0: 6566 6963 6974 220d 0a20 2020 2020 2020  eficit"..       
-000005b0: 207d 0d0a 2020 2020 7d0d 0a7d 0d0a        }..    }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4465 6669 6369 7422 2c0a  le": "Deficit",.
+00000030: 2020 2020 2020 2020 2264 6573 6372 6970          "descrip
+00000040: 7469 6f6e 223a 2022 4578 616d 706c 6520  tion": "Example 
+00000050: 6f66 2064 6566 6963 6974 2070 6172 616d  of deficit param
+00000060: 6574 6572 222c 0a20 2020 2020 2020 2022  eter",.        "
+00000070: 6d69 6e69 6d75 6d5f 7665 7273 696f 6e22  minimum_version"
+00000080: 3a20 2230 2e34 6465 7630 220a 2020 2020  : "0.4dev0".    
+00000090: 7d2c 0a20 2020 2022 7469 6d65 7374 6570  },.    "timestep
+000000a0: 7065 7222 3a20 7b0a 2020 2020 2020 2020  per": {.        
+000000b0: 2273 7461 7274 223a 2022 3230 3135 2d30  "start": "2015-0
+000000c0: 312d 3230 222c 0a20 2020 2020 2020 2022  1-20",.        "
+000000d0: 656e 6422 3a20 2232 3031 352d 3132 2d33  end": "2015-12-3
+000000e0: 3122 2c0a 2020 2020 2020 2020 2274 696d  1",.        "tim
+000000f0: 6573 7465 7022 3a20 3331 0a20 2020 207d  estep": 31.    }
+00000100: 2c0a 2020 2020 226e 6f64 6573 223a 205b  ,.    "nodes": [
+00000110: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+00000120: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
+00000130: 7375 7070 6c79 3122 2c0a 2020 2020 2020  supply1",.      
+00000140: 2020 2020 2020 2274 7970 6522 3a20 2249        "type": "I
+00000150: 6e70 7574 222c 0a20 2020 2020 2020 2020  nput",.         
+00000160: 2020 2022 6d61 785f 666c 6f77 223a 207b     "max_flow": {
+00000170: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000180: 2022 7479 7065 223a 2022 6d6f 6e74 686c   "type": "monthl
+00000190: 7970 726f 6669 6c65 222c 0a20 2020 2020  yprofile",.     
+000001a0: 2020 2020 2020 2020 2020 2022 7661 6c75             "valu
+000001b0: 6573 223a 205b 352c 2036 2c20 372c 2038  es": [5, 6, 7, 8
+000001c0: 2c20 392c 2031 302c 2031 312c 2031 322c  , 9, 10, 11, 12,
+000001d0: 2031 312c 2031 302c 2039 2c20 385d 0a20   11, 10, 9, 8]. 
+000001e0: 2020 2020 2020 2020 2020 207d 0a20 2020             }.   
+000001f0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000200: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+00000210: 616d 6522 3a20 2264 656d 616e 6431 222c  ame": "demand1",
+00000220: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000230: 7065 223a 2022 4f75 7470 7574 222c 0a20  pe": "Output",. 
+00000240: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
+00000250: 666c 6f77 223a 2031 302c 0a20 2020 2020  flow": 10,.     
+00000260: 2020 2020 2020 2022 636f 7374 223a 202d         "cost": -
+00000270: 3130 0a20 2020 2020 2020 207d 2c0a 2020  10.        },.  
+00000280: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000290: 2020 2020 226e 616d 6522 3a20 2279 6573      "name": "yes
+000002a0: 7465 7264 6179 5f64 6566 6963 6974 222c  terday_deficit",
+000002b0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+000002c0: 7065 223a 2022 696e 7075 7422 2c0a 2020  pe": "input",.  
+000002d0: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
+000002e0: 6c6f 7722 3a20 2264 6566 6963 6974 220a  low": "deficit".
+000002f0: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000300: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000310: 2022 6e61 6d65 223a 2022 6475 6d6d 795f   "name": "dummy_
+00000320: 6f75 7470 7574 222c 0a20 2020 2020 2020  output",.       
+00000330: 2020 2020 2022 7479 7065 223a 2022 6f75       "type": "ou
+00000340: 7470 7574 222c 0a20 2020 2020 2020 2020  tput",.         
+00000350: 2020 2022 6d61 785f 666c 6f77 223a 206e     "max_flow": n
+00000360: 756c 6c2c 0a20 2020 2020 2020 2020 2020  ull,.           
+00000370: 2022 636f 7374 223a 202d 3130 0a20 2020   "cost": -10.   
+00000380: 2020 2020 207d 0a20 2020 205d 2c0a 2020       }.    ],.  
+00000390: 2020 2265 6467 6573 223a 205b 0a20 2020    "edges": [.   
+000003a0: 2020 2020 205b 2273 7570 706c 7931 222c       ["supply1",
+000003b0: 2022 6465 6d61 6e64 3122 5d2c 0a20 2020   "demand1"],.   
+000003c0: 2020 2020 205b 2279 6573 7465 7264 6179       ["yesterday
+000003d0: 5f64 6566 6963 6974 222c 2022 6475 6d6d  _deficit", "dumm
+000003e0: 795f 6f75 7470 7574 225d 0a20 2020 205d  y_output"].    ]
+000003f0: 2c0a 2020 2020 2270 6172 616d 6574 6572  ,.    "parameter
+00000400: 7322 3a20 7b0a 2020 2020 2020 2020 2264  s": {.        "d
+00000410: 6566 6963 6974 223a 207b 0a20 2020 2020  eficit": {.     
+00000420: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000430: 6465 6669 6369 7422 2c0a 2020 2020 2020  deficit",.      
+00000440: 2020 2020 2020 226e 6f64 6522 3a20 2264        "node": "d
+00000450: 656d 616e 6431 220a 2020 2020 2020 2020  emand1".        
+00000460: 7d0a 2020 2020 7d2c 0a20 2020 2022 7265  }.    },.    "re
+00000470: 636f 7264 6572 7322 3a20 7b0a 2020 2020  corders": {.    
+00000480: 2020 2020 2264 6566 6963 6974 5f72 6563      "deficit_rec
+00000490: 6f72 6465 7222 3a20 7b0a 2020 2020 2020  order": {.      
+000004a0: 2020 2020 2020 2274 7970 6522 3a20 226e        "type": "n
+000004b0: 756d 7079 6172 7261 7970 6172 616d 6574  umpyarrayparamet
+000004c0: 6572 7265 636f 7264 6572 222c 0a20 2020  errecorder",.   
+000004d0: 2020 2020 2020 2020 2022 7061 7261 6d65           "parame
+000004e0: 7465 7222 3a20 2264 6566 6963 6974 220a  ter": "deficit".
+000004f0: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000500: 2020 2022 7965 7374 6572 6461 795f 7265     "yesterday_re
+00000510: 636f 7264 6572 223a 207b 0a20 2020 2020  corder": {.     
+00000520: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000530: 6e75 6d70 7961 7272 6179 6e6f 6465 7265  numpyarraynodere
+00000540: 636f 7264 6572 222c 0a20 2020 2020 2020  corder",.       
+00000550: 2020 2020 2022 6e6f 6465 223a 2022 7965       "node": "ye
+00000560: 7374 6572 6461 795f 6465 6669 6369 7422  sterday_deficit"
+00000570: 0a20 2020 2020 2020 207d 0a20 2020 207d  .        }.    }
+00000580: 0a7d 0a                                  .}.
```

### Comparing `pywr-1.8.0/tests/models/demand_saving2_with_variables.json` & `pywr-1.9.0/tests/models/demand_saving2_with_variables.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,214 +1,207 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4465 6d61 6e64 2053  itle": "Demand S
-00000030: 6176 696e 6722 2c0d 0a20 2020 2020 2020  aving",..       
-00000040: 2022 6465 7363 7269 7074 696f 6e22 3a20   "description": 
-00000050: 2244 656d 616e 6420 7361 7669 6e67 2075  "Demand saving u
-00000060: 7369 6e67 2061 6e20 496e 6465 7865 6441  sing an IndexedA
-00000070: 7272 6179 5061 7261 6d65 7465 7220 7769  rrayParameter wi
-00000080: 7468 2076 6172 6961 626c 6573 2061 6e64  th variables and
-00000090: 2063 6f6e 7374 7261 696e 7473 222c 0d0a   constraints",..
-000000a0: 2020 2020 2020 2020 226d 696e 696d 756d          "minimum
-000000b0: 5f76 6572 7369 6f6e 223a 2022 302e 3122  _version": "0.1"
-000000c0: 0d0a 2020 2020 7d2c 0d0a 2020 2020 2274  ..    },..    "t
-000000d0: 696d 6573 7465 7070 6572 223a 207b 0d0a  imestepper": {..
-000000e0: 2020 2020 2020 2020 2273 7461 7274 223a          "start":
-000000f0: 2022 3230 3136 2d30 312d 3031 222c 0d0a   "2016-01-01",..
-00000100: 2020 2020 2020 2020 2265 6e64 223a 2022          "end": "
-00000110: 3230 3136 2d31 322d 3331 222c 0d0a 2020  2016-12-31",..  
-00000120: 2020 2020 2020 2274 696d 6573 7465 7022        "timestep"
-00000130: 3a20 310d 0a20 2020 207d 2c0d 0a20 2020  : 1..    },..   
-00000140: 2022 6e6f 6465 7322 3a20 5b0d 0a20 2020   "nodes": [..   
-00000150: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-00000160: 2020 2020 2274 7970 6522 3a20 2263 6174      "type": "cat
-00000170: 6368 6d65 6e74 222c 0d0a 2020 2020 2020  chment",..      
-00000180: 2020 2020 2020 226e 616d 6522 3a20 2249        "name": "I
-00000190: 6e66 6c6f 7722 2c0d 0a20 2020 2020 2020  nflow",..       
-000001a0: 2020 2020 2022 666c 6f77 223a 2030 2e30       "flow": 0.0
-000001b0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-000001c0: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-000001d0: 2020 2020 2022 7479 7065 223a 2022 7265       "type": "re
-000001e0: 7365 7276 6f69 7222 2c0d 0a20 2020 2020  servoir",..     
-000001f0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000200: 5265 7365 7276 6f69 7222 2c0d 0a20 2020  Reservoir",..   
-00000210: 2020 2020 2020 2020 2022 6d61 785f 766f           "max_vo
-00000220: 6c75 6d65 223a 2031 3030 302c 0d0a 2020  lume": 1000,..  
-00000230: 2020 2020 2020 2020 2020 2269 6e69 7469            "initi
-00000240: 616c 5f76 6f6c 756d 6522 3a20 3130 3030  al_volume": 1000
-00000250: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000260: 706f 7369 7469 6f6e 223a 207b 0d0a 2020  position": {..  
-00000270: 2020 2020 2020 2020 2020 2020 2020 2273                "s
-00000280: 6368 656d 6174 6963 223a 205b 312c 2031  chematic": [1, 1
-00000290: 5d0d 0a20 2020 2020 2020 2020 2020 207d  ]..            }
-000002a0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-000002b0: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-000002c0: 2020 2020 2022 7479 7065 223a 2022 6f75       "type": "ou
-000002d0: 7470 7574 222c 0d0a 2020 2020 2020 2020  tput",..        
-000002e0: 2020 2020 226e 616d 6522 3a20 2253 7069      "name": "Spi
-000002f0: 6c6c 222c 0d0a 2020 2020 2020 2020 2020  ll",..          
-00000300: 2020 2263 6f73 7422 3a20 3130 0d0a 2020    "cost": 10..  
-00000310: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000320: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-00000330: 2022 636f 6d6d 656e 7422 3a20 2254 6865   "comment": "The
-00000340: 206f 6e6c 7920 6465 6d61 6e64 2069 6e20   only demand in 
-00000350: 7468 6520 6d6f 6465 6c22 2c0d 0a20 2020  the model",..   
-00000360: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000370: 2022 6f75 7470 7574 222c 0d0a 2020 2020   "output",..    
-00000380: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000390: 2244 656d 616e 6422 2c0d 0a20 2020 2020  "Demand",..     
-000003a0: 2020 2020 2020 2022 6d61 785f 666c 6f77         "max_flow
-000003b0: 223a 2022 6465 6d61 6e64 5f6d 6178 5f66  ": "demand_max_f
-000003c0: 6c6f 7722 2c0d 0a20 2020 2020 2020 2020  low",..         
-000003d0: 2020 2022 636f 7374 223a 202d 3530 300d     "cost": -500.
-000003e0: 0a20 2020 2020 2020 207d 0d0a 2020 2020  .        }..    
-000003f0: 5d2c 0d0a 2020 2020 2265 6467 6573 223a  ],..    "edges":
-00000400: 205b 0d0a 2020 2020 2020 2020 5b22 496e   [..        ["In
-00000410: 666c 6f77 222c 2022 5265 7365 7276 6f69  flow", "Reservoi
-00000420: 7222 5d2c 0d0a 2020 2020 2020 2020 5b22  r"],..        ["
-00000430: 5265 7365 7276 6f69 7222 2c20 2244 656d  Reservoir", "Dem
-00000440: 616e 6422 5d2c 0d0a 2020 2020 2020 2020  and"],..        
-00000450: 5b22 5265 7365 7276 6f69 7222 2c20 2253  ["Reservoir", "S
-00000460: 7069 6c6c 225d 0d0a 2020 2020 5d2c 0d0a  pill"]..    ],..
-00000470: 2020 2020 2270 6172 616d 6574 6572 7322      "parameters"
-00000480: 3a20 7b0d 0a20 2020 2020 2020 2022 6465  : {..        "de
-00000490: 6d61 6e64 5f62 6173 656c 696e 6522 3a20  mand_baseline": 
-000004a0: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-000004b0: 7479 7065 223a 2022 636f 6e73 7461 6e74  type": "constant
-000004c0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000004d0: 2276 616c 7565 223a 2035 300d 0a20 2020  "value": 50..   
-000004e0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-000004f0: 2022 6465 6d61 6e64 5f70 726f 6669 6c65   "demand_profile
-00000500: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000510: 2020 2263 6f6d 6d65 6e74 223a 2022 4d6f    "comment": "Mo
-00000520: 6e74 686c 7920 6465 6d61 6e64 2070 726f  nthly demand pro
-00000530: 6669 6c65 2061 7320 6120 6661 6374 6f72  file as a factor
-00000540: 2061 726f 756e 6420 7468 6520 6d65 616e   around the mean
-00000550: 2064 656d 616e 6422 2c0d 0a20 2020 2020   demand",..     
-00000560: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000570: 6d6f 6e74 686c 7970 726f 6669 6c65 222c  monthlyprofile",
-00000580: 0d0a 2020 2020 2020 2020 2020 2020 2276  ..            "v
-00000590: 616c 7565 7322 3a20 5b30 2e39 2c20 302e  alues": [0.9, 0.
-000005a0: 392c 2030 2e39 2c20 302e 392c 2031 2e32  9, 0.9, 0.9, 1.2
-000005b0: 2c20 312e 322c 2031 2e32 2c20 312e 322c  , 1.2, 1.2, 1.2,
-000005c0: 2030 2e39 2c20 302e 392c 2030 2e39 2c20   0.9, 0.9, 0.9, 
-000005d0: 302e 395d 2c0d 0a20 2020 2020 2020 2020  0.9],..         
-000005e0: 2020 2022 6973 5f76 6172 6961 626c 6522     "is_variable"
-000005f0: 3a20 7472 7565 0d0a 0d0a 2020 2020 2020  : true....      
-00000600: 2020 7d2c 0d0a 2020 2020 2020 2020 226c    },..        "l
-00000610: 6576 656c 3122 3a20 7b0d 0a20 2020 2020  evel1": {..     
-00000620: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000630: 636f 6e73 7461 6e74 222c 0d0a 2020 2020  constant",..    
-00000640: 2020 2020 2020 2020 2276 616c 7565 223a          "value":
-00000650: 2030 2e38 2c0d 0a20 2020 2020 2020 2020   0.8,..         
-00000660: 2020 2022 6973 5f76 6172 6961 626c 6522     "is_variable"
-00000670: 3a20 7472 7565 0d0a 2020 2020 2020 2020  : true..        
-00000680: 7d2c 0d0a 2020 2020 2020 2020 226c 6576  },..        "lev
-00000690: 656c 3222 3a20 7b0d 0a20 2020 2020 2020  el2": {..       
-000006a0: 2020 2020 2022 7479 7065 223a 2022 636f       "type": "co
-000006b0: 6e73 7461 6e74 222c 0d0a 2020 2020 2020  nstant",..      
-000006c0: 2020 2020 2020 2276 616c 7565 223a 2030        "value": 0
-000006d0: 2e35 2c0d 0a20 2020 2020 2020 2020 2020  .5,..           
-000006e0: 2022 6973 5f76 6172 6961 626c 6522 3a20   "is_variable": 
-000006f0: 7472 7565 0d0a 2020 2020 2020 2020 7d2c  true..        },
-00000700: 0d0a 2020 2020 2020 2020 2264 656d 616e  ..        "deman
-00000710: 645f 7361 7669 6e67 5f6c 6576 656c 223a  d_saving_level":
-00000720: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000730: 2263 6f6d 6d65 6e74 223a 2022 5468 6520  "comment": "The 
-00000740: 6465 6d61 6e64 2073 6176 696e 6720 6c65  demand saving le
-00000750: 7665 6c22 2c0d 0a20 2020 2020 2020 2020  vel",..         
-00000760: 2020 2022 7479 7065 223a 2022 636f 6e74     "type": "cont
-00000770: 726f 6c63 7572 7665 696e 6465 7822 2c0d  rolcurveindex",.
-00000780: 0a20 2020 2020 2020 2020 2020 2022 7374  .            "st
-00000790: 6f72 6167 655f 6e6f 6465 223a 2022 5265  orage_node": "Re
-000007a0: 7365 7276 6f69 7222 2c0d 0a20 2020 2020  servoir",..     
-000007b0: 2020 2020 2020 2022 636f 6e74 726f 6c5f         "control_
-000007c0: 6375 7276 6573 223a 205b 0d0a 2020 2020  curves": [..    
-000007d0: 2020 2020 2020 2020 2020 2020 226c 6576              "lev
-000007e0: 656c 3122 2c0d 0a20 2020 2020 2020 2020  el1",..         
-000007f0: 2020 2020 2020 2022 6c65 7665 6c32 220d         "level2".
-00000800: 0a20 2020 2020 2020 2020 2020 205d 0d0a  .            ]..
-00000810: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000820: 2020 2020 2264 656d 616e 645f 7361 7669      "demand_savi
-00000830: 6e67 5f66 6163 746f 7222 3a20 7b0d 0a20  ng_factor": {.. 
-00000840: 2020 2020 2020 2020 2020 2022 636f 6d6d             "comm
-00000850: 656e 7422 3a20 2244 656d 616e 6420 7361  ent": "Demand sa
-00000860: 7669 6e67 2061 7320 6120 6661 6374 6f72  ving as a factor
-00000870: 206f 6620 7468 6520 6261 7365 2064 656d   of the base dem
-00000880: 616e 6422 2c0d 0a20 2020 2020 2020 2020  and",..         
-00000890: 2020 2022 7479 7065 223a 2022 696e 6465     "type": "inde
-000008a0: 7865 6461 7272 6179 222c 0d0a 2020 2020  xedarray",..    
-000008b0: 2020 2020 2020 2020 2269 6e64 6578 5f70          "index_p
-000008c0: 6172 616d 6574 6572 223a 2022 6465 6d61  arameter": "dema
-000008d0: 6e64 5f73 6176 696e 675f 6c65 7665 6c22  nd_saving_level"
-000008e0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000008f0: 7061 7261 6d73 223a 205b 0d0a 2020 2020  params": [..    
-00000900: 2020 2020 2020 2020 2020 2020 7b0d 0a20              {.. 
-00000910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000920: 2020 2022 7479 7065 223a 2022 636f 6e73     "type": "cons
-00000930: 7461 6e74 222c 0d0a 2020 2020 2020 2020  tant",..        
-00000940: 2020 2020 2020 2020 2020 2020 2276 616c              "val
-00000950: 7565 223a 2031 2e30 0d0a 2020 2020 2020  ue": 1.0..      
-00000960: 2020 2020 2020 2020 2020 7d2c 0d0a 2020            },..  
-00000970: 2020 2020 2020 2020 2020 2020 2020 7b0d                {.
-00000980: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000990: 2020 2020 2022 7479 7065 223a 2022 6d6f       "type": "mo
-000009a0: 6e74 686c 7970 726f 6669 6c65 222c 0d0a  nthlyprofile",..
-000009b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000009c0: 2020 2020 2276 616c 7565 7322 3a20 5b30      "values": [0
-000009d0: 2e39 352c 2030 2e39 352c 2030 2e39 352c  .95, 0.95, 0.95,
-000009e0: 2030 2e39 352c 2030 2e39 302c 2030 2e39   0.95, 0.90, 0.9
-000009f0: 302c 2030 2e39 302c 2030 2e39 302c 2030  0, 0.90, 0.90, 0
-00000a00: 2e39 352c 2030 2e39 352c 2030 2e39 352c  .95, 0.95, 0.95,
-00000a10: 2030 2e39 355d 0d0a 2020 2020 2020 2020   0.95]..        
-00000a20: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000a30: 2020 2020 2020 2020 2020 2020 7b0d 0a20              {.. 
-00000a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000a50: 2020 2022 7479 7065 223a 2022 6d6f 6e74     "type": "mont
-00000a60: 686c 7970 726f 6669 6c65 222c 0d0a 2020  hlyprofile",..  
-00000a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000a80: 2020 2276 616c 7565 7322 3a20 5b30 2e35    "values": [0.5
-00000a90: 2c20 302e 352c 2030 2e35 2c20 302e 352c  , 0.5, 0.5, 0.5,
-00000aa0: 2030 2e34 2c20 302e 342c 2030 2e34 2c20   0.4, 0.4, 0.4, 
-00000ab0: 302e 342c 2030 2e35 2c20 302e 352c 2030  0.4, 0.5, 0.5, 0
-00000ac0: 2e35 2c20 302e 355d 0d0a 2020 2020 2020  .5, 0.5]..      
-00000ad0: 2020 2020 2020 2020 2020 7d0d 0a20 2020            }..   
-00000ae0: 2020 2020 2020 2020 205d 0d0a 2020 2020           ]..    
-00000af0: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000b00: 2264 656d 616e 645f 6d61 785f 666c 6f77  "demand_max_flow
-00000b10: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000b20: 2020 2274 7970 6522 3a20 2261 6767 7265    "type": "aggre
-00000b30: 6761 7465 6422 2c0d 0a20 2020 2020 2020  gated",..       
-00000b40: 2020 2020 2022 6167 675f 6675 6e63 223a       "agg_func":
-00000b50: 2022 7072 6f64 7563 7422 2c0d 0a20 2020   "product",..   
-00000b60: 2020 2020 2020 2020 2022 7061 7261 6d65           "parame
-00000b70: 7465 7273 223a 205b 0d0a 2020 2020 2020  ters": [..      
-00000b80: 2020 2020 2020 2020 2020 2264 656d 616e            "deman
-00000b90: 645f 6261 7365 6c69 6e65 222c 0d0a 2020  d_baseline",..  
-00000ba0: 2020 2020 2020 2020 2020 2020 2020 2264                "d
-00000bb0: 656d 616e 645f 7072 6f66 696c 6522 2c0d  emand_profile",.
-00000bc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000bd0: 2022 6465 6d61 6e64 5f73 6176 696e 675f   "demand_saving_
-00000be0: 6661 6374 6f72 220d 0a20 2020 2020 2020  factor"..       
-00000bf0: 2020 2020 205d 0d0a 2020 2020 2020 2020       ]..        
-00000c00: 7d0d 0a20 2020 207d 2c0d 0a20 2020 2022  }..    },..    "
-00000c10: 7265 636f 7264 6572 7322 3a20 7b0d 0a20  recorders": {.. 
-00000c20: 2020 2020 2020 2022 746f 7461 6c5f 6465         "total_de
-00000c30: 6669 6369 7422 3a20 7b0d 0a20 2020 2020  ficit": {..     
-00000c40: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000c50: 746f 7461 6c64 6566 6963 6974 6e6f 6465  totaldeficitnode
-00000c60: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000c70: 226e 6f64 6522 3a20 2244 656d 616e 6422  "node": "Demand"
-00000c80: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000c90: 6973 5f6f 626a 6563 7469 7665 223a 2022  is_objective": "
-00000ca0: 6d69 6e69 6d69 7365 220d 0a20 2020 2020  minimise"..     
-00000cb0: 2020 207d 2c0d 0a20 2020 2020 2020 2022     },..        "
-00000cc0: 6d69 6e5f 766f 6c75 6d65 223a 207b 0d0a  min_volume": {..
-00000cd0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000ce0: 6522 3a20 224d 696e 696d 756d 566f 6c75  e": "MinimumVolu
-00000cf0: 6d65 5374 6f72 6167 6522 2c0d 0a20 2020  meStorage",..   
-00000d00: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
-00000d10: 2022 5265 7365 7276 6f69 7222 2c0d 0a20   "Reservoir",.. 
-00000d20: 2020 2020 2020 2020 2020 2022 636f 6e73             "cons
-00000d30: 7472 6169 6e74 5f6c 6f77 6572 5f62 6f75  traint_lower_bou
-00000d40: 6e64 7322 3a20 3130 300d 0a20 2020 2020  nds": 100..     
-00000d50: 2020 207d 0d0a 2020 2020 7d0d 0a7d 0d0a     }..    }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4465 6d61 6e64 2053 6176  le": "Demand Sav
+00000030: 696e 6722 2c0a 2020 2020 2020 2020 2264  ing",.        "d
+00000040: 6573 6372 6970 7469 6f6e 223a 2022 4465  escription": "De
+00000050: 6d61 6e64 2073 6176 696e 6720 7573 696e  mand saving usin
+00000060: 6720 616e 2049 6e64 6578 6564 4172 7261  g an IndexedArra
+00000070: 7950 6172 616d 6574 6572 2077 6974 6820  yParameter with 
+00000080: 7661 7269 6162 6c65 7320 616e 6420 636f  variables and co
+00000090: 6e73 7472 6169 6e74 7322 2c0a 2020 2020  nstraints",.    
+000000a0: 2020 2020 226d 696e 696d 756d 5f76 6572      "minimum_ver
+000000b0: 7369 6f6e 223a 2022 302e 3122 0a20 2020  sion": "0.1".   
+000000c0: 207d 2c0a 2020 2020 2274 696d 6573 7465   },.    "timeste
+000000d0: 7070 6572 223a 207b 0a20 2020 2020 2020  pper": {.       
+000000e0: 2022 7374 6172 7422 3a20 2232 3031 362d   "start": "2016-
+000000f0: 3031 2d30 3122 2c0a 2020 2020 2020 2020  01-01",.        
+00000100: 2265 6e64 223a 2022 3230 3136 2d31 322d  "end": "2016-12-
+00000110: 3331 222c 0a20 2020 2020 2020 2022 7469  31",.        "ti
+00000120: 6d65 7374 6570 223a 2031 0a20 2020 207d  mestep": 1.    }
+00000130: 2c0a 2020 2020 226e 6f64 6573 223a 205b  ,.    "nodes": [
+00000140: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+00000150: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000160: 6361 7463 686d 656e 7422 2c0a 2020 2020  catchment",.    
+00000170: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
+00000180: 2249 6e66 6c6f 7722 2c0a 2020 2020 2020  "Inflow",.      
+00000190: 2020 2020 2020 2266 6c6f 7722 3a20 302e        "flow": 0.
+000001a0: 300a 2020 2020 2020 2020 7d2c 0a20 2020  0.        },.   
+000001b0: 2020 2020 207b 0a20 2020 2020 2020 2020       {.         
+000001c0: 2020 2022 7479 7065 223a 2022 7265 7365     "type": "rese
+000001d0: 7276 6f69 7222 2c0a 2020 2020 2020 2020  rvoir",.        
+000001e0: 2020 2020 226e 616d 6522 3a20 2252 6573      "name": "Res
+000001f0: 6572 766f 6972 222c 0a20 2020 2020 2020  ervoir",.       
+00000200: 2020 2020 2022 6d61 785f 766f 6c75 6d65       "max_volume
+00000210: 223a 2031 3030 302c 0a20 2020 2020 2020  ": 1000,.       
+00000220: 2020 2020 2022 696e 6974 6961 6c5f 766f       "initial_vo
+00000230: 6c75 6d65 223a 2031 3030 302c 0a20 2020  lume": 1000,.   
+00000240: 2020 2020 2020 2020 2022 706f 7369 7469           "positi
+00000250: 6f6e 223a 207b 0a20 2020 2020 2020 2020  on": {.         
+00000260: 2020 2020 2020 2022 7363 6865 6d61 7469         "schemati
+00000270: 6322 3a20 5b31 2c20 315d 0a20 2020 2020  c": [1, 1].     
+00000280: 2020 2020 2020 207d 0a20 2020 2020 2020         }.       
+00000290: 207d 2c0a 2020 2020 2020 2020 7b0a 2020   },.        {.  
+000002a0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+000002b0: 3a20 226f 7574 7075 7422 2c0a 2020 2020  : "output",.    
+000002c0: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
+000002d0: 2253 7069 6c6c 222c 0a20 2020 2020 2020  "Spill",.       
+000002e0: 2020 2020 2022 636f 7374 223a 2031 300a       "cost": 10.
+000002f0: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+00000300: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000310: 2022 636f 6d6d 656e 7422 3a20 2254 6865   "comment": "The
+00000320: 206f 6e6c 7920 6465 6d61 6e64 2069 6e20   only demand in 
+00000330: 7468 6520 6d6f 6465 6c22 2c0a 2020 2020  the model",.    
+00000340: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000350: 226f 7574 7075 7422 2c0a 2020 2020 2020  "output",.      
+00000360: 2020 2020 2020 226e 616d 6522 3a20 2244        "name": "D
+00000370: 656d 616e 6422 2c0a 2020 2020 2020 2020  emand",.        
+00000380: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
+00000390: 2264 656d 616e 645f 6d61 785f 666c 6f77  "demand_max_flow
+000003a0: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+000003b0: 636f 7374 223a 202d 3530 300a 2020 2020  cost": -500.    
+000003c0: 2020 2020 7d0a 2020 2020 5d2c 0a20 2020      }.    ],.   
+000003d0: 2022 6564 6765 7322 3a20 5b0a 2020 2020   "edges": [.    
+000003e0: 2020 2020 5b22 496e 666c 6f77 222c 2022      ["Inflow", "
+000003f0: 5265 7365 7276 6f69 7222 5d2c 0a20 2020  Reservoir"],.   
+00000400: 2020 2020 205b 2252 6573 6572 766f 6972       ["Reservoir
+00000410: 222c 2022 4465 6d61 6e64 225d 2c0a 2020  ", "Demand"],.  
+00000420: 2020 2020 2020 5b22 5265 7365 7276 6f69        ["Reservoi
+00000430: 7222 2c20 2253 7069 6c6c 225d 0a20 2020  r", "Spill"].   
+00000440: 205d 2c0a 2020 2020 2270 6172 616d 6574   ],.    "paramet
+00000450: 6572 7322 3a20 7b0a 2020 2020 2020 2020  ers": {.        
+00000460: 2264 656d 616e 645f 6261 7365 6c69 6e65  "demand_baseline
+00000470: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
+00000480: 2022 7479 7065 223a 2022 636f 6e73 7461   "type": "consta
+00000490: 6e74 222c 0a20 2020 2020 2020 2020 2020  nt",.           
+000004a0: 2022 7661 6c75 6522 3a20 3530 0a20 2020   "value": 50.   
+000004b0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000004c0: 2264 656d 616e 645f 7072 6f66 696c 6522  "demand_profile"
+000004d0: 3a20 7b0a 2020 2020 2020 2020 2020 2020  : {.            
+000004e0: 2263 6f6d 6d65 6e74 223a 2022 4d6f 6e74  "comment": "Mont
+000004f0: 686c 7920 6465 6d61 6e64 2070 726f 6669  hly demand profi
+00000500: 6c65 2061 7320 6120 6661 6374 6f72 2061  le as a factor a
+00000510: 726f 756e 6420 7468 6520 6d65 616e 2064  round the mean d
+00000520: 656d 616e 6422 2c0a 2020 2020 2020 2020  emand",.        
+00000530: 2020 2020 2274 7970 6522 3a20 226d 6f6e      "type": "mon
+00000540: 7468 6c79 7072 6f66 696c 6522 2c0a 2020  thlyprofile",.  
+00000550: 2020 2020 2020 2020 2020 2276 616c 7565            "value
+00000560: 7322 3a20 5b30 2e39 2c20 302e 392c 2030  s": [0.9, 0.9, 0
+00000570: 2e39 2c20 302e 392c 2031 2e32 2c20 312e  .9, 0.9, 1.2, 1.
+00000580: 322c 2031 2e32 2c20 312e 322c 2030 2e39  2, 1.2, 1.2, 0.9
+00000590: 2c20 302e 392c 2030 2e39 2c20 302e 395d  , 0.9, 0.9, 0.9]
+000005a0: 2c0a 2020 2020 2020 2020 2020 2020 2269  ,.            "i
+000005b0: 735f 7661 7269 6162 6c65 223a 2074 7275  s_variable": tru
+000005c0: 650a 0a20 2020 2020 2020 207d 2c0a 2020  e..        },.  
+000005d0: 2020 2020 2020 226c 6576 656c 3122 3a20        "level1": 
+000005e0: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+000005f0: 7970 6522 3a20 2263 6f6e 7374 616e 7422  ype": "constant"
+00000600: 2c0a 2020 2020 2020 2020 2020 2020 2276  ,.            "v
+00000610: 616c 7565 223a 2030 2e38 2c0a 2020 2020  alue": 0.8,.    
+00000620: 2020 2020 2020 2020 2269 735f 7661 7269          "is_vari
+00000630: 6162 6c65 223a 2074 7275 650a 2020 2020  able": true.    
+00000640: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000650: 6c65 7665 6c32 223a 207b 0a20 2020 2020  level2": {.     
+00000660: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000670: 636f 6e73 7461 6e74 222c 0a20 2020 2020  constant",.     
+00000680: 2020 2020 2020 2022 7661 6c75 6522 3a20         "value": 
+00000690: 302e 352c 0a20 2020 2020 2020 2020 2020  0.5,.           
+000006a0: 2022 6973 5f76 6172 6961 626c 6522 3a20   "is_variable": 
+000006b0: 7472 7565 0a20 2020 2020 2020 207d 2c0a  true.        },.
+000006c0: 2020 2020 2020 2020 2264 656d 616e 645f          "demand_
+000006d0: 7361 7669 6e67 5f6c 6576 656c 223a 207b  saving_level": {
+000006e0: 0a20 2020 2020 2020 2020 2020 2022 636f  .            "co
+000006f0: 6d6d 656e 7422 3a20 2254 6865 2064 656d  mment": "The dem
+00000700: 616e 6420 7361 7669 6e67 206c 6576 656c  and saving level
+00000710: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000720: 7479 7065 223a 2022 636f 6e74 726f 6c63  type": "controlc
+00000730: 7572 7665 696e 6465 7822 2c0a 2020 2020  urveindex",.    
+00000740: 2020 2020 2020 2020 2273 746f 7261 6765          "storage
+00000750: 5f6e 6f64 6522 3a20 2252 6573 6572 766f  _node": "Reservo
+00000760: 6972 222c 0a20 2020 2020 2020 2020 2020  ir",.           
+00000770: 2022 636f 6e74 726f 6c5f 6375 7276 6573   "control_curves
+00000780: 223a 205b 0a20 2020 2020 2020 2020 2020  ": [.           
+00000790: 2020 2020 2022 6c65 7665 6c31 222c 0a20       "level1",. 
+000007a0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+000007b0: 6c65 7665 6c32 220a 2020 2020 2020 2020  level2".        
+000007c0: 2020 2020 5d0a 2020 2020 2020 2020 7d2c      ].        },
+000007d0: 0a20 2020 2020 2020 2022 6465 6d61 6e64  .        "demand
+000007e0: 5f73 6176 696e 675f 6661 6374 6f72 223a  _saving_factor":
+000007f0: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+00000800: 636f 6d6d 656e 7422 3a20 2244 656d 616e  comment": "Deman
+00000810: 6420 7361 7669 6e67 2061 7320 6120 6661  d saving as a fa
+00000820: 6374 6f72 206f 6620 7468 6520 6261 7365  ctor of the base
+00000830: 2064 656d 616e 6422 2c0a 2020 2020 2020   demand",.      
+00000840: 2020 2020 2020 2274 7970 6522 3a20 2269        "type": "i
+00000850: 6e64 6578 6564 6172 7261 7922 2c0a 2020  ndexedarray",.  
+00000860: 2020 2020 2020 2020 2020 2269 6e64 6578            "index
+00000870: 5f70 6172 616d 6574 6572 223a 2022 6465  _parameter": "de
+00000880: 6d61 6e64 5f73 6176 696e 675f 6c65 7665  mand_saving_leve
+00000890: 6c22 2c0a 2020 2020 2020 2020 2020 2020  l",.            
+000008a0: 2270 6172 616d 7322 3a20 5b0a 2020 2020  "params": [.    
+000008b0: 2020 2020 2020 2020 2020 2020 7b0a 2020              {.  
+000008c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000008d0: 2020 2274 7970 6522 3a20 2263 6f6e 7374    "type": "const
+000008e0: 616e 7422 2c0a 2020 2020 2020 2020 2020  ant",.          
+000008f0: 2020 2020 2020 2020 2020 2276 616c 7565            "value
+00000900: 223a 2031 2e30 0a20 2020 2020 2020 2020  ": 1.0.         
+00000910: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+00000920: 2020 2020 2020 2020 2020 7b0a 2020 2020            {.    
+00000930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000940: 2274 7970 6522 3a20 226d 6f6e 7468 6c79  "type": "monthly
+00000950: 7072 6f66 696c 6522 2c0a 2020 2020 2020  profile",.      
+00000960: 2020 2020 2020 2020 2020 2020 2020 2276                "v
+00000970: 616c 7565 7322 3a20 5b30 2e39 352c 2030  alues": [0.95, 0
+00000980: 2e39 352c 2030 2e39 352c 2030 2e39 352c  .95, 0.95, 0.95,
+00000990: 2030 2e39 302c 2030 2e39 302c 2030 2e39   0.90, 0.90, 0.9
+000009a0: 302c 2030 2e39 302c 2030 2e39 352c 2030  0, 0.90, 0.95, 0
+000009b0: 2e39 352c 2030 2e39 352c 2030 2e39 355d  .95, 0.95, 0.95]
+000009c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000009d0: 207d 2c0a 2020 2020 2020 2020 2020 2020   },.            
+000009e0: 2020 2020 7b0a 2020 2020 2020 2020 2020      {.          
+000009f0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000a00: 3a20 226d 6f6e 7468 6c79 7072 6f66 696c  : "monthlyprofil
+00000a10: 6522 2c0a 2020 2020 2020 2020 2020 2020  e",.            
+00000a20: 2020 2020 2020 2020 2276 616c 7565 7322          "values"
+00000a30: 3a20 5b30 2e35 2c20 302e 352c 2030 2e35  : [0.5, 0.5, 0.5
+00000a40: 2c20 302e 352c 2030 2e34 2c20 302e 342c  , 0.5, 0.4, 0.4,
+00000a50: 2030 2e34 2c20 302e 342c 2030 2e35 2c20   0.4, 0.4, 0.5, 
+00000a60: 302e 352c 2030 2e35 2c20 302e 355d 0a20  0.5, 0.5, 0.5]. 
+00000a70: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+00000a80: 0a20 2020 2020 2020 2020 2020 205d 0a20  .            ]. 
+00000a90: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+00000aa0: 2020 2264 656d 616e 645f 6d61 785f 666c    "demand_max_fl
+00000ab0: 6f77 223a 207b 0a20 2020 2020 2020 2020  ow": {.         
+00000ac0: 2020 2022 7479 7065 223a 2022 6167 6772     "type": "aggr
+00000ad0: 6567 6174 6564 222c 0a20 2020 2020 2020  egated",.       
+00000ae0: 2020 2020 2022 6167 675f 6675 6e63 223a       "agg_func":
+00000af0: 2022 7072 6f64 7563 7422 2c0a 2020 2020   "product",.    
+00000b00: 2020 2020 2020 2020 2270 6172 616d 6574          "paramet
+00000b10: 6572 7322 3a20 5b0a 2020 2020 2020 2020  ers": [.        
+00000b20: 2020 2020 2020 2020 2264 656d 616e 645f          "demand_
+00000b30: 6261 7365 6c69 6e65 222c 0a20 2020 2020  baseline",.     
+00000b40: 2020 2020 2020 2020 2020 2022 6465 6d61             "dema
+00000b50: 6e64 5f70 726f 6669 6c65 222c 0a20 2020  nd_profile",.   
+00000b60: 2020 2020 2020 2020 2020 2020 2022 6465               "de
+00000b70: 6d61 6e64 5f73 6176 696e 675f 6661 6374  mand_saving_fact
+00000b80: 6f72 220a 2020 2020 2020 2020 2020 2020  or".            
+00000b90: 5d0a 2020 2020 2020 2020 7d0a 2020 2020  ].        }.    
+00000ba0: 7d2c 0a20 2020 2022 7265 636f 7264 6572  },.    "recorder
+00000bb0: 7322 3a20 7b0a 2020 2020 2020 2020 2274  s": {.        "t
+00000bc0: 6f74 616c 5f64 6566 6963 6974 223a 207b  otal_deficit": {
+00000bd0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000be0: 7065 223a 2022 746f 7461 6c64 6566 6963  pe": "totaldefic
+00000bf0: 6974 6e6f 6465 222c 0a20 2020 2020 2020  itnode",.       
+00000c00: 2020 2020 2022 6e6f 6465 223a 2022 4465       "node": "De
+00000c10: 6d61 6e64 222c 0a20 2020 2020 2020 2020  mand",.         
+00000c20: 2020 2022 6973 5f6f 626a 6563 7469 7665     "is_objective
+00000c30: 223a 2022 6d69 6e69 6d69 7365 220a 2020  ": "minimise".  
+00000c40: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000c50: 2022 6d69 6e5f 766f 6c75 6d65 223a 207b   "min_volume": {
+00000c60: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000c70: 7065 223a 2022 4d69 6e69 6d75 6d56 6f6c  pe": "MinimumVol
+00000c80: 756d 6553 746f 7261 6765 222c 0a20 2020  umeStorage",.   
+00000c90: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
+00000ca0: 2022 5265 7365 7276 6f69 7222 2c0a 2020   "Reservoir",.  
+00000cb0: 2020 2020 2020 2020 2020 2263 6f6e 7374            "const
+00000cc0: 7261 696e 745f 6c6f 7765 725f 626f 756e  raint_lower_boun
+00000cd0: 6473 223a 2031 3030 0a20 2020 2020 2020  ds": 100.       
+00000ce0: 207d 0a20 2020 207d 0a7d 0a               }.    }.}.
```

### Comparing `pywr-1.8.0/tests/models/demand_saving_level.h5` & `pywr-1.9.0/tests/models/demand_saving_level.h5`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/demand_saving_with_tables_recorder.json` & `pywr-1.9.0/tests/models/demand_saving_with_tables_recorder.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,274 +1,264 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4465 6d61 6e64 2053  itle": "Demand S
-00000030: 6176 696e 6722 2c0d 0a20 2020 2020 2020  aving",..       
-00000040: 2022 6465 7363 7269 7074 696f 6e22 3a20   "description": 
-00000050: 2244 656d 616e 6420 7361 7669 6e67 2075  "Demand saving u
-00000060: 7369 6e67 2061 6e20 496e 6465 7865 6441  sing an IndexedA
-00000070: 7272 6179 5061 7261 6d65 7465 7222 2c0d  rrayParameter",.
-00000080: 0a20 2020 2020 2020 2022 6d69 6e69 6d75  .        "minimu
-00000090: 6d5f 7665 7273 696f 6e22 3a20 2230 2e31  m_version": "0.1
-000000a0: 220d 0a20 2020 207d 2c0d 0a20 2020 2022  "..    },..    "
-000000b0: 7469 6d65 7374 6570 7065 7222 3a20 7b0d  timestepper": {.
-000000c0: 0a20 2020 2020 2020 2022 7374 6172 7422  .        "start"
-000000d0: 3a20 2232 3031 362d 3031 2d30 3122 2c0d  : "2016-01-01",.
-000000e0: 0a20 2020 2020 2020 2022 656e 6422 3a20  .        "end": 
-000000f0: 2232 3031 362d 3132 2d33 3122 2c0d 0a20  "2016-12-31",.. 
-00000100: 2020 2020 2020 2022 7469 6d65 7374 6570         "timestep
-00000110: 223a 2031 0d0a 2020 2020 7d2c 0d0a 2020  ": 1..    },..  
-00000120: 2020 226e 6f64 6573 223a 205b 0d0a 2020    "nodes": [..  
-00000130: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000140: 2020 2020 2022 7479 7065 223a 2022 6361       "type": "ca
-00000150: 7463 686d 656e 7422 2c0d 0a20 2020 2020  tchment",..     
-00000160: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000170: 496e 666c 6f77 222c 0d0a 2020 2020 2020  Inflow",..      
-00000180: 2020 2020 2020 2266 6c6f 7722 3a20 302e        "flow": 0.
-00000190: 300d 0a20 2020 2020 2020 207d 2c0d 0a20  0..        },.. 
-000001a0: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-000001b0: 2020 2020 2020 2274 7970 6522 3a20 2272        "type": "r
-000001c0: 6573 6572 766f 6972 222c 0d0a 2020 2020  eservoir",..    
-000001d0: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-000001e0: 2252 6573 6572 766f 6972 222c 0d0a 2020  "Reservoir",..  
-000001f0: 2020 2020 2020 2020 2020 226d 6178 5f76            "max_v
-00000200: 6f6c 756d 6522 3a20 3130 3030 2c0d 0a20  olume": 1000,.. 
-00000210: 2020 2020 2020 2020 2020 2022 696e 6974             "init
-00000220: 6961 6c5f 766f 6c75 6d65 223a 2031 3030  ial_volume": 100
-00000230: 300d 0a20 2020 2020 2020 207d 2c0d 0a20  0..        },.. 
-00000240: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000250: 2020 2020 2020 2274 7970 6522 3a20 226f        "type": "o
-00000260: 7574 7075 7422 2c0d 0a20 2020 2020 2020  utput",..       
-00000270: 2020 2020 2022 6e61 6d65 223a 2022 5370       "name": "Sp
-00000280: 696c 6c22 2c0d 0a20 2020 2020 2020 2020  ill",..         
-00000290: 2020 2022 636f 7374 223a 2031 300d 0a20     "cost": 10.. 
-000002a0: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-000002b0: 2020 207b 0d0a 2020 2020 2020 2020 2020     {..          
-000002c0: 2020 2263 6f6d 6d65 6e74 223a 2022 5468    "comment": "Th
-000002d0: 6520 6f6e 6c79 2064 656d 616e 6420 696e  e only demand in
-000002e0: 2074 6865 206d 6f64 656c 222c 0d0a 2020   the model",..  
-000002f0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000300: 3a20 226f 7574 7075 7422 2c0d 0a20 2020  : "output",..   
-00000310: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000320: 2022 4465 6d61 6e64 222c 0d0a 2020 2020   "Demand",..    
-00000330: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
-00000340: 7722 3a20 2264 656d 616e 645f 6d61 785f  w": "demand_max_
-00000350: 666c 6f77 222c 0d0a 2020 2020 2020 2020  flow",..        
-00000360: 2020 2020 2263 6f73 7422 3a20 2d35 3030      "cost": -500
-00000370: 0d0a 2020 2020 2020 2020 7d0d 0a20 2020  ..        }..   
-00000380: 205d 2c0d 0a20 2020 2022 6564 6765 7322   ],..    "edges"
-00000390: 3a20 5b0d 0a20 2020 2020 2020 205b 2249  : [..        ["I
-000003a0: 6e66 6c6f 7722 2c20 2252 6573 6572 766f  nflow", "Reservo
-000003b0: 6972 225d 2c0d 0a20 2020 2020 2020 205b  ir"],..        [
-000003c0: 2252 6573 6572 766f 6972 222c 2022 4465  "Reservoir", "De
-000003d0: 6d61 6e64 225d 2c0d 0a20 2020 2020 2020  mand"],..       
-000003e0: 205b 2252 6573 6572 766f 6972 222c 2022   ["Reservoir", "
-000003f0: 5370 696c 6c22 5d0d 0a20 2020 205d 2c0d  Spill"]..    ],.
-00000400: 0a20 2020 2022 7061 7261 6d65 7465 7273  .    "parameters
-00000410: 223a 207b 0d0a 2020 2020 2020 2020 2264  ": {..        "d
-00000420: 656d 616e 645f 6261 7365 6c69 6e65 223a  emand_baseline":
-00000430: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000440: 2274 7970 6522 3a20 2263 6f6e 7374 616e  "type": "constan
-00000450: 7422 2c0d 0a20 2020 2020 2020 2020 2020  t",..           
-00000460: 2022 7661 6c75 6522 3a20 3530 0d0a 2020   "value": 50..  
-00000470: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000480: 2020 2264 656d 616e 645f 7072 6f66 696c    "demand_profil
-00000490: 6522 3a20 7b0d 0a20 2020 2020 2020 2020  e": {..         
-000004a0: 2020 2022 636f 6d6d 656e 7422 3a20 224d     "comment": "M
-000004b0: 6f6e 7468 6c79 2064 656d 616e 6420 7072  onthly demand pr
-000004c0: 6f66 696c 6520 6173 2061 2066 6163 746f  ofile as a facto
-000004d0: 7220 6172 6f75 6e64 2074 6865 206d 6561  r around the mea
-000004e0: 6e20 6465 6d61 6e64 222c 0d0a 2020 2020  n demand",..    
-000004f0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000500: 226d 6f6e 7468 6c79 7072 6f66 696c 6522  "monthlyprofile"
-00000510: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000520: 7661 6c75 6573 223a 205b 0d0a 2020 2020  values": [..    
-00000530: 2020 2020 2020 2020 2020 2020 302e 392c              0.9,
-00000540: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000550: 2020 302e 392c 0d0a 2020 2020 2020 2020    0.9,..        
-00000560: 2020 2020 2020 2020 302e 392c 0d0a 2020          0.9,..  
-00000570: 2020 2020 2020 2020 2020 2020 2020 302e                0.
-00000580: 392c 0d0a 2020 2020 2020 2020 2020 2020  9,..            
-00000590: 2020 2020 312e 322c 0d0a 2020 2020 2020      1.2,..      
-000005a0: 2020 2020 2020 2020 2020 312e 322c 0d0a            1.2,..
-000005b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005c0: 312e 322c 0d0a 2020 2020 2020 2020 2020  1.2,..          
-000005d0: 2020 2020 2020 312e 322c 0d0a 2020 2020        1.2,..    
-000005e0: 2020 2020 2020 2020 2020 2020 302e 392c              0.9,
-000005f0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000600: 2020 302e 392c 0d0a 2020 2020 2020 2020    0.9,..        
-00000610: 2020 2020 2020 2020 302e 392c 0d0a 2020          0.9,..  
-00000620: 2020 2020 2020 2020 2020 2020 2020 302e                0.
-00000630: 390d 0a20 2020 2020 2020 2020 2020 205d  9..            ]
-00000640: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-00000650: 2020 2020 2020 226c 6576 656c 3122 3a20        "level1": 
-00000660: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000670: 7479 7065 223a 2022 636f 6e73 7461 6e74  type": "constant
-00000680: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000690: 2276 616c 7565 223a 2030 2e38 0d0a 2020  "value": 0.8..  
-000006a0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000006b0: 2020 226c 6576 656c 3222 3a20 7b0d 0a20    "level2": {.. 
-000006c0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-000006d0: 223a 2022 636f 6e73 7461 6e74 222c 0d0a  ": "constant",..
-000006e0: 2020 2020 2020 2020 2020 2020 2276 616c              "val
-000006f0: 7565 223a 2030 2e35 0d0a 2020 2020 2020  ue": 0.5..      
-00000700: 2020 7d2c 0d0a 2020 2020 2020 2020 2264    },..        "d
-00000710: 656d 616e 645f 7361 7669 6e67 5f6c 6576  emand_saving_lev
-00000720: 656c 223a 207b 0d0a 2020 2020 2020 2020  el": {..        
-00000730: 2020 2020 2263 6f6d 6d65 6e74 223a 2022      "comment": "
-00000740: 5468 6520 6465 6d61 6e64 2073 6176 696e  The demand savin
-00000750: 6720 6c65 7665 6c22 2c0d 0a20 2020 2020  g level",..     
-00000760: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000770: 636f 6e74 726f 6c63 7572 7665 696e 6465  controlcurveinde
-00000780: 7822 2c0d 0a20 2020 2020 2020 2020 2020  x",..           
-00000790: 2022 7374 6f72 6167 655f 6e6f 6465 223a   "storage_node":
-000007a0: 2022 5265 7365 7276 6f69 7222 2c0d 0a20   "Reservoir",.. 
-000007b0: 2020 2020 2020 2020 2020 2022 636f 6e74             "cont
-000007c0: 726f 6c5f 6375 7276 6573 223a 205b 0d0a  rol_curves": [..
-000007d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000007e0: 226c 6576 656c 3122 2c0d 0a20 2020 2020  "level1",..     
-000007f0: 2020 2020 2020 2020 2020 2022 6c65 7665             "leve
-00000800: 6c32 220d 0a20 2020 2020 2020 2020 2020  l2"..           
-00000810: 205d 0d0a 2020 2020 2020 2020 7d2c 0d0a   ]..        },..
-00000820: 2020 2020 2020 2020 2264 656d 616e 645f          "demand_
-00000830: 7361 7669 6e67 5f66 6163 746f 7222 3a20  saving_factor": 
-00000840: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000850: 636f 6d6d 656e 7422 3a20 2244 656d 616e  comment": "Deman
-00000860: 6420 7361 7669 6e67 2061 7320 6120 6661  d saving as a fa
-00000870: 6374 6f72 206f 6620 7468 6520 6261 7365  ctor of the base
-00000880: 2064 656d 616e 6422 2c0d 0a20 2020 2020   demand",..     
-00000890: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000008a0: 696e 6465 7865 6461 7272 6179 222c 0d0a  indexedarray",..
-000008b0: 2020 2020 2020 2020 2020 2020 2269 6e64              "ind
-000008c0: 6578 5f70 6172 616d 6574 6572 223a 2022  ex_parameter": "
-000008d0: 6465 6d61 6e64 5f73 6176 696e 675f 6c65  demand_saving_le
-000008e0: 7665 6c22 2c0d 0a20 2020 2020 2020 2020  vel",..         
-000008f0: 2020 2022 7061 7261 6d73 223a 205b 0d0a     "params": [..
-00000900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000910: 7b0d 0a20 2020 2020 2020 2020 2020 2020  {..             
-00000920: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000930: 636f 6e73 7461 6e74 222c 0d0a 2020 2020  constant",..    
-00000940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000950: 2276 616c 7565 223a 2031 2e30 0d0a 2020  "value": 1.0..  
-00000960: 2020 2020 2020 2020 2020 2020 2020 7d2c                },
-00000970: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000980: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-00000990: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-000009a0: 2022 6d6f 6e74 686c 7970 726f 6669 6c65   "monthlyprofile
-000009b0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000009c0: 2020 2020 2020 2020 2276 616c 7565 7322          "values"
-000009d0: 3a20 5b0d 0a20 2020 2020 2020 2020 2020  : [..           
-000009e0: 2020 2020 2020 2020 2020 2020 2030 2e39               0.9
-000009f0: 352c 0d0a 2020 2020 2020 2020 2020 2020  5,..            
-00000a00: 2020 2020 2020 2020 2020 2020 302e 3935              0.95
-00000a10: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-00000a20: 2020 2020 2020 2020 2020 2030 2e39 352c             0.95,
-00000a30: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000a40: 2020 2020 2020 2020 2020 302e 3935 2c0d            0.95,.
-00000a50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000a60: 2020 2020 2020 2020 2030 2e39 302c 0d0a           0.90,..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4465 6d61 6e64 2053 6176  le": "Demand Sav
+00000030: 696e 6722 2c0a 2020 2020 2020 2020 2264  ing",.        "d
+00000040: 6573 6372 6970 7469 6f6e 223a 2022 4465  escription": "De
+00000050: 6d61 6e64 2073 6176 696e 6720 7573 696e  mand saving usin
+00000060: 6720 616e 2049 6e64 6578 6564 4172 7261  g an IndexedArra
+00000070: 7950 6172 616d 6574 6572 222c 0a20 2020  yParameter",.   
+00000080: 2020 2020 2022 6d69 6e69 6d75 6d5f 7665       "minimum_ve
+00000090: 7273 696f 6e22 3a20 2230 2e31 220a 2020  rsion": "0.1".  
+000000a0: 2020 7d2c 0a20 2020 2022 7469 6d65 7374    },.    "timest
+000000b0: 6570 7065 7222 3a20 7b0a 2020 2020 2020  epper": {.      
+000000c0: 2020 2273 7461 7274 223a 2022 3230 3136    "start": "2016
+000000d0: 2d30 312d 3031 222c 0a20 2020 2020 2020  -01-01",.       
+000000e0: 2022 656e 6422 3a20 2232 3031 362d 3132   "end": "2016-12
+000000f0: 2d33 3122 2c0a 2020 2020 2020 2020 2274  -31",.        "t
+00000100: 696d 6573 7465 7022 3a20 310a 2020 2020  imestep": 1.    
+00000110: 7d2c 0a20 2020 2022 6e6f 6465 7322 3a20  },.    "nodes": 
+00000120: 5b0a 2020 2020 2020 2020 7b0a 2020 2020  [.        {.    
+00000130: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000140: 2263 6174 6368 6d65 6e74 222c 0a20 2020  "catchment",.   
+00000150: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000160: 2022 496e 666c 6f77 222c 0a20 2020 2020   "Inflow",.     
+00000170: 2020 2020 2020 2022 666c 6f77 223a 2030         "flow": 0
+00000180: 2e30 0a20 2020 2020 2020 207d 2c0a 2020  .0.        },.  
+00000190: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+000001a0: 2020 2020 2274 7970 6522 3a20 2272 6573      "type": "res
+000001b0: 6572 766f 6972 222c 0a20 2020 2020 2020  ervoir",.       
+000001c0: 2020 2020 2022 6e61 6d65 223a 2022 5265       "name": "Re
+000001d0: 7365 7276 6f69 7222 2c0a 2020 2020 2020  servoir",.      
+000001e0: 2020 2020 2020 226d 6178 5f76 6f6c 756d        "max_volum
+000001f0: 6522 3a20 3130 3030 2c0a 2020 2020 2020  e": 1000,.      
+00000200: 2020 2020 2020 2269 6e69 7469 616c 5f76        "initial_v
+00000210: 6f6c 756d 6522 3a20 3130 3030 0a20 2020  olume": 1000.   
+00000220: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000230: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000240: 7970 6522 3a20 226f 7574 7075 7422 2c0a  ype": "output",.
+00000250: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000260: 6522 3a20 2253 7069 6c6c 222c 0a20 2020  e": "Spill",.   
+00000270: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
+00000280: 2031 300a 2020 2020 2020 2020 7d2c 0a20   10.        },. 
+00000290: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+000002a0: 2020 2020 2022 636f 6d6d 656e 7422 3a20       "comment": 
+000002b0: 2254 6865 206f 6e6c 7920 6465 6d61 6e64  "The only demand
+000002c0: 2069 6e20 7468 6520 6d6f 6465 6c22 2c0a   in the model",.
+000002d0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000002e0: 6522 3a20 226f 7574 7075 7422 2c0a 2020  e": "output",.  
+000002f0: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
+00000300: 3a20 2244 656d 616e 6422 2c0a 2020 2020  : "Demand",.    
+00000310: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
+00000320: 7722 3a20 2264 656d 616e 645f 6d61 785f  w": "demand_max_
+00000330: 666c 6f77 222c 0a20 2020 2020 2020 2020  flow",.         
+00000340: 2020 2022 636f 7374 223a 202d 3530 300a     "cost": -500.
+00000350: 2020 2020 2020 2020 7d0a 2020 2020 5d2c          }.    ],
+00000360: 0a20 2020 2022 6564 6765 7322 3a20 5b0a  .    "edges": [.
+00000370: 2020 2020 2020 2020 5b22 496e 666c 6f77          ["Inflow
+00000380: 222c 2022 5265 7365 7276 6f69 7222 5d2c  ", "Reservoir"],
+00000390: 0a20 2020 2020 2020 205b 2252 6573 6572  .        ["Reser
+000003a0: 766f 6972 222c 2022 4465 6d61 6e64 225d  voir", "Demand"]
+000003b0: 2c0a 2020 2020 2020 2020 5b22 5265 7365  ,.        ["Rese
+000003c0: 7276 6f69 7222 2c20 2253 7069 6c6c 225d  rvoir", "Spill"]
+000003d0: 0a20 2020 205d 2c0a 2020 2020 2270 6172  .    ],.    "par
+000003e0: 616d 6574 6572 7322 3a20 7b0a 2020 2020  ameters": {.    
+000003f0: 2020 2020 2264 656d 616e 645f 6261 7365      "demand_base
+00000400: 6c69 6e65 223a 207b 0a20 2020 2020 2020  line": {.       
+00000410: 2020 2020 2022 7479 7065 223a 2022 636f       "type": "co
+00000420: 6e73 7461 6e74 222c 0a20 2020 2020 2020  nstant",.       
+00000430: 2020 2020 2022 7661 6c75 6522 3a20 3530       "value": 50
+00000440: 0a20 2020 2020 2020 207d 2c0a 2020 2020  .        },.    
+00000450: 2020 2020 2264 656d 616e 645f 7072 6f66      "demand_prof
+00000460: 696c 6522 3a20 7b0a 2020 2020 2020 2020  ile": {.        
+00000470: 2020 2020 2263 6f6d 6d65 6e74 223a 2022      "comment": "
+00000480: 4d6f 6e74 686c 7920 6465 6d61 6e64 2070  Monthly demand p
+00000490: 726f 6669 6c65 2061 7320 6120 6661 6374  rofile as a fact
+000004a0: 6f72 2061 726f 756e 6420 7468 6520 6d65  or around the me
+000004b0: 616e 2064 656d 616e 6422 2c0a 2020 2020  an demand",.    
+000004c0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+000004d0: 226d 6f6e 7468 6c79 7072 6f66 696c 6522  "monthlyprofile"
+000004e0: 2c0a 2020 2020 2020 2020 2020 2020 2276  ,.            "v
+000004f0: 616c 7565 7322 3a20 5b0a 2020 2020 2020  alues": [.      
+00000500: 2020 2020 2020 2020 2020 302e 392c 0a20            0.9,. 
+00000510: 2020 2020 2020 2020 2020 2020 2020 2030                 0
+00000520: 2e39 2c0a 2020 2020 2020 2020 2020 2020  .9,.            
+00000530: 2020 2020 302e 392c 0a20 2020 2020 2020      0.9,.       
+00000540: 2020 2020 2020 2020 2030 2e39 2c0a 2020           0.9,.  
+00000550: 2020 2020 2020 2020 2020 2020 2020 312e                1.
+00000560: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
+00000570: 2020 2031 2e32 2c0a 2020 2020 2020 2020     1.2,.        
+00000580: 2020 2020 2020 2020 312e 322c 0a20 2020          1.2,.   
+00000590: 2020 2020 2020 2020 2020 2020 2031 2e32               1.2
+000005a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000005b0: 2020 302e 392c 0a20 2020 2020 2020 2020    0.9,.         
+000005c0: 2020 2020 2020 2030 2e39 2c0a 2020 2020         0.9,.    
+000005d0: 2020 2020 2020 2020 2020 2020 302e 392c              0.9,
+000005e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000005f0: 2030 2e39 0a20 2020 2020 2020 2020 2020   0.9.           
+00000600: 205d 0a20 2020 2020 2020 207d 2c0a 2020   ].        },.  
+00000610: 2020 2020 2020 226c 6576 656c 3122 3a20        "level1": 
+00000620: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000630: 7970 6522 3a20 2263 6f6e 7374 616e 7422  ype": "constant"
+00000640: 2c0a 2020 2020 2020 2020 2020 2020 2276  ,.            "v
+00000650: 616c 7565 223a 2030 2e38 0a20 2020 2020  alue": 0.8.     
+00000660: 2020 207d 2c0a 2020 2020 2020 2020 226c     },.        "l
+00000670: 6576 656c 3222 3a20 7b0a 2020 2020 2020  evel2": {.      
+00000680: 2020 2020 2020 2274 7970 6522 3a20 2263        "type": "c
+00000690: 6f6e 7374 616e 7422 2c0a 2020 2020 2020  onstant",.      
+000006a0: 2020 2020 2020 2276 616c 7565 223a 2030        "value": 0
+000006b0: 2e35 0a20 2020 2020 2020 207d 2c0a 2020  .5.        },.  
+000006c0: 2020 2020 2020 2264 656d 616e 645f 7361        "demand_sa
+000006d0: 7669 6e67 5f6c 6576 656c 223a 207b 0a20  ving_level": {. 
+000006e0: 2020 2020 2020 2020 2020 2022 636f 6d6d             "comm
+000006f0: 656e 7422 3a20 2254 6865 2064 656d 616e  ent": "The deman
+00000700: 6420 7361 7669 6e67 206c 6576 656c 222c  d saving level",
+00000710: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000720: 7065 223a 2022 636f 6e74 726f 6c63 7572  pe": "controlcur
+00000730: 7665 696e 6465 7822 2c0a 2020 2020 2020  veindex",.      
+00000740: 2020 2020 2020 2273 746f 7261 6765 5f6e        "storage_n
+00000750: 6f64 6522 3a20 2252 6573 6572 766f 6972  ode": "Reservoir
+00000760: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000770: 636f 6e74 726f 6c5f 6375 7276 6573 223a  control_curves":
+00000780: 205b 0a20 2020 2020 2020 2020 2020 2020   [.             
+00000790: 2020 2022 6c65 7665 6c31 222c 0a20 2020     "level1",.   
+000007a0: 2020 2020 2020 2020 2020 2020 2022 6c65               "le
+000007b0: 7665 6c32 220a 2020 2020 2020 2020 2020  vel2".          
+000007c0: 2020 5d0a 2020 2020 2020 2020 7d2c 0a20    ].        },. 
+000007d0: 2020 2020 2020 2022 6465 6d61 6e64 5f73         "demand_s
+000007e0: 6176 696e 675f 6661 6374 6f72 223a 207b  aving_factor": {
+000007f0: 0a20 2020 2020 2020 2020 2020 2022 636f  .            "co
+00000800: 6d6d 656e 7422 3a20 2244 656d 616e 6420  mment": "Demand 
+00000810: 7361 7669 6e67 2061 7320 6120 6661 6374  saving as a fact
+00000820: 6f72 206f 6620 7468 6520 6261 7365 2064  or of the base d
+00000830: 656d 616e 6422 2c0a 2020 2020 2020 2020  emand",.        
+00000840: 2020 2020 2274 7970 6522 3a20 2269 6e64      "type": "ind
+00000850: 6578 6564 6172 7261 7922 2c0a 2020 2020  exedarray",.    
+00000860: 2020 2020 2020 2020 2269 6e64 6578 5f70          "index_p
+00000870: 6172 616d 6574 6572 223a 2022 6465 6d61  arameter": "dema
+00000880: 6e64 5f73 6176 696e 675f 6c65 7665 6c22  nd_saving_level"
+00000890: 2c0a 2020 2020 2020 2020 2020 2020 2270  ,.            "p
+000008a0: 6172 616d 7322 3a20 5b0a 2020 2020 2020  arams": [.      
+000008b0: 2020 2020 2020 2020 2020 7b0a 2020 2020            {.    
+000008c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000008d0: 2274 7970 6522 3a20 2263 6f6e 7374 616e  "type": "constan
+000008e0: 7422 2c0a 2020 2020 2020 2020 2020 2020  t",.            
+000008f0: 2020 2020 2020 2020 2276 616c 7565 223a          "value":
+00000900: 2031 2e30 0a20 2020 2020 2020 2020 2020   1.0.           
+00000910: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000920: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000930: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+00000940: 7970 6522 3a20 226d 6f6e 7468 6c79 7072  ype": "monthlypr
+00000950: 6f66 696c 6522 2c0a 2020 2020 2020 2020  ofile",.        
+00000960: 2020 2020 2020 2020 2020 2020 2276 616c              "val
+00000970: 7565 7322 3a20 5b0a 2020 2020 2020 2020  ues": [.        
+00000980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000990: 302e 3935 2c0a 2020 2020 2020 2020 2020  0.95,.          
+000009a0: 2020 2020 2020 2020 2020 2020 2020 302e                0.
+000009b0: 3935 2c0a 2020 2020 2020 2020 2020 2020  95,.            
+000009c0: 2020 2020 2020 2020 2020 2020 302e 3935              0.95
+000009d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000009e0: 2020 2020 2020 2020 2020 302e 3935 2c0a            0.95,.
+000009f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000a00: 2020 2020 2020 2020 302e 3930 2c0a 2020          0.90,.  
+00000a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000a20: 2020 2020 2020 302e 3930 2c0a 2020 2020        0.90,.    
+00000a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000a40: 2020 2020 302e 3930 2c0a 2020 2020 2020      0.90,.      
+00000a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000a60: 2020 302e 3930 2c0a 2020 2020 2020 2020    0.90,.        
 00000a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000a80: 2020 2020 2020 2020 302e 3930 2c0d 0a20          0.90,.. 
-00000a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000aa0: 2020 2020 2020 2030 2e39 302c 0d0a 2020         0.90,..  
-00000ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ac0: 2020 2020 2020 302e 3930 2c0d 0a20 2020        0.90,..   
-00000ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ae0: 2020 2020 2030 2e39 352c 0d0a 2020 2020       0.95,..    
-00000af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b00: 2020 2020 302e 3935 2c0d 0a20 2020 2020      0.95,..     
-00000b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b20: 2020 2030 2e39 352c 0d0a 2020 2020 2020     0.95,..      
-00000b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b40: 2020 302e 3935 0d0a 2020 2020 2020 2020    0.95..        
-00000b50: 2020 2020 2020 2020 2020 2020 5d0d 0a20              ].. 
-00000b60: 2020 2020 2020 2020 2020 2020 2020 207d                 }
-00000b70: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-00000b80: 2020 207b 0d0a 2020 2020 2020 2020 2020     {..          
-00000b90: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000ba0: 3a20 226d 6f6e 7468 6c79 7072 6f66 696c  : "monthlyprofil
-00000bb0: 6522 2c0d 0a20 2020 2020 2020 2020 2020  e",..           
-00000bc0: 2020 2020 2020 2020 2022 7661 6c75 6573           "values
-00000bd0: 223a 205b 0d0a 2020 2020 2020 2020 2020  ": [..          
-00000be0: 2020 2020 2020 2020 2020 2020 2020 302e                0.
-00000bf0: 352c 0d0a 2020 2020 2020 2020 2020 2020  5,..            
-00000c00: 2020 2020 2020 2020 2020 2020 302e 352c              0.5,
-00000c10: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000c20: 2020 2020 2020 2020 2020 302e 352c 0d0a            0.5,..
-00000c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c40: 2020 2020 2020 2020 302e 352c 0d0a 2020          0.5,..  
+00000a80: 302e 3935 2c0a 2020 2020 2020 2020 2020  0.95,.          
+00000a90: 2020 2020 2020 2020 2020 2020 2020 302e                0.
+00000aa0: 3935 2c0a 2020 2020 2020 2020 2020 2020  95,.            
+00000ab0: 2020 2020 2020 2020 2020 2020 302e 3935              0.95
+00000ac0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00000ad0: 2020 2020 2020 2020 2020 302e 3935 0a20            0.95. 
+00000ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000af0: 2020 205d 0a20 2020 2020 2020 2020 2020     ].           
+00000b00: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000b10: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000b20: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+00000b30: 7970 6522 3a20 226d 6f6e 7468 6c79 7072  ype": "monthlypr
+00000b40: 6f66 696c 6522 2c0a 2020 2020 2020 2020  ofile",.        
+00000b50: 2020 2020 2020 2020 2020 2020 2276 616c              "val
+00000b60: 7565 7322 3a20 5b0a 2020 2020 2020 2020  ues": [.        
+00000b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000b80: 302e 352c 0a20 2020 2020 2020 2020 2020  0.5,.           
+00000b90: 2020 2020 2020 2020 2020 2020 2030 2e35               0.5
+00000ba0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00000bb0: 2020 2020 2020 2020 2020 302e 352c 0a20            0.5,. 
+00000bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000bd0: 2020 2020 2020 2030 2e35 2c0a 2020 2020         0.5,.    
+00000be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000bf0: 2020 2020 302e 342c 0a20 2020 2020 2020      0.4,.       
+00000c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000c10: 2030 2e34 2c0a 2020 2020 2020 2020 2020   0.4,.          
+00000c20: 2020 2020 2020 2020 2020 2020 2020 302e                0.
+00000c30: 342c 0a20 2020 2020 2020 2020 2020 2020  4,.             
+00000c40: 2020 2020 2020 2020 2020 2030 2e34 2c0a             0.4,.
 00000c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c60: 2020 2020 2020 302e 342c 0d0a 2020 2020        0.4,..    
+00000c60: 2020 2020 2020 2020 302e 352c 0a20 2020          0.5,.   
 00000c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c80: 2020 2020 302e 342c 0d0a 2020 2020 2020      0.4,..      
+00000c80: 2020 2020 2030 2e35 2c0a 2020 2020 2020       0.5,.      
 00000c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ca0: 2020 302e 342c 0d0a 2020 2020 2020 2020    0.4,..        
-00000cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000cc0: 302e 342c 0d0a 2020 2020 2020 2020 2020  0.4,..          
-00000cd0: 2020 2020 2020 2020 2020 2020 2020 302e                0.
-00000ce0: 352c 0d0a 2020 2020 2020 2020 2020 2020  5,..            
-00000cf0: 2020 2020 2020 2020 2020 2020 302e 352c              0.5,
-00000d00: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000d10: 2020 2020 2020 2020 2020 302e 352c 0d0a            0.5,..
-00000d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d30: 2020 2020 2020 2020 302e 350d 0a20 2020          0.5..   
-00000d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d50: 205d 0d0a 2020 2020 2020 2020 2020 2020   ]..            
-00000d60: 2020 2020 7d0d 0a20 2020 2020 2020 2020      }..         
-00000d70: 2020 205d 0d0a 2020 2020 2020 2020 7d2c     ]..        },
-00000d80: 0d0a 2020 2020 2020 2020 2264 656d 616e  ..        "deman
-00000d90: 645f 6d61 785f 666c 6f77 223a 207b 0d0a  d_max_flow": {..
-00000da0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000db0: 6522 3a20 2261 6767 7265 6761 7465 6422  e": "aggregated"
-00000dc0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000dd0: 6167 675f 6675 6e63 223a 2022 7072 6f64  agg_func": "prod
-00000de0: 7563 7422 2c0d 0a20 2020 2020 2020 2020  uct",..         
-00000df0: 2020 2022 7061 7261 6d65 7465 7273 223a     "parameters":
-00000e00: 205b 0d0a 2020 2020 2020 2020 2020 2020   [..            
-00000e10: 2020 2020 2264 656d 616e 645f 6261 7365      "demand_base
-00000e20: 6c69 6e65 222c 0d0a 2020 2020 2020 2020  line",..        
-00000e30: 2020 2020 2020 2020 2264 656d 616e 645f          "demand_
-00000e40: 7072 6f66 696c 6522 2c0d 0a20 2020 2020  profile",..     
-00000e50: 2020 2020 2020 2020 2020 2022 6465 6d61             "dema
-00000e60: 6e64 5f73 6176 696e 675f 6661 6374 6f72  nd_saving_factor
-00000e70: 220d 0a20 2020 2020 2020 2020 2020 205d  "..            ]
-00000e80: 0d0a 2020 2020 2020 2020 7d0d 0a20 2020  ..        }..   
-00000e90: 207d 2c0d 0a20 2020 2022 7265 636f 7264   },..    "record
-00000ea0: 6572 7322 3a20 7b0d 0a20 2020 2020 2020  ers": {..       
-00000eb0: 2022 6461 7461 6261 7365 223a 207b 0d0a   "database": {..
-00000ec0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000ed0: 6522 3a20 2254 6162 6c65 7352 6563 6f72  e": "TablesRecor
-00000ee0: 6465 7222 2c0d 0a20 2020 2020 2020 2020  der",..         
-00000ef0: 2020 2022 7572 6c22 3a20 226f 7574 7075     "url": "outpu
-00000f00: 742e 6835 222c 0d0a 2020 2020 2020 2020  t.h5",..        
-00000f10: 2020 2020 226e 6f64 6573 223a 205b 0d0a      "nodes": [..
-00000f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f30: 5b22 2f6f 7574 7075 7473 2f64 656d 616e  ["/outputs/deman
-00000f40: 6422 2c20 2244 656d 616e 6422 5d2c 0d0a  d", "Demand"],..
-00000f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f60: 5b22 2f73 746f 7261 6765 2f72 6573 6572  ["/storage/reser
-00000f70: 766f 6972 222c 2022 5265 7365 7276 6f69  voir", "Reservoi
-00000f80: 7222 5d0d 0a20 2020 2020 2020 2020 2020  r"]..           
-00000f90: 205d 2c0d 0a20 2020 2020 2020 2020 2020   ],..           
-00000fa0: 2022 7061 7261 6d65 7465 7273 223a 205b   "parameters": [
-00000fb0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000fc0: 2020 5b22 2f70 6172 616d 6574 6572 732f    ["/parameters/
-00000fd0: 6465 6d61 6e64 5f73 6176 696e 675f 6c65  demand_saving_le
-00000fe0: 7665 6c22 2c20 2264 656d 616e 645f 7361  vel", "demand_sa
-00000ff0: 7669 6e67 5f6c 6576 656c 225d 0d0a 2020  ving_level"]..  
-00001000: 2020 2020 2020 2020 2020 5d2c 0d0a 2020            ],..  
-00001010: 2020 2020 2020 2020 2020 226d 6f64 6522            "mode"
-00001020: 3a20 2277 222c 0d0a 2020 2020 2020 2020  : "w",..        
-00001030: 2020 2020 2266 696c 7465 725f 6b77 6473      "filter_kwds
-00001040: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00001050: 2020 2020 2020 2263 6f6d 706c 6576 656c        "complevel
-00001060: 223a 2035 2c0d 0a20 2020 2020 2020 2020  ": 5,..         
-00001070: 2020 2020 2020 2022 636f 6d70 6c69 6222         "complib"
-00001080: 3a20 227a 6c69 6222 0d0a 2020 2020 2020  : "zlib"..      
-00001090: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000010a0: 2020 2020 2020 226d 6574 6164 6174 6122        "metadata"
-000010b0: 3a20 7b0d 0a20 2020 2020 2020 2020 2020  : {..           
-000010c0: 2020 2020 2022 6175 7468 6f72 223a 2022       "author": "
-000010d0: 7079 7465 7374 222c 0d0a 2020 2020 2020  pytest",..      
-000010e0: 2020 2020 2020 2020 2020 2272 756e 5f6e            "run_n
-000010f0: 756d 6265 7222 3a20 300d 0a20 2020 2020  umber": 0..     
-00001100: 2020 2020 2020 207d 0d0a 2020 2020 2020         }..      
-00001110: 2020 7d0d 0a20 2020 207d 0d0a 7d0d 0a      }..    }..}..
+00000ca0: 2020 302e 352c 0a20 2020 2020 2020 2020    0.5,.         
+00000cb0: 2020 2020 2020 2020 2020 2020 2020 2030                 0
+00000cc0: 2e35 0a20 2020 2020 2020 2020 2020 2020  .5.             
+00000cd0: 2020 2020 2020 205d 0a20 2020 2020 2020         ].       
+00000ce0: 2020 2020 2020 2020 207d 0a20 2020 2020           }.     
+00000cf0: 2020 2020 2020 205d 0a20 2020 2020 2020         ].       
+00000d00: 207d 2c0a 2020 2020 2020 2020 2264 656d   },.        "dem
+00000d10: 616e 645f 6d61 785f 666c 6f77 223a 207b  and_max_flow": {
+00000d20: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000d30: 7065 223a 2022 6167 6772 6567 6174 6564  pe": "aggregated
+00000d40: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000d50: 6167 675f 6675 6e63 223a 2022 7072 6f64  agg_func": "prod
+00000d60: 7563 7422 2c0a 2020 2020 2020 2020 2020  uct",.          
+00000d70: 2020 2270 6172 616d 6574 6572 7322 3a20    "parameters": 
+00000d80: 5b0a 2020 2020 2020 2020 2020 2020 2020  [.              
+00000d90: 2020 2264 656d 616e 645f 6261 7365 6c69    "demand_baseli
+00000da0: 6e65 222c 0a20 2020 2020 2020 2020 2020  ne",.           
+00000db0: 2020 2020 2022 6465 6d61 6e64 5f70 726f       "demand_pro
+00000dc0: 6669 6c65 222c 0a20 2020 2020 2020 2020  file",.         
+00000dd0: 2020 2020 2020 2022 6465 6d61 6e64 5f73         "demand_s
+00000de0: 6176 696e 675f 6661 6374 6f72 220a 2020  aving_factor".  
+00000df0: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+00000e00: 2020 2020 7d0a 2020 2020 7d2c 0a20 2020      }.    },.   
+00000e10: 2022 7265 636f 7264 6572 7322 3a20 7b0a   "recorders": {.
+00000e20: 2020 2020 2020 2020 2264 6174 6162 6173          "databas
+00000e30: 6522 3a20 7b0a 2020 2020 2020 2020 2020  e": {.          
+00000e40: 2020 2274 7970 6522 3a20 2254 6162 6c65    "type": "Table
+00000e50: 7352 6563 6f72 6465 7222 2c0a 2020 2020  sRecorder",.    
+00000e60: 2020 2020 2020 2020 2275 726c 223a 2022          "url": "
+00000e70: 6f75 7470 7574 2e68 3522 2c0a 2020 2020  output.h5",.    
+00000e80: 2020 2020 2020 2020 226e 6f64 6573 223a          "nodes":
+00000e90: 205b 0a20 2020 2020 2020 2020 2020 2020   [.             
+00000ea0: 2020 205b 222f 6f75 7470 7574 732f 6465     ["/outputs/de
+00000eb0: 6d61 6e64 222c 2022 4465 6d61 6e64 225d  mand", "Demand"]
+00000ec0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00000ed0: 2020 5b22 2f73 746f 7261 6765 2f72 6573    ["/storage/res
+00000ee0: 6572 766f 6972 222c 2022 5265 7365 7276  ervoir", "Reserv
+00000ef0: 6f69 7222 5d0a 2020 2020 2020 2020 2020  oir"].          
+00000f00: 2020 5d2c 0a20 2020 2020 2020 2020 2020    ],.           
+00000f10: 2022 7061 7261 6d65 7465 7273 223a 205b   "parameters": [
+00000f20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000f30: 205b 222f 7061 7261 6d65 7465 7273 2f64   ["/parameters/d
+00000f40: 656d 616e 645f 7361 7669 6e67 5f6c 6576  emand_saving_lev
+00000f50: 656c 222c 2022 6465 6d61 6e64 5f73 6176  el", "demand_sav
+00000f60: 696e 675f 6c65 7665 6c22 5d0a 2020 2020  ing_level"].    
+00000f70: 2020 2020 2020 2020 5d2c 0a20 2020 2020          ],.     
+00000f80: 2020 2020 2020 2022 6d6f 6465 223a 2022         "mode": "
+00000f90: 7722 2c0a 2020 2020 2020 2020 2020 2020  w",.            
+00000fa0: 2266 696c 7465 725f 6b77 6473 223a 207b  "filter_kwds": {
+00000fb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000fc0: 2022 636f 6d70 6c65 7665 6c22 3a20 352c   "complevel": 5,
+00000fd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000fe0: 2022 636f 6d70 6c69 6222 3a20 227a 6c69   "complib": "zli
+00000ff0: 6222 0a20 2020 2020 2020 2020 2020 207d  b".            }
+00001000: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
+00001010: 6574 6164 6174 6122 3a20 7b0a 2020 2020  etadata": {.    
+00001020: 2020 2020 2020 2020 2020 2020 2261 7574              "aut
+00001030: 686f 7222 3a20 2270 7974 6573 7422 2c0a  hor": "pytest",.
+00001040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001050: 2272 756e 5f6e 756d 6265 7222 3a20 300a  "run_number": 0.
+00001060: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
+00001070: 2020 2020 2020 7d0a 2020 2020 7d0a 7d0a        }.    }.}.
```

### Comparing `pywr-1.8.0/tests/models/discount.json` & `pywr-1.9.0/tests/models/discount.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 25% similar despite different names*

```diff
@@ -1,49 +1,47 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4469 7363 6f75 6e74  itle": "Discount
-00000030: 4661 6374 6f72 5061 7261 6d65 7465 7220  FactorParameter 
-00000040: 7465 7374 206d 6f64 656c 222c 0d0a 2020  test model",..  
-00000050: 2020 2020 2020 226d 696e 696d 756d 5f76        "minimum_v
-00000060: 6572 7369 6f6e 223a 2022 302e 3122 0d0a  ersion": "0.1"..
-00000070: 2020 2020 7d2c 0d0a 2020 2020 2274 696d      },..    "tim
-00000080: 6573 7465 7070 6572 223a 207b 0d0a 2020  estepper": {..  
-00000090: 2020 2020 2020 2273 7461 7274 223a 2022        "start": "
-000000a0: 3230 3135 2d30 312d 3031 222c 0d0a 2020  2015-01-01",..  
-000000b0: 2020 2020 2020 2265 6e64 223a 2022 3230        "end": "20
-000000c0: 3230 2d31 322d 3331 222c 0d0a 2020 2020  20-12-31",..    
-000000d0: 2020 2020 2274 696d 6573 7465 7022 3a20      "timestep": 
-000000e0: 3130 0d0a 2020 2020 7d2c 0d0a 2020 2020  10..    },..    
-000000f0: 226e 6f64 6573 223a 205b 0d0a 2020 2020  "nodes": [..    
-00000100: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-00000110: 2020 2022 6e61 6d65 223a 2022 7375 7070     "name": "supp
-00000120: 6c79 222c 0d0a 2020 2020 2020 2020 2020  ly",..          
-00000130: 2020 2274 7970 6522 3a20 2249 6e70 7574    "type": "Input
-00000140: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000150: 226d 6178 5f66 6c6f 7722 3a20 3130 2c0d  "max_flow": 10,.
-00000160: 0a20 2020 2020 2020 2020 2020 2022 636f  .            "co
-00000170: 7374 223a 2033 0d0a 2020 2020 2020 2020  st": 3..        
-00000180: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-00000190: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000001a0: 223a 2022 6465 6d61 6e64 222c 0d0a 2020  ": "demand",..  
-000001b0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-000001c0: 3a20 224f 7574 7075 7422 2c0d 0a20 2020  : "Output",..   
-000001d0: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
-000001e0: 6f77 223a 2022 6469 7363 6f75 6e74 5f66  ow": "discount_f
-000001f0: 6163 746f 7222 2c0d 0a20 2020 2020 2020  actor",..       
-00000200: 2020 2020 2022 636f 7374 223a 202d 3130       "cost": -10
-00000210: 300d 0a20 2020 2020 2020 207d 0d0a 2020  0..        }..  
-00000220: 2020 5d2c 0d0a 2020 2020 2265 6467 6573    ],..    "edges
-00000230: 223a 205b 0d0a 2020 2020 2020 2020 5b22  ": [..        ["
-00000240: 7375 7070 6c79 222c 2022 6465 6d61 6e64  supply", "demand
-00000250: 225d 0d0a 2020 2020 5d2c 0d0a 2020 2020  "]..    ],..    
-00000260: 2270 6172 616d 6574 6572 7322 3a20 7b0d  "parameters": {.
-00000270: 0a20 2020 2020 2020 2022 6469 7363 6f75  .        "discou
-00000280: 6e74 5f66 6163 746f 7222 3a20 7b0d 0a20  nt_factor": {.. 
-00000290: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-000002a0: 223a 2022 4469 7363 6f75 6e74 4661 6374  ": "DiscountFact
-000002b0: 6f72 5061 7261 6d65 7465 7222 2c0d 0a20  orParameter",.. 
-000002c0: 2020 2020 2020 2020 2020 2022 6261 7365             "base
-000002d0: 5f79 6561 7222 3a20 3230 3135 2c0d 0a20  _year": 2015,.. 
-000002e0: 2020 2020 2020 2020 2020 2022 7261 7465             "rate
-000002f0: 223a 2030 2e30 3335 0d0a 2020 2020 2020  ": 0.035..      
-00000300: 2020 7d0d 0a20 2020 207d 0d0a 7d0d 0a      }..    }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4469 7363 6f75 6e74 4661  le": "DiscountFa
+00000030: 6374 6f72 5061 7261 6d65 7465 7220 7465  ctorParameter te
+00000040: 7374 206d 6f64 656c 222c 0a20 2020 2020  st model",.     
+00000050: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
+00000060: 696f 6e22 3a20 2230 2e31 220a 2020 2020  ion": "0.1".    
+00000070: 7d2c 0a20 2020 2022 7469 6d65 7374 6570  },.    "timestep
+00000080: 7065 7222 3a20 7b0a 2020 2020 2020 2020  per": {.        
+00000090: 2273 7461 7274 223a 2022 3230 3135 2d30  "start": "2015-0
+000000a0: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+000000b0: 656e 6422 3a20 2232 3032 302d 3132 2d33  end": "2020-12-3
+000000c0: 3122 2c0a 2020 2020 2020 2020 2274 696d  1",.        "tim
+000000d0: 6573 7465 7022 3a20 3130 0a20 2020 207d  estep": 10.    }
+000000e0: 2c0a 2020 2020 226e 6f64 6573 223a 205b  ,.    "nodes": [
+000000f0: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+00000100: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
+00000110: 7375 7070 6c79 222c 0a20 2020 2020 2020  supply",.       
+00000120: 2020 2020 2022 7479 7065 223a 2022 496e       "type": "In
+00000130: 7075 7422 2c0a 2020 2020 2020 2020 2020  put",.          
+00000140: 2020 226d 6178 5f66 6c6f 7722 3a20 3130    "max_flow": 10
+00000150: 2c0a 2020 2020 2020 2020 2020 2020 2263  ,.            "c
+00000160: 6f73 7422 3a20 330a 2020 2020 2020 2020  ost": 3.        
+00000170: 7d2c 0a20 2020 2020 2020 207b 0a20 2020  },.        {.   
+00000180: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000190: 2022 6465 6d61 6e64 222c 0a20 2020 2020   "demand",.     
+000001a0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+000001b0: 4f75 7470 7574 222c 0a20 2020 2020 2020  Output",.       
+000001c0: 2020 2020 2022 6d61 785f 666c 6f77 223a       "max_flow":
+000001d0: 2022 6469 7363 6f75 6e74 5f66 6163 746f   "discount_facto
+000001e0: 7222 2c0a 2020 2020 2020 2020 2020 2020  r",.            
+000001f0: 2263 6f73 7422 3a20 2d31 3030 0a20 2020  "cost": -100.   
+00000200: 2020 2020 207d 0a20 2020 205d 2c0a 2020       }.    ],.  
+00000210: 2020 2265 6467 6573 223a 205b 0a20 2020    "edges": [.   
+00000220: 2020 2020 205b 2273 7570 706c 7922 2c20       ["supply", 
+00000230: 2264 656d 616e 6422 5d0a 2020 2020 5d2c  "demand"].    ],
+00000240: 0a20 2020 2022 7061 7261 6d65 7465 7273  .    "parameters
+00000250: 223a 207b 0a20 2020 2020 2020 2022 6469  ": {.        "di
+00000260: 7363 6f75 6e74 5f66 6163 746f 7222 3a20  scount_factor": 
+00000270: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000280: 7970 6522 3a20 2244 6973 636f 756e 7446  ype": "DiscountF
+00000290: 6163 746f 7250 6172 616d 6574 6572 222c  actorParameter",
+000002a0: 0a20 2020 2020 2020 2020 2020 2022 6261  .            "ba
+000002b0: 7365 5f79 6561 7222 3a20 3230 3135 2c0a  se_year": 2015,.
+000002c0: 2020 2020 2020 2020 2020 2020 2272 6174              "rat
+000002d0: 6522 3a20 302e 3033 350a 2020 2020 2020  e": 0.035.      
+000002e0: 2020 7d0a 2020 2020 7d0a 7d0a              }.    }.}.
```

### Comparing `pywr-1.8.0/tests/models/extra1.json` & `pywr-1.9.0/tests/models/reservoir1_pc.json`

 * *Files 24% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7416666666666666%*

 * *Differences: {"'metadata'": "{'title': 'Reservoir 1', 'description': 'A model with a reservoir.'}",*

 * * "'nodes'": "{0: {'type': 'Storage', 'max_volume': 35, 'initial_volume_pc': 1.0, 'outputs': 0, "*

 * *            "delete: ['max_flow']}}",*

 * * 'delete': "['includes']"}*

```diff
@@ -5,27 +5,26 @@
             "link1"
         ],
         [
             "link1",
             "demand1"
         ]
     ],
-    "includes": [
-        "extra2.json"
-    ],
     "metadata": {
-        "description": "A very simple example.",
+        "description": "A model with a reservoir.",
         "minimum_version": "0.1",
-        "title": "Simple 1"
+        "title": "Reservoir 1"
     },
     "nodes": [
         {
-            "max_flow": 15,
+            "initial_volume_pc": 1.0,
+            "max_volume": 35,
             "name": "supply1",
-            "type": "Input"
+            "outputs": 0,
+            "type": "Storage"
         },
         {
             "name": "link1",
             "type": "Link"
         },
         {
             "cost": -10,
```

### Comparing `pywr-1.8.0/tests/models/hydropower_example.json` & `pywr-1.9.0/tests/models/hydropower_example.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,201 +1,194 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4879 6472 6f70 6f77  itle": "Hydropow
-00000030: 6572 2065 7861 6d70 6c65 222c 0d0a 2020  er example",..  
-00000040: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
-00000050: 6f6e 223a 2022 4120 6d6f 6465 6c20 7769  on": "A model wi
-00000060: 7468 2061 2073 696e 676c 6520 7265 7365  th a single rese
-00000070: 7276 6f69 7220 616e 6420 6879 6472 6f2d  rvoir and hydro-
-00000080: 706f 7765 7220 7265 636f 7264 6572 222c  power recorder",
-00000090: 0d0a 2020 2020 2020 2020 226d 696e 696d  ..        "minim
-000000a0: 756d 5f76 6572 7369 6f6e 223a 2022 302e  um_version": "0.
-000000b0: 3422 0d0a 2020 2020 7d2c 0d0a 2020 2020  4"..    },..    
-000000c0: 2274 696d 6573 7465 7070 6572 223a 207b  "timestepper": {
-000000d0: 0d0a 2020 2020 2020 2020 2273 7461 7274  ..        "start
-000000e0: 223a 2022 3231 3030 2d30 312d 3031 222c  ": "2100-01-01",
-000000f0: 0d0a 2020 2020 2020 2020 2265 6e64 223a  ..        "end":
-00000100: 2022 3231 3031 2d30 312d 3031 222c 0d0a   "2101-01-01",..
-00000110: 2020 2020 2020 2020 2274 696d 6573 7465          "timeste
-00000120: 7022 3a20 370d 0a20 2020 207d 2c0d 0a20  p": 7..    },.. 
-00000130: 2020 2022 6e6f 6465 7322 3a20 5b0d 0a20     "nodes": [.. 
-00000140: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000150: 2020 2020 2020 226e 616d 6522 3a20 2263        "name": "c
-00000160: 6174 6368 6d65 6e74 3122 2c0d 0a20 2020  atchment1",..   
-00000170: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000180: 2022 6361 7463 686d 656e 7422 2c0d 0a20   "catchment",.. 
-00000190: 2020 2020 2020 2020 2020 2022 666c 6f77             "flow
-000001a0: 223a 2031 3030 2e30 0d0a 2020 2020 2020  ": 100.0..      
-000001b0: 2020 7d2c 0d0a 2020 2020 2020 2020 7b0d    },..        {.
-000001c0: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
-000001d0: 6d65 223a 2022 7265 7365 7276 6f69 7231  me": "reservoir1
-000001e0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000001f0: 2274 7970 6522 3a20 2273 746f 7261 6765  "type": "storage
-00000200: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000210: 226d 6178 5f76 6f6c 756d 6522 3a20 3230  "max_volume": 20
-00000220: 3030 3030 2c0d 0a20 2020 2020 2020 2020  0000,..         
-00000230: 2020 2022 696e 6974 6961 6c5f 766f 6c75     "initial_volu
-00000240: 6d65 223a 2031 3730 3030 300d 0a20 2020  me": 170000..   
-00000250: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-00000260: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000270: 226e 616d 6522 3a20 2272 656c 6561 7365  "name": "release
-00000280: 3122 2c0d 0a20 2020 2020 2020 2020 2020  1",..           
-00000290: 2022 7479 7065 223a 2022 6c69 6e6b 222c   "type": "link",
-000002a0: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-000002b0: 6178 5f66 6c6f 7722 3a20 3130 2c0d 0a20  ax_flow": 10,.. 
-000002c0: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-000002d0: 223a 202d 3530 300d 0a20 2020 2020 2020  ": -500..       
-000002e0: 207d 2c0d 0a20 2020 2020 2020 207b 0d0a   },..        {..
-000002f0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-00000300: 6522 3a20 2274 7572 6269 6e65 3122 2c0d  e": "turbine1",.
-00000310: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
-00000320: 7065 223a 2022 6c69 6e6b 222c 0d0a 2020  pe": "link",..  
-00000330: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
-00000340: 6c6f 7722 3a20 2274 7572 6269 6e65 315f  low": "turbine1_
-00000350: 6469 7363 6861 7267 6522 2c0d 0a20 2020  discharge",..   
-00000360: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
-00000370: 202d 3230 300d 0a20 2020 2020 2020 207d   -200..        }
-00000380: 2c0d 0a20 2020 2020 2020 207b 0d0a 2020  ,..        {..  
-00000390: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
-000003a0: 3a20 2273 7069 6c6c 3122 2c0d 0a20 2020  : "spill1",..   
-000003b0: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-000003c0: 2022 6c69 6e6b 222c 0d0a 2020 2020 2020   "link",..      
-000003d0: 2020 2020 2020 2263 6f73 7422 3a20 3130        "cost": 10
-000003e0: 3030 0d0a 2020 2020 2020 2020 7d2c 0d0a  00..        },..
-000003f0: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-00000400: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000410: 7265 6163 6831 222c 0d0a 2020 2020 2020  reach1",..      
-00000420: 2020 2020 2020 2274 7970 6522 3a20 226c        "type": "l
-00000430: 696e 6b22 0d0a 2020 2020 2020 2020 7d2c  ink"..        },
-00000440: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-00000450: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000460: 2022 656e 6431 222c 0d0a 2020 2020 2020   "end1",..      
-00000470: 2020 2020 2020 2274 7970 6522 3a20 226f        "type": "o
-00000480: 7574 7075 7422 0d0a 2020 2020 2020 2020  utput"..        
-00000490: 7d0d 0a20 2020 205d 2c0d 0a20 2020 2022  }..    ],..    "
-000004a0: 6564 6765 7322 3a20 5b0d 0a20 2020 2020  edges": [..     
-000004b0: 2020 205b 2263 6174 6368 6d65 6e74 3122     ["catchment1"
-000004c0: 2c20 2272 6573 6572 766f 6972 3122 5d2c  , "reservoir1"],
-000004d0: 0d0a 2020 2020 2020 2020 5b22 7265 7365  ..        ["rese
-000004e0: 7276 6f69 7231 222c 2022 7265 6c65 6173  rvoir1", "releas
-000004f0: 6531 225d 2c0d 0a20 2020 2020 2020 205b  e1"],..        [
-00000500: 2272 6573 6572 766f 6972 3122 2c20 2274  "reservoir1", "t
-00000510: 7572 6269 6e65 3122 5d2c 0d0a 2020 2020  urbine1"],..    
-00000520: 2020 2020 5b22 7265 7365 7276 6f69 7231      ["reservoir1
-00000530: 222c 2022 7370 696c 6c31 225d 2c0d 0a20  ", "spill1"],.. 
-00000540: 2020 2020 2020 205b 2272 656c 6561 7365         ["release
-00000550: 3122 2c20 2272 6561 6368 3122 5d2c 0d0a  1", "reach1"],..
-00000560: 2020 2020 2020 2020 5b22 7475 7262 696e          ["turbin
-00000570: 6531 222c 2022 7265 6163 6831 225d 2c0d  e1", "reach1"],.
-00000580: 0a20 2020 2020 2020 205b 2273 7069 6c6c  .        ["spill
-00000590: 3122 2c20 2272 6561 6368 3122 5d2c 0d0a  1", "reach1"],..
-000005a0: 2020 2020 2020 2020 5b22 7265 6163 6831          ["reach1
-000005b0: 222c 2022 656e 6431 225d 0d0a 2020 2020  ", "end1"]..    
-000005c0: 5d2c 0d0a 2020 2020 2270 6172 616d 6574  ],..    "paramet
-000005d0: 6572 7322 3a20 7b0d 0a20 2020 2020 2020  ers": {..       
-000005e0: 2022 7265 7365 7276 6f69 7231 5f6c 6576   "reservoir1_lev
-000005f0: 656c 223a 207b 0d0a 2020 2020 2020 2020  el": {..        
-00000600: 2020 2274 7970 6522 3a20 2269 6e74 6572    "type": "inter
-00000610: 706f 6c61 7465 6476 6f6c 756d 6522 2c0d  polatedvolume",.
-00000620: 0a20 2020 2020 2020 2020 2022 6e6f 6465  .          "node
-00000630: 223a 2022 7265 7365 7276 6f69 7231 222c  ": "reservoir1",
-00000640: 0d0a 2020 2020 2020 2020 2020 2276 6f6c  ..          "vol
-00000650: 756d 6573 223a 205b 302c 2032 3530 3030  umes": [0, 25000
-00000660: 2c20 3530 3030 302c 2037 3530 3030 2c20  , 50000, 75000, 
-00000670: 3130 3030 3030 2c20 3135 3030 3030 2c20  100000, 150000, 
-00000680: 3230 3030 3030 5d2c 0d0a 2020 2020 2020  200000],..      
-00000690: 2020 2020 2276 616c 7565 7322 3a20 5b30      "values": [0
-000006a0: 2c20 3239 2e32 2c20 3336 2e38 2c20 3432  , 29.2, 36.8, 42
-000006b0: 2e32 2c20 3436 2e34 2c20 3533 2e31 2c20  .2, 46.4, 53.1, 
-000006c0: 3538 2e35 5d2c 0d0a 2020 2020 2020 2020  58.5],..        
-000006d0: 2020 226b 696e 6422 3a20 2263 7562 6963    "kind": "cubic
-000006e0: 220d 0a20 2020 2020 2020 207d 2c0d 0a20  "..        },.. 
-000006f0: 2020 2020 2020 2022 7475 7262 696e 6531         "turbine1
-00000700: 5f64 6973 6368 6172 6765 223a 207b 0d0a  _discharge": {..
-00000710: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000720: 6522 3a20 2269 6e64 6578 6564 6172 7261  e": "indexedarra
-00000730: 7922 2c0d 0a20 2020 2020 2020 2020 2020  y",..           
-00000740: 2022 696e 6465 785f 7061 7261 6d65 7465   "index_paramete
-00000750: 7222 3a20 2274 7572 6269 6e65 315f 636f  r": "turbine1_co
-00000760: 6e74 726f 6c22 2c0d 0a20 2020 2020 2020  ntrol",..       
-00000770: 2020 2020 2022 7061 7261 6d73 223a 205b       "params": [
-00000780: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000790: 2020 3430 2e30 2c0d 0a20 2020 2020 2020    40.0,..       
-000007a0: 2020 2020 2020 2020 2030 2e30 0d0a 2020           0.0..  
-000007b0: 2020 2020 2020 2020 2020 5d0d 0a20 2020            ]..   
-000007c0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-000007d0: 2022 7475 7262 696e 6531 5f63 6f6e 7472   "turbine1_contr
-000007e0: 6f6c 223a 207b 0d0a 2020 2020 2020 2020  ol": {..        
-000007f0: 2020 2020 2274 7970 6522 3a20 2263 6f6e      "type": "con
-00000800: 7472 6f6c 6375 7276 6569 6e64 6578 222c  trolcurveindex",
-00000810: 0d0a 2020 2020 2020 2020 2020 2020 2273  ..            "s
-00000820: 746f 7261 6765 5f6e 6f64 6522 3a20 2272  torage_node": "r
-00000830: 6573 6572 766f 6972 3122 2c0d 0a20 2020  eservoir1",..   
-00000840: 2020 2020 2020 2020 2022 636f 6e74 726f           "contro
-00000850: 6c5f 6375 7276 6573 223a 205b 0d0a 2020  l_curves": [..  
-00000860: 2020 2020 2020 2020 2020 2020 2020 2274                "t
-00000870: 7572 6269 6e65 315f 636f 6e74 726f 6c5f  urbine1_control_
-00000880: 6375 7276 6522 0d0a 2020 2020 2020 2020  curve"..        
-00000890: 2020 2020 5d0d 0a20 2020 2020 2020 207d      ]..        }
-000008a0: 2c0d 0a20 2020 2020 2020 2022 7475 7262  ,..        "turb
-000008b0: 696e 6531 5f63 6f6e 7472 6f6c 5f63 7572  ine1_control_cur
-000008c0: 7665 223a 207b 0d0a 2020 2020 2020 2020  ve": {..        
-000008d0: 2020 2020 2274 7970 6522 3a20 226d 6f6e      "type": "mon
-000008e0: 7468 6c79 7072 6f66 696c 6522 2c0d 0a20  thlyprofile",.. 
-000008f0: 2020 2020 2020 2020 2020 2022 7661 6c75             "valu
-00000900: 6573 223a 205b 302e 382c 2030 2e38 2c20  es": [0.8, 0.8, 
-00000910: 302e 382c 2030 2e38 2c20 302e 382c 2030  0.8, 0.8, 0.8, 0
-00000920: 2e38 2c20 302e 382c 2030 2e38 2c20 302e  .8, 0.8, 0.8, 0.
-00000930: 382c 2030 2e38 2c20 302e 382c 2030 2e38  8, 0.8, 0.8, 0.8
-00000940: 5d0d 0a20 2020 2020 2020 207d 0d0a 2020  ]..        }..  
-00000950: 2020 7d2c 0d0a 2020 2020 2272 6563 6f72    },..    "recor
-00000960: 6465 7273 223a 207b 0d0a 2020 2020 2020  ders": {..      
-00000970: 2020 2274 7572 6269 6e65 315f 656e 6572    "turbine1_ener
-00000980: 6779 223a 207b 0d0a 2020 2020 2020 2020  gy": {..        
-00000990: 2020 2020 2274 7970 6522 3a20 2248 7964      "type": "Hyd
-000009a0: 726f 506f 7765 7252 6563 6f72 6465 7222  roPowerRecorder"
-000009b0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000009c0: 6e6f 6465 223a 2022 7475 7262 696e 6531  node": "turbine1
-000009d0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000009e0: 2277 6174 6572 5f65 6c65 7661 7469 6f6e  "water_elevation
-000009f0: 5f70 6172 616d 6574 6572 223a 2022 7265  _parameter": "re
-00000a00: 7365 7276 6f69 7231 5f6c 6576 656c 222c  servoir1_level",
-00000a10: 0d0a 2020 2020 2020 2020 2020 2020 2274  ..            "t
-00000a20: 7572 6269 6e65 5f65 6c65 7661 7469 6f6e  urbine_elevation
-00000a30: 223a 2033 352e 302c 0d0a 2020 2020 2020  ": 35.0,..      
-00000a40: 2020 2020 2020 2265 6666 6963 6965 6e63        "efficienc
-00000a50: 7922 3a20 302e 3835 2c0d 0a20 2020 2020  y": 0.85,..     
-00000a60: 2020 2020 2020 2022 666c 6f77 5f75 6e69         "flow_uni
-00000a70: 745f 636f 6e76 6572 7369 6f6e 223a 2031  t_conversion": 1
-00000a80: 6533 0d0a 2020 2020 2020 2020 7d2c 0d0a  e3..        },..
-00000a90: 2020 2020 2020 2020 2263 6174 6368 6d65          "catchme
-00000aa0: 6e74 315f 666c 6f77 223a 207b 0d0a 2020  nt1_flow": {..  
-00000ab0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000ac0: 3a20 226e 756d 7079 6172 7261 796e 6f64  : "numpyarraynod
-00000ad0: 6572 6563 6f72 6465 7222 2c0d 0a20 2020  erecorder",..   
-00000ae0: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
-00000af0: 2022 6361 7463 686d 656e 7431 220d 0a20   "catchment1".. 
-00000b00: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000b10: 2020 2022 7265 7365 7276 6f69 7231 5f73     "reservoir1_s
-00000b20: 746f 7261 6765 223a 207b 0d0a 2020 2020  torage": {..    
-00000b30: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000b40: 226e 756d 7079 6172 7261 7973 746f 7261  "numpyarraystora
-00000b50: 6765 7265 636f 7264 6572 222c 0d0a 2020  gerecorder",..  
-00000b60: 2020 2020 2020 2020 2020 226e 6f64 6522            "node"
-00000b70: 3a20 2272 6573 6572 766f 6972 3122 0d0a  : "reservoir1"..
-00000b80: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000b90: 2020 2020 2274 7572 6269 6e65 315f 666c      "turbine1_fl
-00000ba0: 6f77 223a 207b 0d0a 2020 2020 2020 2020  ow": {..        
-00000bb0: 2020 2020 2274 7970 6522 3a20 226e 756d      "type": "num
-00000bc0: 7079 6172 7261 796e 6f64 6572 6563 6f72  pyarraynoderecor
-00000bd0: 6465 7222 2c0d 0a20 2020 2020 2020 2020  der",..         
-00000be0: 2020 2022 6e6f 6465 223a 2022 7475 7262     "node": "turb
-00000bf0: 696e 6531 220d 0a20 2020 2020 2020 207d  ine1"..        }
-00000c00: 2c0d 0a20 2020 2020 2020 2022 7265 6c65  ,..        "rele
-00000c10: 6173 6531 5f66 6c6f 7722 3a20 7b0d 0a20  ase1_flow": {.. 
-00000c20: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000c30: 223a 2022 6e75 6d70 7961 7272 6179 6e6f  ": "numpyarrayno
-00000c40: 6465 7265 636f 7264 6572 222c 0d0a 2020  derecorder",..  
-00000c50: 2020 2020 2020 2020 2020 226e 6f64 6522            "node"
-00000c60: 3a20 2272 656c 6561 7365 3122 0d0a 2020  : "release1"..  
-00000c70: 2020 2020 2020 7d0d 0a20 2020 207d 0d0a        }..    }..
-00000c80: 7d0d 0a                                  }..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4879 6472 6f70 6f77 6572  le": "Hydropower
+00000030: 2065 7861 6d70 6c65 222c 0a20 2020 2020   example",.     
+00000040: 2020 2022 6465 7363 7269 7074 696f 6e22     "description"
+00000050: 3a20 2241 206d 6f64 656c 2077 6974 6820  : "A model with 
+00000060: 6120 7369 6e67 6c65 2072 6573 6572 766f  a single reservo
+00000070: 6972 2061 6e64 2068 7964 726f 2d70 6f77  ir and hydro-pow
+00000080: 6572 2072 6563 6f72 6465 7222 2c0a 2020  er recorder",.  
+00000090: 2020 2020 2020 226d 696e 696d 756d 5f76        "minimum_v
+000000a0: 6572 7369 6f6e 223a 2022 302e 3422 0a20  ersion": "0.4". 
+000000b0: 2020 207d 2c0a 2020 2020 2274 696d 6573     },.    "times
+000000c0: 7465 7070 6572 223a 207b 0a20 2020 2020  tepper": {.     
+000000d0: 2020 2022 7374 6172 7422 3a20 2232 3130     "start": "210
+000000e0: 302d 3031 2d30 3122 2c0a 2020 2020 2020  0-01-01",.      
+000000f0: 2020 2265 6e64 223a 2022 3231 3031 2d30    "end": "2101-0
+00000100: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+00000110: 7469 6d65 7374 6570 223a 2037 0a20 2020  timestep": 7.   
+00000120: 207d 2c0a 2020 2020 226e 6f64 6573 223a   },.    "nodes":
+00000130: 205b 0a20 2020 2020 2020 207b 0a20 2020   [.        {.   
+00000140: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000150: 2022 6361 7463 686d 656e 7431 222c 0a20   "catchment1",. 
+00000160: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000170: 223a 2022 6361 7463 686d 656e 7422 2c0a  ": "catchment",.
+00000180: 2020 2020 2020 2020 2020 2020 2266 6c6f              "flo
+00000190: 7722 3a20 3130 302e 300a 2020 2020 2020  w": 100.0.      
+000001a0: 2020 7d2c 0a20 2020 2020 2020 207b 0a20    },.        {. 
+000001b0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
+000001c0: 223a 2022 7265 7365 7276 6f69 7231 222c  ": "reservoir1",
+000001d0: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+000001e0: 7065 223a 2022 7374 6f72 6167 6522 2c0a  pe": "storage",.
+000001f0: 2020 2020 2020 2020 2020 2020 226d 6178              "max
+00000200: 5f76 6f6c 756d 6522 3a20 3230 3030 3030  _volume": 200000
+00000210: 2c0a 2020 2020 2020 2020 2020 2020 2269  ,.            "i
+00000220: 6e69 7469 616c 5f76 6f6c 756d 6522 3a20  nitial_volume": 
+00000230: 3137 3030 3030 0a20 2020 2020 2020 207d  170000.        }
+00000240: 2c0a 2020 2020 2020 2020 7b0a 2020 2020  ,.        {.    
+00000250: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
+00000260: 2272 656c 6561 7365 3122 2c0a 2020 2020  "release1",.    
+00000270: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000280: 226c 696e 6b22 2c0a 2020 2020 2020 2020  "link",.        
+00000290: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
+000002a0: 3130 2c0a 2020 2020 2020 2020 2020 2020  10,.            
+000002b0: 2263 6f73 7422 3a20 2d35 3030 0a20 2020  "cost": -500.   
+000002c0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000002d0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000002e0: 616d 6522 3a20 2274 7572 6269 6e65 3122  ame": "turbine1"
+000002f0: 2c0a 2020 2020 2020 2020 2020 2020 2274  ,.            "t
+00000300: 7970 6522 3a20 226c 696e 6b22 2c0a 2020  ype": "link",.  
+00000310: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
+00000320: 6c6f 7722 3a20 2274 7572 6269 6e65 315f  low": "turbine1_
+00000330: 6469 7363 6861 7267 6522 2c0a 2020 2020  discharge",.    
+00000340: 2020 2020 2020 2020 2263 6f73 7422 3a20          "cost": 
+00000350: 2d32 3030 0a20 2020 2020 2020 207d 2c0a  -200.        },.
+00000360: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000370: 2020 2020 2020 226e 616d 6522 3a20 2273        "name": "s
+00000380: 7069 6c6c 3122 2c0a 2020 2020 2020 2020  pill1",.        
+00000390: 2020 2020 2274 7970 6522 3a20 226c 696e      "type": "lin
+000003a0: 6b22 2c0a 2020 2020 2020 2020 2020 2020  k",.            
+000003b0: 2263 6f73 7422 3a20 3130 3030 0a20 2020  "cost": 1000.   
+000003c0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000003d0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000003e0: 616d 6522 3a20 2272 6561 6368 3122 2c0a  ame": "reach1",.
+000003f0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000400: 6522 3a20 226c 696e 6b22 0a20 2020 2020  e": "link".     
+00000410: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+00000420: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000430: 6522 3a20 2265 6e64 3122 2c0a 2020 2020  e": "end1",.    
+00000440: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000450: 226f 7574 7075 7422 0a20 2020 2020 2020  "output".       
+00000460: 207d 0a20 2020 205d 2c0a 2020 2020 2265   }.    ],.    "e
+00000470: 6467 6573 223a 205b 0a20 2020 2020 2020  dges": [.       
+00000480: 205b 2263 6174 6368 6d65 6e74 3122 2c20   ["catchment1", 
+00000490: 2272 6573 6572 766f 6972 3122 5d2c 0a20  "reservoir1"],. 
+000004a0: 2020 2020 2020 205b 2272 6573 6572 766f         ["reservo
+000004b0: 6972 3122 2c20 2272 656c 6561 7365 3122  ir1", "release1"
+000004c0: 5d2c 0a20 2020 2020 2020 205b 2272 6573  ],.        ["res
+000004d0: 6572 766f 6972 3122 2c20 2274 7572 6269  ervoir1", "turbi
+000004e0: 6e65 3122 5d2c 0a20 2020 2020 2020 205b  ne1"],.        [
+000004f0: 2272 6573 6572 766f 6972 3122 2c20 2273  "reservoir1", "s
+00000500: 7069 6c6c 3122 5d2c 0a20 2020 2020 2020  pill1"],.       
+00000510: 205b 2272 656c 6561 7365 3122 2c20 2272   ["release1", "r
+00000520: 6561 6368 3122 5d2c 0a20 2020 2020 2020  each1"],.       
+00000530: 205b 2274 7572 6269 6e65 3122 2c20 2272   ["turbine1", "r
+00000540: 6561 6368 3122 5d2c 0a20 2020 2020 2020  each1"],.       
+00000550: 205b 2273 7069 6c6c 3122 2c20 2272 6561   ["spill1", "rea
+00000560: 6368 3122 5d2c 0a20 2020 2020 2020 205b  ch1"],.        [
+00000570: 2272 6561 6368 3122 2c20 2265 6e64 3122  "reach1", "end1"
+00000580: 5d0a 2020 2020 5d2c 0a20 2020 2022 7061  ].    ],.    "pa
+00000590: 7261 6d65 7465 7273 223a 207b 0a20 2020  rameters": {.   
+000005a0: 2020 2020 2022 7265 7365 7276 6f69 7231       "reservoir1
+000005b0: 5f6c 6576 656c 223a 207b 0a20 2020 2020  _level": {.     
+000005c0: 2020 2020 2022 7479 7065 223a 2022 696e       "type": "in
+000005d0: 7465 7270 6f6c 6174 6564 766f 6c75 6d65  terpolatedvolume
+000005e0: 222c 0a20 2020 2020 2020 2020 2022 6e6f  ",.          "no
+000005f0: 6465 223a 2022 7265 7365 7276 6f69 7231  de": "reservoir1
+00000600: 222c 0a20 2020 2020 2020 2020 2022 766f  ",.          "vo
+00000610: 6c75 6d65 7322 3a20 5b30 2c20 3235 3030  lumes": [0, 2500
+00000620: 302c 2035 3030 3030 2c20 3735 3030 302c  0, 50000, 75000,
+00000630: 2031 3030 3030 302c 2031 3530 3030 302c   100000, 150000,
+00000640: 2032 3030 3030 305d 2c0a 2020 2020 2020   200000],.      
+00000650: 2020 2020 2276 616c 7565 7322 3a20 5b30      "values": [0
+00000660: 2c20 3239 2e32 2c20 3336 2e38 2c20 3432  , 29.2, 36.8, 42
+00000670: 2e32 2c20 3436 2e34 2c20 3533 2e31 2c20  .2, 46.4, 53.1, 
+00000680: 3538 2e35 5d2c 0a20 2020 2020 2020 2020  58.5],.         
+00000690: 2022 6b69 6e64 223a 2022 6375 6269 6322   "kind": "cubic"
+000006a0: 0a20 2020 2020 2020 207d 2c0a 2020 2020  .        },.    
+000006b0: 2020 2020 2274 7572 6269 6e65 315f 6469      "turbine1_di
+000006c0: 7363 6861 7267 6522 3a20 7b0a 2020 2020  scharge": {.    
+000006d0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+000006e0: 2269 6e64 6578 6564 6172 7261 7922 2c0a  "indexedarray",.
+000006f0: 2020 2020 2020 2020 2020 2020 2269 6e64              "ind
+00000700: 6578 5f70 6172 616d 6574 6572 223a 2022  ex_parameter": "
+00000710: 7475 7262 696e 6531 5f63 6f6e 7472 6f6c  turbine1_control
+00000720: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000730: 7061 7261 6d73 223a 205b 0a20 2020 2020  params": [.     
+00000740: 2020 2020 2020 2020 2020 2034 302e 302c             40.0,
+00000750: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000760: 2030 2e30 0a20 2020 2020 2020 2020 2020   0.0.           
+00000770: 205d 0a20 2020 2020 2020 207d 2c0a 2020   ].        },.  
+00000780: 2020 2020 2020 2274 7572 6269 6e65 315f        "turbine1_
+00000790: 636f 6e74 726f 6c22 3a20 7b0a 2020 2020  control": {.    
+000007a0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+000007b0: 2263 6f6e 7472 6f6c 6375 7276 6569 6e64  "controlcurveind
+000007c0: 6578 222c 0a20 2020 2020 2020 2020 2020  ex",.           
+000007d0: 2022 7374 6f72 6167 655f 6e6f 6465 223a   "storage_node":
+000007e0: 2022 7265 7365 7276 6f69 7231 222c 0a20   "reservoir1",. 
+000007f0: 2020 2020 2020 2020 2020 2022 636f 6e74             "cont
+00000800: 726f 6c5f 6375 7276 6573 223a 205b 0a20  rol_curves": [. 
+00000810: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00000820: 7475 7262 696e 6531 5f63 6f6e 7472 6f6c  turbine1_control
+00000830: 5f63 7572 7665 220a 2020 2020 2020 2020  _curve".        
+00000840: 2020 2020 5d0a 2020 2020 2020 2020 7d2c      ].        },
+00000850: 0a20 2020 2020 2020 2022 7475 7262 696e  .        "turbin
+00000860: 6531 5f63 6f6e 7472 6f6c 5f63 7572 7665  e1_control_curve
+00000870: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
+00000880: 2022 7479 7065 223a 2022 6d6f 6e74 686c   "type": "monthl
+00000890: 7970 726f 6669 6c65 222c 0a20 2020 2020  yprofile",.     
+000008a0: 2020 2020 2020 2022 7661 6c75 6573 223a         "values":
+000008b0: 205b 302e 382c 2030 2e38 2c20 302e 382c   [0.8, 0.8, 0.8,
+000008c0: 2030 2e38 2c20 302e 382c 2030 2e38 2c20   0.8, 0.8, 0.8, 
+000008d0: 302e 382c 2030 2e38 2c20 302e 382c 2030  0.8, 0.8, 0.8, 0
+000008e0: 2e38 2c20 302e 382c 2030 2e38 5d0a 2020  .8, 0.8, 0.8].  
+000008f0: 2020 2020 2020 7d0a 2020 2020 7d2c 0a20        }.    },. 
+00000900: 2020 2022 7265 636f 7264 6572 7322 3a20     "recorders": 
+00000910: 7b0a 2020 2020 2020 2020 2274 7572 6269  {.        "turbi
+00000920: 6e65 315f 656e 6572 6779 223a 207b 0a20  ne1_energy": {. 
+00000930: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000940: 223a 2022 4879 6472 6f50 6f77 6572 5265  ": "HydroPowerRe
+00000950: 636f 7264 6572 222c 0a20 2020 2020 2020  corder",.       
+00000960: 2020 2020 2022 6e6f 6465 223a 2022 7475       "node": "tu
+00000970: 7262 696e 6531 222c 0a20 2020 2020 2020  rbine1",.       
+00000980: 2020 2020 2022 7761 7465 725f 656c 6576       "water_elev
+00000990: 6174 696f 6e5f 7061 7261 6d65 7465 7222  ation_parameter"
+000009a0: 3a20 2272 6573 6572 766f 6972 315f 6c65  : "reservoir1_le
+000009b0: 7665 6c22 2c0a 2020 2020 2020 2020 2020  vel",.          
+000009c0: 2020 2274 7572 6269 6e65 5f65 6c65 7661    "turbine_eleva
+000009d0: 7469 6f6e 223a 2033 352e 302c 0a20 2020  tion": 35.0,.   
+000009e0: 2020 2020 2020 2020 2022 6566 6669 6369           "effici
+000009f0: 656e 6379 223a 2030 2e38 352c 0a20 2020  ency": 0.85,.   
+00000a00: 2020 2020 2020 2020 2022 666c 6f77 5f75           "flow_u
+00000a10: 6e69 745f 636f 6e76 6572 7369 6f6e 223a  nit_conversion":
+00000a20: 2031 6533 0a20 2020 2020 2020 207d 2c0a   1e3.        },.
+00000a30: 2020 2020 2020 2020 2263 6174 6368 6d65          "catchme
+00000a40: 6e74 315f 666c 6f77 223a 207b 0a20 2020  nt1_flow": {.   
+00000a50: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+00000a60: 2022 6e75 6d70 7961 7272 6179 6e6f 6465   "numpyarraynode
+00000a70: 7265 636f 7264 6572 222c 0a20 2020 2020  recorder",.     
+00000a80: 2020 2020 2020 2022 6e6f 6465 223a 2022         "node": "
+00000a90: 6361 7463 686d 656e 7431 220a 2020 2020  catchment1".    
+00000aa0: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000ab0: 7265 7365 7276 6f69 7231 5f73 746f 7261  reservoir1_stora
+00000ac0: 6765 223a 207b 0a20 2020 2020 2020 2020  ge": {.         
+00000ad0: 2020 2022 7479 7065 223a 2022 6e75 6d70     "type": "nump
+00000ae0: 7961 7272 6179 7374 6f72 6167 6572 6563  yarraystoragerec
+00000af0: 6f72 6465 7222 2c0a 2020 2020 2020 2020  order",.        
+00000b00: 2020 2020 226e 6f64 6522 3a20 2272 6573      "node": "res
+00000b10: 6572 766f 6972 3122 0a20 2020 2020 2020  ervoir1".       
+00000b20: 207d 2c0a 2020 2020 2020 2020 2274 7572   },.        "tur
+00000b30: 6269 6e65 315f 666c 6f77 223a 207b 0a20  bine1_flow": {. 
+00000b40: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000b50: 223a 2022 6e75 6d70 7961 7272 6179 6e6f  ": "numpyarrayno
+00000b60: 6465 7265 636f 7264 6572 222c 0a20 2020  derecorder",.   
+00000b70: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
+00000b80: 2022 7475 7262 696e 6531 220a 2020 2020   "turbine1".    
+00000b90: 2020 2020 7d2c 0a20 2020 2020 2020 2022      },.        "
+00000ba0: 7265 6c65 6173 6531 5f66 6c6f 7722 3a20  release1_flow": 
+00000bb0: 7b0a 2020 2020 2020 2020 2020 2020 2274  {.            "t
+00000bc0: 7970 6522 3a20 226e 756d 7079 6172 7261  ype": "numpyarra
+00000bd0: 796e 6f64 6572 6563 6f72 6465 7222 2c0a  ynoderecorder",.
+00000be0: 2020 2020 2020 2020 2020 2020 226e 6f64              "nod
+00000bf0: 6522 3a20 2272 656c 6561 7365 3122 0a20  e": "release1". 
+00000c00: 2020 2020 2020 207d 0a20 2020 207d 0a7d         }.    }.}
+00000c10: 0a                                       .
```

### Comparing `pywr-1.8.0/tests/models/hydropower_target_example.json` & `pywr-1.9.0/tests/models/hydropower_target_example.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 26% similar despite different names*

```diff
@@ -1,191 +1,184 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 4879 6472 6f70 6f77  itle": "Hydropow
-00000030: 6572 2065 7861 6d70 6c65 222c 0d0a 2020  er example",..  
-00000040: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
-00000050: 6f6e 223a 2022 4120 6d6f 6465 6c20 7769  on": "A model wi
-00000060: 7468 2061 2073 696e 676c 6520 7265 7365  th a single rese
-00000070: 7276 6f69 7220 616e 6420 6879 6472 6f2d  rvoir and hydro-
-00000080: 706f 7765 7220 7265 636f 7264 6572 222c  power recorder",
-00000090: 0d0a 2020 2020 2020 2020 226d 696e 696d  ..        "minim
-000000a0: 756d 5f76 6572 7369 6f6e 223a 2022 302e  um_version": "0.
-000000b0: 3622 0d0a 2020 2020 7d2c 0d0a 2020 2020  6"..    },..    
-000000c0: 2274 696d 6573 7465 7070 6572 223a 207b  "timestepper": {
-000000d0: 0d0a 2020 2020 2020 2020 2273 7461 7274  ..        "start
-000000e0: 223a 2022 3231 3030 2d30 312d 3031 222c  ": "2100-01-01",
-000000f0: 0d0a 2020 2020 2020 2020 2265 6e64 223a  ..        "end":
-00000100: 2022 3231 3031 2d30 312d 3031 222c 0d0a   "2101-01-01",..
-00000110: 2020 2020 2020 2020 2274 696d 6573 7465          "timeste
-00000120: 7022 3a20 370d 0a20 2020 207d 2c0d 0a20  p": 7..    },.. 
-00000130: 2020 2022 6e6f 6465 7322 3a20 5b0d 0a20     "nodes": [.. 
-00000140: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000150: 2020 2020 2020 226e 616d 6522 3a20 2263        "name": "c
-00000160: 6174 6368 6d65 6e74 3122 2c0d 0a20 2020  atchment1",..   
-00000170: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000180: 2022 6361 7463 686d 656e 7422 2c0d 0a20   "catchment",.. 
-00000190: 2020 2020 2020 2020 2020 2022 666c 6f77             "flow
-000001a0: 223a 2030 2e30 0d0a 2020 2020 2020 2020  ": 0.0..        
-000001b0: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-000001c0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000001d0: 223a 2022 7265 7365 7276 6f69 7231 222c  ": "reservoir1",
-000001e0: 0d0a 2020 2020 2020 2020 2020 2020 2274  ..            "t
-000001f0: 7970 6522 3a20 2273 746f 7261 6765 222c  ype": "storage",
-00000200: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-00000210: 6178 5f76 6f6c 756d 6522 3a20 3230 3030  ax_volume": 2000
-00000220: 3030 2c0d 0a20 2020 2020 2020 2020 2020  00,..           
-00000230: 2022 696e 6974 6961 6c5f 766f 6c75 6d65   "initial_volume
-00000240: 223a 2032 3030 3030 300d 0a20 2020 2020  ": 200000..     
-00000250: 2020 207d 2c0d 0a20 2020 2020 2020 207b     },..        {
-00000260: 0d0a 2020 2020 2020 2020 2020 2020 226e  ..            "n
-00000270: 616d 6522 3a20 2272 656c 6561 7365 3122  ame": "release1"
-00000280: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000290: 7479 7065 223a 2022 6c69 6e6b 222c 0d0a  type": "link",..
-000002a0: 2020 2020 2020 2020 2020 2020 226d 6178              "max
-000002b0: 5f66 6c6f 7722 3a20 3130 2c0d 0a20 2020  _flow": 10,..   
-000002c0: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
-000002d0: 202d 3530 300d 0a20 2020 2020 2020 207d   -500..        }
-000002e0: 2c0d 0a20 2020 2020 2020 207b 0d0a 2020  ,..        {..  
-000002f0: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
-00000300: 3a20 2274 7572 6269 6e65 3122 2c0d 0a20  : "turbine1",.. 
-00000310: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000320: 223a 2022 6c69 6e6b 222c 0d0a 2020 2020  ": "link",..    
-00000330: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
-00000340: 7722 3a20 2274 7572 6269 6e65 315f 6469  w": "turbine1_di
-00000350: 7363 6861 7267 6522 2c0d 0a20 2020 2020  scharge",..     
-00000360: 2020 2020 2020 2022 636f 7374 223a 202d         "cost": -
-00000370: 3230 300d 0a20 2020 2020 2020 207d 2c0d  200..        },.
-00000380: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000390: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-000003a0: 2273 7069 6c6c 3122 2c0d 0a20 2020 2020  "spill1",..     
-000003b0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000003c0: 6c69 6e6b 222c 0d0a 2020 2020 2020 2020  link",..        
-000003d0: 2020 2020 2263 6f73 7422 3a20 3130 3030      "cost": 1000
-000003e0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-000003f0: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000400: 2020 2020 2022 6e61 6d65 223a 2022 7265       "name": "re
-00000410: 6163 6831 222c 0d0a 2020 2020 2020 2020  ach1",..        
-00000420: 2020 2020 2274 7970 6522 3a20 226c 696e      "type": "lin
-00000430: 6b22 0d0a 2020 2020 2020 2020 7d2c 0d0a  k"..        },..
-00000440: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-00000450: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000460: 656e 6431 222c 0d0a 2020 2020 2020 2020  end1",..        
-00000470: 2020 2020 2274 7970 6522 3a20 226f 7574      "type": "out
-00000480: 7075 7422 0d0a 2020 2020 2020 2020 7d0d  put"..        }.
-00000490: 0a20 2020 205d 2c0d 0a20 2020 2022 6564  .    ],..    "ed
-000004a0: 6765 7322 3a20 5b0d 0a20 2020 2020 2020  ges": [..       
-000004b0: 205b 2263 6174 6368 6d65 6e74 3122 2c20   ["catchment1", 
-000004c0: 2272 6573 6572 766f 6972 3122 5d2c 0d0a  "reservoir1"],..
-000004d0: 2020 2020 2020 2020 5b22 7265 7365 7276          ["reserv
-000004e0: 6f69 7231 222c 2022 7265 6c65 6173 6531  oir1", "release1
-000004f0: 225d 2c0d 0a20 2020 2020 2020 205b 2272  "],..        ["r
-00000500: 6573 6572 766f 6972 3122 2c20 2274 7572  eservoir1", "tur
-00000510: 6269 6e65 3122 5d2c 0d0a 2020 2020 2020  bine1"],..      
-00000520: 2020 5b22 7265 7365 7276 6f69 7231 222c    ["reservoir1",
-00000530: 2022 7370 696c 6c31 225d 2c0d 0a20 2020   "spill1"],..   
-00000540: 2020 2020 205b 2272 656c 6561 7365 3122       ["release1"
-00000550: 2c20 2272 6561 6368 3122 5d2c 0d0a 2020  , "reach1"],..  
-00000560: 2020 2020 2020 5b22 7475 7262 696e 6531        ["turbine1
-00000570: 222c 2022 7265 6163 6831 225d 2c0d 0a20  ", "reach1"],.. 
-00000580: 2020 2020 2020 205b 2273 7069 6c6c 3122         ["spill1"
-00000590: 2c20 2272 6561 6368 3122 5d2c 0d0a 2020  , "reach1"],..  
-000005a0: 2020 2020 2020 5b22 7265 6163 6831 222c        ["reach1",
-000005b0: 2022 656e 6431 225d 0d0a 2020 2020 5d2c   "end1"]..    ],
-000005c0: 0d0a 2020 2020 2270 6172 616d 6574 6572  ..    "parameter
-000005d0: 7322 3a20 7b0d 0a20 2020 2020 2020 2022  s": {..        "
-000005e0: 7265 7365 7276 6f69 7231 5f6c 6576 656c  reservoir1_level
-000005f0: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000600: 2274 7970 6522 3a20 2269 6e74 6572 706f  "type": "interpo
-00000610: 6c61 7465 6476 6f6c 756d 6522 2c0d 0a20  latedvolume",.. 
-00000620: 2020 2020 2020 2020 2022 6e6f 6465 223a           "node":
-00000630: 2022 7265 7365 7276 6f69 7231 222c 0d0a   "reservoir1",..
-00000640: 2020 2020 2020 2020 2020 2276 6f6c 756d            "volum
-00000650: 6573 223a 205b 302c 2032 3530 3030 2c20  es": [0, 25000, 
-00000660: 3530 3030 302c 2037 3530 3030 2c20 3130  50000, 75000, 10
-00000670: 3030 3030 2c20 3135 3030 3030 2c20 3230  0000, 150000, 20
-00000680: 3030 3030 5d2c 0d0a 2020 2020 2020 2020  0000],..        
-00000690: 2020 2276 616c 7565 7322 3a20 5b30 2c20    "values": [0, 
-000006a0: 3239 2e32 2c20 3336 2e38 2c20 3432 2e32  29.2, 36.8, 42.2
-000006b0: 2c20 3436 2e34 2c20 3533 2e31 2c20 3538  , 46.4, 53.1, 58
-000006c0: 2e35 5d2c 0d0a 2020 2020 2020 2020 2020  .5],..          
-000006d0: 226b 696e 6422 3a20 2263 7562 6963 220d  "kind": "cubic".
-000006e0: 0a20 2020 2020 2020 207d 2c0d 0a20 2020  .        },..   
-000006f0: 2020 2020 2022 7475 7262 696e 6531 5f64       "turbine1_d
-00000700: 6973 6368 6172 6765 223a 207b 0d0a 2020  ischarge": {..  
-00000710: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000720: 3a20 2248 7964 726f 506f 7765 7254 6172  : "HydroPowerTar
-00000730: 6765 7422 2c0d 0a20 2020 2020 2020 2020  get",..         
-00000740: 2020 2022 7761 7465 725f 656c 6576 6174     "water_elevat
-00000750: 696f 6e5f 7061 7261 6d65 7465 7222 3a20  ion_parameter": 
-00000760: 2272 6573 6572 766f 6972 315f 6c65 7665  "reservoir1_leve
-00000770: 6c22 2c0d 0a20 2020 2020 2020 2020 2020  l",..           
-00000780: 2022 7475 7262 696e 655f 656c 6576 6174   "turbine_elevat
-00000790: 696f 6e22 3a20 3335 2e30 2c0d 0a20 2020  ion": 35.0,..   
-000007a0: 2020 2020 2020 2020 2022 6566 6669 6369           "effici
-000007b0: 656e 6379 223a 2030 2e38 352c 0d0a 2020  ency": 0.85,..  
-000007c0: 2020 2020 2020 2020 2020 2266 6c6f 775f            "flow_
-000007d0: 756e 6974 5f63 6f6e 7665 7273 696f 6e22  unit_conversion"
-000007e0: 3a20 3165 332c 0d0a 2020 2020 2020 2020  : 1e3,..        
-000007f0: 2020 2020 2274 6172 6765 7422 3a20 7b20      "target": { 
-00000800: 2274 7970 6522 3a20 2263 6f6e 7374 616e  "type": "constan
-00000810: 7422 2c20 2276 616c 7565 223a 2031 6535  t", "value": 1e5
-00000820: 7d2c 0d0a 2020 2020 2020 2020 2020 2020  },..            
-00000830: 226d 6178 5f66 6c6f 7722 3a20 7b20 2274  "max_flow": { "t
-00000840: 7970 6522 3a20 2263 6f6e 7374 616e 7422  ype": "constant"
-00000850: 2c20 2276 616c 7565 223a 2031 3030 302e  , "value": 1000.
-00000860: 307d 2c0d 0a20 2020 2020 2020 2020 2020  0},..           
-00000870: 2022 6d69 6e5f 666c 6f77 223a 207b 2022   "min_flow": { "
-00000880: 7479 7065 223a 2022 636f 6e73 7461 6e74  type": "constant
-00000890: 222c 2022 7661 6c75 6522 3a20 3530 302e  ", "value": 500.
-000008a0: 307d 0d0a 2020 2020 2020 2020 7d0d 0a20  0}..        }.. 
-000008b0: 2020 207d 2c0d 0a20 2020 2022 7265 636f     },..    "reco
-000008c0: 7264 6572 7322 3a20 7b0d 0a20 2020 2020  rders": {..     
-000008d0: 2020 2022 7475 7262 696e 6531 5f65 6e65     "turbine1_ene
-000008e0: 7267 7922 3a20 7b0d 0a20 2020 2020 2020  rgy": {..       
-000008f0: 2020 2020 2022 7479 7065 223a 2022 4879       "type": "Hy
-00000900: 6472 6f50 6f77 6572 5265 636f 7264 6572  droPowerRecorder
-00000910: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000920: 226e 6f64 6522 3a20 2274 7572 6269 6e65  "node": "turbine
-00000930: 3122 2c0d 0a20 2020 2020 2020 2020 2020  1",..           
-00000940: 2022 7761 7465 725f 656c 6576 6174 696f   "water_elevatio
-00000950: 6e5f 7061 7261 6d65 7465 7222 3a20 2272  n_parameter": "r
-00000960: 6573 6572 766f 6972 315f 6c65 7665 6c22  eservoir1_level"
-00000970: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000980: 7475 7262 696e 655f 656c 6576 6174 696f  turbine_elevatio
-00000990: 6e22 3a20 3335 2e30 2c0d 0a20 2020 2020  n": 35.0,..     
-000009a0: 2020 2020 2020 2022 6566 6669 6369 656e         "efficien
-000009b0: 6379 223a 2030 2e38 352c 0d0a 2020 2020  cy": 0.85,..    
-000009c0: 2020 2020 2020 2020 2266 6c6f 775f 756e          "flow_un
-000009d0: 6974 5f63 6f6e 7665 7273 696f 6e22 3a20  it_conversion": 
-000009e0: 3165 330d 0a20 2020 2020 2020 207d 2c0d  1e3..        },.
-000009f0: 0a20 2020 2020 2020 2022 6361 7463 686d  .        "catchm
-00000a00: 656e 7431 5f66 6c6f 7722 3a20 7b0d 0a20  ent1_flow": {.. 
-00000a10: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000a20: 223a 2022 6e75 6d70 7961 7272 6179 6e6f  ": "numpyarrayno
-00000a30: 6465 7265 636f 7264 6572 222c 0d0a 2020  derecorder",..  
-00000a40: 2020 2020 2020 2020 2020 226e 6f64 6522            "node"
-00000a50: 3a20 2263 6174 6368 6d65 6e74 3122 0d0a  : "catchment1"..
-00000a60: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000a70: 2020 2020 2272 6573 6572 766f 6972 315f      "reservoir1_
-00000a80: 7374 6f72 6167 6522 3a20 7b0d 0a20 2020  storage": {..   
-00000a90: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000aa0: 2022 6e75 6d70 7961 7272 6179 7374 6f72   "numpyarraystor
-00000ab0: 6167 6572 6563 6f72 6465 7222 2c0d 0a20  agerecorder",.. 
-00000ac0: 2020 2020 2020 2020 2020 2022 6e6f 6465             "node
-00000ad0: 223a 2022 7265 7365 7276 6f69 7231 220d  ": "reservoir1".
-00000ae0: 0a20 2020 2020 2020 207d 2c0d 0a20 2020  .        },..   
-00000af0: 2020 2020 2022 7475 7262 696e 6531 5f66       "turbine1_f
-00000b00: 6c6f 7722 3a20 7b0d 0a20 2020 2020 2020  low": {..       
-00000b10: 2020 2020 2022 7479 7065 223a 2022 6e75       "type": "nu
-00000b20: 6d70 7961 7272 6179 6e6f 6465 7265 636f  mpyarraynodereco
-00000b30: 7264 6572 222c 0d0a 2020 2020 2020 2020  rder",..        
-00000b40: 2020 2020 226e 6f64 6522 3a20 2274 7572      "node": "tur
-00000b50: 6269 6e65 3122 0d0a 2020 2020 2020 2020  bine1"..        
-00000b60: 7d2c 0d0a 2020 2020 2020 2020 2272 656c  },..        "rel
-00000b70: 6561 7365 315f 666c 6f77 223a 207b 0d0a  ease1_flow": {..
-00000b80: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000b90: 6522 3a20 226e 756d 7079 6172 7261 796e  e": "numpyarrayn
-00000ba0: 6f64 6572 6563 6f72 6465 7222 2c0d 0a20  oderecorder",.. 
-00000bb0: 2020 2020 2020 2020 2020 2022 6e6f 6465             "node
-00000bc0: 223a 2022 7265 6c65 6173 6531 220d 0a20  ": "release1".. 
-00000bd0: 2020 2020 2020 207d 0d0a 2020 2020 7d0d         }..    }.
-00000be0: 0a7d 0d0a                                .}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 4879 6472 6f70 6f77 6572  le": "Hydropower
+00000030: 2065 7861 6d70 6c65 222c 0a20 2020 2020   example",.     
+00000040: 2020 2022 6465 7363 7269 7074 696f 6e22     "description"
+00000050: 3a20 2241 206d 6f64 656c 2077 6974 6820  : "A model with 
+00000060: 6120 7369 6e67 6c65 2072 6573 6572 766f  a single reservo
+00000070: 6972 2061 6e64 2068 7964 726f 2d70 6f77  ir and hydro-pow
+00000080: 6572 2072 6563 6f72 6465 7222 2c0a 2020  er recorder",.  
+00000090: 2020 2020 2020 226d 696e 696d 756d 5f76        "minimum_v
+000000a0: 6572 7369 6f6e 223a 2022 302e 3622 0a20  ersion": "0.6". 
+000000b0: 2020 207d 2c0a 2020 2020 2274 696d 6573     },.    "times
+000000c0: 7465 7070 6572 223a 207b 0a20 2020 2020  tepper": {.     
+000000d0: 2020 2022 7374 6172 7422 3a20 2232 3130     "start": "210
+000000e0: 302d 3031 2d30 3122 2c0a 2020 2020 2020  0-01-01",.      
+000000f0: 2020 2265 6e64 223a 2022 3231 3031 2d30    "end": "2101-0
+00000100: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+00000110: 7469 6d65 7374 6570 223a 2037 0a20 2020  timestep": 7.   
+00000120: 207d 2c0a 2020 2020 226e 6f64 6573 223a   },.    "nodes":
+00000130: 205b 0a20 2020 2020 2020 207b 0a20 2020   [.        {.   
+00000140: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000150: 2022 6361 7463 686d 656e 7431 222c 0a20   "catchment1",. 
+00000160: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000170: 223a 2022 6361 7463 686d 656e 7422 2c0a  ": "catchment",.
+00000180: 2020 2020 2020 2020 2020 2020 2266 6c6f              "flo
+00000190: 7722 3a20 302e 300a 2020 2020 2020 2020  w": 0.0.        
+000001a0: 7d2c 0a20 2020 2020 2020 207b 0a20 2020  },.        {.   
+000001b0: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+000001c0: 2022 7265 7365 7276 6f69 7231 222c 0a20   "reservoir1",. 
+000001d0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+000001e0: 223a 2022 7374 6f72 6167 6522 2c0a 2020  ": "storage",.  
+000001f0: 2020 2020 2020 2020 2020 226d 6178 5f76            "max_v
+00000200: 6f6c 756d 6522 3a20 3230 3030 3030 2c0a  olume": 200000,.
+00000210: 2020 2020 2020 2020 2020 2020 2269 6e69              "ini
+00000220: 7469 616c 5f76 6f6c 756d 6522 3a20 3230  tial_volume": 20
+00000230: 3030 3030 0a20 2020 2020 2020 207d 2c0a  0000.        },.
+00000240: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000250: 2020 2020 2020 226e 616d 6522 3a20 2272        "name": "r
+00000260: 656c 6561 7365 3122 2c0a 2020 2020 2020  elease1",.      
+00000270: 2020 2020 2020 2274 7970 6522 3a20 226c        "type": "l
+00000280: 696e 6b22 2c0a 2020 2020 2020 2020 2020  ink",.          
+00000290: 2020 226d 6178 5f66 6c6f 7722 3a20 3130    "max_flow": 10
+000002a0: 2c0a 2020 2020 2020 2020 2020 2020 2263  ,.            "c
+000002b0: 6f73 7422 3a20 2d35 3030 0a20 2020 2020  ost": -500.     
+000002c0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+000002d0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+000002e0: 6522 3a20 2274 7572 6269 6e65 3122 2c0a  e": "turbine1",.
+000002f0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000300: 6522 3a20 226c 696e 6b22 2c0a 2020 2020  e": "link",.    
+00000310: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
+00000320: 7722 3a20 2274 7572 6269 6e65 315f 6469  w": "turbine1_di
+00000330: 7363 6861 7267 6522 2c0a 2020 2020 2020  scharge",.      
+00000340: 2020 2020 2020 2263 6f73 7422 3a20 2d32        "cost": -2
+00000350: 3030 0a20 2020 2020 2020 207d 2c0a 2020  00.        },.  
+00000360: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000370: 2020 2020 226e 616d 6522 3a20 2273 7069      "name": "spi
+00000380: 6c6c 3122 2c0a 2020 2020 2020 2020 2020  ll1",.          
+00000390: 2020 2274 7970 6522 3a20 226c 696e 6b22    "type": "link"
+000003a0: 2c0a 2020 2020 2020 2020 2020 2020 2263  ,.            "c
+000003b0: 6f73 7422 3a20 3130 3030 0a20 2020 2020  ost": 1000.     
+000003c0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+000003d0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+000003e0: 6522 3a20 2272 6561 6368 3122 2c0a 2020  e": "reach1",.  
+000003f0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000400: 3a20 226c 696e 6b22 0a20 2020 2020 2020  : "link".       
+00000410: 207d 2c0a 2020 2020 2020 2020 7b0a 2020   },.        {.  
+00000420: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
+00000430: 3a20 2265 6e64 3122 2c0a 2020 2020 2020  : "end1",.      
+00000440: 2020 2020 2020 2274 7970 6522 3a20 226f        "type": "o
+00000450: 7574 7075 7422 0a20 2020 2020 2020 207d  utput".        }
+00000460: 0a20 2020 205d 2c0a 2020 2020 2265 6467  .    ],.    "edg
+00000470: 6573 223a 205b 0a20 2020 2020 2020 205b  es": [.        [
+00000480: 2263 6174 6368 6d65 6e74 3122 2c20 2272  "catchment1", "r
+00000490: 6573 6572 766f 6972 3122 5d2c 0a20 2020  eservoir1"],.   
+000004a0: 2020 2020 205b 2272 6573 6572 766f 6972       ["reservoir
+000004b0: 3122 2c20 2272 656c 6561 7365 3122 5d2c  1", "release1"],
+000004c0: 0a20 2020 2020 2020 205b 2272 6573 6572  .        ["reser
+000004d0: 766f 6972 3122 2c20 2274 7572 6269 6e65  voir1", "turbine
+000004e0: 3122 5d2c 0a20 2020 2020 2020 205b 2272  1"],.        ["r
+000004f0: 6573 6572 766f 6972 3122 2c20 2273 7069  eservoir1", "spi
+00000500: 6c6c 3122 5d2c 0a20 2020 2020 2020 205b  ll1"],.        [
+00000510: 2272 656c 6561 7365 3122 2c20 2272 6561  "release1", "rea
+00000520: 6368 3122 5d2c 0a20 2020 2020 2020 205b  ch1"],.        [
+00000530: 2274 7572 6269 6e65 3122 2c20 2272 6561  "turbine1", "rea
+00000540: 6368 3122 5d2c 0a20 2020 2020 2020 205b  ch1"],.        [
+00000550: 2273 7069 6c6c 3122 2c20 2272 6561 6368  "spill1", "reach
+00000560: 3122 5d2c 0a20 2020 2020 2020 205b 2272  1"],.        ["r
+00000570: 6561 6368 3122 2c20 2265 6e64 3122 5d0a  each1", "end1"].
+00000580: 2020 2020 5d2c 0a20 2020 2022 7061 7261      ],.    "para
+00000590: 6d65 7465 7273 223a 207b 0a20 2020 2020  meters": {.     
+000005a0: 2020 2022 7265 7365 7276 6f69 7231 5f6c     "reservoir1_l
+000005b0: 6576 656c 223a 207b 0a20 2020 2020 2020  evel": {.       
+000005c0: 2020 2022 7479 7065 223a 2022 696e 7465     "type": "inte
+000005d0: 7270 6f6c 6174 6564 766f 6c75 6d65 222c  rpolatedvolume",
+000005e0: 0a20 2020 2020 2020 2020 2022 6e6f 6465  .          "node
+000005f0: 223a 2022 7265 7365 7276 6f69 7231 222c  ": "reservoir1",
+00000600: 0a20 2020 2020 2020 2020 2022 766f 6c75  .          "volu
+00000610: 6d65 7322 3a20 5b30 2c20 3235 3030 302c  mes": [0, 25000,
+00000620: 2035 3030 3030 2c20 3735 3030 302c 2031   50000, 75000, 1
+00000630: 3030 3030 302c 2031 3530 3030 302c 2032  00000, 150000, 2
+00000640: 3030 3030 305d 2c0a 2020 2020 2020 2020  00000],.        
+00000650: 2020 2276 616c 7565 7322 3a20 5b30 2c20    "values": [0, 
+00000660: 3239 2e32 2c20 3336 2e38 2c20 3432 2e32  29.2, 36.8, 42.2
+00000670: 2c20 3436 2e34 2c20 3533 2e31 2c20 3538  , 46.4, 53.1, 58
+00000680: 2e35 5d2c 0a20 2020 2020 2020 2020 2022  .5],.          "
+00000690: 6b69 6e64 223a 2022 6375 6269 6322 0a20  kind": "cubic". 
+000006a0: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+000006b0: 2020 2274 7572 6269 6e65 315f 6469 7363    "turbine1_disc
+000006c0: 6861 7267 6522 3a20 7b0a 2020 2020 2020  harge": {.      
+000006d0: 2020 2020 2020 2274 7970 6522 3a20 2248        "type": "H
+000006e0: 7964 726f 506f 7765 7254 6172 6765 7422  ydroPowerTarget"
+000006f0: 2c0a 2020 2020 2020 2020 2020 2020 2277  ,.            "w
+00000700: 6174 6572 5f65 6c65 7661 7469 6f6e 5f70  ater_elevation_p
+00000710: 6172 616d 6574 6572 223a 2022 7265 7365  arameter": "rese
+00000720: 7276 6f69 7231 5f6c 6576 656c 222c 0a20  rvoir1_level",. 
+00000730: 2020 2020 2020 2020 2020 2022 7475 7262             "turb
+00000740: 696e 655f 656c 6576 6174 696f 6e22 3a20  ine_elevation": 
+00000750: 3335 2e30 2c0a 2020 2020 2020 2020 2020  35.0,.          
+00000760: 2020 2265 6666 6963 6965 6e63 7922 3a20    "efficiency": 
+00000770: 302e 3835 2c0a 2020 2020 2020 2020 2020  0.85,.          
+00000780: 2020 2266 6c6f 775f 756e 6974 5f63 6f6e    "flow_unit_con
+00000790: 7665 7273 696f 6e22 3a20 3165 332c 0a20  version": 1e3,. 
+000007a0: 2020 2020 2020 2020 2020 2022 7461 7267             "targ
+000007b0: 6574 223a 207b 2022 7479 7065 223a 2022  et": { "type": "
+000007c0: 636f 6e73 7461 6e74 222c 2022 7661 6c75  constant", "valu
+000007d0: 6522 3a20 3165 357d 2c0a 2020 2020 2020  e": 1e5},.      
+000007e0: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
+000007f0: 3a20 7b20 2274 7970 6522 3a20 2263 6f6e  : { "type": "con
+00000800: 7374 616e 7422 2c20 2276 616c 7565 223a  stant", "value":
+00000810: 2031 3030 302e 307d 2c0a 2020 2020 2020   1000.0},.      
+00000820: 2020 2020 2020 226d 696e 5f66 6c6f 7722        "min_flow"
+00000830: 3a20 7b20 2274 7970 6522 3a20 2263 6f6e  : { "type": "con
+00000840: 7374 616e 7422 2c20 2276 616c 7565 223a  stant", "value":
+00000850: 2035 3030 2e30 7d0a 2020 2020 2020 2020   500.0}.        
+00000860: 7d0a 2020 2020 7d2c 0a20 2020 2022 7265  }.    },.    "re
+00000870: 636f 7264 6572 7322 3a20 7b0a 2020 2020  corders": {.    
+00000880: 2020 2020 2274 7572 6269 6e65 315f 656e      "turbine1_en
+00000890: 6572 6779 223a 207b 0a20 2020 2020 2020  ergy": {.       
+000008a0: 2020 2020 2022 7479 7065 223a 2022 4879       "type": "Hy
+000008b0: 6472 6f50 6f77 6572 5265 636f 7264 6572  droPowerRecorder
+000008c0: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+000008d0: 6e6f 6465 223a 2022 7475 7262 696e 6531  node": "turbine1
+000008e0: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+000008f0: 7761 7465 725f 656c 6576 6174 696f 6e5f  water_elevation_
+00000900: 7061 7261 6d65 7465 7222 3a20 2272 6573  parameter": "res
+00000910: 6572 766f 6972 315f 6c65 7665 6c22 2c0a  ervoir1_level",.
+00000920: 2020 2020 2020 2020 2020 2020 2274 7572              "tur
+00000930: 6269 6e65 5f65 6c65 7661 7469 6f6e 223a  bine_elevation":
+00000940: 2033 352e 302c 0a20 2020 2020 2020 2020   35.0,.         
+00000950: 2020 2022 6566 6669 6369 656e 6379 223a     "efficiency":
+00000960: 2030 2e38 352c 0a20 2020 2020 2020 2020   0.85,.         
+00000970: 2020 2022 666c 6f77 5f75 6e69 745f 636f     "flow_unit_co
+00000980: 6e76 6572 7369 6f6e 223a 2031 6533 0a20  nversion": 1e3. 
+00000990: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+000009a0: 2020 2263 6174 6368 6d65 6e74 315f 666c    "catchment1_fl
+000009b0: 6f77 223a 207b 0a20 2020 2020 2020 2020  ow": {.         
+000009c0: 2020 2022 7479 7065 223a 2022 6e75 6d70     "type": "nump
+000009d0: 7961 7272 6179 6e6f 6465 7265 636f 7264  yarraynoderecord
+000009e0: 6572 222c 0a20 2020 2020 2020 2020 2020  er",.           
+000009f0: 2022 6e6f 6465 223a 2022 6361 7463 686d   "node": "catchm
+00000a00: 656e 7431 220a 2020 2020 2020 2020 7d2c  ent1".        },
+00000a10: 0a20 2020 2020 2020 2022 7265 7365 7276  .        "reserv
+00000a20: 6f69 7231 5f73 746f 7261 6765 223a 207b  oir1_storage": {
+00000a30: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000a40: 7065 223a 2022 6e75 6d70 7961 7272 6179  pe": "numpyarray
+00000a50: 7374 6f72 6167 6572 6563 6f72 6465 7222  storagerecorder"
+00000a60: 2c0a 2020 2020 2020 2020 2020 2020 226e  ,.            "n
+00000a70: 6f64 6522 3a20 2272 6573 6572 766f 6972  ode": "reservoir
+00000a80: 3122 0a20 2020 2020 2020 207d 2c0a 2020  1".        },.  
+00000a90: 2020 2020 2020 2274 7572 6269 6e65 315f        "turbine1_
+00000aa0: 666c 6f77 223a 207b 0a20 2020 2020 2020  flow": {.       
+00000ab0: 2020 2020 2022 7479 7065 223a 2022 6e75       "type": "nu
+00000ac0: 6d70 7961 7272 6179 6e6f 6465 7265 636f  mpyarraynodereco
+00000ad0: 7264 6572 222c 0a20 2020 2020 2020 2020  rder",.         
+00000ae0: 2020 2022 6e6f 6465 223a 2022 7475 7262     "node": "turb
+00000af0: 696e 6531 220a 2020 2020 2020 2020 7d2c  ine1".        },
+00000b00: 0a20 2020 2020 2020 2022 7265 6c65 6173  .        "releas
+00000b10: 6531 5f66 6c6f 7722 3a20 7b0a 2020 2020  e1_flow": {.    
+00000b20: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000b30: 226e 756d 7079 6172 7261 796e 6f64 6572  "numpyarraynoder
+00000b40: 6563 6f72 6465 7222 2c0a 2020 2020 2020  ecorder",.      
+00000b50: 2020 2020 2020 226e 6f64 6522 3a20 2272        "node": "r
+00000b60: 656c 6561 7365 3122 0a20 2020 2020 2020  elease1".       
+00000b70: 207d 0a20 2020 207d 0a7d 0a               }.    }.}.
```

### Comparing `pywr-1.8.0/tests/models/invalid_include.json` & `pywr-1.9.0/tests/models/invalid_include.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 22% similar despite different names*

```diff
@@ -1,45 +1,43 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 496e 7661 6c69 6420  itle": "Invalid 
-00000030: 696e 636c 7564 6522 2c0d 0a20 2020 2020  include",..     
-00000040: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
-00000050: 696f 6e22 3a20 2230 2e32 6465 7622 0d0a  ion": "0.2dev"..
-00000060: 2020 2020 7d2c 0d0a 2020 2020 2274 696d      },..    "tim
-00000070: 6573 7465 7070 6572 223a 207b 0d0a 2020  estepper": {..  
-00000080: 2020 2020 2020 2273 7461 7274 223a 2022        "start": "
-00000090: 3230 3135 2d30 312d 3031 222c 0d0a 2020  2015-01-01",..  
-000000a0: 2020 2020 2020 2265 6e64 223a 2022 3230        "end": "20
-000000b0: 3135 2d31 322d 3331 222c 0d0a 2020 2020  15-12-31",..    
-000000c0: 2020 2020 2274 696d 6573 7465 7022 3a20      "timestep": 
-000000d0: 310d 0a20 2020 207d 2c0d 0a20 2020 2022  1..    },..    "
-000000e0: 696e 636c 7564 6573 223a 205b 0d0a 2020  includes": [..  
-000000f0: 2020 2020 2020 2269 6e76 616c 6964 2e6a        "invalid.j
-00000100: 736f 6e22 0d0a 2020 2020 5d2c 0d0a 2020  son"..    ],..  
-00000110: 2020 226e 6f64 6573 223a 205b 0d0a 2020    "nodes": [..  
-00000120: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000130: 2020 2020 2022 6e61 6d65 223a 2022 7375       "name": "su
-00000140: 7070 6c79 3122 2c0d 0a20 2020 2020 2020  pply1",..       
-00000150: 2020 2020 2022 7479 7065 223a 2022 496e       "type": "In
-00000160: 7075 7422 2c0d 0a20 2020 2020 2020 2020  put",..         
-00000170: 2020 2022 6d61 785f 666c 6f77 223a 2031     "max_flow": 1
-00000180: 350d 0a20 2020 2020 2020 207d 2c0d 0a20  5..        },.. 
-00000190: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-000001a0: 2020 2020 2020 226e 616d 6522 3a20 226c        "name": "l
-000001b0: 696e 6b31 222c 0d0a 2020 2020 2020 2020  ink1",..        
-000001c0: 2020 2020 2274 7970 6522 3a20 224c 696e      "type": "Lin
-000001d0: 6b22 0d0a 2020 2020 2020 2020 7d2c 0d0a  k"..        },..
-000001e0: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-000001f0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000200: 6465 6d61 6e64 3122 2c0d 0a20 2020 2020  demand1",..     
-00000210: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000220: 4f75 7470 7574 222c 0d0a 2020 2020 2020  Output",..      
-00000230: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
-00000240: 3a20 3130 2c0d 0a20 2020 2020 2020 2020  : 10,..         
-00000250: 2020 2022 636f 7374 223a 202d 3130 0d0a     "cost": -10..
-00000260: 2020 2020 2020 2020 7d0d 0a20 2020 205d          }..    ]
-00000270: 2c0d 0a20 2020 2022 6564 6765 7322 3a20  ,..    "edges": 
-00000280: 5b0d 0a20 2020 2020 2020 205b 2273 7570  [..        ["sup
-00000290: 706c 7931 222c 2022 6c69 6e6b 3122 5d2c  ply1", "link1"],
-000002a0: 0d0a 2020 2020 2020 2020 5b22 6c69 6e6b  ..        ["link
-000002b0: 3122 2c20 2264 656d 616e 6431 225d 0d0a  1", "demand1"]..
-000002c0: 2020 2020 5d0d 0a7d 0d0a                     ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 496e 7661 6c69 6420 696e  le": "Invalid in
+00000030: 636c 7564 6522 2c0a 2020 2020 2020 2020  clude",.        
+00000040: 226d 696e 696d 756d 5f76 6572 7369 6f6e  "minimum_version
+00000050: 223a 2022 302e 3264 6576 220a 2020 2020  ": "0.2dev".    
+00000060: 7d2c 0a20 2020 2022 7469 6d65 7374 6570  },.    "timestep
+00000070: 7065 7222 3a20 7b0a 2020 2020 2020 2020  per": {.        
+00000080: 2273 7461 7274 223a 2022 3230 3135 2d30  "start": "2015-0
+00000090: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+000000a0: 656e 6422 3a20 2232 3031 352d 3132 2d33  end": "2015-12-3
+000000b0: 3122 2c0a 2020 2020 2020 2020 2274 696d  1",.        "tim
+000000c0: 6573 7465 7022 3a20 310a 2020 2020 7d2c  estep": 1.    },
+000000d0: 0a20 2020 2022 696e 636c 7564 6573 223a  .    "includes":
+000000e0: 205b 0a20 2020 2020 2020 2022 696e 7661   [.        "inva
+000000f0: 6c69 642e 6a73 6f6e 220a 2020 2020 5d2c  lid.json".    ],
+00000100: 0a20 2020 2022 6e6f 6465 7322 3a20 5b0a  .    "nodes": [.
+00000110: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000120: 2020 2020 2020 226e 616d 6522 3a20 2273        "name": "s
+00000130: 7570 706c 7931 222c 0a20 2020 2020 2020  upply1",.       
+00000140: 2020 2020 2022 7479 7065 223a 2022 496e       "type": "In
+00000150: 7075 7422 2c0a 2020 2020 2020 2020 2020  put",.          
+00000160: 2020 226d 6178 5f66 6c6f 7722 3a20 3135    "max_flow": 15
+00000170: 0a20 2020 2020 2020 207d 2c0a 2020 2020  .        },.    
+00000180: 2020 2020 7b0a 2020 2020 2020 2020 2020      {.          
+00000190: 2020 226e 616d 6522 3a20 226c 696e 6b31    "name": "link1
+000001a0: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+000001b0: 7479 7065 223a 2022 4c69 6e6b 220a 2020  type": "Link".  
+000001c0: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+000001d0: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+000001e0: 6e61 6d65 223a 2022 6465 6d61 6e64 3122  name": "demand1"
+000001f0: 2c0a 2020 2020 2020 2020 2020 2020 2274  ,.            "t
+00000200: 7970 6522 3a20 224f 7574 7075 7422 2c0a  ype": "Output",.
+00000210: 2020 2020 2020 2020 2020 2020 226d 6178              "max
+00000220: 5f66 6c6f 7722 3a20 3130 2c0a 2020 2020  _flow": 10,.    
+00000230: 2020 2020 2020 2020 2263 6f73 7422 3a20          "cost": 
+00000240: 2d31 300a 2020 2020 2020 2020 7d0a 2020  -10.        }.  
+00000250: 2020 5d2c 0a20 2020 2022 6564 6765 7322    ],.    "edges"
+00000260: 3a20 5b0a 2020 2020 2020 2020 5b22 7375  : [.        ["su
+00000270: 7070 6c79 3122 2c20 226c 696e 6b31 225d  pply1", "link1"]
+00000280: 2c0a 2020 2020 2020 2020 5b22 6c69 6e6b  ,.        ["link
+00000290: 3122 2c20 2264 656d 616e 6431 225d 0a20  1", "demand1"]. 
+000002a0: 2020 205d 0a7d 0a                           ].}.
```

### Comparing `pywr-1.8.0/tests/models/multiindex_df.json` & `pywr-1.9.0/tests/models/simple_df_shared.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7451388888888889%*

 * *Differences: {"'metadata'": "{'title': 'Simple 1', 'description': 'A very simple example using an external data "*

 * *               "file shared between two parameters.', 'minimum_version': '0.2dev'}",*

 * * "'nodes'": "{2: {'max_flow': {'index': 'demand1', 'table': 'simple_data', delete: ['url', "*

 * *            "'index_col']}, 'cost': {'index': 'demand1', 'table': 'simple_data', delete: ['url', "*

 * *            "'index_col']}}}",*

 * * "'tables'": "OrderedDict([('simple_data', OrderedDict([('url', 'simple_data.csv'), ('index_col', "*

 * *    […]*

```diff
@@ -6,58 +6,50 @@
         ],
         [
             "link1",
             "demand1"
         ]
     ],
     "metadata": {
-        "description": "An example of loading a multi-index parameter.",
-        "minimum_version": "0.1",
-        "title": "Multiindex 1"
+        "description": "A very simple example using an external data file shared between two parameters.",
+        "minimum_version": "0.2dev",
+        "title": "Simple 1"
     },
     "nodes": [
         {
             "max_flow": 15,
             "name": "supply1",
             "type": "Input"
         },
         {
             "name": "link1",
             "type": "Link"
         },
         {
             "cost": {
                 "column": "cost",
-                "index": [
-                    1,
-                    "demand1"
-                ],
-                "index_col": [
-                    "level",
-                    "node"
-                ],
-                "type": "constant",
-                "url": "multiindex_data.csv"
+                "index": "demand1",
+                "table": "simple_data",
+                "type": "constant"
             },
             "max_flow": {
                 "column": "max_flow",
-                "index": [
-                    0,
-                    "demand1"
-                ],
-                "index_col": [
-                    "level",
-                    "node"
-                ],
-                "type": "constant",
-                "url": "multiindex_data.csv"
+                "index": "demand1",
+                "table": "simple_data",
+                "type": "constant"
             },
             "name": "demand1",
             "type": "Output"
         }
     ],
+    "tables": {
+        "simple_data": {
+            "index_col": "node",
+            "url": "simple_data.csv"
+        }
+    },
     "timestepper": {
         "end": "2015-12-31",
         "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/tests/models/parameter_reference.json` & `pywr-1.9.0/tests/models/reservoir2.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.6316666666666666%*

 * *Differences: {"'edges'": "{insert: [(2, ['catchment1', 'abs1']), (3, ['abs1', 'supply1']), (4, ['abs1', "*

 * *            "'term1'])]}",*

 * * "'metadata'": "{'title': 'Reservoir 2', 'description': 'Model with a reservoir, fed by a river "*

 * *               "abstraction'}",*

 * * "'nodes'": "{2: {'name': 'demand1', 'type': 'Output', 'max_flow': 15, 'cost': -10}, 4: {'name': "*

 * *            "'abs1', 'type': 'Link', 'max_flow': 5}, 5: {'name': 'term1', 'cost': 1, delete: "*

 * *            "['max_flow']}, insert: [(0, OrderedDict([('name', 'supp […]*

```diff
@@ -3,56 +3,74 @@
         [
             "supply1",
             "link1"
         ],
         [
             "link1",
             "demand1"
+        ],
+        [
+            "catchment1",
+            "abs1"
+        ],
+        [
+            "abs1",
+            "supply1"
+        ],
+        [
+            "abs1",
+            "term1"
         ]
     ],
     "metadata": {
-        "description": "Most basic example of parameter references",
+        "description": "Model with a reservoir, fed by a river abstraction",
         "minimum_version": "0.1",
-        "title": "Parameter reference"
+        "title": "Reservoir 2"
     },
     "nodes": [
         {
-            "cost": {
+            "initial_volume": 35,
+            "initial_volume_pc": 1.0,
+            "max_volume": {
                 "type": "constant",
-                "values": 0.0
+                "value": 35
+            },
+            "min_volume": {
+                "type": "constant",
+                "value": 0
             },
-            "max_flow": "supply_max_flow",
             "name": "supply1",
-            "type": "Input"
+            "type": "Storage"
         },
         {
-            "max_flow": {
-                "name": "link_max_flow",
-                "type": "constant",
-                "values": 9999
-            },
             "name": "link1",
-            "type": "link"
+            "type": "Link"
         },
         {
-            "cost": "demand_cost",
-            "max_flow": 10,
+            "cost": -10,
+            "max_flow": 15,
             "name": "demand1",
             "type": "Output"
-        }
-    ],
-    "parameters": {
-        "demand_cost": {
-            "type": "constant",
-            "values": -10
         },
-        "supply_max_flow": {
-            "type": "constant",
-            "values": 125.0
+        {
+            "max_flow": 5,
+            "min_flow": 5,
+            "name": "catchment1",
+            "type": "Input"
+        },
+        {
+            "max_flow": 5,
+            "name": "abs1",
+            "type": "Link"
+        },
+        {
+            "cost": 1,
+            "name": "term1",
+            "type": "Output"
         }
-    },
+    ],
     "timestepper": {
         "end": "2015-12-31",
         "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/tests/models/piecewise1.json` & `pywr-1.9.0/tests/models/piecewise1.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 21% similar despite different names*

```diff
@@ -1,50 +1,48 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5069 6563 6577 6973  itle": "Piecewis
-00000030: 6520 6c69 6e6b 222c 0d0a 2020 2020 2020  e link",..      
-00000040: 2020 2264 6573 6372 6970 7469 6f6e 223a    "description":
-00000050: 2022 4578 616d 706c 6520 6f66 2061 2070   "Example of a p
-00000060: 6965 6365 7769 7365 206c 696e 6b22 2c0d  iecewise link",.
-00000070: 0a20 2020 2020 2020 2022 6d69 6e69 6d75  .        "minimu
-00000080: 6d5f 7665 7273 696f 6e22 3a20 2230 2e31  m_version": "0.1
-00000090: 220d 0a20 2020 207d 2c0d 0a20 2020 2022  "..    },..    "
-000000a0: 7469 6d65 7374 6570 7065 7222 3a20 7b0d  timestepper": {.
-000000b0: 0a20 2020 2020 2020 2022 7374 6172 7422  .        "start"
-000000c0: 3a20 2232 3031 352d 3031 2d30 3122 2c0d  : "2015-01-01",.
-000000d0: 0a20 2020 2020 2020 2022 656e 6422 3a20  .        "end": 
-000000e0: 2232 3031 352d 3132 2d33 3122 2c0d 0a20  "2015-12-31",.. 
-000000f0: 2020 2020 2020 2022 7469 6d65 7374 6570         "timestep
-00000100: 223a 2031 0d0a 2020 2020 7d2c 0d0a 2020  ": 1..    },..  
-00000110: 2020 226e 6f64 6573 223a 205b 0d0a 2020    "nodes": [..  
-00000120: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000130: 2020 2020 2022 6e61 6d65 223a 2022 7375       "name": "su
-00000140: 7070 6c79 3122 2c0d 0a20 2020 2020 2020  pply1",..       
-00000150: 2020 2020 2022 7479 7065 223a 2022 496e       "type": "In
-00000160: 7075 7422 2c0d 0a20 2020 2020 2020 2020  put",..         
-00000170: 2020 2022 6d61 785f 666c 6f77 223a 2031     "max_flow": 1
-00000180: 3030 0d0a 2020 2020 2020 2020 7d2c 0d0a  00..        },..
-00000190: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-000001a0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-000001b0: 6c69 6e6b 3122 2c0d 0a20 2020 2020 2020  link1",..       
-000001c0: 2020 2020 2022 7479 7065 223a 2022 7069       "type": "pi
-000001d0: 6563 6577 6973 656c 696e 6b22 2c0d 0a20  ecewiselink",.. 
-000001e0: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-000001f0: 223a 205b 2d31 302e 302c 2035 2e30 5d2c  ": [-10.0, 5.0],
-00000200: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-00000210: 6178 5f66 6c6f 7722 3a20 5b32 302e 302c  ax_flow": [20.0,
-00000220: 206e 756c 6c5d 0d0a 2020 2020 2020 2020   null]..        
-00000230: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-00000240: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000250: 223a 2022 6465 6d61 6e64 3122 2c0d 0a20  ": "demand1",.. 
-00000260: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000270: 223a 2022 4f75 7470 7574 222c 0d0a 2020  ": "Output",..  
-00000280: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
-00000290: 6c6f 7722 3a20 3530 2c0d 0a20 2020 2020  low": 50,..     
-000002a0: 2020 2020 2020 2022 636f 7374 223a 2030         "cost": 0
-000002b0: 2e30 0d0a 2020 2020 2020 2020 7d0d 0a20  .0..        }.. 
-000002c0: 2020 205d 2c0d 0a20 2020 2022 6564 6765     ],..    "edge
-000002d0: 7322 3a20 5b0d 0a20 2020 2020 2020 205b  s": [..        [
-000002e0: 2273 7570 706c 7931 222c 2022 6c69 6e6b  "supply1", "link
-000002f0: 3122 5d2c 0d0a 2020 2020 2020 2020 5b22  1"],..        ["
-00000300: 6c69 6e6b 3122 2c20 2264 656d 616e 6431  link1", "demand1
-00000310: 225d 0d0a 2020 2020 5d0d 0a7d 0d0a       "]..    ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5069 6563 6577 6973 6520  le": "Piecewise 
+00000030: 6c69 6e6b 222c 0a20 2020 2020 2020 2022  link",.        "
+00000040: 6465 7363 7269 7074 696f 6e22 3a20 2245  description": "E
+00000050: 7861 6d70 6c65 206f 6620 6120 7069 6563  xample of a piec
+00000060: 6577 6973 6520 6c69 6e6b 222c 0a20 2020  ewise link",.   
+00000070: 2020 2020 2022 6d69 6e69 6d75 6d5f 7665       "minimum_ve
+00000080: 7273 696f 6e22 3a20 2230 2e31 220a 2020  rsion": "0.1".  
+00000090: 2020 7d2c 0a20 2020 2022 7469 6d65 7374    },.    "timest
+000000a0: 6570 7065 7222 3a20 7b0a 2020 2020 2020  epper": {.      
+000000b0: 2020 2273 7461 7274 223a 2022 3230 3135    "start": "2015
+000000c0: 2d30 312d 3031 222c 0a20 2020 2020 2020  -01-01",.       
+000000d0: 2022 656e 6422 3a20 2232 3031 352d 3132   "end": "2015-12
+000000e0: 2d33 3122 2c0a 2020 2020 2020 2020 2274  -31",.        "t
+000000f0: 696d 6573 7465 7022 3a20 310a 2020 2020  imestep": 1.    
+00000100: 7d2c 0a20 2020 2022 6e6f 6465 7322 3a20  },.    "nodes": 
+00000110: 5b0a 2020 2020 2020 2020 7b0a 2020 2020  [.        {.    
+00000120: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
+00000130: 2273 7570 706c 7931 222c 0a20 2020 2020  "supply1",.     
+00000140: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000150: 496e 7075 7422 2c0a 2020 2020 2020 2020  Input",.        
+00000160: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
+00000170: 3130 300a 2020 2020 2020 2020 7d2c 0a20  100.        },. 
+00000180: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000190: 2020 2020 2022 6e61 6d65 223a 2022 6c69       "name": "li
+000001a0: 6e6b 3122 2c0a 2020 2020 2020 2020 2020  nk1",.          
+000001b0: 2020 2274 7970 6522 3a20 2270 6965 6365    "type": "piece
+000001c0: 7769 7365 6c69 6e6b 222c 0a20 2020 2020  wiselink",.     
+000001d0: 2020 2020 2020 2022 636f 7374 223a 205b         "cost": [
+000001e0: 2d31 302e 302c 2035 2e30 5d2c 0a20 2020  -10.0, 5.0],.   
+000001f0: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+00000200: 6f77 223a 205b 3230 2e30 2c20 6e75 6c6c  ow": [20.0, null
+00000210: 5d0a 2020 2020 2020 2020 7d2c 0a20 2020  ].        },.   
+00000220: 2020 2020 207b 0a20 2020 2020 2020 2020       {.         
+00000230: 2020 2022 6e61 6d65 223a 2022 6465 6d61     "name": "dema
+00000240: 6e64 3122 2c0a 2020 2020 2020 2020 2020  nd1",.          
+00000250: 2020 2274 7970 6522 3a20 224f 7574 7075    "type": "Outpu
+00000260: 7422 2c0a 2020 2020 2020 2020 2020 2020  t",.            
+00000270: 226d 6178 5f66 6c6f 7722 3a20 3530 2c0a  "max_flow": 50,.
+00000280: 2020 2020 2020 2020 2020 2020 2263 6f73              "cos
+00000290: 7422 3a20 302e 300a 2020 2020 2020 2020  t": 0.0.        
+000002a0: 7d0a 2020 2020 5d2c 0a20 2020 2022 6564  }.    ],.    "ed
+000002b0: 6765 7322 3a20 5b0a 2020 2020 2020 2020  ges": [.        
+000002c0: 5b22 7375 7070 6c79 3122 2c20 226c 696e  ["supply1", "lin
+000002d0: 6b31 225d 2c0a 2020 2020 2020 2020 5b22  k1"],.        ["
+000002e0: 6c69 6e6b 3122 2c20 2264 656d 616e 6431  link1", "demand1
+000002f0: 225d 0a20 2020 205d 0a7d 0a              "].    ].}.
```

### Comparing `pywr-1.8.0/tests/models/piecewise1_with_parameters.json` & `pywr-1.9.0/tests/models/piecewise1_with_parameters.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 22% similar despite different names*

```diff
@@ -1,73 +1,70 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5069 6563 6577 6973  itle": "Piecewis
-00000030: 6520 6c69 6e6b 2077 6974 6820 7061 7261  e link with para
-00000040: 6d65 7465 7273 222c 0d0a 2020 2020 2020  meters",..      
-00000050: 2020 2264 6573 6372 6970 7469 6f6e 223a    "description":
-00000060: 2022 4578 616d 706c 6520 6f66 2061 2070   "Example of a p
-00000070: 6965 6365 7769 7365 206c 696e 6b20 7468  iecewise link th
-00000080: 6174 2075 7365 7320 7061 7261 6d65 7465  at uses paramete
-00000090: 7273 222c 0d0a 2020 2020 2020 2020 226d  rs",..        "m
-000000a0: 696e 696d 756d 5f76 6572 7369 6f6e 223a  inimum_version":
-000000b0: 2022 312e 322e 3122 0d0a 2020 2020 7d2c   "1.2.1"..    },
-000000c0: 0d0a 2020 2020 2274 696d 6573 7465 7070  ..    "timestepp
-000000d0: 6572 223a 207b 0d0a 2020 2020 2020 2020  er": {..        
-000000e0: 2273 7461 7274 223a 2022 3230 3135 2d30  "start": "2015-0
-000000f0: 312d 3031 222c 0d0a 2020 2020 2020 2020  1-01",..        
-00000100: 2265 6e64 223a 2022 3230 3135 2d31 322d  "end": "2015-12-
-00000110: 3331 222c 0d0a 2020 2020 2020 2020 2274  31",..        "t
-00000120: 696d 6573 7465 7022 3a20 310d 0a20 2020  imestep": 1..   
-00000130: 207d 2c0d 0a20 2020 2022 6e6f 6465 7322   },..    "nodes"
-00000140: 3a20 5b0d 0a20 2020 2020 2020 207b 0d0a  : [..        {..
-00000150: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-00000160: 6522 3a20 2273 7570 706c 7931 222c 0d0a  e": "supply1",..
-00000170: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000180: 6522 3a20 2249 6e70 7574 222c 0d0a 2020  e": "Input",..  
-00000190: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
-000001a0: 6c6f 7722 3a20 3130 300d 0a20 2020 2020  low": 100..     
-000001b0: 2020 207d 2c0d 0a20 2020 2020 2020 207b     },..        {
-000001c0: 0d0a 2020 2020 2020 2020 2020 2020 226e  ..            "n
-000001d0: 616d 6522 3a20 226c 696e 6b31 222c 0d0a  ame": "link1",..
-000001e0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-000001f0: 6522 3a20 2270 6965 6365 7769 7365 6c69  e": "piecewiseli
-00000200: 6e6b 222c 0d0a 2020 2020 2020 2020 2020  nk",..          
-00000210: 2020 2263 6f73 7422 3a20 5b22 636f 7374    "cost": ["cost
-00000220: 3122 2c20 2263 6f73 7432 225d 2c0d 0a20  1", "cost2"],.. 
-00000230: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
-00000240: 666c 6f77 223a 205b 226d 6178 5f66 6c6f  flow": ["max_flo
-00000250: 7731 222c 206e 756c 6c5d 0d0a 2020 2020  w1", null]..    
-00000260: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000270: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000280: 6e61 6d65 223a 2022 6465 6d61 6e64 3122  name": "demand1"
-00000290: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000002a0: 7479 7065 223a 2022 4f75 7470 7574 222c  type": "Output",
-000002b0: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-000002c0: 6178 5f66 6c6f 7722 3a20 3530 2c0d 0a20  ax_flow": 50,.. 
-000002d0: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-000002e0: 223a 2030 2e30 0d0a 2020 2020 2020 2020  ": 0.0..        
-000002f0: 7d0d 0a20 2020 205d 2c0d 0a20 2020 2022  }..    ],..    "
-00000300: 6564 6765 7322 3a20 5b0d 0a20 2020 2020  edges": [..     
-00000310: 2020 205b 2273 7570 706c 7931 222c 2022     ["supply1", "
-00000320: 6c69 6e6b 3122 5d2c 0d0a 2020 2020 2020  link1"],..      
-00000330: 2020 5b22 6c69 6e6b 3122 2c20 2264 656d    ["link1", "dem
-00000340: 616e 6431 225d 0d0a 2020 2020 5d2c 0d0a  and1"]..    ],..
-00000350: 2020 2020 2270 6172 616d 6574 6572 7322      "parameters"
-00000360: 3a20 7b0d 0a20 2020 2020 2020 2022 636f  : {..        "co
-00000370: 7374 3122 3a20 7b0d 0a20 2020 2020 2020  st1": {..       
-00000380: 2020 2020 2022 7479 7065 223a 2022 636f       "type": "co
-00000390: 6e73 7461 6e74 222c 0d0a 2020 2020 2020  nstant",..      
-000003a0: 2020 2020 2020 2276 616c 7565 223a 202d        "value": -
-000003b0: 3130 2e30 0d0a 2020 2020 2020 2020 7d2c  10.0..        },
-000003c0: 0d0a 2020 2020 2020 2020 2263 6f73 7432  ..        "cost2
-000003d0: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-000003e0: 2020 2274 7970 6522 3a20 2263 6f6e 7374    "type": "const
-000003f0: 616e 7422 2c0d 0a20 2020 2020 2020 2020  ant",..         
-00000400: 2020 2022 7661 6c75 6522 3a20 352e 300d     "value": 5.0.
-00000410: 0a20 2020 2020 2020 207d 2c0d 0a20 2020  .        },..   
-00000420: 2020 2020 2022 6d61 785f 666c 6f77 3122       "max_flow1"
-00000430: 3a20 7b0d 0a20 2020 2020 2020 2020 2020  : {..           
-00000440: 2022 7479 7065 223a 2022 636f 6e73 7461   "type": "consta
-00000450: 6e74 222c 0d0a 2020 2020 2020 2020 2020  nt",..          
-00000460: 2020 2276 616c 7565 223a 2032 302e 300d    "value": 20.0.
-00000470: 0a20 2020 2020 2020 207d 0d0a 2020 2020  .        }..    
-00000480: 7d0d 0a7d 0d0a                           }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5069 6563 6577 6973 6520  le": "Piecewise 
+00000030: 6c69 6e6b 2077 6974 6820 7061 7261 6d65  link with parame
+00000040: 7465 7273 222c 0a20 2020 2020 2020 2022  ters",.        "
+00000050: 6465 7363 7269 7074 696f 6e22 3a20 2245  description": "E
+00000060: 7861 6d70 6c65 206f 6620 6120 7069 6563  xample of a piec
+00000070: 6577 6973 6520 6c69 6e6b 2074 6861 7420  ewise link that 
+00000080: 7573 6573 2070 6172 616d 6574 6572 7322  uses parameters"
+00000090: 2c0a 2020 2020 2020 2020 226d 696e 696d  ,.        "minim
+000000a0: 756d 5f76 6572 7369 6f6e 223a 2022 312e  um_version": "1.
+000000b0: 322e 3122 0a20 2020 207d 2c0a 2020 2020  2.1".    },.    
+000000c0: 2274 696d 6573 7465 7070 6572 223a 207b  "timestepper": {
+000000d0: 0a20 2020 2020 2020 2022 7374 6172 7422  .        "start"
+000000e0: 3a20 2232 3031 352d 3031 2d30 3122 2c0a  : "2015-01-01",.
+000000f0: 2020 2020 2020 2020 2265 6e64 223a 2022          "end": "
+00000100: 3230 3135 2d31 322d 3331 222c 0a20 2020  2015-12-31",.   
+00000110: 2020 2020 2022 7469 6d65 7374 6570 223a       "timestep":
+00000120: 2031 0a20 2020 207d 2c0a 2020 2020 226e   1.    },.    "n
+00000130: 6f64 6573 223a 205b 0a20 2020 2020 2020  odes": [.       
+00000140: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+00000150: 6e61 6d65 223a 2022 7375 7070 6c79 3122  name": "supply1"
+00000160: 2c0a 2020 2020 2020 2020 2020 2020 2274  ,.            "t
+00000170: 7970 6522 3a20 2249 6e70 7574 222c 0a20  ype": "Input",. 
+00000180: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
+00000190: 666c 6f77 223a 2031 3030 0a20 2020 2020  flow": 100.     
+000001a0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+000001b0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+000001c0: 6522 3a20 226c 696e 6b31 222c 0a20 2020  e": "link1",.   
+000001d0: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+000001e0: 2022 7069 6563 6577 6973 656c 696e 6b22   "piecewiselink"
+000001f0: 2c0a 2020 2020 2020 2020 2020 2020 2263  ,.            "c
+00000200: 6f73 7422 3a20 5b22 636f 7374 3122 2c20  ost": ["cost1", 
+00000210: 2263 6f73 7432 225d 2c0a 2020 2020 2020  "cost2"],.      
+00000220: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
+00000230: 3a20 5b22 6d61 785f 666c 6f77 3122 2c20  : ["max_flow1", 
+00000240: 6e75 6c6c 5d0a 2020 2020 2020 2020 7d2c  null].        },
+00000250: 0a20 2020 2020 2020 207b 0a20 2020 2020  .        {.     
+00000260: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
+00000270: 6465 6d61 6e64 3122 2c0a 2020 2020 2020  demand1",.      
+00000280: 2020 2020 2020 2274 7970 6522 3a20 224f        "type": "O
+00000290: 7574 7075 7422 2c0a 2020 2020 2020 2020  utput",.        
+000002a0: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
+000002b0: 3530 2c0a 2020 2020 2020 2020 2020 2020  50,.            
+000002c0: 2263 6f73 7422 3a20 302e 300a 2020 2020  "cost": 0.0.    
+000002d0: 2020 2020 7d0a 2020 2020 5d2c 0a20 2020      }.    ],.   
+000002e0: 2022 6564 6765 7322 3a20 5b0a 2020 2020   "edges": [.    
+000002f0: 2020 2020 5b22 7375 7070 6c79 3122 2c20      ["supply1", 
+00000300: 226c 696e 6b31 225d 2c0a 2020 2020 2020  "link1"],.      
+00000310: 2020 5b22 6c69 6e6b 3122 2c20 2264 656d    ["link1", "dem
+00000320: 616e 6431 225d 0a20 2020 205d 2c0a 2020  and1"].    ],.  
+00000330: 2020 2270 6172 616d 6574 6572 7322 3a20    "parameters": 
+00000340: 7b0a 2020 2020 2020 2020 2263 6f73 7431  {.        "cost1
+00000350: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
+00000360: 2022 7479 7065 223a 2022 636f 6e73 7461   "type": "consta
+00000370: 6e74 222c 0a20 2020 2020 2020 2020 2020  nt",.           
+00000380: 2022 7661 6c75 6522 3a20 2d31 302e 300a   "value": -10.0.
+00000390: 2020 2020 2020 2020 7d2c 0a20 2020 2020          },.     
+000003a0: 2020 2022 636f 7374 3222 3a20 7b0a 2020     "cost2": {.  
+000003b0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+000003c0: 3a20 2263 6f6e 7374 616e 7422 2c0a 2020  : "constant",.  
+000003d0: 2020 2020 2020 2020 2020 2276 616c 7565            "value
+000003e0: 223a 2035 2e30 0a20 2020 2020 2020 207d  ": 5.0.        }
+000003f0: 2c0a 2020 2020 2020 2020 226d 6178 5f66  ,.        "max_f
+00000400: 6c6f 7731 223a 207b 0a20 2020 2020 2020  low1": {.       
+00000410: 2020 2020 2022 7479 7065 223a 2022 636f       "type": "co
+00000420: 6e73 7461 6e74 222c 0a20 2020 2020 2020  nstant",.       
+00000430: 2020 2020 2022 7661 6c75 6522 3a20 3230       "value": 20
+00000440: 2e30 0a20 2020 2020 2020 207d 0a20 2020  .0.        }.   
+00000450: 207d 0a7d 0a                              }.}.
```

### Comparing `pywr-1.8.0/tests/models/reservoir1.json` & `pywr-1.9.0/tests/models/reservoir1.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 21% similar despite different names*

```diff
@@ -1,49 +1,47 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5265 7365 7276 6f69  itle": "Reservoi
-00000030: 7220 3122 2c0d 0a20 2020 2020 2020 2022  r 1",..        "
-00000040: 6465 7363 7269 7074 696f 6e22 3a20 2241  description": "A
-00000050: 206d 6f64 656c 2077 6974 6820 6120 7265   model with a re
-00000060: 7365 7276 6f69 722e 222c 0d0a 2020 2020  servoir.",..    
-00000070: 2020 2020 226d 696e 696d 756d 5f76 6572      "minimum_ver
-00000080: 7369 6f6e 223a 2022 302e 3122 0d0a 2020  sion": "0.1"..  
-00000090: 2020 7d2c 0d0a 2020 2020 2274 696d 6573    },..    "times
-000000a0: 7465 7070 6572 223a 207b 0d0a 2020 2020  tepper": {..    
-000000b0: 2020 2020 2273 7461 7274 223a 2022 3230      "start": "20
-000000c0: 3135 2d30 312d 3031 222c 0d0a 2020 2020  15-01-01",..    
-000000d0: 2020 2020 2265 6e64 223a 2022 3230 3135      "end": "2015
-000000e0: 2d31 322d 3331 222c 0d0a 2020 2020 2020  -12-31",..      
-000000f0: 2020 2274 696d 6573 7465 7022 3a20 310d    "timestep": 1.
-00000100: 0a20 2020 207d 2c0d 0a20 2020 2022 6e6f  .    },..    "no
-00000110: 6465 7322 3a20 5b0d 0a20 2020 2020 2020  des": [..       
-00000120: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000130: 226e 616d 6522 3a20 2273 7570 706c 7931  "name": "supply1
-00000140: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000150: 2274 7970 6522 3a20 2253 746f 7261 6765  "type": "Storage
-00000160: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000170: 226d 6178 5f76 6f6c 756d 6522 3a20 3335  "max_volume": 35
-00000180: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000190: 696e 6974 6961 6c5f 766f 6c75 6d65 223a  initial_volume":
-000001a0: 2033 352c 0d0a 2020 2020 2020 2020 2020   35,..          
-000001b0: 2020 226f 7574 7075 7473 223a 2030 0d0a    "outputs": 0..
-000001c0: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-000001d0: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-000001e0: 2020 2022 6e61 6d65 223a 2022 6c69 6e6b     "name": "link
-000001f0: 3122 2c0d 0a20 2020 2020 2020 2020 2020  1",..           
-00000200: 2022 7479 7065 223a 2022 4c69 6e6b 220d   "type": "Link".
-00000210: 0a20 2020 2020 2020 207d 2c0d 0a20 2020  .        },..   
-00000220: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-00000230: 2020 2020 226e 616d 6522 3a20 2264 656d      "name": "dem
-00000240: 616e 6431 222c 0d0a 2020 2020 2020 2020  and1",..        
-00000250: 2020 2020 2274 7970 6522 3a20 224f 7574      "type": "Out
-00000260: 7075 7422 2c0d 0a20 2020 2020 2020 2020  put",..         
-00000270: 2020 2022 6d61 785f 666c 6f77 223a 2031     "max_flow": 1
-00000280: 302c 0d0a 2020 2020 2020 2020 2020 2020  0,..            
-00000290: 2263 6f73 7422 3a20 2d31 300d 0a20 2020  "cost": -10..   
-000002a0: 2020 2020 207d 0d0a 2020 2020 5d2c 0d0a       }..    ],..
-000002b0: 2020 2020 2265 6467 6573 223a 205b 0d0a      "edges": [..
-000002c0: 2020 2020 2020 2020 5b22 7375 7070 6c79          ["supply
-000002d0: 3122 2c20 226c 696e 6b31 225d 2c0d 0a20  1", "link1"],.. 
-000002e0: 2020 2020 2020 205b 226c 696e 6b31 222c         ["link1",
-000002f0: 2022 6465 6d61 6e64 3122 5d0d 0a20 2020   "demand1"]..   
-00000300: 205d 0d0a 7d0d 0a                         ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5265 7365 7276 6f69 7220  le": "Reservoir 
+00000030: 3122 2c0a 2020 2020 2020 2020 2264 6573  1",.        "des
+00000040: 6372 6970 7469 6f6e 223a 2022 4120 6d6f  cription": "A mo
+00000050: 6465 6c20 7769 7468 2061 2072 6573 6572  del with a reser
+00000060: 766f 6972 2e22 2c0a 2020 2020 2020 2020  voir.",.        
+00000070: 226d 696e 696d 756d 5f76 6572 7369 6f6e  "minimum_version
+00000080: 223a 2022 302e 3122 0a20 2020 207d 2c0a  ": "0.1".    },.
+00000090: 2020 2020 2274 696d 6573 7465 7070 6572      "timestepper
+000000a0: 223a 207b 0a20 2020 2020 2020 2022 7374  ": {.        "st
+000000b0: 6172 7422 3a20 2232 3031 352d 3031 2d30  art": "2015-01-0
+000000c0: 3122 2c0a 2020 2020 2020 2020 2265 6e64  1",.        "end
+000000d0: 223a 2022 3230 3135 2d31 322d 3331 222c  ": "2015-12-31",
+000000e0: 0a20 2020 2020 2020 2022 7469 6d65 7374  .        "timest
+000000f0: 6570 223a 2031 0a20 2020 207d 2c0a 2020  ep": 1.    },.  
+00000100: 2020 226e 6f64 6573 223a 205b 0a20 2020    "nodes": [.   
+00000110: 2020 2020 207b 0a20 2020 2020 2020 2020       {.         
+00000120: 2020 2022 6e61 6d65 223a 2022 7375 7070     "name": "supp
+00000130: 6c79 3122 2c0a 2020 2020 2020 2020 2020  ly1",.          
+00000140: 2020 2274 7970 6522 3a20 2253 746f 7261    "type": "Stora
+00000150: 6765 222c 0a20 2020 2020 2020 2020 2020  ge",.           
+00000160: 2022 6d61 785f 766f 6c75 6d65 223a 2033   "max_volume": 3
+00000170: 352c 0a20 2020 2020 2020 2020 2020 2022  5,.            "
+00000180: 696e 6974 6961 6c5f 766f 6c75 6d65 223a  initial_volume":
+00000190: 2033 352c 0a20 2020 2020 2020 2020 2020   35,.           
+000001a0: 2022 6f75 7470 7574 7322 3a20 300a 2020   "outputs": 0.  
+000001b0: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+000001c0: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+000001d0: 6e61 6d65 223a 2022 6c69 6e6b 3122 2c0a  name": "link1",.
+000001e0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000001f0: 6522 3a20 224c 696e 6b22 0a20 2020 2020  e": "Link".     
+00000200: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+00000210: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000220: 6522 3a20 2264 656d 616e 6431 222c 0a20  e": "demand1",. 
+00000230: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000240: 223a 2022 4f75 7470 7574 222c 0a20 2020  ": "Output",.   
+00000250: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+00000260: 6f77 223a 2031 302c 0a20 2020 2020 2020  ow": 10,.       
+00000270: 2020 2020 2022 636f 7374 223a 202d 3130       "cost": -10
+00000280: 0a20 2020 2020 2020 207d 0a20 2020 205d  .        }.    ]
+00000290: 2c0a 2020 2020 2265 6467 6573 223a 205b  ,.    "edges": [
+000002a0: 0a20 2020 2020 2020 205b 2273 7570 706c  .        ["suppl
+000002b0: 7931 222c 2022 6c69 6e6b 3122 5d2c 0a20  y1", "link1"],. 
+000002c0: 2020 2020 2020 205b 226c 696e 6b31 222c         ["link1",
+000002d0: 2022 6465 6d61 6e64 3122 5d0a 2020 2020   "demand1"].    
+000002e0: 5d0a 7d0a                                ].}.
```

### Comparing `pywr-1.8.0/tests/models/reservoir_evaporation.json` & `pywr-1.9.0/tests/models/reservoir_evaporation_without_area_property.json`

 * *Files 18% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9944444444444445%*

 * *Differences: {"'nodes'": "{0: {delete: ['area']}}"}*

```diff
@@ -12,15 +12,14 @@
     "metadata": {
         "description": "A model with a reservoir, with evaporation proportional to surface area",
         "minimum_version": "0.5dev0",
         "title": "Reservoir Evaporation"
     },
     "nodes": [
         {
-            "area": "reservoir_area",
             "initial_volume": 1000,
             "max_volume": 1000,
             "name": "reservoir1",
             "outputs": 0,
             "type": "Storage"
         },
         {
```

### Comparing `pywr-1.8.0/tests/models/reservoir_with_cc.json` & `pywr-1.9.0/tests/models/reservoir_with_cc.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,70 +1,68 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5265 7365 7276 6f69  itle": "Reservoi
-00000030: 7220 7769 7468 2063 6322 2c0d 0a20 2020  r with cc",..   
-00000040: 2020 2020 2022 6465 7363 7269 7074 696f       "descriptio
-00000050: 6e22 3a20 2241 206d 6f64 656c 2077 6974  n": "A model wit
-00000060: 6820 6120 7265 7365 7276 6f69 7220 616e  h a reservoir an
-00000070: 6420 636f 6e74 726f 6c20 6375 7276 6522  d control curve"
-00000080: 2c0d 0a20 2020 2020 2020 2022 6d69 6e69  ,..        "mini
-00000090: 6d75 6d5f 7665 7273 696f 6e22 3a20 2230  mum_version": "0
-000000a0: 2e31 220d 0a20 2020 207d 2c0d 0a20 2020  .1"..    },..   
-000000b0: 2022 7469 6d65 7374 6570 7065 7222 3a20   "timestepper": 
-000000c0: 7b0d 0a20 2020 2020 2020 2022 7374 6172  {..        "star
-000000d0: 7422 3a20 2232 3031 352d 3031 2d30 3122  t": "2015-01-01"
-000000e0: 2c0d 0a20 2020 2020 2020 2022 656e 6422  ,..        "end"
-000000f0: 3a20 2232 3031 352d 3132 2d33 3122 2c0d  : "2015-12-31",.
-00000100: 0a20 2020 2020 2020 2022 7469 6d65 7374  .        "timest
-00000110: 6570 223a 2031 0d0a 2020 2020 7d2c 0d0a  ep": 1..    },..
-00000120: 2020 2020 226e 6f64 6573 223a 205b 0d0a      "nodes": [..
-00000130: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-00000140: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-00000150: 7265 7365 7276 6f69 7231 222c 0d0a 2020  reservoir1",..  
-00000160: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000170: 3a20 2253 746f 7261 6765 222c 0d0a 2020  : "Storage",..  
-00000180: 2020 2020 2020 2020 2020 226d 6178 5f76            "max_v
-00000190: 6f6c 756d 6522 3a20 3130 3030 2c0d 0a20  olume": 1000,.. 
-000001a0: 2020 2020 2020 2020 2020 2022 696e 6974             "init
-000001b0: 6961 6c5f 766f 6c75 6d65 223a 2031 3030  ial_volume": 100
-000001c0: 302c 0d0a 2020 2020 2020 2020 2020 2020  0,..            
-000001d0: 226f 7574 7075 7473 223a 2030 2c0d 0a20  "outputs": 0,.. 
-000001e0: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-000001f0: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000200: 2020 2020 2020 2274 7970 6522 3a20 2243        "type": "C
-00000210: 6f6e 7472 6f6c 4375 7276 6549 6e74 6572  ontrolCurveInter
-00000220: 706f 6c61 7465 6422 2c0d 0a20 2020 2020  polated",..     
-00000230: 2020 2020 2020 2020 2020 2022 636f 6e74             "cont
-00000240: 726f 6c5f 6375 7276 6522 3a20 7b0d 0a20  rol_curve": {.. 
-00000250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000260: 2020 2022 7479 7065 223a 2022 6461 696c     "type": "dail
-00000270: 7970 726f 6669 6c65 222c 0d0a 2020 2020  yprofile",..    
-00000280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000290: 2275 726c 223a 2022 636f 6e74 726f 6c5f  "url": "control_
-000002a0: 6375 7276 652e 6373 7622 2c0d 0a20 2020  curve.csv",..   
-000002b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000002c0: 2022 7061 7273 655f 6461 7465 7322 3a20   "parse_dates": 
-000002d0: 6661 6c73 652c 0d0a 2020 2020 2020 2020  false,..        
-000002e0: 2020 2020 2020 2020 2020 2020 2263 6f6c              "col
-000002f0: 756d 6e22 3a20 2243 6f6e 7472 6f6c 2043  umn": "Control C
-00000300: 7572 7665 220d 0a20 2020 2020 2020 2020  urve"..         
-00000310: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000320: 2020 2020 2020 2020 2020 2022 7374 6f72             "stor
-00000330: 6167 655f 6e6f 6465 223a 2022 7265 7365  age_node": "rese
-00000340: 7276 6f69 7231 222c 0d0a 2020 2020 2020  rvoir1",..      
-00000350: 2020 2020 2020 2020 2020 2276 616c 7565            "value
-00000360: 7322 3a20 5b2d 382c 202d 362c 202d 345d  s": [-8, -6, -4]
-00000370: 0d0a 2020 2020 2020 2020 2020 2020 7d0d  ..            }.
-00000380: 0a20 2020 2020 2020 207d 2c0d 0a20 2020  .        },..   
-00000390: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-000003a0: 2020 2020 226e 616d 6522 3a20 2264 656d      "name": "dem
-000003b0: 616e 6431 222c 0d0a 2020 2020 2020 2020  and1",..        
-000003c0: 2020 2020 2274 7970 6522 3a20 224f 7574      "type": "Out
-000003d0: 7075 7422 2c0d 0a20 2020 2020 2020 2020  put",..         
-000003e0: 2020 2022 6d61 785f 666c 6f77 223a 2033     "max_flow": 3
-000003f0: 3030 2c0d 0a20 2020 2020 2020 2020 2020  00,..           
-00000400: 2022 636f 7374 223a 202d 3130 0d0a 2020   "cost": -10..  
-00000410: 2020 2020 2020 7d0d 0a20 2020 205d 2c0d        }..    ],.
-00000420: 0a20 2020 2022 6564 6765 7322 3a20 5b0d  .    "edges": [.
-00000430: 0a20 2020 2020 2020 205b 2272 6573 6572  .        ["reser
-00000440: 766f 6972 3122 2c20 2264 656d 616e 6431  voir1", "demand1
-00000450: 225d 0d0a 2020 2020 5d0d 0a7d 0d0a       "]..    ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5265 7365 7276 6f69 7220  le": "Reservoir 
+00000030: 7769 7468 2063 6322 2c0a 2020 2020 2020  with cc",.      
+00000040: 2020 2264 6573 6372 6970 7469 6f6e 223a    "description":
+00000050: 2022 4120 6d6f 6465 6c20 7769 7468 2061   "A model with a
+00000060: 2072 6573 6572 766f 6972 2061 6e64 2063   reservoir and c
+00000070: 6f6e 7472 6f6c 2063 7572 7665 222c 0a20  ontrol curve",. 
+00000080: 2020 2020 2020 2022 6d69 6e69 6d75 6d5f         "minimum_
+00000090: 7665 7273 696f 6e22 3a20 2230 2e31 220a  version": "0.1".
+000000a0: 2020 2020 7d2c 0a20 2020 2022 7469 6d65      },.    "time
+000000b0: 7374 6570 7065 7222 3a20 7b0a 2020 2020  stepper": {.    
+000000c0: 2020 2020 2273 7461 7274 223a 2022 3230      "start": "20
+000000d0: 3135 2d30 312d 3031 222c 0a20 2020 2020  15-01-01",.     
+000000e0: 2020 2022 656e 6422 3a20 2232 3031 352d     "end": "2015-
+000000f0: 3132 2d33 3122 2c0a 2020 2020 2020 2020  12-31",.        
+00000100: 2274 696d 6573 7465 7022 3a20 310a 2020  "timestep": 1.  
+00000110: 2020 7d2c 0a20 2020 2022 6e6f 6465 7322    },.    "nodes"
+00000120: 3a20 5b0a 2020 2020 2020 2020 7b0a 2020  : [.        {.  
+00000130: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
+00000140: 3a20 2272 6573 6572 766f 6972 3122 2c0a  : "reservoir1",.
+00000150: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000160: 6522 3a20 2253 746f 7261 6765 222c 0a20  e": "Storage",. 
+00000170: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
+00000180: 766f 6c75 6d65 223a 2031 3030 302c 0a20  volume": 1000,. 
+00000190: 2020 2020 2020 2020 2020 2022 696e 6974             "init
+000001a0: 6961 6c5f 766f 6c75 6d65 223a 2031 3030  ial_volume": 100
+000001b0: 302c 0a20 2020 2020 2020 2020 2020 2022  0,.            "
+000001c0: 6f75 7470 7574 7322 3a20 302c 0a20 2020  outputs": 0,.   
+000001d0: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
+000001e0: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
+000001f0: 2020 2022 7479 7065 223a 2022 436f 6e74     "type": "Cont
+00000200: 726f 6c43 7572 7665 496e 7465 7270 6f6c  rolCurveInterpol
+00000210: 6174 6564 222c 0a20 2020 2020 2020 2020  ated",.         
+00000220: 2020 2020 2020 2022 636f 6e74 726f 6c5f         "control_
+00000230: 6375 7276 6522 3a20 7b0a 2020 2020 2020  curve": {.      
+00000240: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+00000250: 7970 6522 3a20 2264 6169 6c79 7072 6f66  ype": "dailyprof
+00000260: 696c 6522 2c0a 2020 2020 2020 2020 2020  ile",.          
+00000270: 2020 2020 2020 2020 2020 2275 726c 223a            "url":
+00000280: 2022 636f 6e74 726f 6c5f 6375 7276 652e   "control_curve.
+00000290: 6373 7622 2c0a 2020 2020 2020 2020 2020  csv",.          
+000002a0: 2020 2020 2020 2020 2020 2270 6172 7365            "parse
+000002b0: 5f64 6174 6573 223a 2066 616c 7365 2c0a  _dates": false,.
+000002c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000002d0: 2020 2020 2263 6f6c 756d 6e22 3a20 2243      "column": "C
+000002e0: 6f6e 7472 6f6c 2043 7572 7665 220a 2020  ontrol Curve".  
+000002f0: 2020 2020 2020 2020 2020 2020 2020 7d2c                },
+00000300: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000310: 2022 7374 6f72 6167 655f 6e6f 6465 223a   "storage_node":
+00000320: 2022 7265 7365 7276 6f69 7231 222c 0a20   "reservoir1",. 
+00000330: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00000340: 7661 6c75 6573 223a 205b 2d38 2c20 2d36  values": [-8, -6
+00000350: 2c20 2d34 5d0a 2020 2020 2020 2020 2020  , -4].          
+00000360: 2020 7d0a 2020 2020 2020 2020 7d2c 0a20    }.        },. 
+00000370: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000380: 2020 2020 2022 6e61 6d65 223a 2022 6465       "name": "de
+00000390: 6d61 6e64 3122 2c0a 2020 2020 2020 2020  mand1",.        
+000003a0: 2020 2020 2274 7970 6522 3a20 224f 7574      "type": "Out
+000003b0: 7075 7422 2c0a 2020 2020 2020 2020 2020  put",.          
+000003c0: 2020 226d 6178 5f66 6c6f 7722 3a20 3330    "max_flow": 30
+000003d0: 302c 0a20 2020 2020 2020 2020 2020 2022  0,.            "
+000003e0: 636f 7374 223a 202d 3130 0a20 2020 2020  cost": -10.     
+000003f0: 2020 207d 0a20 2020 205d 2c0a 2020 2020     }.    ],.    
+00000400: 2265 6467 6573 223a 205b 0a20 2020 2020  "edges": [.     
+00000410: 2020 205b 2272 6573 6572 766f 6972 3122     ["reservoir1"
+00000420: 2c20 2264 656d 616e 6431 225d 0a20 2020  , "demand1"].   
+00000430: 205d 0a7d 0a                              ].}.
```

### Comparing `pywr-1.8.0/tests/models/river1.json` & `pywr-1.9.0/tests/models/river_discharge1.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8637152777777778%*

 * *Differences: {"'edges'": "{2: {insert: [(0, 'discharge1')], delete: [0]}, insert: [(1, ['river1', "*

 * *            "'discharge1'])]}",*

 * * "'metadata'": "{'title': 'River Discharge 1', 'description': 'A model with a discharge upstream "*

 * *               "from an abstraction.'}",*

 * * "'nodes'": "{2: {'type': 'link'}, 3: {delete: ['max_flow']}, 4: {'type': 'Link'}, insert: [(1, "*

 * *            "OrderedDict([('name', 'discharge1'), ('type', 'input'), ('min_flow', 3), ('max_flow', "*

 * *            "3)])), (6, OrderedDict([('name', 'term1') […]*

```diff
@@ -2,14 +2,18 @@
     "edges": [
         [
             "catchment1",
             "river1"
         ],
         [
             "river1",
+            "discharge1"
+        ],
+        [
+            "discharge1",
             "abs1"
         ],
         [
             "abs1",
             "link1"
         ],
         [
@@ -18,46 +22,51 @@
         ],
         [
             "abs1",
             "term1"
         ]
     ],
     "metadata": {
-        "description": "A model with a river abstraction.",
+        "description": "A model with a discharge upstream from an abstraction.",
         "minimum_version": "0.1",
-        "title": "River 1"
+        "title": "River Discharge 1"
     },
     "nodes": [
         {
             "flow": 5,
             "name": "catchment1",
             "type": "catchment"
         },
         {
+            "max_flow": 3,
+            "min_flow": 3,
+            "name": "discharge1",
+            "type": "input"
+        },
+        {
             "name": "river1",
-            "type": "river"
+            "type": "link"
         },
         {
-            "max_flow": 15,
             "name": "abs1",
             "type": "link"
         },
         {
             "name": "link1",
-            "type": "river"
-        },
-        {
-            "name": "term1",
-            "type": "output"
+            "type": "Link"
         },
         {
             "cost": -10,
             "max_flow": 10,
             "name": "demand1",
             "type": "Output"
+        },
+        {
+            "name": "term1",
+            "type": "output"
         }
     ],
     "timestepper": {
         "end": "2015-12-31",
         "start": "2015-01-01",
         "timestep": 1
     }
```

### Comparing `pywr-1.8.0/tests/models/river2.json` & `pywr-1.9.0/tests/models/river2.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 22% similar despite different names*

```diff
@@ -1,110 +1,106 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5269 7665 7220 3122  itle": "River 1"
-00000030: 2c0d 0a20 2020 2020 2020 2022 6465 7363  ,..        "desc
-00000040: 7269 7074 696f 6e22 3a20 2241 206d 6f72  ription": "A mor
-00000050: 6520 636f 6d70 6c65 7820 7269 7665 7220  e complex river 
-00000060: 7379 7374 656d 2e22 2c0d 0a20 2020 2020  system.",..     
-00000070: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
-00000080: 696f 6e22 3a20 2230 2e31 220d 0a20 2020  ion": "0.1"..   
-00000090: 207d 2c0d 0a20 2020 2022 7469 6d65 7374   },..    "timest
-000000a0: 6570 7065 7222 3a20 7b0d 0a20 2020 2020  epper": {..     
-000000b0: 2020 2022 7374 6172 7422 3a20 2232 3031     "start": "201
-000000c0: 352d 3031 2d30 3122 2c0d 0a20 2020 2020  5-01-01",..     
-000000d0: 2020 2022 656e 6422 3a20 2232 3031 352d     "end": "2015-
-000000e0: 3132 2d33 3122 2c0d 0a20 2020 2020 2020  12-31",..       
-000000f0: 2022 7469 6d65 7374 6570 223a 2031 0d0a   "timestep": 1..
-00000100: 2020 2020 7d2c 0d0a 2020 2020 226e 6f64      },..    "nod
-00000110: 6573 223a 205b 0d0a 2020 2020 2020 2020  es": [..        
-00000120: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000130: 6e61 6d65 223a 2022 6361 7463 686d 656e  name": "catchmen
-00000140: 7431 222c 0d0a 2020 2020 2020 2020 2020  t1",..          
-00000150: 2020 2274 7970 6522 3a20 2263 6174 6368    "type": "catch
-00000160: 6d65 6e74 222c 0d0a 2020 2020 2020 2020  ment",..        
-00000170: 2020 2020 2266 6c6f 7722 3a20 350d 0a20      "flow": 5.. 
-00000180: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000190: 2020 207b 0d0a 2020 2020 2020 2020 2020     {..          
-000001a0: 2020 226e 616d 6522 3a20 2263 6174 6368    "name": "catch
-000001b0: 6d65 6e74 3222 2c0d 0a20 2020 2020 2020  ment2",..       
-000001c0: 2020 2020 2022 7479 7065 223a 2022 6361       "type": "ca
-000001d0: 7463 686d 656e 7422 2c0d 0a20 2020 2020  tchment",..     
-000001e0: 2020 2020 2020 2022 666c 6f77 223a 2035         "flow": 5
-000001f0: 0d0a 2020 2020 2020 2020 7d2c 0d0a 2020  ..        },..  
-00000200: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000210: 2020 2020 2022 6e61 6d65 223a 2022 7269       "name": "ri
-00000220: 7665 7231 222c 0d0a 2020 2020 2020 2020  ver1",..        
-00000230: 2020 2020 2274 7970 6522 3a20 2272 6976      "type": "riv
-00000240: 6572 220d 0a20 2020 2020 2020 207d 2c0d  er"..        },.
-00000250: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000260: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000270: 2272 6976 6572 3222 2c0d 0a20 2020 2020  "river2",..     
-00000280: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000290: 7269 7665 7273 706c 6974 222c 0d0a 2020  riversplit",..  
-000002a0: 2020 2020 2020 2020 2020 2266 6163 746f            "facto
-000002b0: 7273 223a 205b 312c 2033 5d2c 0d0a 2020  rs": [1, 3],..  
-000002c0: 2020 2020 2020 2020 2020 2273 6c6f 745f            "slot_
-000002d0: 6e61 6d65 7322 3a20 5b22 7465 726d 222c  names": ["term",
-000002e0: 2022 6272 616e 6368 225d 0d0a 2020 2020   "branch"]..    
-000002f0: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000300: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000310: 6e61 6d65 223a 2022 6162 7331 222c 0d0a  name": "abs1",..
-00000320: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000330: 6522 3a20 226c 696e 6b22 2c0d 0a20 2020  e": "link",..   
-00000340: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
-00000350: 6f77 223a 2031 350d 0a20 2020 2020 2020  ow": 15..       
-00000360: 207d 2c0d 0a20 2020 2020 2020 207b 0d0a   },..        {..
-00000370: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-00000380: 6522 3a20 2261 6273 3222 2c0d 0a20 2020  e": "abs2",..   
-00000390: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-000003a0: 2022 6c69 6e6b 222c 0d0a 2020 2020 2020   "link",..      
-000003b0: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
-000003c0: 3a20 3135 0d0a 2020 2020 2020 2020 7d2c  : 15..        },
-000003d0: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-000003e0: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-000003f0: 2022 7465 726d 3122 2c0d 0a20 2020 2020   "term1",..     
-00000400: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-00000410: 6f75 7470 7574 220d 0a20 2020 2020 2020  output"..       
-00000420: 207d 2c0d 0a20 2020 2020 2020 207b 0d0a   },..        {..
-00000430: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-00000440: 6522 3a20 2274 6572 6d32 222c 0d0a 2020  e": "term2",..  
-00000450: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-00000460: 3a20 226f 7574 7075 7422 0d0a 2020 2020  : "output"..    
-00000470: 2020 2020 7d2c 0d0a 2020 2020 2020 2020      },..        
-00000480: 7b0d 0a20 2020 2020 2020 2020 2020 2022  {..            "
-00000490: 6e61 6d65 223a 2022 6465 6d61 6e64 3122  name": "demand1"
-000004a0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000004b0: 7479 7065 223a 2022 4f75 7470 7574 222c  type": "Output",
-000004c0: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-000004d0: 6178 5f66 6c6f 7722 3a20 3130 2c0d 0a20  ax_flow": 10,.. 
-000004e0: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-000004f0: 223a 202d 3130 0d0a 2020 2020 2020 2020  ": -10..        
-00000500: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-00000510: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000520: 223a 2022 6465 6d61 6e64 3222 2c0d 0a20  ": "demand2",.. 
-00000530: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000540: 223a 2022 4f75 7470 7574 222c 0d0a 2020  ": "Output",..  
-00000550: 2020 2020 2020 2020 2020 226d 6178 5f66            "max_f
-00000560: 6c6f 7722 3a20 322c 0d0a 2020 2020 2020  low": 2,..      
-00000570: 2020 2020 2020 2263 6f73 7422 3a20 2d31        "cost": -1
-00000580: 300d 0a20 2020 2020 2020 207d 0d0a 2020  0..        }..  
-00000590: 2020 5d2c 0d0a 2020 2020 2265 6467 6573    ],..    "edges
-000005a0: 223a 205b 0d0a 2020 2020 2020 2020 5b22  ": [..        ["
-000005b0: 6361 7463 686d 656e 7432 222c 2022 6162  catchment2", "ab
-000005c0: 7332 225d 2c0d 0a20 2020 2020 2020 205b  s2"],..        [
-000005d0: 2261 6273 3222 2c20 2272 6976 6572 3222  "abs2", "river2"
-000005e0: 5d2c 0d0a 2020 2020 2020 2020 5b22 6162  ],..        ["ab
-000005f0: 7332 222c 2022 6465 6d61 6e64 3222 5d2c  s2", "demand2"],
-00000600: 0d0a 2020 2020 2020 2020 5b22 7269 7665  ..        ["rive
-00000610: 7232 222c 2022 7269 7665 7231 222c 2022  r2", "river1", "
-00000620: 6272 616e 6368 222c 206e 756c 6c5d 2c0d  branch", null],.
-00000630: 0a20 2020 2020 2020 205b 2272 6976 6572  .        ["river
-00000640: 3222 2c20 2274 6572 6d32 222c 2022 7465  2", "term2", "te
-00000650: 726d 222c 206e 756c 6c5d 2c0d 0a20 2020  rm", null],..   
-00000660: 2020 2020 205b 2263 6174 6368 6d65 6e74       ["catchment
-00000670: 3122 2c20 2272 6976 6572 3122 5d2c 0d0a  1", "river1"],..
-00000680: 2020 2020 2020 2020 5b22 7269 7665 7231          ["river1
-00000690: 222c 2022 6162 7331 225d 2c0d 0a20 2020  ", "abs1"],..   
-000006a0: 2020 2020 205b 2261 6273 3122 2c20 2264       ["abs1", "d
-000006b0: 656d 616e 6431 225d 2c0d 0a20 2020 2020  emand1"],..     
-000006c0: 2020 205b 2261 6273 3122 2c20 2274 6572     ["abs1", "ter
-000006d0: 6d31 225d 0d0a 2020 2020 5d0d 0a7d 0d0a  m1"]..    ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5269 7665 7220 3122 2c0a  le": "River 1",.
+00000030: 2020 2020 2020 2020 2264 6573 6372 6970          "descrip
+00000040: 7469 6f6e 223a 2022 4120 6d6f 7265 2063  tion": "A more c
+00000050: 6f6d 706c 6578 2072 6976 6572 2073 7973  omplex river sys
+00000060: 7465 6d2e 222c 0a20 2020 2020 2020 2022  tem.",.        "
+00000070: 6d69 6e69 6d75 6d5f 7665 7273 696f 6e22  minimum_version"
+00000080: 3a20 2230 2e31 220a 2020 2020 7d2c 0a20  : "0.1".    },. 
+00000090: 2020 2022 7469 6d65 7374 6570 7065 7222     "timestepper"
+000000a0: 3a20 7b0a 2020 2020 2020 2020 2273 7461  : {.        "sta
+000000b0: 7274 223a 2022 3230 3135 2d30 312d 3031  rt": "2015-01-01
+000000c0: 222c 0a20 2020 2020 2020 2022 656e 6422  ",.        "end"
+000000d0: 3a20 2232 3031 352d 3132 2d33 3122 2c0a  : "2015-12-31",.
+000000e0: 2020 2020 2020 2020 2274 696d 6573 7465          "timeste
+000000f0: 7022 3a20 310a 2020 2020 7d2c 0a20 2020  p": 1.    },.   
+00000100: 2022 6e6f 6465 7322 3a20 5b0a 2020 2020   "nodes": [.    
+00000110: 2020 2020 7b0a 2020 2020 2020 2020 2020      {.          
+00000120: 2020 226e 616d 6522 3a20 2263 6174 6368    "name": "catch
+00000130: 6d65 6e74 3122 2c0a 2020 2020 2020 2020  ment1",.        
+00000140: 2020 2020 2274 7970 6522 3a20 2263 6174      "type": "cat
+00000150: 6368 6d65 6e74 222c 0a20 2020 2020 2020  chment",.       
+00000160: 2020 2020 2022 666c 6f77 223a 2035 0a20       "flow": 5. 
+00000170: 2020 2020 2020 207d 2c0a 2020 2020 2020         },.      
+00000180: 2020 7b0a 2020 2020 2020 2020 2020 2020    {.            
+00000190: 226e 616d 6522 3a20 2263 6174 6368 6d65  "name": "catchme
+000001a0: 6e74 3222 2c0a 2020 2020 2020 2020 2020  nt2",.          
+000001b0: 2020 2274 7970 6522 3a20 2263 6174 6368    "type": "catch
+000001c0: 6d65 6e74 222c 0a20 2020 2020 2020 2020  ment",.         
+000001d0: 2020 2022 666c 6f77 223a 2035 0a20 2020     "flow": 5.   
+000001e0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000001f0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+00000200: 616d 6522 3a20 2272 6976 6572 3122 2c0a  ame": "river1",.
+00000210: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000220: 6522 3a20 2272 6976 6572 220a 2020 2020  e": "river".    
+00000230: 2020 2020 7d2c 0a20 2020 2020 2020 207b      },.        {
+00000240: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
+00000250: 6d65 223a 2022 7269 7665 7232 222c 0a20  me": "river2",. 
+00000260: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000270: 223a 2022 7269 7665 7273 706c 6974 222c  ": "riversplit",
+00000280: 0a20 2020 2020 2020 2020 2020 2022 6661  .            "fa
+00000290: 6374 6f72 7322 3a20 5b31 2c20 335d 2c0a  ctors": [1, 3],.
+000002a0: 2020 2020 2020 2020 2020 2020 2273 6c6f              "slo
+000002b0: 745f 6e61 6d65 7322 3a20 5b22 7465 726d  t_names": ["term
+000002c0: 222c 2022 6272 616e 6368 225d 0a20 2020  ", "branch"].   
+000002d0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000002e0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000002f0: 616d 6522 3a20 2261 6273 3122 2c0a 2020  ame": "abs1",.  
+00000300: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000310: 3a20 226c 696e 6b22 2c0a 2020 2020 2020  : "link",.      
+00000320: 2020 2020 2020 226d 6178 5f66 6c6f 7722        "max_flow"
+00000330: 3a20 3135 0a20 2020 2020 2020 207d 2c0a  : 15.        },.
+00000340: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000350: 2020 2020 2020 226e 616d 6522 3a20 2261        "name": "a
+00000360: 6273 3222 2c0a 2020 2020 2020 2020 2020  bs2",.          
+00000370: 2020 2274 7970 6522 3a20 226c 696e 6b22    "type": "link"
+00000380: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
+00000390: 6178 5f66 6c6f 7722 3a20 3135 0a20 2020  ax_flow": 15.   
+000003a0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000003b0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000003c0: 616d 6522 3a20 2274 6572 6d31 222c 0a20  ame": "term1",. 
+000003d0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+000003e0: 223a 2022 6f75 7470 7574 220a 2020 2020  ": "output".    
+000003f0: 2020 2020 7d2c 0a20 2020 2020 2020 207b      },.        {
+00000400: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
+00000410: 6d65 223a 2022 7465 726d 3222 2c0a 2020  me": "term2",.  
+00000420: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000430: 3a20 226f 7574 7075 7422 0a20 2020 2020  : "output".     
+00000440: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+00000450: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000460: 6522 3a20 2264 656d 616e 6431 222c 0a20  e": "demand1",. 
+00000470: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+00000480: 223a 2022 4f75 7470 7574 222c 0a20 2020  ": "Output",.   
+00000490: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+000004a0: 6f77 223a 2031 302c 0a20 2020 2020 2020  ow": 10,.       
+000004b0: 2020 2020 2022 636f 7374 223a 202d 3130       "cost": -10
+000004c0: 0a20 2020 2020 2020 207d 2c0a 2020 2020  .        },.    
+000004d0: 2020 2020 7b0a 2020 2020 2020 2020 2020      {.          
+000004e0: 2020 226e 616d 6522 3a20 2264 656d 616e    "name": "deman
+000004f0: 6432 222c 0a20 2020 2020 2020 2020 2020  d2",.           
+00000500: 2022 7479 7065 223a 2022 4f75 7470 7574   "type": "Output
+00000510: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+00000520: 6d61 785f 666c 6f77 223a 2032 2c0a 2020  max_flow": 2,.  
+00000530: 2020 2020 2020 2020 2020 2263 6f73 7422            "cost"
+00000540: 3a20 2d31 300a 2020 2020 2020 2020 7d0a  : -10.        }.
+00000550: 2020 2020 5d2c 0a20 2020 2022 6564 6765      ],.    "edge
+00000560: 7322 3a20 5b0a 2020 2020 2020 2020 5b22  s": [.        ["
+00000570: 6361 7463 686d 656e 7432 222c 2022 6162  catchment2", "ab
+00000580: 7332 225d 2c0a 2020 2020 2020 2020 5b22  s2"],.        ["
+00000590: 6162 7332 222c 2022 7269 7665 7232 225d  abs2", "river2"]
+000005a0: 2c0a 2020 2020 2020 2020 5b22 6162 7332  ,.        ["abs2
+000005b0: 222c 2022 6465 6d61 6e64 3222 5d2c 0a20  ", "demand2"],. 
+000005c0: 2020 2020 2020 205b 2272 6976 6572 3222         ["river2"
+000005d0: 2c20 2272 6976 6572 3122 2c20 2262 7261  , "river1", "bra
+000005e0: 6e63 6822 2c20 6e75 6c6c 5d2c 0a20 2020  nch", null],.   
+000005f0: 2020 2020 205b 2272 6976 6572 3222 2c20       ["river2", 
+00000600: 2274 6572 6d32 222c 2022 7465 726d 222c  "term2", "term",
+00000610: 206e 756c 6c5d 2c0a 2020 2020 2020 2020   null],.        
+00000620: 5b22 6361 7463 686d 656e 7431 222c 2022  ["catchment1", "
+00000630: 7269 7665 7231 225d 2c0a 2020 2020 2020  river1"],.      
+00000640: 2020 5b22 7269 7665 7231 222c 2022 6162    ["river1", "ab
+00000650: 7331 225d 2c0a 2020 2020 2020 2020 5b22  s1"],.        ["
+00000660: 6162 7331 222c 2022 6465 6d61 6e64 3122  abs1", "demand1"
+00000670: 5d2c 0a20 2020 2020 2020 205b 2261 6273  ],.        ["abs
+00000680: 3122 2c20 2274 6572 6d31 225d 0a20 2020  1", "term1"].   
+00000690: 205d 0a7d 0a                              ].}.
```

### Comparing `pywr-1.8.0/tests/models/river_discharge1.json` & `pywr-1.9.0/tests/models/timeseries1_xlsx.json`

 * *Files 22% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7857142857142857%*

 * *Differences: {"'edges'": "{1: {insert: [(0, 'river1')], delete: [1]}, 2: {insert: [(0, 'abs1')], delete: [0]}, "*

 * *            "3: {insert: [(0, 'river1')], delete: [0]}, delete: [2, 1]}",*

 * * "'metadata'": "{'title': 'Timeseries example', 'description': 'A model with a timeseries'}",*

 * * "'nodes'": "{0: {'name': 'catchment1', 'type': 'Input', 'max_flow': {replace: "*

 * *            "OrderedDict([('type', 'dataframe'), ('url', 'test_data1.xlsx'), ('sheet_name', "*

 * *            "'timeseries'), ('index_col', 0)])}, delete: ['min_flow'] […]*

```diff
@@ -2,72 +2,61 @@
     "edges": [
         [
             "catchment1",
             "river1"
         ],
         [
             "river1",
-            "discharge1"
-        ],
-        [
-            "discharge1",
             "abs1"
         ],
         [
             "abs1",
-            "link1"
-        ],
-        [
-            "link1",
             "demand1"
         ],
         [
-            "abs1",
+            "river1",
             "term1"
         ]
     ],
     "metadata": {
-        "description": "A model with a discharge upstream from an abstraction.",
+        "description": "A model with a timeseries",
         "minimum_version": "0.1",
-        "title": "River Discharge 1"
+        "title": "Timeseries example"
     },
     "nodes": [
         {
-            "flow": 5,
+            "max_flow": {
+                "index_col": 0,
+                "sheet_name": "timeseries",
+                "type": "dataframe",
+                "url": "test_data1.xlsx"
+            },
             "name": "catchment1",
-            "type": "catchment"
-        },
-        {
-            "max_flow": 3,
-            "min_flow": 3,
-            "name": "discharge1",
-            "type": "input"
+            "type": "Input"
         },
         {
             "name": "river1",
-            "type": "link"
+            "type": "Link"
         },
         {
+            "max_flow": 50,
             "name": "abs1",
             "type": "link"
         },
         {
-            "name": "link1",
-            "type": "Link"
-        },
-        {
             "cost": -10,
-            "max_flow": 10,
+            "max_flow": 23.0,
             "name": "demand1",
             "type": "Output"
         },
         {
+            "cost": -5,
             "name": "term1",
-            "type": "output"
+            "type": "Output"
         }
     ],
     "timestepper": {
-        "end": "2015-12-31",
+        "end": "2015-01-31",
         "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/tests/models/river_discharge2.json` & `pywr-1.9.0/tests/models/timeseries3.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.6168650793650794%*

 * *Differences: {"'edges'": "{0: {insert: [(1, 'reservoir1')], delete: [1]}, 1: {insert: [(0, 'reservoir1')], "*

 * *            "delete: [1]}, 2: {insert: [(1, 'demand1')], delete: [1]}, 3: {insert: [(0, "*

 * *            "'reservoir1')], delete: [0]}, delete: [3, 1]}",*

 * * "'metadata'": "{'title': 'Timeseries example', 'description': 'A model with a timeseries'}",*

 * * "'nodes'": "{0: {'flow': {replace: OrderedDict([('type', 'dataframe'), ('url', "*

 * *            "'timeseries2.csv'), ('scenario', 'scenario A'), ('parse_dates', True), ('day […]*

```diff
@@ -1,73 +1,72 @@
 {
     "edges": [
         [
             "catchment1",
-            "river1"
+            "reservoir1"
         ],
         [
-            "river1",
+            "reservoir1",
             "abs1"
         ],
         [
             "abs1",
-            "link1"
-        ],
-        [
-            "link1",
             "demand1"
         ],
         [
-            "abs1",
-            "discharge1"
-        ],
-        [
-            "discharge1",
+            "reservoir1",
             "term1"
         ]
     ],
     "metadata": {
-        "description": "A model with a discharge upstream from an abstraction.",
+        "description": "A model with a timeseries",
         "minimum_version": "0.1",
-        "title": "River Discharge 2"
+        "title": "Timeseries example"
     },
     "nodes": [
         {
-            "flow": 5,
+            "flow": {
+                "dayfirst": true,
+                "index_col": 0,
+                "parse_dates": true,
+                "scenario": "scenario A",
+                "type": "dataframe",
+                "url": "timeseries2.csv"
+            },
             "name": "catchment1",
             "type": "catchment"
         },
         {
-            "max_flow": 3,
-            "min_flow": 3,
-            "name": "discharge1",
-            "type": "input"
-        },
-        {
-            "name": "river1",
-            "type": "link"
+            "initial_volume": 50.0,
+            "max_volume": 9999,
+            "name": "reservoir1",
+            "type": "Storage"
         },
         {
+            "max_flow": 50,
             "name": "abs1",
             "type": "link"
         },
         {
-            "name": "link1",
-            "type": "Link"
-        },
-        {
             "cost": -10,
-            "max_flow": 10,
+            "max_flow": 23.0,
             "name": "demand1",
             "type": "Output"
         },
         {
+            "cost": 5,
             "name": "term1",
-            "type": "output"
+            "type": "Output"
+        }
+    ],
+    "scenarios": [
+        {
+            "name": "scenario A",
+            "size": 10
         }
     ],
     "timestepper": {
-        "end": "2015-12-31",
+        "end": "2015-01-31",
         "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/tests/models/river_split_with_gauge1.json` & `pywr-1.9.0/tests/models/river_split_with_gauge1.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 25% similar despite different names*

```diff
@@ -1,76 +1,73 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5269 7665 7253 706c  itle": "RiverSpl
-00000030: 6974 5769 7468 4761 7567 6522 2c0d 0a20  itWithGauge",.. 
-00000040: 2020 2020 2020 2022 6465 7363 7269 7074         "descript
-00000050: 696f 6e22 3a20 2245 7861 6d70 6c65 206f  ion": "Example o
-00000060: 6620 616e 2061 6273 7472 6163 7469 6f6e  f an abstraction
-00000070: 2077 6974 6820 616e 204d 5246 206f 6620   with an MRF of 
-00000080: 666f 726d 2079 3d6d 782b 6322 2c0d 0a20  form y=mx+c",.. 
-00000090: 2020 2020 2020 2022 6d69 6e69 6d75 6d5f         "minimum_
-000000a0: 7665 7273 696f 6e22 3a20 2230 2e31 220d  version": "0.1".
-000000b0: 0a20 2020 207d 2c0d 0a20 2020 2022 7469  .    },..    "ti
-000000c0: 6d65 7374 6570 7065 7222 3a20 7b0d 0a20  mestepper": {.. 
-000000d0: 2020 2020 2020 2022 7374 6172 7422 3a20         "start": 
-000000e0: 2232 3031 352d 3031 2d30 3122 2c0d 0a20  "2015-01-01",.. 
-000000f0: 2020 2020 2020 2022 656e 6422 3a20 2232         "end": "2
-00000100: 3031 352d 3132 2d33 3122 2c0d 0a20 2020  015-12-31",..   
-00000110: 2020 2020 2022 7469 6d65 7374 6570 223a       "timestep":
-00000120: 2031 0d0a 2020 2020 7d2c 0d0a 2020 2020   1..    },..    
-00000130: 226e 6f64 6573 223a 205b 0d0a 2020 2020  "nodes": [..    
-00000140: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-00000150: 2020 2022 6e61 6d65 223a 2022 4361 7463     "name": "Catc
-00000160: 686d 656e 7422 2c0d 0a20 2020 2020 2020  hment",..       
-00000170: 2020 2020 2022 7479 7065 223a 2022 6361       "type": "ca
-00000180: 7463 686d 656e 7422 2c0d 0a20 2020 2020  tchment",..     
-00000190: 2020 2020 2020 2022 666c 6f77 223a 2031         "flow": 1
-000001a0: 3030 0d0a 2020 2020 2020 2020 7d2c 0d0a  00..        },..
-000001b0: 2020 2020 2020 2020 7b0d 0a20 2020 2020          {..     
-000001c0: 2020 2020 2020 2022 6e61 6d65 223a 2022         "name": "
-000001d0: 4761 7567 6522 2c0d 0a20 2020 2020 2020  Gauge",..       
-000001e0: 2020 2020 2022 7479 7065 223a 2022 5269       "type": "Ri
-000001f0: 7665 7253 706c 6974 5769 7468 4761 7567  verSplitWithGaug
-00000200: 6522 2c0d 0a20 2020 2020 2020 2020 2020  e",..           
-00000210: 2022 6d72 6622 3a20 7b0d 0a20 2020 2020   "mrf": {..     
-00000220: 2020 2020 2020 2020 2020 2022 7479 7065             "type
-00000230: 223a 2022 6d6f 6e74 686c 7970 726f 6669  ": "monthlyprofi
-00000240: 6c65 222c 0d0a 2020 2020 2020 2020 2020  le",..          
-00000250: 2020 2020 2020 2276 616c 7565 7322 3a20        "values": 
-00000260: 5b34 302e 302c 2034 302e 302c 2034 302e  [40.0, 40.0, 40.
-00000270: 302c 2034 302e 302c 2034 302e 302c 2034  0, 40.0, 40.0, 4
-00000280: 302e 302c 2034 302e 302c 2034 302e 302c  0.0, 40.0, 40.0,
-00000290: 2034 302e 302c 2034 302e 302c 2034 302e   40.0, 40.0, 40.
-000002a0: 302c 2034 302e 305d 0d0a 2020 2020 2020  0, 40.0]..      
-000002b0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000002c0: 2020 2020 2020 226d 7266 5f63 6f73 7422        "mrf_cost"
-000002d0: 3a20 2d31 3030 302c 0d0a 2020 2020 2020  : -1000,..      
-000002e0: 2020 2020 2020 2266 6163 746f 7273 223a        "factors":
-000002f0: 205b 332c 2031 5d2c 0d0a 2020 2020 2020   [3, 1],..      
-00000300: 2020 2020 2020 2273 6c6f 745f 6e61 6d65        "slot_name
-00000310: 7322 3a20 5b22 7269 7665 7222 2c20 2261  s": ["river", "a
-00000320: 6273 7472 6163 7469 6f6e 225d 0d0a 2020  bstraction"]..  
-00000330: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-00000340: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-00000350: 2022 6e61 6d65 223a 2022 4573 7475 6172   "name": "Estuar
-00000360: 7922 2c0d 0a20 2020 2020 2020 2020 2020  y",..           
-00000370: 2022 7479 7065 223a 2022 6f75 7470 7574   "type": "output
-00000380: 220d 0a20 2020 2020 2020 207d 2c0d 0a20  "..        },.. 
-00000390: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-000003a0: 2020 2020 2020 226e 616d 6522 3a20 2244        "name": "D
-000003b0: 656d 616e 6422 2c0d 0a20 2020 2020 2020  emand",..       
-000003c0: 2020 2020 2022 7479 7065 223a 2022 4f75       "type": "Ou
-000003d0: 7470 7574 222c 0d0a 2020 2020 2020 2020  tput",..        
-000003e0: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
-000003f0: 3530 2c0d 0a20 2020 2020 2020 2020 2020  50,..           
-00000400: 2022 636f 7374 223a 202d 3130 0d0a 2020   "cost": -10..  
-00000410: 2020 2020 2020 7d0d 0a20 2020 205d 2c0d        }..    ],.
-00000420: 0a20 2020 2022 6564 6765 7322 3a20 5b0d  .    "edges": [.
-00000430: 0a20 2020 2020 2020 205b 2243 6174 6368  .        ["Catch
-00000440: 6d65 6e74 222c 2022 4761 7567 6522 5d2c  ment", "Gauge"],
-00000450: 0d0a 2020 2020 2020 2020 5b22 4761 7567  ..        ["Gaug
-00000460: 6522 2c20 2245 7374 7561 7279 222c 2022  e", "Estuary", "
-00000470: 7269 7665 7222 2c20 6e75 6c6c 5d2c 0d0a  river", null],..
-00000480: 2020 2020 2020 2020 5b22 4761 7567 6522          ["Gauge"
-00000490: 2c20 2244 656d 616e 6422 2c20 2261 6273  , "Demand", "abs
-000004a0: 7472 6163 7469 6f6e 222c 206e 756c 6c5d  traction", null]
-000004b0: 0d0a 2020 2020 5d0d 0a7d 0d0a            ..    ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5269 7665 7253 706c 6974  le": "RiverSplit
+00000030: 5769 7468 4761 7567 6522 2c0a 2020 2020  WithGauge",.    
+00000040: 2020 2020 2264 6573 6372 6970 7469 6f6e      "description
+00000050: 223a 2022 4578 616d 706c 6520 6f66 2061  ": "Example of a
+00000060: 6e20 6162 7374 7261 6374 696f 6e20 7769  n abstraction wi
+00000070: 7468 2061 6e20 4d52 4620 6f66 2066 6f72  th an MRF of for
+00000080: 6d20 793d 6d78 2b63 222c 0a20 2020 2020  m y=mx+c",.     
+00000090: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
+000000a0: 696f 6e22 3a20 2230 2e31 220a 2020 2020  ion": "0.1".    
+000000b0: 7d2c 0a20 2020 2022 7469 6d65 7374 6570  },.    "timestep
+000000c0: 7065 7222 3a20 7b0a 2020 2020 2020 2020  per": {.        
+000000d0: 2273 7461 7274 223a 2022 3230 3135 2d30  "start": "2015-0
+000000e0: 312d 3031 222c 0a20 2020 2020 2020 2022  1-01",.        "
+000000f0: 656e 6422 3a20 2232 3031 352d 3132 2d33  end": "2015-12-3
+00000100: 3122 2c0a 2020 2020 2020 2020 2274 696d  1",.        "tim
+00000110: 6573 7465 7022 3a20 310a 2020 2020 7d2c  estep": 1.    },
+00000120: 0a20 2020 2022 6e6f 6465 7322 3a20 5b0a  .    "nodes": [.
+00000130: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000140: 2020 2020 2020 226e 616d 6522 3a20 2243        "name": "C
+00000150: 6174 6368 6d65 6e74 222c 0a20 2020 2020  atchment",.     
+00000160: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
+00000170: 6361 7463 686d 656e 7422 2c0a 2020 2020  catchment",.    
+00000180: 2020 2020 2020 2020 2266 6c6f 7722 3a20          "flow": 
+00000190: 3130 300a 2020 2020 2020 2020 7d2c 0a20  100.        },. 
+000001a0: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+000001b0: 2020 2020 2022 6e61 6d65 223a 2022 4761       "name": "Ga
+000001c0: 7567 6522 2c0a 2020 2020 2020 2020 2020  uge",.          
+000001d0: 2020 2274 7970 6522 3a20 2252 6976 6572    "type": "River
+000001e0: 5370 6c69 7457 6974 6847 6175 6765 222c  SplitWithGauge",
+000001f0: 0a20 2020 2020 2020 2020 2020 2022 6d72  .            "mr
+00000200: 6622 3a20 7b0a 2020 2020 2020 2020 2020  f": {.          
+00000210: 2020 2020 2020 2274 7970 6522 3a20 226d        "type": "m
+00000220: 6f6e 7468 6c79 7072 6f66 696c 6522 2c0a  onthlyprofile",.
+00000230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000240: 2276 616c 7565 7322 3a20 5b34 302e 302c  "values": [40.0,
+00000250: 2034 302e 302c 2034 302e 302c 2034 302e   40.0, 40.0, 40.
+00000260: 302c 2034 302e 302c 2034 302e 302c 2034  0, 40.0, 40.0, 4
+00000270: 302e 302c 2034 302e 302c 2034 302e 302c  0.0, 40.0, 40.0,
+00000280: 2034 302e 302c 2034 302e 302c 2034 302e   40.0, 40.0, 40.
+00000290: 305d 0a20 2020 2020 2020 2020 2020 207d  0].            }
+000002a0: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
+000002b0: 7266 5f63 6f73 7422 3a20 2d31 3030 302c  rf_cost": -1000,
+000002c0: 0a20 2020 2020 2020 2020 2020 2022 6661  .            "fa
+000002d0: 6374 6f72 7322 3a20 5b33 2c20 315d 2c0a  ctors": [3, 1],.
+000002e0: 2020 2020 2020 2020 2020 2020 2273 6c6f              "slo
+000002f0: 745f 6e61 6d65 7322 3a20 5b22 7269 7665  t_names": ["rive
+00000300: 7222 2c20 2261 6273 7472 6163 7469 6f6e  r", "abstraction
+00000310: 225d 0a20 2020 2020 2020 207d 2c0a 2020  "].        },.  
+00000320: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000330: 2020 2020 226e 616d 6522 3a20 2245 7374      "name": "Est
+00000340: 7561 7279 222c 0a20 2020 2020 2020 2020  uary",.         
+00000350: 2020 2022 7479 7065 223a 2022 6f75 7470     "type": "outp
+00000360: 7574 220a 2020 2020 2020 2020 7d2c 0a20  ut".        },. 
+00000370: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000380: 2020 2020 2022 6e61 6d65 223a 2022 4465       "name": "De
+00000390: 6d61 6e64 222c 0a20 2020 2020 2020 2020  mand",.         
+000003a0: 2020 2022 7479 7065 223a 2022 4f75 7470     "type": "Outp
+000003b0: 7574 222c 0a20 2020 2020 2020 2020 2020  ut",.           
+000003c0: 2022 6d61 785f 666c 6f77 223a 2035 302c   "max_flow": 50,
+000003d0: 0a20 2020 2020 2020 2020 2020 2022 636f  .            "co
+000003e0: 7374 223a 202d 3130 0a20 2020 2020 2020  st": -10.       
+000003f0: 207d 0a20 2020 205d 2c0a 2020 2020 2265   }.    ],.    "e
+00000400: 6467 6573 223a 205b 0a20 2020 2020 2020  dges": [.       
+00000410: 205b 2243 6174 6368 6d65 6e74 222c 2022   ["Catchment", "
+00000420: 4761 7567 6522 5d2c 0a20 2020 2020 2020  Gauge"],.       
+00000430: 205b 2247 6175 6765 222c 2022 4573 7475   ["Gauge", "Estu
+00000440: 6172 7922 2c20 2272 6976 6572 222c 206e  ary", "river", n
+00000450: 756c 6c5d 2c0a 2020 2020 2020 2020 5b22  ull],.        ["
+00000460: 4761 7567 6522 2c20 2244 656d 616e 6422  Gauge", "Demand"
+00000470: 2c20 2261 6273 7472 6163 7469 6f6e 222c  , "abstraction",
+00000480: 206e 756c 6c5d 0a20 2020 205d 0a7d 0a     null].    ].}.
```

### Comparing `pywr-1.8.0/tests/models/scenario_with_slices.json` & `pywr-1.9.0/tests/models/scenario_with_slices.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 21% similar despite different names*

```diff
@@ -1,40 +1,38 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5363 656e 6172 696f  itle": "Scenario
-00000030: 2077 6974 6820 736c 6963 6573 222c 0d0a   with slices",..
-00000040: 2020 2020 2020 2020 2264 6573 6372 6970          "descrip
-00000050: 7469 6f6e 223a 2022 4578 616d 706c 6520  tion": "Example 
-00000060: 6f66 2073 6365 6e61 7269 6f20 7769 7468  of scenario with
-00000070: 2073 6c69 6365 7322 2c0d 0a20 2020 2020   slices",..     
-00000080: 2020 2022 6d69 6e69 6d75 6d5f 7665 7273     "minimum_vers
-00000090: 696f 6e22 3a20 2230 2e32 6465 7622 0d0a  ion": "0.2dev"..
-000000a0: 2020 2020 7d2c 0d0a 2020 2020 2274 696d      },..    "tim
-000000b0: 6573 7465 7070 6572 223a 207b 0d0a 2020  estepper": {..  
-000000c0: 2020 2020 2020 2273 7461 7274 223a 2022        "start": "
-000000d0: 3230 3135 2d30 312d 3031 222c 0d0a 2020  2015-01-01",..  
-000000e0: 2020 2020 2020 2265 6e64 223a 2022 3230        "end": "20
-000000f0: 3135 2d31 322d 3331 222c 0d0a 2020 2020  15-12-31",..    
-00000100: 2020 2020 2274 696d 6573 7465 7022 3a20      "timestep": 
-00000110: 310d 0a20 2020 207d 2c0d 0a20 2020 2022  1..    },..    "
-00000120: 7363 656e 6172 696f 7322 3a20 5b0d 0a20  scenarios": [.. 
-00000130: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000140: 2020 2020 2020 226e 616d 6522 3a20 2273        "name": "s
-00000150: 6365 6e61 7269 6f20 4122 2c0d 0a20 2020  cenario A",..   
-00000160: 2020 2020 2020 2020 2022 7369 7a65 223a           "size":
-00000170: 2031 302c 0d0a 2020 2020 2020 2020 2020   10,..          
-00000180: 2020 2273 6c69 6365 223a 205b 302c 206e    "slice": [0, n
-00000190: 756c 6c2c 2032 5d0d 0a20 2020 2020 2020  ull, 2]..       
-000001a0: 207d 2c0d 0a20 2020 2020 2020 207b 0d0a   },..        {..
-000001b0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
-000001c0: 6522 3a20 2273 6365 6e61 7269 6f20 4222  e": "scenario B"
-000001d0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000001e0: 7369 7a65 223a 2032 2c0d 0a20 2020 2020  size": 2,..     
-000001f0: 2020 2020 2020 2022 736c 6963 6522 3a20         "slice": 
-00000200: 5b30 2c20 312c 2031 5d0d 0a20 2020 2020  [0, 1, 1]..     
-00000210: 2020 207d 0d0a 2020 2020 5d2c 0d0a 2020     }..    ],..  
-00000220: 2020 226e 6f64 6573 223a 205b 0d0a 0d0a    "nodes": [....
-00000230: 2020 2020 5d2c 0d0a 2020 2020 2265 6467      ],..    "edg
-00000240: 6573 223a 205b 0d0a 0d0a 2020 2020 5d2c  es": [....    ],
-00000250: 0d0a 2020 2020 2272 6563 6f72 6465 7273  ..    "recorders
-00000260: 223a 207b 0d0a 0d0a 2020 2020 7d0d 0a7d  ": {....    }..}
-00000270: 0d0a                                     ..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5363 656e 6172 696f 2077  le": "Scenario w
+00000030: 6974 6820 736c 6963 6573 222c 0a20 2020  ith slices",.   
+00000040: 2020 2020 2022 6465 7363 7269 7074 696f       "descriptio
+00000050: 6e22 3a20 2245 7861 6d70 6c65 206f 6620  n": "Example of 
+00000060: 7363 656e 6172 696f 2077 6974 6820 736c  scenario with sl
+00000070: 6963 6573 222c 0a20 2020 2020 2020 2022  ices",.        "
+00000080: 6d69 6e69 6d75 6d5f 7665 7273 696f 6e22  minimum_version"
+00000090: 3a20 2230 2e32 6465 7622 0a20 2020 207d  : "0.2dev".    }
+000000a0: 2c0a 2020 2020 2274 696d 6573 7465 7070  ,.    "timestepp
+000000b0: 6572 223a 207b 0a20 2020 2020 2020 2022  er": {.        "
+000000c0: 7374 6172 7422 3a20 2232 3031 352d 3031  start": "2015-01
+000000d0: 2d30 3122 2c0a 2020 2020 2020 2020 2265  -01",.        "e
+000000e0: 6e64 223a 2022 3230 3135 2d31 322d 3331  nd": "2015-12-31
+000000f0: 222c 0a20 2020 2020 2020 2022 7469 6d65  ",.        "time
+00000100: 7374 6570 223a 2031 0a20 2020 207d 2c0a  step": 1.    },.
+00000110: 2020 2020 2273 6365 6e61 7269 6f73 223a      "scenarios":
+00000120: 205b 0a20 2020 2020 2020 207b 0a20 2020   [.        {.   
+00000130: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
+00000140: 2022 7363 656e 6172 696f 2041 222c 0a20   "scenario A",. 
+00000150: 2020 2020 2020 2020 2020 2022 7369 7a65             "size
+00000160: 223a 2031 302c 0a20 2020 2020 2020 2020  ": 10,.         
+00000170: 2020 2022 736c 6963 6522 3a20 5b30 2c20     "slice": [0, 
+00000180: 6e75 6c6c 2c20 325d 0a20 2020 2020 2020  null, 2].       
+00000190: 207d 2c0a 2020 2020 2020 2020 7b0a 2020   },.        {.  
+000001a0: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
+000001b0: 3a20 2273 6365 6e61 7269 6f20 4222 2c0a  : "scenario B",.
+000001c0: 2020 2020 2020 2020 2020 2020 2273 697a              "siz
+000001d0: 6522 3a20 322c 0a20 2020 2020 2020 2020  e": 2,.         
+000001e0: 2020 2022 736c 6963 6522 3a20 5b30 2c20     "slice": [0, 
+000001f0: 312c 2031 5d0a 2020 2020 2020 2020 7d0a  1, 1].        }.
+00000200: 2020 2020 5d2c 0a20 2020 2022 6e6f 6465      ],.    "node
+00000210: 7322 3a20 5b0a 0a20 2020 205d 2c0a 2020  s": [..    ],.  
+00000220: 2020 2265 6467 6573 223a 205b 0a0a 2020    "edges": [..  
+00000230: 2020 5d2c 0a20 2020 2022 7265 636f 7264    ],.    "record
+00000240: 6572 7322 3a20 7b0a 0a20 2020 207d 0a7d  ers": {..    }.}
+00000250: 0a                                       .
```

### Comparing `pywr-1.8.0/tests/models/simple1.json` & `pywr-1.9.0/tests/models/version1.json`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8333333333333334%*

 * *Differences: {"'edges'": "{0: {insert: [(0, 'supply1')], delete: [0]}, delete: [0]}",*

 * * "'metadata'": "{'minimum_version': '99999'}",*

 * * "'nodes'": '{delete: [1]}'}*

```diff
@@ -1,34 +1,26 @@
 {
     "edges": [
         [
             "supply1",
-            "link1"
-        ],
-        [
-            "link1",
             "demand1"
         ]
     ],
     "metadata": {
         "description": "A very simple example.",
-        "minimum_version": "0.1",
+        "minimum_version": "99999",
         "title": "Simple 1"
     },
     "nodes": [
         {
             "max_flow": 15,
             "name": "supply1",
             "type": "Input"
         },
         {
-            "name": "link1",
-            "type": "Link"
-        },
-        {
             "cost": -10,
             "max_flow": 10,
             "name": "demand1",
             "type": "Output"
         }
     ],
     "timestepper": {
```

### Comparing `pywr-1.8.0/tests/models/test_data1.xlsx` & `pywr-1.9.0/tests/models/test_data1.xlsx`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/timeseries1_weekly.h5` & `pywr-1.9.0/tests/models/timeseries1_weekly.h5`

 * *Files 21% similar despite different names*

#### h5dump {}

```diff
@@ -1,8 +1,8 @@
-HDF5 "/tmp/diffoscope_o93tibvj_/tmpsru2c6uj_TarContainer/0/210.h5" {
+HDF5 "/tmp/diffoscope_o93tibvj_/tmpwh65j8qg_TarContainer/0/217.h5" {
 GROUP "/" {
    ATTRIBUTE "CLASS" {
       DATATYPE  H5T_STRING {
          STRSIZE 5;
          STRPAD H5T_STR_NULLTERM;
          CSET H5T_CSET_UTF8;
          CTYPE H5T_C_S1;
@@ -127,14 +127,26 @@
             CTYPE H5T_C_S1;
          }
          DATASPACE  SCALAR
          DATA {
          (0): "UTF-8"
          }
       }
+      ATTRIBUTE "errors" {
+         DATATYPE  H5T_STRING {
+            STRSIZE 6;
+            STRPAD H5T_STR_NULLTERM;
+            CSET H5T_CSET_UTF8;
+            CTYPE H5T_C_S1;
+         }
+         DATASPACE  SCALAR
+         DATA {
+         (0): "strict"
+         }
+      }
       ATTRIBUTE "nblocks" {
          DATATYPE  H5T_STD_I64LE
          DATASPACE  SCALAR
          DATA {
          (0): 1
          }
       }
@@ -311,56 +323,29 @@
             DATASPACE  SCALAR
             DATA {
             (0): "2.4"
             }
          }
          ATTRIBUTE "freq" {
             DATATYPE  H5T_STRING {
-               STRSIZE 234;
+               STRSIZE 53;
                STRPAD H5T_STR_NULLTERM;
                CSET H5T_CSET_ASCII;
                CTYPE H5T_C_S1;
             }
             DATASPACE  SCALAR
             DATA {
-            (0): "ccopy_reg
-           _reconstructor
-           p0
-           (cpandas.tseries.offsets
+            (0): "cpandas._libs.tslibs.offsets
            Day
-           p1
-           c__builtin__
-           object
-           p2
-           Ntp3
-           Rp4
-           (dp5
-           Vkwds
-           p6
-           (dp7
-           sV_use_relativedelta
-           p8
-           I00
-           sVnormalize
-           p9
+           p0
+           (I7
            I00
-           sVn
-           p10
-           L7L
-           sV_offset
-           p11
-           cdatetime
-           timedelta
-           p12
-           (L1L
-           L0L
-           L0L
-           tp13
-           Rp14
-           sb."
+           tp1
+           Rp2
+           ."
             }
          }
          ATTRIBUTE "index_class" {
             DATATYPE  H5T_STRING {
                STRSIZE 8;
                STRPAD H5T_STR_NULLTERM;
                CSET H5T_CSET_UTF8;
```

### Comparing `pywr-1.8.0/tests/models/timeseries2.csv` & `pywr-1.9.0/tests/models/timeseries2.csv`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/timeseries2.h5` & `pywr-1.9.0/tests/models/timeseries2.h5`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/tests/models/timeseries2_hdf.json` & `pywr-1.9.0/tests/models/timeseries2_hdf.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 24% similar despite different names*

```diff
@@ -1,91 +1,87 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5469 6d65 7365 7269  itle": "Timeseri
-00000030: 6573 2065 7861 6d70 6c65 222c 0d0a 2020  es example",..  
-00000040: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
-00000050: 6f6e 223a 2022 4120 6d6f 6465 6c20 7769  on": "A model wi
-00000060: 7468 2061 2074 696d 6573 6572 6965 7322  th a timeseries"
-00000070: 2c0d 0a20 2020 2020 2020 2022 6d69 6e69  ,..        "mini
-00000080: 6d75 6d5f 7665 7273 696f 6e22 3a20 2230  mum_version": "0
-00000090: 2e31 220d 0a20 2020 207d 2c0d 0a20 2020  .1"..    },..   
-000000a0: 2022 7469 6d65 7374 6570 7065 7222 3a20   "timestepper": 
-000000b0: 7b0d 0a20 2020 2020 2020 2022 7374 6172  {..        "star
-000000c0: 7422 3a20 2232 3031 352d 3031 2d30 3122  t": "2015-01-01"
-000000d0: 2c0d 0a20 2020 2020 2020 2022 656e 6422  ,..        "end"
-000000e0: 3a20 2232 3031 352d 3031 2d33 3122 2c0d  : "2015-01-31",.
-000000f0: 0a20 2020 2020 2020 2022 7469 6d65 7374  .        "timest
-00000100: 6570 223a 2031 0d0a 2020 2020 7d2c 0d0a  ep": 1..    },..
-00000110: 2020 2020 2273 6365 6e61 7269 6f73 223a      "scenarios":
-00000120: 205b 0d0a 2020 2020 2020 2020 7b0d 0a20   [..        {.. 
-00000130: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000140: 223a 2022 7363 656e 6172 696f 2041 222c  ": "scenario A",
-00000150: 0d0a 2020 2020 2020 2020 2020 2020 2273  ..            "s
-00000160: 697a 6522 3a20 3130 0d0a 2020 2020 2020  ize": 10..      
-00000170: 2020 7d0d 0a20 2020 205d 2c0d 0a20 2020    }..    ],..   
-00000180: 2022 6e6f 6465 7322 3a20 5b0d 0a20 2020   "nodes": [..   
-00000190: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-000001a0: 2020 2020 226e 616d 6522 3a20 2263 6174      "name": "cat
-000001b0: 6368 6d65 6e74 3122 2c0d 0a20 2020 2020  chment1",..     
-000001c0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000001d0: 6361 7463 686d 656e 7422 2c0d 0a20 2020  catchment",..   
-000001e0: 2020 2020 2020 2020 2022 666c 6f77 223a           "flow":
-000001f0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000200: 2020 2020 2274 7970 6522 3a20 2274 6162      "type": "tab
-00000210: 6c65 7361 7272 6179 222c 0d0a 2020 2020  lesarray",..    
-00000220: 2020 2020 2020 2020 2020 2020 2273 6365              "sce
-00000230: 6e61 7269 6f22 3a20 2273 6365 6e61 7269  nario": "scenari
-00000240: 6f20 4122 2c0d 0a20 2020 2020 2020 2020  o A",..         
-00000250: 2020 2020 2020 2022 6368 6563 6b73 756d         "checksum
-00000260: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000270: 2020 2020 2020 2020 2020 226d 6435 223a            "md5":
-00000280: 2022 3066 3663 3635 6133 3638 3531 6338   "0f6c65a36851c8
-00000290: 3963 3763 3465 3633 6162 3138 3933 3535  9c7c4e63ab189355
-000002a0: 3462 220d 0a20 2020 2020 2020 2020 2020  4b"..           
-000002b0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-000002c0: 2020 2020 2020 2020 2022 7572 6c22 203a           "url" :
-000002d0: 2022 7469 6d65 7365 7269 6573 322e 6835   "timeseries2.h5
-000002e0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000002f0: 2020 2020 2277 6865 7265 223a 2022 2f74      "where": "/t
-00000300: 696d 6573 6572 6965 7332 222c 0d0a 2020  imeseries2",..  
-00000310: 2020 2020 2020 2020 2020 2020 2020 226e                "n
-00000320: 6f64 6522 3a20 2262 6c6f 636b 305f 7661  ode": "block0_va
-00000330: 6c75 6573 220d 0a20 2020 2020 2020 2020  lues"..         
-00000340: 2020 207d 0d0a 2020 2020 2020 2020 7d2c     }..        },
-00000350: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-00000360: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000370: 2022 7269 7665 7231 222c 0d0a 2020 2020   "river1",..    
-00000380: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000390: 224c 696e 6b22 0d0a 2020 2020 2020 2020  "Link"..        
-000003a0: 7d2c 0d0a 2020 2020 2020 2020 7b0d 0a20  },..        {.. 
-000003b0: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-000003c0: 223a 2022 6162 7331 222c 0d0a 2020 2020  ": "abs1",..    
-000003d0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-000003e0: 226c 696e 6b22 2c0d 0a20 2020 2020 2020  "link",..       
-000003f0: 2020 2020 2022 6d61 785f 666c 6f77 223a       "max_flow":
-00000400: 2035 300d 0a20 2020 2020 2020 207d 2c0d   50..        },.
-00000410: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000420: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-00000430: 2264 656d 616e 6431 222c 0d0a 2020 2020  "demand1",..    
-00000440: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000450: 224f 7574 7075 7422 2c0d 0a20 2020 2020  "Output",..     
-00000460: 2020 2020 2020 2022 6d61 785f 666c 6f77         "max_flow
-00000470: 223a 2032 332e 302c 0d0a 2020 2020 2020  ": 23.0,..      
-00000480: 2020 2020 2020 2263 6f73 7422 3a20 2d31        "cost": -1
-00000490: 300d 0a20 2020 2020 2020 207d 2c0d 0a20  0..        },.. 
-000004a0: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-000004b0: 2020 2020 2020 226e 616d 6522 3a20 2274        "name": "t
-000004c0: 6572 6d31 222c 0d0a 2020 2020 2020 2020  erm1",..        
-000004d0: 2020 2020 2274 7970 6522 3a20 224f 7574      "type": "Out
-000004e0: 7075 7422 2c0d 0a20 2020 2020 2020 2020  put",..         
-000004f0: 2020 2022 636f 7374 223a 202d 350d 0a20     "cost": -5.. 
-00000500: 2020 2020 2020 207d 0d0a 2020 2020 5d2c         }..    ],
-00000510: 0d0a 2020 2020 2265 6467 6573 223a 205b  ..    "edges": [
-00000520: 0d0a 2020 2020 2020 2020 5b22 6361 7463  ..        ["catc
-00000530: 686d 656e 7431 222c 2022 7269 7665 7231  hment1", "river1
-00000540: 225d 2c0d 0a20 2020 2020 2020 205b 2272  "],..        ["r
-00000550: 6976 6572 3122 2c20 2261 6273 3122 5d2c  iver1", "abs1"],
-00000560: 0d0a 2020 2020 2020 2020 5b22 6162 7331  ..        ["abs1
-00000570: 222c 2022 6465 6d61 6e64 3122 5d2c 0d0a  ", "demand1"],..
-00000580: 2020 2020 2020 2020 5b22 7269 7665 7231          ["river1
-00000590: 222c 2022 7465 726d 3122 5d0d 0a20 2020  ", "term1"]..   
-000005a0: 205d 0d0a 7d0d 0a                         ]..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5469 6d65 7365 7269 6573  le": "Timeseries
+00000030: 2065 7861 6d70 6c65 222c 0a20 2020 2020   example",.     
+00000040: 2020 2022 6465 7363 7269 7074 696f 6e22     "description"
+00000050: 3a20 2241 206d 6f64 656c 2077 6974 6820  : "A model with 
+00000060: 6120 7469 6d65 7365 7269 6573 222c 0a20  a timeseries",. 
+00000070: 2020 2020 2020 2022 6d69 6e69 6d75 6d5f         "minimum_
+00000080: 7665 7273 696f 6e22 3a20 2230 2e31 220a  version": "0.1".
+00000090: 2020 2020 7d2c 0a20 2020 2022 7469 6d65      },.    "time
+000000a0: 7374 6570 7065 7222 3a20 7b0a 2020 2020  stepper": {.    
+000000b0: 2020 2020 2273 7461 7274 223a 2022 3230      "start": "20
+000000c0: 3135 2d30 312d 3031 222c 0a20 2020 2020  15-01-01",.     
+000000d0: 2020 2022 656e 6422 3a20 2232 3031 352d     "end": "2015-
+000000e0: 3031 2d33 3122 2c0a 2020 2020 2020 2020  01-31",.        
+000000f0: 2274 696d 6573 7465 7022 3a20 310a 2020  "timestep": 1.  
+00000100: 2020 7d2c 0a20 2020 2022 7363 656e 6172    },.    "scenar
+00000110: 696f 7322 3a20 5b0a 2020 2020 2020 2020  ios": [.        
+00000120: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+00000130: 616d 6522 3a20 2273 6365 6e61 7269 6f20  ame": "scenario 
+00000140: 4122 2c0a 2020 2020 2020 2020 2020 2020  A",.            
+00000150: 2273 697a 6522 3a20 3130 0a20 2020 2020  "size": 10.     
+00000160: 2020 207d 0a20 2020 205d 2c0a 2020 2020     }.    ],.    
+00000170: 226e 6f64 6573 223a 205b 0a20 2020 2020  "nodes": [.     
+00000180: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000190: 2022 6e61 6d65 223a 2022 6361 7463 686d   "name": "catchm
+000001a0: 656e 7431 222c 0a20 2020 2020 2020 2020  ent1",.         
+000001b0: 2020 2022 7479 7065 223a 2022 6361 7463     "type": "catc
+000001c0: 686d 656e 7422 2c0a 2020 2020 2020 2020  hment",.        
+000001d0: 2020 2020 2266 6c6f 7722 3a20 7b0a 2020      "flow": {.  
+000001e0: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+000001f0: 7970 6522 3a20 2274 6162 6c65 7361 7272  ype": "tablesarr
+00000200: 6179 222c 0a20 2020 2020 2020 2020 2020  ay",.           
+00000210: 2020 2020 2022 7363 656e 6172 696f 223a       "scenario":
+00000220: 2022 7363 656e 6172 696f 2041 222c 0a20   "scenario A",. 
+00000230: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00000240: 6368 6563 6b73 756d 223a 207b 0a20 2020  checksum": {.   
+00000250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000260: 2022 6d64 3522 3a20 2230 6636 6336 3561   "md5": "0f6c65a
+00000270: 3336 3835 3163 3839 6337 6334 6536 3361  36851c89c7c4e63a
+00000280: 6231 3839 3335 3534 6222 0a20 2020 2020  b1893554b".     
+00000290: 2020 2020 2020 2020 2020 207d 2c0a 2020             },.  
+000002a0: 2020 2020 2020 2020 2020 2020 2020 2275                "u
+000002b0: 726c 2220 3a20 2274 696d 6573 6572 6965  rl" : "timeserie
+000002c0: 7332 2e68 3522 2c0a 2020 2020 2020 2020  s2.h5",.        
+000002d0: 2020 2020 2020 2020 2277 6865 7265 223a          "where":
+000002e0: 2022 2f74 696d 6573 6572 6965 7332 222c   "/timeseries2",
+000002f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000300: 2022 6e6f 6465 223a 2022 626c 6f63 6b30   "node": "block0
+00000310: 5f76 616c 7565 7322 0a20 2020 2020 2020  _values".       
+00000320: 2020 2020 207d 0a20 2020 2020 2020 207d       }.        }
+00000330: 2c0a 2020 2020 2020 2020 7b0a 2020 2020  ,.        {.    
+00000340: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
+00000350: 2272 6976 6572 3122 2c0a 2020 2020 2020  "river1",.      
+00000360: 2020 2020 2020 2274 7970 6522 3a20 224c        "type": "L
+00000370: 696e 6b22 0a20 2020 2020 2020 207d 2c0a  ink".        },.
+00000380: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000390: 2020 2020 2020 226e 616d 6522 3a20 2261        "name": "a
+000003a0: 6273 3122 2c0a 2020 2020 2020 2020 2020  bs1",.          
+000003b0: 2020 2274 7970 6522 3a20 226c 696e 6b22    "type": "link"
+000003c0: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
+000003d0: 6178 5f66 6c6f 7722 3a20 3530 0a20 2020  ax_flow": 50.   
+000003e0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000003f0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+00000400: 616d 6522 3a20 2264 656d 616e 6431 222c  ame": "demand1",
+00000410: 0a20 2020 2020 2020 2020 2020 2022 7479  .            "ty
+00000420: 7065 223a 2022 4f75 7470 7574 222c 0a20  pe": "Output",. 
+00000430: 2020 2020 2020 2020 2020 2022 6d61 785f             "max_
+00000440: 666c 6f77 223a 2032 332e 302c 0a20 2020  flow": 23.0,.   
+00000450: 2020 2020 2020 2020 2022 636f 7374 223a           "cost":
+00000460: 202d 3130 0a20 2020 2020 2020 207d 2c0a   -10.        },.
+00000470: 2020 2020 2020 2020 7b0a 2020 2020 2020          {.      
+00000480: 2020 2020 2020 226e 616d 6522 3a20 2274        "name": "t
+00000490: 6572 6d31 222c 0a20 2020 2020 2020 2020  erm1",.         
+000004a0: 2020 2022 7479 7065 223a 2022 4f75 7470     "type": "Outp
+000004b0: 7574 222c 0a20 2020 2020 2020 2020 2020  ut",.           
+000004c0: 2022 636f 7374 223a 202d 350a 2020 2020   "cost": -5.    
+000004d0: 2020 2020 7d0a 2020 2020 5d2c 0a20 2020      }.    ],.   
+000004e0: 2022 6564 6765 7322 3a20 5b0a 2020 2020   "edges": [.    
+000004f0: 2020 2020 5b22 6361 7463 686d 656e 7431      ["catchment1
+00000500: 222c 2022 7269 7665 7231 225d 2c0a 2020  ", "river1"],.  
+00000510: 2020 2020 2020 5b22 7269 7665 7231 222c        ["river1",
+00000520: 2022 6162 7331 225d 2c0a 2020 2020 2020   "abs1"],.      
+00000530: 2020 5b22 6162 7331 222c 2022 6465 6d61    ["abs1", "dema
+00000540: 6e64 3122 5d2c 0a20 2020 2020 2020 205b  nd1"],.        [
+00000550: 2272 6976 6572 3122 2c20 2274 6572 6d31  "river1", "term1
+00000560: 225d 0a20 2020 205d 0a7d 0a              "].    ].}.
```

### Comparing `pywr-1.8.0/tests/models/timeseries2_with_fdc.json` & `pywr-1.9.0/tests/models/timeseries2_with_fdc.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 17% similar despite different names*

```diff
@@ -1,199 +1,193 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 5469 6d65 7365 7269  itle": "Timeseri
-00000030: 6573 2065 7861 6d70 6c65 222c 0d0a 2020  es example",..  
-00000040: 2020 2020 2020 2264 6573 6372 6970 7469        "descripti
-00000050: 6f6e 223a 2022 4120 6d6f 6465 6c20 7769  on": "A model wi
-00000060: 7468 2061 2074 696d 6573 6572 6965 7322  th a timeseries"
-00000070: 2c0d 0a20 2020 2020 2020 2022 6d69 6e69  ,..        "mini
-00000080: 6d75 6d5f 7665 7273 696f 6e22 3a20 2230  mum_version": "0
-00000090: 2e31 220d 0a20 2020 207d 2c0d 0a20 2020  .1"..    },..   
-000000a0: 2022 7469 6d65 7374 6570 7065 7222 3a20   "timestepper": 
-000000b0: 7b0d 0a20 2020 2020 2020 2022 7374 6172  {..        "star
-000000c0: 7422 3a20 2232 3031 352d 3031 2d30 3122  t": "2015-01-01"
-000000d0: 2c0d 0a20 2020 2020 2020 2022 656e 6422  ,..        "end"
-000000e0: 3a20 2232 3031 352d 3031 2d33 3122 2c0d  : "2015-01-31",.
-000000f0: 0a20 2020 2020 2020 2022 7469 6d65 7374  .        "timest
-00000100: 6570 223a 2031 0d0a 2020 2020 7d2c 0d0a  ep": 1..    },..
-00000110: 2020 2020 2273 6365 6e61 7269 6f73 223a      "scenarios":
-00000120: 205b 0d0a 2020 2020 2020 2020 7b0d 0a20   [..        {.. 
-00000130: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000140: 223a 2022 7363 656e 6172 696f 2041 222c  ": "scenario A",
-00000150: 0d0a 2020 2020 2020 2020 2020 2020 2273  ..            "s
-00000160: 697a 6522 3a20 3130 0d0a 2020 2020 2020  ize": 10..      
-00000170: 2020 7d0d 0a20 2020 205d 2c0d 0a20 2020    }..    ],..   
-00000180: 2022 6e6f 6465 7322 3a20 5b0d 0a20 2020   "nodes": [..   
-00000190: 2020 2020 207b 0d0a 2020 2020 2020 2020       {..        
-000001a0: 2020 2020 226e 616d 6522 3a20 2263 6174      "name": "cat
-000001b0: 6368 6d65 6e74 3122 2c0d 0a20 2020 2020  chment1",..     
-000001c0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000001d0: 496e 7075 7422 2c0d 0a20 2020 2020 2020  Input",..       
-000001e0: 2020 2020 2022 6d61 785f 666c 6f77 223a       "max_flow":
-000001f0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000200: 2020 2020 2274 7970 6522 3a20 2264 6174      "type": "dat
-00000210: 6166 7261 6d65 222c 0d0a 2020 2020 2020  aframe",..      
-00000220: 2020 2020 2020 2020 2020 2275 726c 2220            "url" 
-00000230: 3a20 2274 696d 6573 6572 6965 7332 2e63  : "timeseries2.c
-00000240: 7376 222c 0d0a 2020 2020 2020 2020 2020  sv",..          
-00000250: 2020 2020 2020 2263 6865 636b 7375 6d22        "checksum"
-00000260: 3a20 7b0d 0a20 2020 2020 2020 2020 2020  : {..           
-00000270: 2020 2020 2020 2020 2022 6d64 3522 3a20           "md5": 
-00000280: 2261 3563 3430 3332 6532 6438 6635 3230  "a5c4032e2d8f520
-00000290: 3563 6139 3964 6564 6366 6134 6364 3138  5ca99dedcfa4cd18
-000002a0: 6522 2c0d 0a20 2020 2020 2020 2020 2020  e",..           
-000002b0: 2020 2020 2020 2020 2022 7368 6132 3536           "sha256
-000002c0: 223a 2022 3066 3735 6233 6365 6533 3235  ": "0f75b3cee325
-000002d0: 6433 3731 3132 3638 3764 3364 3130 3539  d37112687d3d1059
-000002e0: 3666 3434 6530 6164 6433 3734 6634 6534  6f44e0add374f4e4
-000002f0: 3061 3162 3636 3837 3931 3263 3035 6536  0a1b6687912c05e6
-00000300: 3533 3636 220d 0a20 2020 2020 2020 2020  5366"..         
-00000310: 2020 2020 2020 207d 2c0d 0a20 2020 2020         },..     
-00000320: 2020 2020 2020 2020 2020 2022 7363 656e             "scen
-00000330: 6172 696f 223a 2022 7363 656e 6172 696f  ario": "scenario
-00000340: 2041 222c 0d0a 2020 2020 2020 2020 2020   A",..          
-00000350: 2020 2020 2020 2270 6172 7365 5f64 6174        "parse_dat
-00000360: 6573 223a 2074 7275 652c 0d0a 2020 2020  es": true,..    
-00000370: 2020 2020 2020 2020 2020 2020 2264 6179              "day
-00000380: 6669 7273 7422 3a20 7472 7565 2c0d 0a20  first": true,.. 
-00000390: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-000003a0: 696e 6465 785f 636f 6c22 3a20 300d 0a20  index_col": 0.. 
-000003b0: 2020 2020 2020 2020 2020 207d 0d0a 2020             }..  
-000003c0: 2020 2020 2020 7d2c 0d0a 2020 2020 2020        },..      
-000003d0: 2020 7b0d 0a20 2020 2020 2020 2020 2020    {..           
-000003e0: 2022 6e61 6d65 223a 2022 7269 7665 7231   "name": "river1
-000003f0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000400: 2274 7970 6522 3a20 224c 696e 6b22 0d0a  "type": "Link"..
-00000410: 2020 2020 2020 2020 7d2c 0d0a 2020 2020          },..    
-00000420: 2020 2020 7b0d 0a20 2020 2020 2020 2020      {..         
-00000430: 2020 2022 6e61 6d65 223a 2022 6162 7331     "name": "abs1
-00000440: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000450: 2274 7970 6522 3a20 226c 696e 6b22 2c0d  "type": "link",.
-00000460: 0a20 2020 2020 2020 2020 2020 2022 6d61  .            "ma
-00000470: 785f 666c 6f77 223a 2035 300d 0a20 2020  x_flow": 50..   
-00000480: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-00000490: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-000004a0: 226e 616d 6522 3a20 2264 656d 616e 6431  "name": "demand1
-000004b0: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-000004c0: 2274 7970 6522 3a20 224f 7574 7075 7422  "type": "Output"
-000004d0: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-000004e0: 6d61 785f 666c 6f77 223a 2032 332e 302c  max_flow": 23.0,
-000004f0: 0d0a 2020 2020 2020 2020 2020 2020 2263  ..            "c
-00000500: 6f73 7422 3a20 2d31 300d 0a20 2020 2020  ost": -10..     
-00000510: 2020 207d 2c0d 0a20 2020 2020 2020 207b     },..        {
-00000520: 0d0a 2020 2020 2020 2020 2020 2020 226e  ..            "n
-00000530: 616d 6522 3a20 2274 6572 6d31 222c 0d0a  ame": "term1",..
-00000540: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000550: 6522 3a20 224f 7574 7075 7422 2c0d 0a20  e": "Output",.. 
-00000560: 2020 2020 2020 2020 2020 2022 636f 7374             "cost
-00000570: 223a 202d 350d 0a20 2020 2020 2020 207d  ": -5..        }
-00000580: 0d0a 2020 2020 5d2c 0d0a 2020 2020 2265  ..    ],..    "e
-00000590: 6467 6573 223a 205b 0d0a 2020 2020 2020  dges": [..      
-000005a0: 2020 5b22 6361 7463 686d 656e 7431 222c    ["catchment1",
-000005b0: 2022 7269 7665 7231 225d 2c0d 0a20 2020   "river1"],..   
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 5469 6d65 7365 7269 6573  le": "Timeseries
+00000030: 2065 7861 6d70 6c65 222c 0a20 2020 2020   example",.     
+00000040: 2020 2022 6465 7363 7269 7074 696f 6e22     "description"
+00000050: 3a20 2241 206d 6f64 656c 2077 6974 6820  : "A model with 
+00000060: 6120 7469 6d65 7365 7269 6573 222c 0a20  a timeseries",. 
+00000070: 2020 2020 2020 2022 6d69 6e69 6d75 6d5f         "minimum_
+00000080: 7665 7273 696f 6e22 3a20 2230 2e31 220a  version": "0.1".
+00000090: 2020 2020 7d2c 0a20 2020 2022 7469 6d65      },.    "time
+000000a0: 7374 6570 7065 7222 3a20 7b0a 2020 2020  stepper": {.    
+000000b0: 2020 2020 2273 7461 7274 223a 2022 3230      "start": "20
+000000c0: 3135 2d30 312d 3031 222c 0a20 2020 2020  15-01-01",.     
+000000d0: 2020 2022 656e 6422 3a20 2232 3031 352d     "end": "2015-
+000000e0: 3031 2d33 3122 2c0a 2020 2020 2020 2020  01-31",.        
+000000f0: 2274 696d 6573 7465 7022 3a20 310a 2020  "timestep": 1.  
+00000100: 2020 7d2c 0a20 2020 2022 7363 656e 6172    },.    "scenar
+00000110: 696f 7322 3a20 5b0a 2020 2020 2020 2020  ios": [.        
+00000120: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+00000130: 616d 6522 3a20 2273 6365 6e61 7269 6f20  ame": "scenario 
+00000140: 4122 2c0a 2020 2020 2020 2020 2020 2020  A",.            
+00000150: 2273 697a 6522 3a20 3130 0a20 2020 2020  "size": 10.     
+00000160: 2020 207d 0a20 2020 205d 2c0a 2020 2020     }.    ],.    
+00000170: 226e 6f64 6573 223a 205b 0a20 2020 2020  "nodes": [.     
+00000180: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000190: 2022 6e61 6d65 223a 2022 6361 7463 686d   "name": "catchm
+000001a0: 656e 7431 222c 0a20 2020 2020 2020 2020  ent1",.         
+000001b0: 2020 2022 7479 7065 223a 2022 496e 7075     "type": "Inpu
+000001c0: 7422 2c0a 2020 2020 2020 2020 2020 2020  t",.            
+000001d0: 226d 6178 5f66 6c6f 7722 3a20 7b0a 2020  "max_flow": {.  
+000001e0: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+000001f0: 7970 6522 3a20 2264 6174 6166 7261 6d65  ype": "dataframe
+00000200: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00000210: 2020 2022 7572 6c22 203a 2022 7469 6d65     "url" : "time
+00000220: 7365 7269 6573 322e 6373 7622 2c0a 2020  series2.csv",.  
+00000230: 2020 2020 2020 2020 2020 2020 2020 2263                "c
+00000240: 6865 636b 7375 6d22 3a20 7b0a 2020 2020  hecksum": {.    
+00000250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000260: 226d 6435 223a 2022 6135 6334 3033 3265  "md5": "a5c4032e
+00000270: 3264 3866 3532 3035 6361 3939 6465 6463  2d8f5205ca99dedc
+00000280: 6661 3463 6431 3865 222c 0a20 2020 2020  fa4cd18e",.     
+00000290: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+000002a0: 7368 6132 3536 223a 2022 3066 3735 6233  sha256": "0f75b3
+000002b0: 6365 6533 3235 6433 3731 3132 3638 3764  cee325d37112687d
+000002c0: 3364 3130 3539 3666 3434 6530 6164 6433  3d10596f44e0add3
+000002d0: 3734 6634 6534 3061 3162 3636 3837 3931  74f4e40a1b668791
+000002e0: 3263 3035 6536 3533 3636 220a 2020 2020  2c05e65366".    
+000002f0: 2020 2020 2020 2020 2020 2020 7d2c 0a20              },. 
+00000300: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00000310: 7363 656e 6172 696f 223a 2022 7363 656e  scenario": "scen
+00000320: 6172 696f 2041 222c 0a20 2020 2020 2020  ario A",.       
+00000330: 2020 2020 2020 2020 2022 7061 7273 655f           "parse_
+00000340: 6461 7465 7322 3a20 7472 7565 2c0a 2020  dates": true,.  
+00000350: 2020 2020 2020 2020 2020 2020 2020 2264                "d
+00000360: 6179 6669 7273 7422 3a20 7472 7565 2c0a  ayfirst": true,.
+00000370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000380: 2269 6e64 6578 5f63 6f6c 223a 2030 0a20  "index_col": 0. 
+00000390: 2020 2020 2020 2020 2020 207d 0a20 2020             }.   
+000003a0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000003b0: 7b0a 2020 2020 2020 2020 2020 2020 226e  {.            "n
+000003c0: 616d 6522 3a20 2272 6976 6572 3122 2c0a  ame": "river1",.
+000003d0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000003e0: 6522 3a20 224c 696e 6b22 0a20 2020 2020  e": "Link".     
+000003f0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+00000400: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000410: 6522 3a20 2261 6273 3122 2c0a 2020 2020  e": "abs1",.    
+00000420: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
+00000430: 226c 696e 6b22 2c0a 2020 2020 2020 2020  "link",.        
+00000440: 2020 2020 226d 6178 5f66 6c6f 7722 3a20      "max_flow": 
+00000450: 3530 0a20 2020 2020 2020 207d 2c0a 2020  50.        },.  
+00000460: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000470: 2020 2020 226e 616d 6522 3a20 2264 656d      "name": "dem
+00000480: 616e 6431 222c 0a20 2020 2020 2020 2020  and1",.         
+00000490: 2020 2022 7479 7065 223a 2022 4f75 7470     "type": "Outp
+000004a0: 7574 222c 0a20 2020 2020 2020 2020 2020  ut",.           
+000004b0: 2022 6d61 785f 666c 6f77 223a 2032 332e   "max_flow": 23.
+000004c0: 302c 0a20 2020 2020 2020 2020 2020 2022  0,.            "
+000004d0: 636f 7374 223a 202d 3130 0a20 2020 2020  cost": -10.     
+000004e0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+000004f0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+00000500: 6522 3a20 2274 6572 6d31 222c 0a20 2020  e": "term1",.   
+00000510: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+00000520: 2022 4f75 7470 7574 222c 0a20 2020 2020   "Output",.     
+00000530: 2020 2020 2020 2022 636f 7374 223a 202d         "cost": -
+00000540: 350a 2020 2020 2020 2020 7d0a 2020 2020  5.        }.    
+00000550: 5d2c 0a20 2020 2022 6564 6765 7322 3a20  ],.    "edges": 
+00000560: 5b0a 2020 2020 2020 2020 5b22 6361 7463  [.        ["catc
+00000570: 686d 656e 7431 222c 2022 7269 7665 7231  hment1", "river1
+00000580: 225d 2c0a 2020 2020 2020 2020 5b22 7269  "],.        ["ri
+00000590: 7665 7231 222c 2022 6162 7331 225d 2c0a  ver1", "abs1"],.
+000005a0: 2020 2020 2020 2020 5b22 6162 7331 222c          ["abs1",
+000005b0: 2022 6465 6d61 6e64 3122 5d2c 0a20 2020   "demand1"],.   
 000005c0: 2020 2020 205b 2272 6976 6572 3122 2c20       ["river1", 
-000005d0: 2261 6273 3122 5d2c 0d0a 2020 2020 2020  "abs1"],..      
-000005e0: 2020 5b22 6162 7331 222c 2022 6465 6d61    ["abs1", "dema
-000005f0: 6e64 3122 5d2c 0d0a 2020 2020 2020 2020  nd1"],..        
-00000600: 5b22 7269 7665 7231 222c 2022 7465 726d  ["river1", "term
-00000610: 3122 5d0d 0a20 2020 205d 2c0d 0a20 2020  1"]..    ],..   
-00000620: 2022 7265 636f 7264 6572 7322 3a20 7b0d   "recorders": {.
-00000630: 0a20 2020 2020 2020 2022 6664 635f 6465  .        "fdc_de
-00000640: 7631 223a 207b 0d0a 2020 2020 2020 2020  v1": {..        
-00000650: 2020 2020 2274 7970 6522 3a20 2266 6c6f      "type": "flo
-00000660: 7764 7572 6174 696f 6e63 7572 7665 6465  wdurationcurvede
-00000670: 7669 6174 696f 6e22 2c0d 0a20 2020 2020  viation",..     
-00000680: 2020 2020 2020 2022 6e6f 6465 223a 2022         "node": "
-00000690: 7465 726d 3122 2c0d 0a20 2020 2020 2020  term1",..       
-000006a0: 2020 2020 2022 7065 7263 656e 7469 6c65       "percentile
-000006b0: 7322 3a20 5b32 302c 2034 302c 2036 302c  s": [20, 40, 60,
-000006c0: 2038 302c 2031 3030 5d2c 0d0a 2020 2020   80, 100],..    
-000006d0: 2020 2020 2020 2020 226c 6f77 6572 5f74          "lower_t
-000006e0: 6172 6765 745f 6664 6322 3a20 5b0d 0a20  arget_fdc": [.. 
-000006f0: 2020 2020 2020 2020 2020 2020 2020 205b                 [
-00000700: 3136 2e33 3336 2c20 3136 2e33 3932 2c20  16.336, 16.392, 
-00000710: 3138 2e30 3936 2c20 3137 2e36 3332 2c20  18.096, 17.632, 
-00000720: 3136 2e34 3438 2c20 3136 2e32 3438 2c20  16.448, 16.248, 
-00000730: 3136 2e30 3038 2c20 3137 2e30 3420 2c20  16.008, 17.04 , 
-00000740: 3139 2e37 3638 2c20 3139 2e34 3038 5d2c  19.768, 19.408],
-00000750: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000760: 2020 5b31 372e 3432 342c 2031 372e 3438    [17.424, 17.48
-00000770: 382c 2031 392e 3239 362c 2031 382e 3820  8, 19.296, 18.8 
-00000780: 202c 2031 372e 3534 342c 2031 372e 3332   , 17.544, 17.32
-00000790: 382c 2031 372e 3037 322c 2031 382e 3137  8, 17.072, 18.17
-000007a0: 362c 2032 312e 3038 202c 2032 302e 3639  6, 21.08 , 20.69
-000007b0: 365d 2c0d 0a20 2020 2020 2020 2020 2020  6],..           
-000007c0: 2020 2020 205b 3138 2e35 3736 2c20 3138       [18.576, 18
-000007d0: 2e36 3438 2c20 3230 2e35 3736 2c20 3230  .648, 20.576, 20
-000007e0: 2e30 3438 2c20 3138 2e37 3132 2c20 3138  .048, 18.712, 18
-000007f0: 2e34 3820 2c20 3138 2e32 3038 2c20 3139  .48 , 18.208, 19
-00000800: 2e33 3834 2c20 3232 2e34 3820 2c20 3232  .384, 22.48 , 22
-00000810: 2e30 3732 5d2c 0d0a 2020 2020 2020 2020  .072],..        
-00000820: 2020 2020 2020 2020 5b32 312e 3137 362c          [21.176,
-00000830: 2032 312e 3235 362c 2032 332e 3435 362c   21.256, 23.456,
-00000840: 2032 322e 3835 362c 2032 312e 3332 382c   22.856, 21.328,
-00000850: 2032 312e 3036 342c 2032 302e 3735 322c   21.064, 20.752,
-00000860: 2032 322e 3039 362c 2032 352e 3632 342c   22.096, 25.624,
-00000870: 2032 352e 3136 205d 2c0d 0a20 2020 2020   25.16 ],..     
-00000880: 2020 2020 2020 2020 2020 205b 3233 2e34             [23.4
-00000890: 3438 2c20 3233 2e35 3336 2c20 3235 2e39  48, 23.536, 25.9
-000008a0: 3736 2c20 3235 2e33 3034 2c20 3233 2e36  76, 25.304, 23.6
-000008b0: 3136 2c20 3233 2e33 3220 2c20 3232 2e39  16, 23.32 , 22.9
-000008c0: 3834 2c20 3234 2e34 3634 2c20 3238 2e33  84, 24.464, 28.3
-000008d0: 3736 2c20 3237 2e38 3536 5d0d 0a20 2020  76, 27.856]..   
-000008e0: 2020 2020 2020 2020 205d 2c0d 0a20 2020           ],..   
-000008f0: 2020 2020 2020 2020 2022 7570 7065 725f           "upper_
-00000900: 7461 7267 6574 5f66 6463 223a 205b 0d0a  target_fdc": [..
-00000910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000920: 5b32 322e 3436 322c 2032 322e 3533 392c  [22.462, 22.539,
-00000930: 2032 342e 3838 322c 2032 342e 3234 342c   24.882, 24.244,
-00000940: 2032 322e 3631 362c 2032 322e 3334 312c   22.616, 22.341,
-00000950: 2032 322e 3031 312c 2032 332e 3433 202c   22.011, 23.43 ,
-00000960: 2032 372e 3138 312c 2032 362e 3638 365d   27.181, 26.686]
-00000970: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-00000980: 2020 205b 3233 2e39 3538 2c20 3234 2e30     [23.958, 24.0
-00000990: 3436 2c20 3236 2e35 3332 2c20 3235 2e38  46, 26.532, 25.8
-000009a0: 3520 2c20 3234 2e31 3233 2c20 3233 2e38  5 , 24.123, 23.8
-000009b0: 3236 2c20 3233 2e34 3734 2c20 3234 2e39  26, 23.474, 24.9
-000009c0: 3932 2c20 3238 2e39 3835 2c20 3238 2e34  92, 28.985, 28.4
-000009d0: 3537 5d2c 0d0a 2020 2020 2020 2020 2020  57],..          
-000009e0: 2020 2020 2020 5b32 352e 3534 322c 2032        [25.542, 2
-000009f0: 352e 3634 312c 2032 382e 3239 322c 2032  5.641, 28.292, 2
-00000a00: 372e 3536 362c 2032 352e 3732 392c 2032  7.566, 25.729, 2
-00000a10: 352e 3431 202c 2032 352e 3033 362c 2032  5.41 , 25.036, 2
-00000a20: 362e 3635 332c 2033 302e 3931 202c 2033  6.653, 30.91 , 3
-00000a30: 302e 3334 395d 2c0d 0a20 2020 2020 2020  0.349],..       
-00000a40: 2020 2020 2020 2020 205b 3239 2e31 3137           [29.117
-00000a50: 2c20 3239 2e32 3237 2c20 3332 2e32 3532  , 29.227, 32.252
-00000a60: 2c20 3331 2e34 3237 2c20 3239 2e33 3236  , 31.427, 29.326
-00000a70: 2c20 3238 2e39 3633 2c20 3238 2e35 3334  , 28.963, 28.534
-00000a80: 2c20 3330 2e33 3832 2c20 3335 2e32 3333  , 30.382, 35.233
-00000a90: 2c20 3334 2e35 3935 5d2c 0d0a 2020 2020  , 34.595],..    
-00000aa0: 2020 2020 2020 2020 2020 2020 5b33 322e              [32.
-00000ab0: 3234 312c 2033 322e 3336 322c 2033 352e  241, 32.362, 35.
-00000ac0: 3731 372c 2033 342e 3739 332c 2033 322e  717, 34.793, 32.
-00000ad0: 3437 322c 2033 322e 3036 352c 2033 312e  472, 32.065, 31.
-00000ae0: 3630 332c 2033 332e 3633 382c 2033 392e  603, 33.638, 39.
-00000af0: 3031 372c 2033 382e 3330 325d 0d0a 2020  017, 38.302]..  
-00000b00: 2020 2020 2020 2020 2020 5d0d 0a20 2020            ]..   
-00000b10: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-00000b20: 2022 6664 635f 6465 7632 223a 207b 0d0a   "fdc_dev2": {..
-00000b30: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-00000b40: 6522 3a20 2266 6c6f 7764 7572 6174 696f  e": "flowduratio
-00000b50: 6e63 7572 7665 6465 7669 6174 696f 6e22  ncurvedeviation"
-00000b60: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000b70: 6e6f 6465 223a 2022 7465 726d 3122 2c0d  node": "term1",.
-00000b80: 0a20 2020 2020 2020 2020 2020 2022 7065  .            "pe
-00000b90: 7263 656e 7469 6c65 7322 3a20 5b32 302c  rcentiles": [20,
-00000ba0: 2034 302c 2036 302c 2038 302c 2031 3030   40, 60, 80, 100
-00000bb0: 5d2c 0d0a 2020 2020 2020 2020 2020 2020  ],..            
-00000bc0: 226c 6f77 6572 5f74 6172 6765 745f 6664  "lower_target_fd
-00000bd0: 6322 3a20 5b31 372e 3333 3736 2c20 3138  c": [17.3376, 18
-00000be0: 2e34 3930 342c 2031 392e 3731 3834 2c20  .4904, 19.7184, 
-00000bf0: 3232 2e34 3736 382c 2032 342e 3838 385d  22.4768, 24.888]
-00000c00: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000c10: 7570 7065 725f 7461 7267 6574 5f66 6463  upper_target_fdc
-00000c20: 223a 205b 3233 2e38 3339 322c 2032 352e  ": [23.8392, 25.
-00000c30: 3432 3433 2c20 3237 2e31 3132 382c 2033  4243, 27.1128, 3
-00000c40: 302e 3930 3536 2c20 3334 2e32 3231 5d0d  0.9056, 34.221].
-00000c50: 0a20 2020 2020 2020 207d 0d0a 2020 2020  .        }..    
-00000c60: 7d0d 0a7d 0d0a                           }..}..
+000005d0: 2274 6572 6d31 225d 0a20 2020 205d 2c0a  "term1"].    ],.
+000005e0: 2020 2020 2272 6563 6f72 6465 7273 223a      "recorders":
+000005f0: 207b 0a20 2020 2020 2020 2022 6664 635f   {.        "fdc_
+00000600: 6465 7631 223a 207b 0a20 2020 2020 2020  dev1": {.       
+00000610: 2020 2020 2022 7479 7065 223a 2022 666c       "type": "fl
+00000620: 6f77 6475 7261 7469 6f6e 6375 7276 6564  owdurationcurved
+00000630: 6576 6961 7469 6f6e 222c 0a20 2020 2020  eviation",.     
+00000640: 2020 2020 2020 2022 6e6f 6465 223a 2022         "node": "
+00000650: 7465 726d 3122 2c0a 2020 2020 2020 2020  term1",.        
+00000660: 2020 2020 2270 6572 6365 6e74 696c 6573      "percentiles
+00000670: 223a 205b 3230 2c20 3430 2c20 3630 2c20  ": [20, 40, 60, 
+00000680: 3830 2c20 3130 305d 2c0a 2020 2020 2020  80, 100],.      
+00000690: 2020 2020 2020 226c 6f77 6572 5f74 6172        "lower_tar
+000006a0: 6765 745f 6664 6322 3a20 5b0a 2020 2020  get_fdc": [.    
+000006b0: 2020 2020 2020 2020 2020 2020 5b31 362e              [16.
+000006c0: 3333 362c 2031 362e 3339 322c 2031 382e  336, 16.392, 18.
+000006d0: 3039 362c 2031 372e 3633 322c 2031 362e  096, 17.632, 16.
+000006e0: 3434 382c 2031 362e 3234 382c 2031 362e  448, 16.248, 16.
+000006f0: 3030 382c 2031 372e 3034 202c 2031 392e  008, 17.04 , 19.
+00000700: 3736 382c 2031 392e 3430 385d 2c0a 2020  768, 19.408],.  
+00000710: 2020 2020 2020 2020 2020 2020 2020 5b31                [1
+00000720: 372e 3432 342c 2031 372e 3438 382c 2031  7.424, 17.488, 1
+00000730: 392e 3239 362c 2031 382e 3820 202c 2031  9.296, 18.8  , 1
+00000740: 372e 3534 342c 2031 372e 3332 382c 2031  7.544, 17.328, 1
+00000750: 372e 3037 322c 2031 382e 3137 362c 2032  7.072, 18.176, 2
+00000760: 312e 3038 202c 2032 302e 3639 365d 2c0a  1.08 , 20.696],.
+00000770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000780: 5b31 382e 3537 362c 2031 382e 3634 382c  [18.576, 18.648,
+00000790: 2032 302e 3537 362c 2032 302e 3034 382c   20.576, 20.048,
+000007a0: 2031 382e 3731 322c 2031 382e 3438 202c   18.712, 18.48 ,
+000007b0: 2031 382e 3230 382c 2031 392e 3338 342c   18.208, 19.384,
+000007c0: 2032 322e 3438 202c 2032 322e 3037 325d   22.48 , 22.072]
+000007d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000007e0: 2020 5b32 312e 3137 362c 2032 312e 3235    [21.176, 21.25
+000007f0: 362c 2032 332e 3435 362c 2032 322e 3835  6, 23.456, 22.85
+00000800: 362c 2032 312e 3332 382c 2032 312e 3036  6, 21.328, 21.06
+00000810: 342c 2032 302e 3735 322c 2032 322e 3039  4, 20.752, 22.09
+00000820: 362c 2032 352e 3632 342c 2032 352e 3136  6, 25.624, 25.16
+00000830: 205d 2c0a 2020 2020 2020 2020 2020 2020   ],.            
+00000840: 2020 2020 5b32 332e 3434 382c 2032 332e      [23.448, 23.
+00000850: 3533 362c 2032 352e 3937 362c 2032 352e  536, 25.976, 25.
+00000860: 3330 342c 2032 332e 3631 362c 2032 332e  304, 23.616, 23.
+00000870: 3332 202c 2032 322e 3938 342c 2032 342e  32 , 22.984, 24.
+00000880: 3436 342c 2032 382e 3337 362c 2032 372e  464, 28.376, 27.
+00000890: 3835 365d 0a20 2020 2020 2020 2020 2020  856].           
+000008a0: 205d 2c0a 2020 2020 2020 2020 2020 2020   ],.            
+000008b0: 2275 7070 6572 5f74 6172 6765 745f 6664  "upper_target_fd
+000008c0: 6322 3a20 5b0a 2020 2020 2020 2020 2020  c": [.          
+000008d0: 2020 2020 2020 5b32 322e 3436 322c 2032        [22.462, 2
+000008e0: 322e 3533 392c 2032 342e 3838 322c 2032  2.539, 24.882, 2
+000008f0: 342e 3234 342c 2032 322e 3631 362c 2032  4.244, 22.616, 2
+00000900: 322e 3334 312c 2032 322e 3031 312c 2032  2.341, 22.011, 2
+00000910: 332e 3433 202c 2032 372e 3138 312c 2032  3.43 , 27.181, 2
+00000920: 362e 3638 365d 2c0a 2020 2020 2020 2020  6.686],.        
+00000930: 2020 2020 2020 2020 5b32 332e 3935 382c          [23.958,
+00000940: 2032 342e 3034 362c 2032 362e 3533 322c   24.046, 26.532,
+00000950: 2032 352e 3835 202c 2032 342e 3132 332c   25.85 , 24.123,
+00000960: 2032 332e 3832 362c 2032 332e 3437 342c   23.826, 23.474,
+00000970: 2032 342e 3939 322c 2032 382e 3938 352c   24.992, 28.985,
+00000980: 2032 382e 3435 375d 2c0a 2020 2020 2020   28.457],.      
+00000990: 2020 2020 2020 2020 2020 5b32 352e 3534            [25.54
+000009a0: 322c 2032 352e 3634 312c 2032 382e 3239  2, 25.641, 28.29
+000009b0: 322c 2032 372e 3536 362c 2032 352e 3732  2, 27.566, 25.72
+000009c0: 392c 2032 352e 3431 202c 2032 352e 3033  9, 25.41 , 25.03
+000009d0: 362c 2032 362e 3635 332c 2033 302e 3931  6, 26.653, 30.91
+000009e0: 202c 2033 302e 3334 395d 2c0a 2020 2020   , 30.349],.    
+000009f0: 2020 2020 2020 2020 2020 2020 5b32 392e              [29.
+00000a00: 3131 372c 2032 392e 3232 372c 2033 322e  117, 29.227, 32.
+00000a10: 3235 322c 2033 312e 3432 372c 2032 392e  252, 31.427, 29.
+00000a20: 3332 362c 2032 382e 3936 332c 2032 382e  326, 28.963, 28.
+00000a30: 3533 342c 2033 302e 3338 322c 2033 352e  534, 30.382, 35.
+00000a40: 3233 332c 2033 342e 3539 355d 2c0a 2020  233, 34.595],.  
+00000a50: 2020 2020 2020 2020 2020 2020 2020 5b33                [3
+00000a60: 322e 3234 312c 2033 322e 3336 322c 2033  2.241, 32.362, 3
+00000a70: 352e 3731 372c 2033 342e 3739 332c 2033  5.717, 34.793, 3
+00000a80: 322e 3437 322c 2033 322e 3036 352c 2033  2.472, 32.065, 3
+00000a90: 312e 3630 332c 2033 332e 3633 382c 2033  1.603, 33.638, 3
+00000aa0: 392e 3031 372c 2033 382e 3330 325d 0a20  9.017, 38.302]. 
+00000ab0: 2020 2020 2020 2020 2020 205d 0a20 2020             ].   
+00000ac0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+00000ad0: 2266 6463 5f64 6576 3222 3a20 7b0a 2020  "fdc_dev2": {.  
+00000ae0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+00000af0: 3a20 2266 6c6f 7764 7572 6174 696f 6e63  : "flowdurationc
+00000b00: 7572 7665 6465 7669 6174 696f 6e22 2c0a  urvedeviation",.
+00000b10: 2020 2020 2020 2020 2020 2020 226e 6f64              "nod
+00000b20: 6522 3a20 2274 6572 6d31 222c 0a20 2020  e": "term1",.   
+00000b30: 2020 2020 2020 2020 2022 7065 7263 656e           "percen
+00000b40: 7469 6c65 7322 3a20 5b32 302c 2034 302c  tiles": [20, 40,
+00000b50: 2036 302c 2038 302c 2031 3030 5d2c 0a20   60, 80, 100],. 
+00000b60: 2020 2020 2020 2020 2020 2022 6c6f 7765             "lowe
+00000b70: 725f 7461 7267 6574 5f66 6463 223a 205b  r_target_fdc": [
+00000b80: 3137 2e33 3337 362c 2031 382e 3439 3034  17.3376, 18.4904
+00000b90: 2c20 3139 2e37 3138 342c 2032 322e 3437  , 19.7184, 22.47
+00000ba0: 3638 2c20 3234 2e38 3838 5d2c 0a20 2020  68, 24.888],.   
+00000bb0: 2020 2020 2020 2020 2022 7570 7065 725f           "upper_
+00000bc0: 7461 7267 6574 5f66 6463 223a 205b 3233  target_fdc": [23
+00000bd0: 2e38 3339 322c 2032 352e 3432 3433 2c20  .8392, 25.4243, 
+00000be0: 3237 2e31 3132 382c 2033 302e 3930 3536  27.1128, 30.9056
+00000bf0: 2c20 3334 2e32 3231 5d0a 2020 2020 2020  , 34.221].      
+00000c00: 2020 7d0a 2020 2020 7d0a 7d0a              }.    }.}.
```

### Comparing `pywr-1.8.0/tests/models/timeseries3.csv` & `pywr-1.9.0/tests/models/timeseries3.csv`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,366 +1,366 @@
-date,flows
-01/01/2014,44.81093713
-02/01/2014,40.33752949
-03/01/2014,60.59716333
-04/01/2014,57.61234908
-05/01/2014,57.74215377
-06/01/2014,45.50064063
-07/01/2014,40.19159242
-08/01/2014,55.04835942
-09/01/2014,47.79556239
-10/01/2014,41.67490036
-11/01/2014,59.71563266
-12/01/2014,42.48101049
-13/01/2014,46.79406195
-14/01/2014,52.95070011
-15/01/2014,41.70396414
-16/01/2014,39.38645859
-17/01/2014,43.89897191
-18/01/2014,31.41294658
-19/01/2014,40.27021222
-20/01/2014,36.80813029
-21/01/2014,39.18570985
-22/01/2014,47.66167226
-23/01/2014,44.66833131
-24/01/2014,49.56909335
-25/01/2014,41.15492288
-26/01/2014,43.50084322
-27/01/2014,42.95667432
-28/01/2014,52.93634498
-29/01/2014,46.68721953
-30/01/2014,44.22013879
-31/01/2014,49.2153261
-01/02/2014,47.90746237
-02/02/2014,41.98614797
-03/02/2014,35.73605364
-04/02/2014,44.25636619
-05/02/2014,53.10383168
-06/02/2014,59.08787639
-07/02/2014,44.47816846
-08/02/2014,25.85087246
-09/02/2014,33.13401352
-10/02/2014,37.55917441
-11/02/2014,48.14157596
-12/02/2014,49.61577406
-13/02/2014,49.1578266
-14/02/2014,41.18969966
-15/02/2014,47.07289588
-16/02/2014,52.51228476
-17/02/2014,52.97387949
-18/02/2014,42.68222261
-19/02/2014,40.56409874
-20/02/2014,54.48996399
-21/02/2014,65.27900738
-22/02/2014,41.6144538
-23/02/2014,34.9292785
-24/02/2014,44.4747646
-25/02/2014,35.53298428
-26/02/2014,45.34752601
-27/02/2014,46.45692407
-28/02/2014,44.93305789
-01/03/2014,33.82153414
-02/03/2014,37.67702876
-03/03/2014,42.57072709
-04/03/2014,45.59764104
-05/03/2014,29.9157932
-06/03/2014,35.15704783
-07/03/2014,38.14903705
-08/03/2014,44.1680903
-09/03/2014,46.21456169
-10/03/2014,57.51160531
-11/03/2014,33.6232624
-12/03/2014,49.21846065
-13/03/2014,44.37578475
-14/03/2014,43.95173203
-15/03/2014,47.68002383
-16/03/2014,40.7515425
-17/03/2014,36.58074768
-18/03/2014,49.6689794
-19/03/2014,48.37733899
-20/03/2014,59.67397149
-21/03/2014,67.06980509
-22/03/2014,46.24531846
-23/03/2014,49.66943407
-24/03/2014,49.79329258
-25/03/2014,50.04422088
-26/03/2014,59.98285127
-27/03/2014,42.67188966
-28/03/2014,56.73331376
-29/03/2014,55.41787204
-30/03/2014,44.88264093
-31/03/2014,62.57161236
-01/04/2014,55.58739828
-02/04/2014,46.72738365
-03/04/2014,59.4136335
-04/04/2014,56.91726319
-05/04/2014,66.23570155
-06/04/2014,70.49168914
-07/04/2014,47.40666688
-08/04/2014,37.11458296
-09/04/2014,57.30937563
-10/04/2014,50.29852167
-11/04/2014,43.35753655
-12/04/2014,54.30559127
-13/04/2014,55.60416451
-14/04/2014,51.71350413
-15/04/2014,52.64640864
-16/04/2014,52.53744558
-17/04/2014,63.57199852
-18/04/2014,63.27579809
-19/04/2014,54.03441164
-20/04/2014,33.42048494
-21/04/2014,47.92204031
-22/04/2014,59.00521823
-23/04/2014,56.56812556
-24/04/2014,48.52523951
-25/04/2014,53.33873218
-26/04/2014,53.45502743
-27/04/2014,50.13236284
-28/04/2014,47.98376804
-29/04/2014,51.41716151
-30/04/2014,50.57063007
-01/05/2014,54.36548343
-02/05/2014,50.67920866
-03/05/2014,50.21894849
-04/05/2014,47.38842075
-05/05/2014,43.512239
-06/05/2014,45.24150145
-07/05/2014,55.14984763
-08/05/2014,53.35704938
-09/05/2014,36.27426472
-10/05/2014,37.46235184
-11/05/2014,36.04936159
-12/05/2014,49.92342718
-13/05/2014,44.26440514
-14/05/2014,45.33623348
-15/05/2014,48.71692617
-16/05/2014,54.88993077
-17/05/2014,39.69173434
-18/05/2014,45.6321967
-19/05/2014,59.87539742
-20/05/2014,53.68051936
-21/05/2014,44.77735938
-22/05/2014,48.54959268
-23/05/2014,32.2064994
-24/05/2014,40.25233104
-25/05/2014,40.59910526
-26/05/2014,37.77408822
-27/05/2014,40.92524091
-28/05/2014,40.32881753
-29/05/2014,37.77250912
-30/05/2014,43.16304151
-31/05/2014,41.38658717
-01/06/2014,36.73321234
-02/06/2014,38.01039106
-03/06/2014,39.02826901
-04/06/2014,39.61813682
-05/06/2014,36.54646925
-06/06/2014,33.49390113
-07/06/2014,32.38550138
-08/06/2014,37.16064286
-09/06/2014,45.82774659
-10/06/2014,40.75842735
-11/06/2014,39.33407038
-12/06/2014,39.38892152
-13/06/2014,37.34747749
-14/06/2014,47.58782148
-15/06/2014,48.97604706
-16/06/2014,45.90550506
-17/06/2014,47.00137685
-18/06/2014,37.75238698
-19/06/2014,38.40249371
-20/06/2014,33.98338917
-21/06/2014,40.72586519
-22/06/2014,31.90335367
-23/06/2014,36.74416346
-24/06/2014,28.53727482
-25/06/2014,42.81802245
-26/06/2014,37.69134305
-27/06/2014,26.02912802
-28/06/2014,34.64844296
-29/06/2014,43.90010795
-30/06/2014,37.73507662
-01/07/2014,41.07843469
-02/07/2014,38.05346476
-03/07/2014,34.46145289
-04/07/2014,33.98251086
-05/07/2014,39.55619422
-06/07/2014,34.5946326
-07/07/2014,31.52902462
-08/07/2014,35.86955791
-09/07/2014,38.94547821
-10/07/2014,35.22608416
-11/07/2014,35.97479408
-12/07/2014,30.86462638
-13/07/2014,36.92015217
-14/07/2014,32.90366944
-15/07/2014,30.38089545
-16/07/2014,37.63666291
-17/07/2014,44.36381601
-18/07/2014,39.61647911
-19/07/2014,29.58931725
-20/07/2014,31.13525566
-21/07/2014,35.00370597
-22/07/2014,36.32803923
-23/07/2014,42.11053458
-24/07/2014,38.02667714
-25/07/2014,43.09298872
-26/07/2014,37.46617622
-27/07/2014,33.83900304
-28/07/2014,40.03511002
-29/07/2014,39.55247689
-30/07/2014,37.19849326
-31/07/2014,40.45597614
-01/08/2014,36.91929951
-02/08/2014,39.90490172
-03/08/2014,43.40689707
-04/08/2014,37.70501418
-05/08/2014,41.36439132
-06/08/2014,36.62132555
-07/08/2014,42.2664032
-08/08/2014,37.07452538
-09/08/2014,37.03452779
-10/08/2014,30.64407435
-11/08/2014,36.91999851
-12/08/2014,38.65997036
-13/08/2014,38.86997304
-14/08/2014,34.9367546
-15/08/2014,32.23407073
-16/08/2014,34.9958687
-17/08/2014,33.56207291
-18/08/2014,28.25464766
-19/08/2014,31.12271717
-20/08/2014,30.17731487
-21/08/2014,29.87053145
-22/08/2014,23.58458101
-23/08/2014,25.74776031
-24/08/2014,26.69652792
-25/08/2014,19.74642885
-26/08/2014,16.25570825
-27/08/2014,21.13180849
-28/08/2014,22.92988114
-29/08/2014,21.24629064
-30/08/2014,21.45996929
-31/08/2014,21.03967944
-01/09/2014,21.38947029
-02/09/2014,23.22888328
-03/09/2014,21.97146131
-04/09/2014,22.74047745
-05/09/2014,23.91497727
-06/09/2014,21.11388008
-07/09/2014,25.09310154
-08/09/2014,26.77652775
-09/09/2014,23.5601372
-10/09/2014,24.91724662
-11/09/2014,19.48734981
-12/09/2014,19.85786534
-13/09/2014,23.34715742
-14/09/2014,24.11207173
-15/09/2014,23.34457863
-16/09/2014,23.72585844
-17/09/2014,18.10395584
-18/09/2014,22.07165166
-19/09/2014,29.53372497
-20/09/2014,29.24651229
-21/09/2014,24.30438889
-22/09/2014,21.89620451
-23/09/2014,30.12534913
-24/09/2014,25.0214914
-25/09/2014,20.18433812
-26/09/2014,20.22621669
-27/09/2014,22.30946946
-28/09/2014,24.89118292
-29/09/2014,23.26089446
-30/09/2014,23.83910846
-01/10/2014,25.48437838
-02/10/2014,26.27406941
-03/10/2014,28.23443101
-04/10/2014,27.91406123
-05/10/2014,23.4560862
-06/10/2014,26.74052666
-07/10/2014,29.60822828
-08/10/2014,33.70406315
-09/10/2014,35.51838834
-10/10/2014,35.09165473
-11/10/2014,29.59030699
-12/10/2014,32.88118575
-13/10/2014,31.29015891
-14/10/2014,35.06469811
-15/10/2014,28.70597258
-16/10/2014,34.76529688
-17/10/2014,36.61236381
-18/10/2014,40.52364668
-19/10/2014,35.8176273
-20/10/2014,36.43945789
-21/10/2014,30.76618996
-22/10/2014,35.3916902
-23/10/2014,44.3976019
-24/10/2014,39.04957484
-25/10/2014,34.70338177
-26/10/2014,38.31121525
-27/10/2014,31.99650045
-28/10/2014,25.38892451
-29/10/2014,30.40421473
-30/10/2014,45.92434575
-31/10/2014,42.54129601
-01/11/2014,43.79726568
-02/11/2014,42.34957519
-03/11/2014,42.93068633
-04/11/2014,33.02753444
-05/11/2014,36.22990946
-06/11/2014,30.87712067
-07/11/2014,29.39466667
-08/11/2014,36.26843539
-09/11/2014,37.86326233
-10/11/2014,34.0509161
-11/11/2014,30.38786541
-12/11/2014,38.24441461
-13/11/2014,30.39018354
-14/11/2014,22.65902604
-15/11/2014,30.71487179
-16/11/2014,28.57999319
-17/11/2014,31.39195258
-18/11/2014,30.88088327
-19/11/2014,40.62749636
-20/11/2014,38.58348927
-21/11/2014,46.51501704
-22/11/2014,49.90736568
-23/11/2014,34.08812981
-24/11/2014,36.07375359
-25/11/2014,31.77823102
-26/11/2014,32.63920835
-27/11/2014,40.001269
-28/11/2014,40.26666084
-29/11/2014,36.47127932
-30/11/2014,39.8204594
-01/12/2014,37.04779681
-02/12/2014,35.45726563
-03/12/2014,37.91692434
-04/12/2014,43.65432965
-05/12/2014,41.14269752
-06/12/2014,39.71090542
-07/12/2014,44.19176337
-08/12/2014,41.837378
-09/12/2014,40.73630312
-10/12/2014,46.10335228
-11/12/2014,48.27936313
-12/12/2014,35.55184654
-13/12/2014,42.9446145
-14/12/2014,39.92556058
-15/12/2014,43.05361347
-16/12/2014,39.38960683
-17/12/2014,38.74020977
-18/12/2014,46.15384295
-19/12/2014,39.23390622
-20/12/2014,37.16319877
-21/12/2014,29.92239952
-22/12/2014,35.49340738
-23/12/2014,22.12997903
-24/12/2014,44.52507932
-25/12/2014,37.20338332
-26/12/2014,31.74353987
-27/12/2014,37.31422804
-28/12/2014,38.44826962
-29/12/2014,47.86425358
-30/12/2014,48.16468279
-31/12/2014,54.16141351
+date,flows
+01/01/2014,44.81093713
+02/01/2014,40.33752949
+03/01/2014,60.59716333
+04/01/2014,57.61234908
+05/01/2014,57.74215377
+06/01/2014,45.50064063
+07/01/2014,40.19159242
+08/01/2014,55.04835942
+09/01/2014,47.79556239
+10/01/2014,41.67490036
+11/01/2014,59.71563266
+12/01/2014,42.48101049
+13/01/2014,46.79406195
+14/01/2014,52.95070011
+15/01/2014,41.70396414
+16/01/2014,39.38645859
+17/01/2014,43.89897191
+18/01/2014,31.41294658
+19/01/2014,40.27021222
+20/01/2014,36.80813029
+21/01/2014,39.18570985
+22/01/2014,47.66167226
+23/01/2014,44.66833131
+24/01/2014,49.56909335
+25/01/2014,41.15492288
+26/01/2014,43.50084322
+27/01/2014,42.95667432
+28/01/2014,52.93634498
+29/01/2014,46.68721953
+30/01/2014,44.22013879
+31/01/2014,49.2153261
+01/02/2014,47.90746237
+02/02/2014,41.98614797
+03/02/2014,35.73605364
+04/02/2014,44.25636619
+05/02/2014,53.10383168
+06/02/2014,59.08787639
+07/02/2014,44.47816846
+08/02/2014,25.85087246
+09/02/2014,33.13401352
+10/02/2014,37.55917441
+11/02/2014,48.14157596
+12/02/2014,49.61577406
+13/02/2014,49.1578266
+14/02/2014,41.18969966
+15/02/2014,47.07289588
+16/02/2014,52.51228476
+17/02/2014,52.97387949
+18/02/2014,42.68222261
+19/02/2014,40.56409874
+20/02/2014,54.48996399
+21/02/2014,65.27900738
+22/02/2014,41.6144538
+23/02/2014,34.9292785
+24/02/2014,44.4747646
+25/02/2014,35.53298428
+26/02/2014,45.34752601
+27/02/2014,46.45692407
+28/02/2014,44.93305789
+01/03/2014,33.82153414
+02/03/2014,37.67702876
+03/03/2014,42.57072709
+04/03/2014,45.59764104
+05/03/2014,29.9157932
+06/03/2014,35.15704783
+07/03/2014,38.14903705
+08/03/2014,44.1680903
+09/03/2014,46.21456169
+10/03/2014,57.51160531
+11/03/2014,33.6232624
+12/03/2014,49.21846065
+13/03/2014,44.37578475
+14/03/2014,43.95173203
+15/03/2014,47.68002383
+16/03/2014,40.7515425
+17/03/2014,36.58074768
+18/03/2014,49.6689794
+19/03/2014,48.37733899
+20/03/2014,59.67397149
+21/03/2014,67.06980509
+22/03/2014,46.24531846
+23/03/2014,49.66943407
+24/03/2014,49.79329258
+25/03/2014,50.04422088
+26/03/2014,59.98285127
+27/03/2014,42.67188966
+28/03/2014,56.73331376
+29/03/2014,55.41787204
+30/03/2014,44.88264093
+31/03/2014,62.57161236
+01/04/2014,55.58739828
+02/04/2014,46.72738365
+03/04/2014,59.4136335
+04/04/2014,56.91726319
+05/04/2014,66.23570155
+06/04/2014,70.49168914
+07/04/2014,47.40666688
+08/04/2014,37.11458296
+09/04/2014,57.30937563
+10/04/2014,50.29852167
+11/04/2014,43.35753655
+12/04/2014,54.30559127
+13/04/2014,55.60416451
+14/04/2014,51.71350413
+15/04/2014,52.64640864
+16/04/2014,52.53744558
+17/04/2014,63.57199852
+18/04/2014,63.27579809
+19/04/2014,54.03441164
+20/04/2014,33.42048494
+21/04/2014,47.92204031
+22/04/2014,59.00521823
+23/04/2014,56.56812556
+24/04/2014,48.52523951
+25/04/2014,53.33873218
+26/04/2014,53.45502743
+27/04/2014,50.13236284
+28/04/2014,47.98376804
+29/04/2014,51.41716151
+30/04/2014,50.57063007
+01/05/2014,54.36548343
+02/05/2014,50.67920866
+03/05/2014,50.21894849
+04/05/2014,47.38842075
+05/05/2014,43.512239
+06/05/2014,45.24150145
+07/05/2014,55.14984763
+08/05/2014,53.35704938
+09/05/2014,36.27426472
+10/05/2014,37.46235184
+11/05/2014,36.04936159
+12/05/2014,49.92342718
+13/05/2014,44.26440514
+14/05/2014,45.33623348
+15/05/2014,48.71692617
+16/05/2014,54.88993077
+17/05/2014,39.69173434
+18/05/2014,45.6321967
+19/05/2014,59.87539742
+20/05/2014,53.68051936
+21/05/2014,44.77735938
+22/05/2014,48.54959268
+23/05/2014,32.2064994
+24/05/2014,40.25233104
+25/05/2014,40.59910526
+26/05/2014,37.77408822
+27/05/2014,40.92524091
+28/05/2014,40.32881753
+29/05/2014,37.77250912
+30/05/2014,43.16304151
+31/05/2014,41.38658717
+01/06/2014,36.73321234
+02/06/2014,38.01039106
+03/06/2014,39.02826901
+04/06/2014,39.61813682
+05/06/2014,36.54646925
+06/06/2014,33.49390113
+07/06/2014,32.38550138
+08/06/2014,37.16064286
+09/06/2014,45.82774659
+10/06/2014,40.75842735
+11/06/2014,39.33407038
+12/06/2014,39.38892152
+13/06/2014,37.34747749
+14/06/2014,47.58782148
+15/06/2014,48.97604706
+16/06/2014,45.90550506
+17/06/2014,47.00137685
+18/06/2014,37.75238698
+19/06/2014,38.40249371
+20/06/2014,33.98338917
+21/06/2014,40.72586519
+22/06/2014,31.90335367
+23/06/2014,36.74416346
+24/06/2014,28.53727482
+25/06/2014,42.81802245
+26/06/2014,37.69134305
+27/06/2014,26.02912802
+28/06/2014,34.64844296
+29/06/2014,43.90010795
+30/06/2014,37.73507662
+01/07/2014,41.07843469
+02/07/2014,38.05346476
+03/07/2014,34.46145289
+04/07/2014,33.98251086
+05/07/2014,39.55619422
+06/07/2014,34.5946326
+07/07/2014,31.52902462
+08/07/2014,35.86955791
+09/07/2014,38.94547821
+10/07/2014,35.22608416
+11/07/2014,35.97479408
+12/07/2014,30.86462638
+13/07/2014,36.92015217
+14/07/2014,32.90366944
+15/07/2014,30.38089545
+16/07/2014,37.63666291
+17/07/2014,44.36381601
+18/07/2014,39.61647911
+19/07/2014,29.58931725
+20/07/2014,31.13525566
+21/07/2014,35.00370597
+22/07/2014,36.32803923
+23/07/2014,42.11053458
+24/07/2014,38.02667714
+25/07/2014,43.09298872
+26/07/2014,37.46617622
+27/07/2014,33.83900304
+28/07/2014,40.03511002
+29/07/2014,39.55247689
+30/07/2014,37.19849326
+31/07/2014,40.45597614
+01/08/2014,36.91929951
+02/08/2014,39.90490172
+03/08/2014,43.40689707
+04/08/2014,37.70501418
+05/08/2014,41.36439132
+06/08/2014,36.62132555
+07/08/2014,42.2664032
+08/08/2014,37.07452538
+09/08/2014,37.03452779
+10/08/2014,30.64407435
+11/08/2014,36.91999851
+12/08/2014,38.65997036
+13/08/2014,38.86997304
+14/08/2014,34.9367546
+15/08/2014,32.23407073
+16/08/2014,34.9958687
+17/08/2014,33.56207291
+18/08/2014,28.25464766
+19/08/2014,31.12271717
+20/08/2014,30.17731487
+21/08/2014,29.87053145
+22/08/2014,23.58458101
+23/08/2014,25.74776031
+24/08/2014,26.69652792
+25/08/2014,19.74642885
+26/08/2014,16.25570825
+27/08/2014,21.13180849
+28/08/2014,22.92988114
+29/08/2014,21.24629064
+30/08/2014,21.45996929
+31/08/2014,21.03967944
+01/09/2014,21.38947029
+02/09/2014,23.22888328
+03/09/2014,21.97146131
+04/09/2014,22.74047745
+05/09/2014,23.91497727
+06/09/2014,21.11388008
+07/09/2014,25.09310154
+08/09/2014,26.77652775
+09/09/2014,23.5601372
+10/09/2014,24.91724662
+11/09/2014,19.48734981
+12/09/2014,19.85786534
+13/09/2014,23.34715742
+14/09/2014,24.11207173
+15/09/2014,23.34457863
+16/09/2014,23.72585844
+17/09/2014,18.10395584
+18/09/2014,22.07165166
+19/09/2014,29.53372497
+20/09/2014,29.24651229
+21/09/2014,24.30438889
+22/09/2014,21.89620451
+23/09/2014,30.12534913
+24/09/2014,25.0214914
+25/09/2014,20.18433812
+26/09/2014,20.22621669
+27/09/2014,22.30946946
+28/09/2014,24.89118292
+29/09/2014,23.26089446
+30/09/2014,23.83910846
+01/10/2014,25.48437838
+02/10/2014,26.27406941
+03/10/2014,28.23443101
+04/10/2014,27.91406123
+05/10/2014,23.4560862
+06/10/2014,26.74052666
+07/10/2014,29.60822828
+08/10/2014,33.70406315
+09/10/2014,35.51838834
+10/10/2014,35.09165473
+11/10/2014,29.59030699
+12/10/2014,32.88118575
+13/10/2014,31.29015891
+14/10/2014,35.06469811
+15/10/2014,28.70597258
+16/10/2014,34.76529688
+17/10/2014,36.61236381
+18/10/2014,40.52364668
+19/10/2014,35.8176273
+20/10/2014,36.43945789
+21/10/2014,30.76618996
+22/10/2014,35.3916902
+23/10/2014,44.3976019
+24/10/2014,39.04957484
+25/10/2014,34.70338177
+26/10/2014,38.31121525
+27/10/2014,31.99650045
+28/10/2014,25.38892451
+29/10/2014,30.40421473
+30/10/2014,45.92434575
+31/10/2014,42.54129601
+01/11/2014,43.79726568
+02/11/2014,42.34957519
+03/11/2014,42.93068633
+04/11/2014,33.02753444
+05/11/2014,36.22990946
+06/11/2014,30.87712067
+07/11/2014,29.39466667
+08/11/2014,36.26843539
+09/11/2014,37.86326233
+10/11/2014,34.0509161
+11/11/2014,30.38786541
+12/11/2014,38.24441461
+13/11/2014,30.39018354
+14/11/2014,22.65902604
+15/11/2014,30.71487179
+16/11/2014,28.57999319
+17/11/2014,31.39195258
+18/11/2014,30.88088327
+19/11/2014,40.62749636
+20/11/2014,38.58348927
+21/11/2014,46.51501704
+22/11/2014,49.90736568
+23/11/2014,34.08812981
+24/11/2014,36.07375359
+25/11/2014,31.77823102
+26/11/2014,32.63920835
+27/11/2014,40.001269
+28/11/2014,40.26666084
+29/11/2014,36.47127932
+30/11/2014,39.8204594
+01/12/2014,37.04779681
+02/12/2014,35.45726563
+03/12/2014,37.91692434
+04/12/2014,43.65432965
+05/12/2014,41.14269752
+06/12/2014,39.71090542
+07/12/2014,44.19176337
+08/12/2014,41.837378
+09/12/2014,40.73630312
+10/12/2014,46.10335228
+11/12/2014,48.27936313
+12/12/2014,35.55184654
+13/12/2014,42.9446145
+14/12/2014,39.92556058
+15/12/2014,43.05361347
+16/12/2014,39.38960683
+17/12/2014,38.74020977
+18/12/2014,46.15384295
+19/12/2014,39.23390622
+20/12/2014,37.16319877
+21/12/2014,29.92239952
+22/12/2014,35.49340738
+23/12/2014,22.12997903
+24/12/2014,44.52507932
+25/12/2014,37.20338332
+26/12/2014,31.74353987
+27/12/2014,37.31422804
+28/12/2014,38.44826962
+29/12/2014,47.86425358
+30/12/2014,48.16468279
+31/12/2014,54.16141351
```

### Comparing `pywr-1.8.0/tests/models/timeseries4.json` & `pywr-1.9.0/tests/models/timeseries2_hdf_wrong_hash.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.5794753086419753%*

 * *Differences: {"'edges'": "{0: {insert: [(1, 'river1')], delete: [1]}, 1: {insert: [(0, 'river1')], delete: "*

 * *            "[0]}, 3: {insert: [(0, 'river1')], delete: [0]}}",*

 * * "'nodes'": "{0: {'flow': {'type': 'tablesarray', 'url': 'timeseries2.h5', 'scenario': 'scenario "*

 * *            "A', 'checksum': OrderedDict([('md5', 'this hash is incorrect!')]), 'where': "*

 * *            "'/timeseries2', 'node': 'block0_values', delete: ['parse_dates', 'dayfirst', "*

 * *            "'index_col']}}, 1: {'name': 'river1', 'type': 'Link', del […]*

```diff
@@ -1,83 +1,72 @@
 {
     "edges": [
         [
             "catchment1",
-            "reservoir1"
+            "river1"
         ],
         [
-            "reservoir1",
+            "river1",
             "abs1"
         ],
         [
             "abs1",
             "demand1"
         ],
         [
-            "reservoir1",
+            "river1",
             "term1"
         ]
     ],
     "metadata": {
         "description": "A model with a timeseries",
         "minimum_version": "0.1",
         "title": "Timeseries example"
     },
     "nodes": [
         {
             "flow": {
-                "dayfirst": true,
-                "index_col": 0,
-                "parse_dates": true,
-                "type": "dataframe",
-                "url": "timeseries3.csv"
+                "checksum": {
+                    "md5": "this hash is incorrect!"
+                },
+                "node": "block0_values",
+                "scenario": "scenario A",
+                "type": "tablesarray",
+                "url": "timeseries2.h5",
+                "where": "/timeseries2"
             },
             "name": "catchment1",
             "type": "catchment"
         },
         {
-            "initial_volume": 50.0,
-            "max_volume": 9999,
-            "name": "reservoir1",
-            "type": "Storage"
+            "name": "river1",
+            "type": "Link"
         },
         {
             "max_flow": 50,
             "name": "abs1",
             "type": "link"
         },
         {
             "cost": -10,
             "max_flow": 23.0,
             "name": "demand1",
             "type": "Output"
         },
         {
-            "cost": 5,
+            "cost": -5,
             "name": "term1",
             "type": "Output"
         }
     ],
-    "recorders": {
-        "seasonal_fdc": {
-            "months": [
-                6,
-                7,
-                8
-            ],
-            "node": "catchment1",
-            "percentiles": [
-                20.0,
-                40.0,
-                60.0,
-                80.0,
-                100.0
-            ],
-            "type": "seasonalflowdurationcurverecorder"
+    "scenarios": [
+        {
+            "name": "scenario A",
+            "size": 10
         }
-    },
+    ],
     "timestepper": {
-        "end": "2014-12-31",
-        "start": "2014-01-01",
+        "end": "2015-01-31",
+        "start": "2015-01-01",
         "timestep": 1
     }
 }
```

### Comparing `pywr-1.8.0/tests/models/virtual_storage2.json` & `pywr-1.9.0/tests/models/virtual_storage2.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 25% similar despite different names*

```diff
@@ -1,118 +1,113 @@
-00000000: 7b0d 0a20 2020 2022 6d65 7461 6461 7461  {..    "metadata
-00000010: 223a 207b 0d0a 2020 2020 2020 2020 2274  ": {..        "t
-00000020: 6974 6c65 223a 2022 416e 6e75 616c 2076  itle": "Annual v
-00000030: 6972 7475 616c 2073 746f 7261 6765 222c  irtual storage",
-00000040: 0d0a 2020 2020 2020 2020 2264 6573 6372  ..        "descr
-00000050: 6970 7469 6f6e 223a 2022 416e 6e75 616c  iption": "Annual
-00000060: 2061 6273 7472 6163 7469 6f6e 206c 6963   abstraction lic
-00000070: 656e 6365 2069 6d70 6c65 6d65 6e74 6564  ence implemented
-00000080: 2061 7320 616e 2061 6e6e 7561 6c20 7669   as an annual vi
-00000090: 7274 7561 6c20 7374 6f72 6167 6520 7769  rtual storage wi
-000000a0: 7468 2061 2064 796e 616d 6963 2063 6f73  th a dynamic cos
-000000b0: 742e 222c 0d0a 2020 2020 2020 2020 226d  t.",..        "m
-000000c0: 696e 696d 756d 5f76 6572 7369 6f6e 223a  inimum_version":
-000000d0: 2022 302e 3122 0d0a 2020 2020 7d2c 0d0a   "0.1"..    },..
-000000e0: 2020 2020 2274 696d 6573 7465 7070 6572      "timestepper
-000000f0: 223a 207b 0d0a 2020 2020 2020 2020 2273  ": {..        "s
-00000100: 7461 7274 223a 2022 3230 3135 2d30 342d  tart": "2015-04-
-00000110: 3031 222c 0d0a 2020 2020 2020 2020 2265  01",..        "e
-00000120: 6e64 223a 2022 3230 3136 2d30 342d 3031  nd": "2016-04-01
-00000130: 222c 0d0a 2020 2020 2020 2020 2274 696d  ",..        "tim
-00000140: 6573 7465 7022 3a20 310d 0a20 2020 207d  estep": 1..    }
-00000150: 2c0d 0a20 2020 2022 6e6f 6465 7322 3a20  ,..    "nodes": 
-00000160: 5b0d 0a20 2020 2020 2020 207b 0d0a 2020  [..        {..  
-00000170: 2020 2020 2020 2020 2020 226e 616d 6522            "name"
-00000180: 3a20 2273 7570 706c 7931 222c 0d0a 2020  : "supply1",..  
-00000190: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
-000001a0: 3a20 2249 6e70 7574 222c 0d0a 2020 2020  : "Input",..    
-000001b0: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
-000001c0: 7722 3a20 3130 2c0d 0a20 2020 2020 2020  w": 10,..       
-000001d0: 2020 2020 2022 636f 7374 223a 2022 7375       "cost": "su
-000001e0: 7070 6c79 315f 636f 7374 220d 0a20 2020  pply1_cost"..   
-000001f0: 2020 2020 207d 2c0d 0a20 2020 2020 2020       },..       
-00000200: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000210: 226e 616d 6522 3a20 2273 7570 706c 7932  "name": "supply2
-00000220: 222c 0d0a 2020 2020 2020 2020 2020 2020  ",..            
-00000230: 2274 7970 6522 3a20 2249 6e70 7574 222c  "type": "Input",
-00000240: 0d0a 2020 2020 2020 2020 2020 2020 226d  ..            "m
-00000250: 6178 5f66 6c6f 7722 3a20 352c 0d0a 2020  ax_flow": 5,..  
-00000260: 2020 2020 2020 2020 2020 2263 6f73 7422            "cost"
-00000270: 3a20 350d 0a20 2020 2020 2020 207d 2c0d  : 5..        },.
-00000280: 0a20 2020 2020 2020 207b 0d0a 2020 2020  .        {..    
-00000290: 2020 2020 2020 2020 226e 616d 6522 3a20          "name": 
-000002a0: 226c 696e 6b31 222c 0d0a 2020 2020 2020  "link1",..      
-000002b0: 2020 2020 2020 2274 7970 6522 3a20 224c        "type": "L
-000002c0: 696e 6b22 0d0a 2020 2020 2020 2020 7d2c  ink"..        },
-000002d0: 0d0a 2020 2020 2020 2020 7b0d 0a20 2020  ..        {..   
-000002e0: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-000002f0: 2022 6465 6d61 6e64 3122 2c0d 0a20 2020   "demand1",..   
-00000300: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
-00000310: 2022 4f75 7470 7574 222c 0d0a 2020 2020   "Output",..    
-00000320: 2020 2020 2020 2020 226d 6178 5f66 6c6f          "max_flo
-00000330: 7722 3a20 3130 2c0d 0a20 2020 2020 2020  w": 10,..       
-00000340: 2020 2020 2022 636f 7374 223a 202d 3130       "cost": -10
-00000350: 300d 0a20 2020 2020 2020 207d 2c0d 0a20  0..        },.. 
-00000360: 2020 2020 2020 207b 0d0a 2020 2020 2020         {..      
-00000370: 2020 2020 2020 226e 616d 6522 3a20 226c        "name": "l
-00000380: 6963 656e 6365 3122 2c0d 0a20 2020 2020  icence1",..     
-00000390: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000003a0: 416e 6e75 616c 5669 7274 7561 6c53 746f  AnnualVirtualSto
-000003b0: 7261 6765 222c 0d0a 2020 2020 2020 2020  rage",..        
-000003c0: 2020 2020 226d 6178 5f76 6f6c 756d 6522      "max_volume"
-000003d0: 3a20 3330 3030 2c0d 0a20 2020 2020 2020  : 3000,..       
-000003e0: 2020 2020 2022 696e 6974 6961 6c5f 766f       "initial_vo
-000003f0: 6c75 6d65 223a 2033 3030 302c 0d0a 2020  lume": 3000,..  
-00000400: 2020 2020 2020 2020 2020 226e 6f64 6573            "nodes
-00000410: 223a 205b 0d0a 2020 2020 2020 2020 2020  ": [..          
-00000420: 2020 2020 2020 2273 7570 706c 7931 220d        "supply1".
-00000430: 0a20 2020 2020 2020 2020 2020 205d 2c0d  .            ],.
-00000440: 0a20 2020 2020 2020 2020 2020 2022 6661  .            "fa
-00000450: 6374 6f72 7322 3a20 5b0d 0a20 2020 2020  ctors": [..     
-00000460: 2020 2020 2020 2020 2020 2031 2e30 0d0a             1.0..
-00000470: 2020 2020 2020 2020 2020 2020 5d2c 0d0a              ],..
-00000480: 2020 2020 2020 2020 2020 2020 2272 6573              "res
-00000490: 6574 5f64 6179 223a 2031 2c0d 0a20 2020  et_day": 1,..   
-000004a0: 2020 2020 2020 2020 2022 7265 7365 745f           "reset_
-000004b0: 6d6f 6e74 6822 3a20 340d 0a20 2020 2020  month": 4..     
-000004c0: 2020 207d 0d0a 2020 2020 5d2c 0d0a 2020     }..    ],..  
-000004d0: 2020 2265 6467 6573 223a 205b 0d0a 2020    "edges": [..  
-000004e0: 2020 2020 2020 5b22 7375 7070 6c79 3122        ["supply1"
-000004f0: 2c20 226c 696e 6b31 225d 2c0d 0a20 2020  , "link1"],..   
-00000500: 2020 2020 205b 2273 7570 706c 7932 222c       ["supply2",
-00000510: 2022 6c69 6e6b 3122 5d2c 0d0a 2020 2020   "link1"],..    
-00000520: 2020 2020 5b22 6c69 6e6b 3122 2c20 2264      ["link1", "d
-00000530: 656d 616e 6431 225d 0d0a 2020 2020 5d2c  emand1"]..    ],
-00000540: 0d0a 2020 2020 2270 6172 616d 6574 6572  ..    "parameter
-00000550: 7322 3a20 7b0d 0a20 2020 2020 2020 2022  s": {..        "
-00000560: 7375 7070 6c79 315f 636f 7374 223a 207b  supply1_cost": {
-00000570: 0d0a 2020 2020 2020 2020 2020 2020 2274  ..            "t
-00000580: 7970 6522 3a20 2263 6f6e 7472 6f6c 6375  ype": "controlcu
-00000590: 7276 6522 2c0d 0a20 2020 2020 2020 2020  rve",..         
-000005a0: 2020 2022 7374 6f72 6167 655f 6e6f 6465     "storage_node
-000005b0: 223a 2022 6c69 6365 6e63 6531 222c 0d0a  ": "licence1",..
-000005c0: 2020 2020 2020 2020 2020 2020 2263 6f6e              "con
-000005d0: 7472 6f6c 5f63 7572 7665 7322 3a20 5b0d  trol_curves": [.
-000005e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000005f0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000600: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000610: 2275 6e69 666f 726d 6472 6177 646f 776e  "uniformdrawdown
-00000620: 7072 6f66 696c 6522 2c0d 0a20 2020 2020  profile",..     
-00000630: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00000640: 7265 7365 745f 6461 7922 3a20 312c 0d0a  reset_day": 1,..
-00000650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000660: 2020 2020 2272 6573 6574 5f6d 6f6e 7468      "reset_month
-00000670: 223a 2034 0d0a 2020 2020 2020 2020 2020  ": 4..          
-00000680: 2020 2020 2020 7d0d 0a20 2020 2020 2020        }..       
-00000690: 2020 2020 205d 2c0d 0a20 2020 2020 2020       ],..       
-000006a0: 2020 2020 2022 7661 6c75 6573 223a 5b30       "values":[0
-000006b0: 2c20 3130 5d0d 0a20 2020 2020 2020 207d  , 10]..        }
-000006c0: 0d0a 2020 2020 7d2c 0d0a 2020 2020 2272  ..    },..    "r
-000006d0: 6563 6f72 6465 7273 223a 207b 0d0a 2020  ecorders": {..  
-000006e0: 2020 2020 2020 2273 7570 706c 7931 223a        "supply1":
-000006f0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000700: 2274 7970 6522 3a20 226e 756d 7079 6172  "type": "numpyar
-00000710: 7261 796e 6f64 6572 6563 6f72 6465 7222  raynoderecorder"
-00000720: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000730: 6e6f 6465 223a 2022 7375 7070 6c79 3122  node": "supply1"
-00000740: 0d0a 2020 2020 2020 2020 7d0d 0a20 2020  ..        }..   
-00000750: 207d 0d0a 7d0d 0a                         }..}..
+00000000: 7b0a 2020 2020 226d 6574 6164 6174 6122  {.    "metadata"
+00000010: 3a20 7b0a 2020 2020 2020 2020 2274 6974  : {.        "tit
+00000020: 6c65 223a 2022 416e 6e75 616c 2076 6972  le": "Annual vir
+00000030: 7475 616c 2073 746f 7261 6765 222c 0a20  tual storage",. 
+00000040: 2020 2020 2020 2022 6465 7363 7269 7074         "descript
+00000050: 696f 6e22 3a20 2241 6e6e 7561 6c20 6162  ion": "Annual ab
+00000060: 7374 7261 6374 696f 6e20 6c69 6365 6e63  straction licenc
+00000070: 6520 696d 706c 656d 656e 7465 6420 6173  e implemented as
+00000080: 2061 6e20 616e 6e75 616c 2076 6972 7475   an annual virtu
+00000090: 616c 2073 746f 7261 6765 2077 6974 6820  al storage with 
+000000a0: 6120 6479 6e61 6d69 6320 636f 7374 2e22  a dynamic cost."
+000000b0: 2c0a 2020 2020 2020 2020 226d 696e 696d  ,.        "minim
+000000c0: 756d 5f76 6572 7369 6f6e 223a 2022 302e  um_version": "0.
+000000d0: 3122 0a20 2020 207d 2c0a 2020 2020 2274  1".    },.    "t
+000000e0: 696d 6573 7465 7070 6572 223a 207b 0a20  imestepper": {. 
+000000f0: 2020 2020 2020 2022 7374 6172 7422 3a20         "start": 
+00000100: 2232 3031 352d 3034 2d30 3122 2c0a 2020  "2015-04-01",.  
+00000110: 2020 2020 2020 2265 6e64 223a 2022 3230        "end": "20
+00000120: 3136 2d30 342d 3031 222c 0a20 2020 2020  16-04-01",.     
+00000130: 2020 2022 7469 6d65 7374 6570 223a 2031     "timestep": 1
+00000140: 0a20 2020 207d 2c0a 2020 2020 226e 6f64  .    },.    "nod
+00000150: 6573 223a 205b 0a20 2020 2020 2020 207b  es": [.        {
+00000160: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
+00000170: 6d65 223a 2022 7375 7070 6c79 3122 2c0a  me": "supply1",.
+00000180: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000190: 6522 3a20 2249 6e70 7574 222c 0a20 2020  e": "Input",.   
+000001a0: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+000001b0: 6f77 223a 2031 302c 0a20 2020 2020 2020  ow": 10,.       
+000001c0: 2020 2020 2022 636f 7374 223a 2022 7375       "cost": "su
+000001d0: 7070 6c79 315f 636f 7374 220a 2020 2020  pply1_cost".    
+000001e0: 2020 2020 7d2c 0a20 2020 2020 2020 207b      },.        {
+000001f0: 0a20 2020 2020 2020 2020 2020 2022 6e61  .            "na
+00000200: 6d65 223a 2022 7375 7070 6c79 3222 2c0a  me": "supply2",.
+00000210: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000220: 6522 3a20 2249 6e70 7574 222c 0a20 2020  e": "Input",.   
+00000230: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+00000240: 6f77 223a 2035 2c0a 2020 2020 2020 2020  ow": 5,.        
+00000250: 2020 2020 2263 6f73 7422 3a20 350a 2020      "cost": 5.  
+00000260: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+00000270: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+00000280: 6e61 6d65 223a 2022 6c69 6e6b 3122 2c0a  name": "link1",.
+00000290: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000002a0: 6522 3a20 224c 696e 6b22 0a20 2020 2020  e": "Link".     
+000002b0: 2020 207d 2c0a 2020 2020 2020 2020 7b0a     },.        {.
+000002c0: 2020 2020 2020 2020 2020 2020 226e 616d              "nam
+000002d0: 6522 3a20 2264 656d 616e 6431 222c 0a20  e": "demand1",. 
+000002e0: 2020 2020 2020 2020 2020 2022 7479 7065             "type
+000002f0: 223a 2022 4f75 7470 7574 222c 0a20 2020  ": "Output",.   
+00000300: 2020 2020 2020 2020 2022 6d61 785f 666c           "max_fl
+00000310: 6f77 223a 2031 302c 0a20 2020 2020 2020  ow": 10,.       
+00000320: 2020 2020 2022 636f 7374 223a 202d 3130       "cost": -10
+00000330: 300a 2020 2020 2020 2020 7d2c 0a20 2020  0.        },.   
+00000340: 2020 2020 207b 0a20 2020 2020 2020 2020       {.         
+00000350: 2020 2022 6e61 6d65 223a 2022 6c69 6365     "name": "lice
+00000360: 6e63 6531 222c 0a20 2020 2020 2020 2020  nce1",.         
+00000370: 2020 2022 7479 7065 223a 2022 416e 6e75     "type": "Annu
+00000380: 616c 5669 7274 7561 6c53 746f 7261 6765  alVirtualStorage
+00000390: 222c 0a20 2020 2020 2020 2020 2020 2022  ",.            "
+000003a0: 6d61 785f 766f 6c75 6d65 223a 2033 3030  max_volume": 300
+000003b0: 302c 0a20 2020 2020 2020 2020 2020 2022  0,.            "
+000003c0: 696e 6974 6961 6c5f 766f 6c75 6d65 223a  initial_volume":
+000003d0: 2033 3030 302c 0a20 2020 2020 2020 2020   3000,.         
+000003e0: 2020 2022 6e6f 6465 7322 3a20 5b0a 2020     "nodes": [.  
+000003f0: 2020 2020 2020 2020 2020 2020 2020 2273                "s
+00000400: 7570 706c 7931 220a 2020 2020 2020 2020  upply1".        
+00000410: 2020 2020 5d2c 0a20 2020 2020 2020 2020      ],.         
+00000420: 2020 2022 6661 6374 6f72 7322 3a20 5b0a     "factors": [.
+00000430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000440: 312e 300a 2020 2020 2020 2020 2020 2020  1.0.            
+00000450: 5d2c 0a20 2020 2020 2020 2020 2020 2022  ],.            "
+00000460: 7265 7365 745f 6461 7922 3a20 312c 0a20  reset_day": 1,. 
+00000470: 2020 2020 2020 2020 2020 2022 7265 7365             "rese
+00000480: 745f 6d6f 6e74 6822 3a20 340a 2020 2020  t_month": 4.    
+00000490: 2020 2020 7d0a 2020 2020 5d2c 0a20 2020      }.    ],.   
+000004a0: 2022 6564 6765 7322 3a20 5b0a 2020 2020   "edges": [.    
+000004b0: 2020 2020 5b22 7375 7070 6c79 3122 2c20      ["supply1", 
+000004c0: 226c 696e 6b31 225d 2c0a 2020 2020 2020  "link1"],.      
+000004d0: 2020 5b22 7375 7070 6c79 3222 2c20 226c    ["supply2", "l
+000004e0: 696e 6b31 225d 2c0a 2020 2020 2020 2020  ink1"],.        
+000004f0: 5b22 6c69 6e6b 3122 2c20 2264 656d 616e  ["link1", "deman
+00000500: 6431 225d 0a20 2020 205d 2c0a 2020 2020  d1"].    ],.    
+00000510: 2270 6172 616d 6574 6572 7322 3a20 7b0a  "parameters": {.
+00000520: 2020 2020 2020 2020 2273 7570 706c 7931          "supply1
+00000530: 5f63 6f73 7422 3a20 7b0a 2020 2020 2020  _cost": {.      
+00000540: 2020 2020 2020 2274 7970 6522 3a20 2263        "type": "c
+00000550: 6f6e 7472 6f6c 6375 7276 6522 2c0a 2020  ontrolcurve",.  
+00000560: 2020 2020 2020 2020 2020 2273 746f 7261            "stora
+00000570: 6765 5f6e 6f64 6522 3a20 226c 6963 656e  ge_node": "licen
+00000580: 6365 3122 2c0a 2020 2020 2020 2020 2020  ce1",.          
+00000590: 2020 2263 6f6e 7472 6f6c 5f63 7572 7665    "control_curve
+000005a0: 7322 3a20 5b0a 2020 2020 2020 2020 2020  s": [.          
+000005b0: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+000005c0: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+000005d0: 6522 3a20 2275 6e69 666f 726d 6472 6177  e": "uniformdraw
+000005e0: 646f 776e 7072 6f66 696c 6522 2c0a 2020  downprofile",.  
+000005f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000600: 2020 2272 6573 6574 5f64 6179 223a 2031    "reset_day": 1
+00000610: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00000620: 2020 2020 2020 2272 6573 6574 5f6d 6f6e        "reset_mon
+00000630: 7468 223a 2034 0a20 2020 2020 2020 2020  th": 4.         
+00000640: 2020 2020 2020 207d 0a20 2020 2020 2020         }.       
+00000650: 2020 2020 205d 2c0a 2020 2020 2020 2020       ],.        
+00000660: 2020 2020 2276 616c 7565 7322 3a5b 302c      "values":[0,
+00000670: 2031 305d 0a20 2020 2020 2020 207d 0a20   10].        }. 
+00000680: 2020 207d 2c0a 2020 2020 2272 6563 6f72     },.    "recor
+00000690: 6465 7273 223a 207b 0a20 2020 2020 2020  ders": {.       
+000006a0: 2022 7375 7070 6c79 3122 3a20 7b0a 2020   "supply1": {.  
+000006b0: 2020 2020 2020 2020 2020 2274 7970 6522            "type"
+000006c0: 3a20 226e 756d 7079 6172 7261 796e 6f64  : "numpyarraynod
+000006d0: 6572 6563 6f72 6465 7222 2c0a 2020 2020  erecorder",.    
+000006e0: 2020 2020 2020 2020 226e 6f64 6522 3a20          "node": 
+000006f0: 2273 7570 706c 7931 220a 2020 2020 2020  "supply1".      
+00000700: 2020 7d0a 2020 2020 7d0a 7d0a              }.    }.}.
```

### Comparing `pywr-1.8.0/tests/test_analytical.py` & `pywr-1.9.0/tests/test_analytical.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,366 +1,366 @@
-# -*- coding: utf-8 -*-
-"""
-A series of test based on an analytical solution to simple
-network problem.
-
-
-"""
-import pywr.core
-import datetime
-import numpy as np
-import pytest
-from helpers import assert_model
-from fixtures import simple_linear_model
-
-import pywr.parameters
-
-
-@pytest.mark.parametrize("in_flow, out_flow, benefit",
-                         [(10.0, 10.0, 10.0), (10.0, 0.0, 0.0)])
-def test_linear_model(simple_linear_model, in_flow, out_flow, benefit):
-    """
-    Test the simple_linear_model with different basic input and output values
-    """
-
-    simple_linear_model.nodes["Input"].max_flow = in_flow
-    simple_linear_model.nodes["Output"].min_flow = out_flow
-    simple_linear_model.nodes["Output"].cost = -benefit
-
-    expected_sent = in_flow if benefit > 1.0 else out_flow
-
-    expected_node_results = {
-        "Input": expected_sent,
-        "Link": expected_sent,
-        "Output": expected_sent,
-    }
-    assert_model(simple_linear_model, expected_node_results)
-
-
-@pytest.fixture(params=[
-    (10.0, 5.0, 5.0, 0.0, 0.0, 0.0),
-    (10.0, 5.0, 5.0, 0.0, 10.0, 0.0),
-    (10.0, 5.0, 5.0, 0.0, 10.0, 2.0),
-    (10.0, 5.0, 0.0, 5.0, 10.0, 2.0),
-    ])
-def linear_model_with_storage(request):
-    """
-    Make a simple model with a single Input and Output and an offline Storage Node
-
-    Input -> Link -> Output
-               |     ^
-               v     |
-               Storage
-    """
-    in_flow, out_flow, out_benefit, strg_benefit, current_volume, min_volume = request.param
-    max_strg_out = 10.0
-    max_volume = 10.0
-
-    model = pywr.core.Model()
-    inpt = pywr.core.Input(model, name="Input", min_flow=in_flow, max_flow=in_flow)
-    lnk = pywr.core.Link(model, name="Link", cost=0.1)
-    inpt.connect(lnk)
-    otpt = pywr.core.Output(model, name="Output", min_flow=out_flow, cost=-out_benefit)
-    lnk.connect(otpt)
-
-    strg = pywr.core.Storage(model, name="Storage", max_volume=max_volume, min_volume=min_volume,
-                             initial_volume=current_volume, cost=-strg_benefit)
-
-    strg.connect(otpt)
-    lnk.connect(strg)
-    avail_volume = max(current_volume - min_volume, 0.0)
-    avail_refill = max_volume - current_volume
-    expected_sent = in_flow+min(max_strg_out, avail_volume) if out_benefit > strg_benefit else max(out_flow, in_flow-avail_refill)
-
-    expected_node_results = {
-        "Input": in_flow,
-        "Link": in_flow,
-        "Output": expected_sent,
-        "Storage Output": 0.0,
-        "Storage Input": min(max_strg_out, avail_volume) if out_benefit > 1.0 else 0.0,
-        "Storage": min_volume if out_benefit > strg_benefit else max_volume,
-    }
-    return model, expected_node_results
-
-def test_linear_model_with_storage(linear_model_with_storage):
-    assert_model(*linear_model_with_storage)
-
-@pytest.fixture
-def two_domain_linear_model(request):
-    """
-    Make a simple model with two domains, each with a single Input and Output
-
-    Input -> Link -> Output  : river
-                        | across the domain
-    Output <- Link <- Input  : grid
-
-    """
-    river_flow = 864.0  # Ml/d
-    power_plant_cap = 24  # GWh/d
-    power_plant_flow_req = 18.0  # Ml/GWh
-    power_demand = 12  # GWh/d
-    power_benefit = 10.0  # £/GWh
-
-    river_domain = pywr.core.Domain('river')
-    grid_domain = pywr.core.Domain('grid')
-
-    model = pywr.core.Model()
-    # Create river network
-    river_inpt = pywr.core.Input(model, name="Catchment", max_flow=river_flow, domain=river_domain)
-    river_lnk = pywr.core.Link(model, name="Reach", domain=river_domain)
-    river_inpt.connect(river_lnk)
-    river_otpt = pywr.core.Output(model, name="Abstraction", domain=river_domain, cost=0.0)
-    river_lnk.connect(river_otpt)
-    # Create grid network
-    grid_inpt = pywr.core.Input(model, name="Power Plant", max_flow=power_plant_cap, domain=grid_domain,
-                                               conversion_factor=1/power_plant_flow_req)
-    grid_lnk = pywr.core.Link(model, name="Transmission", cost=1.0, domain=grid_domain)
-    grid_inpt.connect(grid_lnk)
-    grid_otpt = pywr.core.Output(model, name="Substation", max_flow=power_demand,
-                                 cost=-power_benefit, domain=grid_domain)
-    grid_lnk.connect(grid_otpt)
-    # Connect grid to river
-    river_otpt.connect(grid_inpt)
-
-    expected_requested = {'river': 0.0, 'grid': 0.0}
-    expected_sent = {'river': power_demand*power_plant_flow_req, 'grid': power_demand}
-
-    expected_node_results = {
-        "Catchment": power_demand*power_plant_flow_req,
-        "Reach": power_demand*power_plant_flow_req,
-        "Abstraction": power_demand*power_plant_flow_req,
-        "Power Plant": power_demand,
-        "Transmission": power_demand,
-        "Substation": power_demand,
-    }
-
-    return model, expected_node_results
-
-
-def test_two_domain_linear_model(two_domain_linear_model):
-    assert_model(*two_domain_linear_model)
-
-
-@pytest.fixture
-def two_cross_domain_output_single_input(request):
-    """
-    Make a simple model with two domains. Thre are two Output nodes
-    both connect to an Input node in a different domain.
-
-    In this example the rivers should be able to provide flow to the grid
-    with a total flow equal to the sum of their respective parts.
-
-    Input -> Link -> Output  : river
-                        | across the domain
-                        Input -> Link -> Output : grid
-                        | across the domain
-    Input -> Link -> Output  : river
-
-    """
-    river_flow = 10.0
-    expected_node_results = {}
-
-    model = pywr.core.Model()
-    # Create grid network
-    grid_inpt = pywr.core.Input(model, name="Input", domain='grid',)
-    grid_lnk = pywr.core.Link(model, name="Link", cost=1.0, domain='grid')
-    grid_inpt.connect(grid_lnk)
-    grid_otpt = pywr.core.Output(model, name="Output", max_flow=50.0, cost=-10.0, domain='grid')
-    grid_lnk.connect(grid_otpt)
-    # Create river network
-    for i in range(2):
-        river_inpt = pywr.core.Input(model, name="Catchment {}".format(i), max_flow=river_flow, domain='river')
-        river_lnk = pywr.core.Link(model, name="Reach {}".format(i), domain='river')
-        river_inpt.connect(river_lnk)
-        river_otpt = pywr.core.Output(model, name="Abstraction {}".format(i), domain='river', cost=0.0)
-        river_lnk.connect(river_otpt)
-        # Connect grid to river
-        river_otpt.connect(grid_inpt)
-
-        expected_node_results.update({
-            "Catchment {}".format(i): river_flow,
-            "Reach {}".format(i): river_flow,
-            "Abstraction {}".format(i): river_flow
-        })
-
-    expected_node_results.update({
-        "Input": river_flow*2,
-        "Link": river_flow*2,
-        "Output": river_flow*2,
-    })
-
-    return model, expected_node_results
-
-
-@pytest.mark.xfail
-def test_two_cross_domain_output_single_input(two_cross_domain_output_single_input):
-    # TODO This test currently fails because of the simple way in which the cross
-    # domain paths work. It can not cope with two Outputs connected to one
-    # input.
-    assert_model(*two_cross_domain_output_single_input)
-
-
-@pytest.fixture()
-def simple_linear_inline_model(request):
-    """
-    Make a simple model with a single Input and Output nodes inline of a route.
-
-    Input 0 -> Input 1 -> Link -> Output 0 -> Output 1
-
-    """
-    model = pywr.core.Model()
-    inpt0 = pywr.core.Input(model, name="Input 0")
-    inpt1 = pywr.core.Input(model, name="Input 1")
-    inpt0.connect(inpt1)
-    lnk = pywr.core.Link(model, name="Link", cost=1.0)
-    inpt1.connect(lnk)
-    otpt0 = pywr.core.Output(model, name="Output 0")
-    lnk.connect(otpt0)
-    otpt1 = pywr.core.Output(model, name="Output 1")
-    otpt0.connect(otpt1)
-
-    return model
-
-
-@pytest.mark.skipif(pywr.core.Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-@pytest.mark.parametrize("in_flow_1, out_flow_0, link_flow",
-                         [(10.0, 10.0, 15.0),
-                          (0.0, 0.0, 10.0)])
-def test_simple_linear_inline_model(simple_linear_inline_model, in_flow_1, out_flow_0, link_flow):
-    """
-    Test the test_simple_linear_inline_model with different flow constraints
-    """
-    model = simple_linear_inline_model
-    model.nodes["Input 0"].max_flow = 10.0
-    model.nodes["Input 1"].max_flow = in_flow_1
-    model.nodes["Link"].max_flow = link_flow
-    model.nodes["Output 0"].max_flow = out_flow_0
-    model.nodes["Input 1"].cost = 1.0
-    model.nodes["Output 0"].cost = -10.0
-    model.nodes["Output 1"].cost = -5.0
-
-    expected_sent = min(link_flow, 10+in_flow_1)
-
-    expected_node_results = {
-        "Input 0": 10.0,
-        "Input 1": max(expected_sent-10.0, 0.0),
-        "Link": expected_sent,
-        "Output 0": min(expected_sent, out_flow_0),
-        "Output 1": max(expected_sent - out_flow_0, 0.0),
-    }
-    assert_model(model, expected_node_results)
-
-
-@pytest.fixture()
-def bidirectional_model(request):
-    """
-    Make a simple model with a single Input and Output.
-
-    Input 0 -> Link 0 -> Output 0
-               |   ^
-               v   |
-    Input 1 -> Link 1 -> Output 1
-
-    """
-    model = pywr.core.Model()
-    for i in range(2):
-        inpt = pywr.core.Input(model, name="Input {}".format(i))
-        lnk = pywr.core.Link(model, name="Link {}".format(i))
-        inpt.connect(lnk)
-        otpt = pywr.core.Output(model, name="Output {}".format(i))
-        lnk.connect(otpt)
-
-    # Create bidirectional link (i.e. a cycle)
-    model.nodes['Link 0'].connect(model.nodes['Link 1'])
-    model.nodes['Link 1'].connect(model.nodes['Link 0'])
-
-    return model
-
-
-def test_bidirectional_model(bidirectional_model):
-    """
-    Test the simple_linear_model with different basic input and output values
-    """
-    model = bidirectional_model
-    model.nodes["Input 0"].max_flow = 10.0
-    model.nodes["Input 1"].max_flow = 10.0
-    model.nodes["Output 0"].max_flow = 10.0
-    model.nodes["Output 1"].max_flow = 15.0
-    model.nodes["Output 0"].cost = -5.0
-    model.nodes["Output 1"].cost = -10.0
-    model.nodes["Link 0"].cost = 1.0
-    model.nodes["Link 1"].cost = 1.0
-
-    expected_node_results = {
-        "Input 0": 10.0,
-        "Input 1": 10.0,
-        "Link 0": 10.0,
-        "Link 1": 15.0,
-        "Output 0": 5.0,
-        "Output 1": 15.0,
-    }
-    assert_model(model, expected_node_results)
-
-
-def make_simple_model(supply_amplitude, demand, frequency, initial_volume):
-    """
-    Make a simple model,
-        supply -> reservoir -> demand.
-
-    supply is a annual cosine function with amplitude supply_amplitude and
-    frequency
-
-    """
-
-    model = pywr.core.Model()
-
-    S = supply_amplitude
-    w = frequency
-
-    class SupplyFunc(pywr.parameters.Parameter):
-        def value(self, ts, si):
-            # Take the mean flow of the day (i.e. offset by half a day)
-            t = ts.dayofyear - 0.5
-            v = S*np.cos(t*w)+S
-            return v
-
-    max_flow = SupplyFunc(model)
-    supply = pywr.core.Input(model, name='supply', max_flow=max_flow, min_flow=max_flow)
-    demand = pywr.core.Output(model, name='demand', max_flow=demand, cost=-10)
-    res = pywr.core.Storage(model, name='reservoir', max_volume=1e6,
-                            initial_volume=initial_volume)
-
-    supply_res_link = pywr.core.Link(model, name='link1')
-    res_demand_link = pywr.core.Link(model, name='link2')
-
-    supply.connect(supply_res_link)
-    supply_res_link.connect(res)
-    res.connect(res_demand_link)
-    res_demand_link.connect(demand)
-
-    return model
-
-
-def test_analytical():
-    """
-    Run the test model though a year with analytical solution values to
-    ensure reservoir just contains sufficient volume.
-    """
-
-    S = 100.0  # supply amplitude
-    D = S  # demand
-    w = 2*np.pi/365  # frequency (annual)
-    V0 = S/w  # initial reservoir level
-
-    model = make_simple_model(S, D, w, V0)
-
-    T = np.arange(1, 365)
-    V_anal = S*(np.sin(w*T)/w+T) - D*T + V0
-    V_model = np.empty(T.shape)
-
-    for i, t in enumerate(T):
-        model.step()
-        V_model[i] = model.nodes['reservoir'].volume[0]
-
-    # Relative error from initial volume
-    error = np.abs(V_model - V_anal) / V0
-    assert np.all(error < 1e-4)
+# -*- coding: utf-8 -*-
+"""
+A series of test based on an analytical solution to simple
+network problem.
+
+
+"""
+import pywr.core
+import datetime
+import numpy as np
+import pytest
+from helpers import assert_model
+from fixtures import simple_linear_model
+
+import pywr.parameters
+
+
+@pytest.mark.parametrize("in_flow, out_flow, benefit",
+                         [(10.0, 10.0, 10.0), (10.0, 0.0, 0.0)])
+def test_linear_model(simple_linear_model, in_flow, out_flow, benefit):
+    """
+    Test the simple_linear_model with different basic input and output values
+    """
+
+    simple_linear_model.nodes["Input"].max_flow = in_flow
+    simple_linear_model.nodes["Output"].min_flow = out_flow
+    simple_linear_model.nodes["Output"].cost = -benefit
+
+    expected_sent = in_flow if benefit > 1.0 else out_flow
+
+    expected_node_results = {
+        "Input": expected_sent,
+        "Link": expected_sent,
+        "Output": expected_sent,
+    }
+    assert_model(simple_linear_model, expected_node_results)
+
+
+@pytest.fixture(params=[
+    (10.0, 5.0, 5.0, 0.0, 0.0, 0.0),
+    (10.0, 5.0, 5.0, 0.0, 10.0, 0.0),
+    (10.0, 5.0, 5.0, 0.0, 10.0, 2.0),
+    (10.0, 5.0, 0.0, 5.0, 10.0, 2.0),
+    ])
+def linear_model_with_storage(request):
+    """
+    Make a simple model with a single Input and Output and an offline Storage Node
+
+    Input -> Link -> Output
+               |     ^
+               v     |
+               Storage
+    """
+    in_flow, out_flow, out_benefit, strg_benefit, current_volume, min_volume = request.param
+    max_strg_out = 10.0
+    max_volume = 10.0
+
+    model = pywr.core.Model()
+    inpt = pywr.core.Input(model, name="Input", min_flow=in_flow, max_flow=in_flow)
+    lnk = pywr.core.Link(model, name="Link", cost=0.1)
+    inpt.connect(lnk)
+    otpt = pywr.core.Output(model, name="Output", min_flow=out_flow, cost=-out_benefit)
+    lnk.connect(otpt)
+
+    strg = pywr.core.Storage(model, name="Storage", max_volume=max_volume, min_volume=min_volume,
+                             initial_volume=current_volume, cost=-strg_benefit)
+
+    strg.connect(otpt)
+    lnk.connect(strg)
+    avail_volume = max(current_volume - min_volume, 0.0)
+    avail_refill = max_volume - current_volume
+    expected_sent = in_flow+min(max_strg_out, avail_volume) if out_benefit > strg_benefit else max(out_flow, in_flow-avail_refill)
+
+    expected_node_results = {
+        "Input": in_flow,
+        "Link": in_flow,
+        "Output": expected_sent,
+        "Storage Output": 0.0,
+        "Storage Input": min(max_strg_out, avail_volume) if out_benefit > 1.0 else 0.0,
+        "Storage": min_volume if out_benefit > strg_benefit else max_volume,
+    }
+    return model, expected_node_results
+
+def test_linear_model_with_storage(linear_model_with_storage):
+    assert_model(*linear_model_with_storage)
+
+@pytest.fixture
+def two_domain_linear_model(request):
+    """
+    Make a simple model with two domains, each with a single Input and Output
+
+    Input -> Link -> Output  : river
+                        | across the domain
+    Output <- Link <- Input  : grid
+
+    """
+    river_flow = 864.0  # Ml/d
+    power_plant_cap = 24  # GWh/d
+    power_plant_flow_req = 18.0  # Ml/GWh
+    power_demand = 12  # GWh/d
+    power_benefit = 10.0  # £/GWh
+
+    river_domain = pywr.core.Domain('river')
+    grid_domain = pywr.core.Domain('grid')
+
+    model = pywr.core.Model()
+    # Create river network
+    river_inpt = pywr.core.Input(model, name="Catchment", max_flow=river_flow, domain=river_domain)
+    river_lnk = pywr.core.Link(model, name="Reach", domain=river_domain)
+    river_inpt.connect(river_lnk)
+    river_otpt = pywr.core.Output(model, name="Abstraction", domain=river_domain, cost=0.0)
+    river_lnk.connect(river_otpt)
+    # Create grid network
+    grid_inpt = pywr.core.Input(model, name="Power Plant", max_flow=power_plant_cap, domain=grid_domain,
+                                               conversion_factor=1/power_plant_flow_req)
+    grid_lnk = pywr.core.Link(model, name="Transmission", cost=1.0, domain=grid_domain)
+    grid_inpt.connect(grid_lnk)
+    grid_otpt = pywr.core.Output(model, name="Substation", max_flow=power_demand,
+                                 cost=-power_benefit, domain=grid_domain)
+    grid_lnk.connect(grid_otpt)
+    # Connect grid to river
+    river_otpt.connect(grid_inpt)
+
+    expected_requested = {'river': 0.0, 'grid': 0.0}
+    expected_sent = {'river': power_demand*power_plant_flow_req, 'grid': power_demand}
+
+    expected_node_results = {
+        "Catchment": power_demand*power_plant_flow_req,
+        "Reach": power_demand*power_plant_flow_req,
+        "Abstraction": power_demand*power_plant_flow_req,
+        "Power Plant": power_demand,
+        "Transmission": power_demand,
+        "Substation": power_demand,
+    }
+
+    return model, expected_node_results
+
+
+def test_two_domain_linear_model(two_domain_linear_model):
+    assert_model(*two_domain_linear_model)
+
+
+@pytest.fixture
+def two_cross_domain_output_single_input(request):
+    """
+    Make a simple model with two domains. Thre are two Output nodes
+    both connect to an Input node in a different domain.
+
+    In this example the rivers should be able to provide flow to the grid
+    with a total flow equal to the sum of their respective parts.
+
+    Input -> Link -> Output  : river
+                        | across the domain
+                        Input -> Link -> Output : grid
+                        | across the domain
+    Input -> Link -> Output  : river
+
+    """
+    river_flow = 10.0
+    expected_node_results = {}
+
+    model = pywr.core.Model()
+    # Create grid network
+    grid_inpt = pywr.core.Input(model, name="Input", domain='grid',)
+    grid_lnk = pywr.core.Link(model, name="Link", cost=1.0, domain='grid')
+    grid_inpt.connect(grid_lnk)
+    grid_otpt = pywr.core.Output(model, name="Output", max_flow=50.0, cost=-10.0, domain='grid')
+    grid_lnk.connect(grid_otpt)
+    # Create river network
+    for i in range(2):
+        river_inpt = pywr.core.Input(model, name="Catchment {}".format(i), max_flow=river_flow, domain='river')
+        river_lnk = pywr.core.Link(model, name="Reach {}".format(i), domain='river')
+        river_inpt.connect(river_lnk)
+        river_otpt = pywr.core.Output(model, name="Abstraction {}".format(i), domain='river', cost=0.0)
+        river_lnk.connect(river_otpt)
+        # Connect grid to river
+        river_otpt.connect(grid_inpt)
+
+        expected_node_results.update({
+            "Catchment {}".format(i): river_flow,
+            "Reach {}".format(i): river_flow,
+            "Abstraction {}".format(i): river_flow
+        })
+
+    expected_node_results.update({
+        "Input": river_flow*2,
+        "Link": river_flow*2,
+        "Output": river_flow*2,
+    })
+
+    return model, expected_node_results
+
+
+@pytest.mark.xfail
+def test_two_cross_domain_output_single_input(two_cross_domain_output_single_input):
+    # TODO This test currently fails because of the simple way in which the cross
+    # domain paths work. It can not cope with two Outputs connected to one
+    # input.
+    assert_model(*two_cross_domain_output_single_input)
+
+
+@pytest.fixture()
+def simple_linear_inline_model(request):
+    """
+    Make a simple model with a single Input and Output nodes inline of a route.
+
+    Input 0 -> Input 1 -> Link -> Output 0 -> Output 1
+
+    """
+    model = pywr.core.Model()
+    inpt0 = pywr.core.Input(model, name="Input 0")
+    inpt1 = pywr.core.Input(model, name="Input 1")
+    inpt0.connect(inpt1)
+    lnk = pywr.core.Link(model, name="Link", cost=1.0)
+    inpt1.connect(lnk)
+    otpt0 = pywr.core.Output(model, name="Output 0")
+    lnk.connect(otpt0)
+    otpt1 = pywr.core.Output(model, name="Output 1")
+    otpt0.connect(otpt1)
+
+    return model
+
+
+@pytest.mark.skipif(pywr.core.Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+@pytest.mark.parametrize("in_flow_1, out_flow_0, link_flow",
+                         [(10.0, 10.0, 15.0),
+                          (0.0, 0.0, 10.0)])
+def test_simple_linear_inline_model(simple_linear_inline_model, in_flow_1, out_flow_0, link_flow):
+    """
+    Test the test_simple_linear_inline_model with different flow constraints
+    """
+    model = simple_linear_inline_model
+    model.nodes["Input 0"].max_flow = 10.0
+    model.nodes["Input 1"].max_flow = in_flow_1
+    model.nodes["Link"].max_flow = link_flow
+    model.nodes["Output 0"].max_flow = out_flow_0
+    model.nodes["Input 1"].cost = 1.0
+    model.nodes["Output 0"].cost = -10.0
+    model.nodes["Output 1"].cost = -5.0
+
+    expected_sent = min(link_flow, 10+in_flow_1)
+
+    expected_node_results = {
+        "Input 0": 10.0,
+        "Input 1": max(expected_sent-10.0, 0.0),
+        "Link": expected_sent,
+        "Output 0": min(expected_sent, out_flow_0),
+        "Output 1": max(expected_sent - out_flow_0, 0.0),
+    }
+    assert_model(model, expected_node_results)
+
+
+@pytest.fixture()
+def bidirectional_model(request):
+    """
+    Make a simple model with a single Input and Output.
+
+    Input 0 -> Link 0 -> Output 0
+               |   ^
+               v   |
+    Input 1 -> Link 1 -> Output 1
+
+    """
+    model = pywr.core.Model()
+    for i in range(2):
+        inpt = pywr.core.Input(model, name="Input {}".format(i))
+        lnk = pywr.core.Link(model, name="Link {}".format(i))
+        inpt.connect(lnk)
+        otpt = pywr.core.Output(model, name="Output {}".format(i))
+        lnk.connect(otpt)
+
+    # Create bidirectional link (i.e. a cycle)
+    model.nodes['Link 0'].connect(model.nodes['Link 1'])
+    model.nodes['Link 1'].connect(model.nodes['Link 0'])
+
+    return model
+
+
+def test_bidirectional_model(bidirectional_model):
+    """
+    Test the simple_linear_model with different basic input and output values
+    """
+    model = bidirectional_model
+    model.nodes["Input 0"].max_flow = 10.0
+    model.nodes["Input 1"].max_flow = 10.0
+    model.nodes["Output 0"].max_flow = 10.0
+    model.nodes["Output 1"].max_flow = 15.0
+    model.nodes["Output 0"].cost = -5.0
+    model.nodes["Output 1"].cost = -10.0
+    model.nodes["Link 0"].cost = 1.0
+    model.nodes["Link 1"].cost = 1.0
+
+    expected_node_results = {
+        "Input 0": 10.0,
+        "Input 1": 10.0,
+        "Link 0": 10.0,
+        "Link 1": 15.0,
+        "Output 0": 5.0,
+        "Output 1": 15.0,
+    }
+    assert_model(model, expected_node_results)
+
+
+def make_simple_model(supply_amplitude, demand, frequency, initial_volume):
+    """
+    Make a simple model,
+        supply -> reservoir -> demand.
+
+    supply is a annual cosine function with amplitude supply_amplitude and
+    frequency
+
+    """
+
+    model = pywr.core.Model()
+
+    S = supply_amplitude
+    w = frequency
+
+    class SupplyFunc(pywr.parameters.Parameter):
+        def value(self, ts, si):
+            # Take the mean flow of the day (i.e. offset by half a day)
+            t = ts.dayofyear - 0.5
+            v = S*np.cos(t*w)+S
+            return v
+
+    max_flow = SupplyFunc(model)
+    supply = pywr.core.Input(model, name='supply', max_flow=max_flow, min_flow=max_flow)
+    demand = pywr.core.Output(model, name='demand', max_flow=demand, cost=-10)
+    res = pywr.core.Storage(model, name='reservoir', max_volume=1e6,
+                            initial_volume=initial_volume)
+
+    supply_res_link = pywr.core.Link(model, name='link1')
+    res_demand_link = pywr.core.Link(model, name='link2')
+
+    supply.connect(supply_res_link)
+    supply_res_link.connect(res)
+    res.connect(res_demand_link)
+    res_demand_link.connect(demand)
+
+    return model
+
+
+def test_analytical():
+    """
+    Run the test model though a year with analytical solution values to
+    ensure reservoir just contains sufficient volume.
+    """
+
+    S = 100.0  # supply amplitude
+    D = S  # demand
+    w = 2*np.pi/365  # frequency (annual)
+    V0 = S/w  # initial reservoir level
+
+    model = make_simple_model(S, D, w, V0)
+
+    T = np.arange(1, 365)
+    V_anal = S*(np.sin(w*T)/w+T) - D*T + V0
+    V_model = np.empty(T.shape)
+
+    for i, t in enumerate(T):
+        model.step()
+        V_model[i] = model.nodes['reservoir'].volume[0]
+
+    # Relative error from initial volume
+    error = np.abs(V_model - V_anal) / V0
+    assert np.all(error < 1e-4)
```

### Comparing `pywr-1.8.0/tests/test_components.py` & `pywr-1.9.0/tests/test_components.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,116 +1,116 @@
-"""
-Tests for `pywr._component.Component` class behaviour and its use in `pywr.core.Model`.
-"""
-
-from pywr.core import Component, Model, ModelStructureError
-from collections import defaultdict
-from fixtures import simple_linear_model
-import pytest
-
-
-class DummyComponent(Component):
-    def __init__(self, *args, **kwargs):
-        super(DummyComponent, self).__init__(*args, **kwargs)
-        self.func_counts = defaultdict(lambda: 0)
-
-    def setup(self):
-        self.func_counts['setup'] += 1
-
-    def reset(self):
-        self.func_counts['reset'] += 1
-
-    def before(self):
-        self.func_counts['before'] += 1
-        # Check that all children have been called first.
-        for c in self.children:
-            assert c.func_counts['before'] == self.func_counts['before']
-        # Check that all parents have yet to called.
-        for p in self.parents:
-            assert p.func_counts['before'] + 1 == self.func_counts['before']
-
-
-    def after(self):
-        self.func_counts['after'] += 1
-
-    def finish(self):
-        self.func_counts['finish'] += 1
-
-
-def test_single_component(simple_linear_model):
-    """ Test that a component's methods are called the correct number of times """
-    m = simple_linear_model
-    nt = len(m.timestepper)
-    c = DummyComponent(m)
-
-    m.run()
-
-    assert c.func_counts['setup'] == 1
-    assert c.func_counts['reset'] == 1
-    assert c.func_counts['before'] == nt
-    assert c.func_counts['after'] == nt
-    assert c.func_counts['finish'] == 1
-
-
-def test_shared_parent_component(simple_linear_model):
-    """ Test two components sharing the same parent """
-
-    m = simple_linear_model
-    nt = len(m.timestepper)
-    c1 = DummyComponent(m, name='c1')
-    c2 = DummyComponent(m, name='c2')
-    cp = DummyComponent(m, name='cp')
-
-    c1.parents.add(cp)
-    c2.parents.add(cp)
-
-    assert cp in c1.parents
-    assert cp in c2.parents
-    assert c1 in cp.children
-    assert c2 in cp.children
-
-    m.run()
-
-    assert len(m.components) == 3
-
-    for c in m.components:
-        assert c.func_counts['setup'] == 1
-        assert c.func_counts['reset'] == 1
-        assert c.func_counts['before'] == nt
-        assert c.func_counts['after'] == nt
-        assert c.func_counts['finish'] == 1
-
-    # Simulate a change and re-run.
-    m.setup()
-    m.run()
-
-    assert len(m.components) == 3
-
-    for c in m.components:
-        assert c.func_counts['setup'] == 2
-        assert c.func_counts['reset'] == 3
-        assert c.func_counts['before'] == 2*nt
-        assert c.func_counts['after'] == 2*nt
-        assert c.func_counts['finish'] == 2
-
-
-def test_circular_components_error(simple_linear_model):
-    """ Test that circular components raise an error """
-    m = simple_linear_model
-    c1 = DummyComponent(m)
-    c2 = DummyComponent(m)
-
-    c1.parents.add(c2)
-    c2.parents.add(c1)
-
-    with pytest.raises(ModelStructureError):
-        m.run()
-
-
-def test_selfloop_components_error(simple_linear_model):
-    """ Test that self-looping components raise an error """
-    m = simple_linear_model
-    c1 = DummyComponent(m)
-    c1.parents.add(c1)
-
-    with pytest.raises(ModelStructureError):
-        m.run()
+"""
+Tests for `pywr._component.Component` class behaviour and its use in `pywr.core.Model`.
+"""
+
+from pywr.core import Component, Model, ModelStructureError
+from collections import defaultdict
+from fixtures import simple_linear_model
+import pytest
+
+
+class DummyComponent(Component):
+    def __init__(self, *args, **kwargs):
+        super(DummyComponent, self).__init__(*args, **kwargs)
+        self.func_counts = defaultdict(lambda: 0)
+
+    def setup(self):
+        self.func_counts['setup'] += 1
+
+    def reset(self):
+        self.func_counts['reset'] += 1
+
+    def before(self):
+        self.func_counts['before'] += 1
+        # Check that all children have been called first.
+        for c in self.children:
+            assert c.func_counts['before'] == self.func_counts['before']
+        # Check that all parents have yet to called.
+        for p in self.parents:
+            assert p.func_counts['before'] + 1 == self.func_counts['before']
+
+
+    def after(self):
+        self.func_counts['after'] += 1
+
+    def finish(self):
+        self.func_counts['finish'] += 1
+
+
+def test_single_component(simple_linear_model):
+    """ Test that a component's methods are called the correct number of times """
+    m = simple_linear_model
+    nt = len(m.timestepper)
+    c = DummyComponent(m)
+
+    m.run()
+
+    assert c.func_counts['setup'] == 1
+    assert c.func_counts['reset'] == 1
+    assert c.func_counts['before'] == nt
+    assert c.func_counts['after'] == nt
+    assert c.func_counts['finish'] == 1
+
+
+def test_shared_parent_component(simple_linear_model):
+    """ Test two components sharing the same parent """
+
+    m = simple_linear_model
+    nt = len(m.timestepper)
+    c1 = DummyComponent(m, name='c1')
+    c2 = DummyComponent(m, name='c2')
+    cp = DummyComponent(m, name='cp')
+
+    c1.parents.add(cp)
+    c2.parents.add(cp)
+
+    assert cp in c1.parents
+    assert cp in c2.parents
+    assert c1 in cp.children
+    assert c2 in cp.children
+
+    m.run()
+
+    assert len(m.components) == 3
+
+    for c in m.components:
+        assert c.func_counts['setup'] == 1
+        assert c.func_counts['reset'] == 1
+        assert c.func_counts['before'] == nt
+        assert c.func_counts['after'] == nt
+        assert c.func_counts['finish'] == 1
+
+    # Simulate a change and re-run.
+    m.setup()
+    m.run()
+
+    assert len(m.components) == 3
+
+    for c in m.components:
+        assert c.func_counts['setup'] == 2
+        assert c.func_counts['reset'] == 3
+        assert c.func_counts['before'] == 2*nt
+        assert c.func_counts['after'] == 2*nt
+        assert c.func_counts['finish'] == 2
+
+
+def test_circular_components_error(simple_linear_model):
+    """ Test that circular components raise an error """
+    m = simple_linear_model
+    c1 = DummyComponent(m)
+    c2 = DummyComponent(m)
+
+    c1.parents.add(c2)
+    c2.parents.add(c1)
+
+    with pytest.raises(ModelStructureError):
+        m.run()
+
+
+def test_selfloop_components_error(simple_linear_model):
+    """ Test that self-looping components raise an error """
+    m = simple_linear_model
+    c1 = DummyComponent(m)
+    c1.parents.add(c1)
+
+    with pytest.raises(ModelStructureError):
+        m.run()
```

### Comparing `pywr-1.8.0/tests/test_control_curves.py` & `pywr-1.9.0/tests/test_control_curves.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,546 +1,546 @@
-from pywr.core import Model, Storage, Link, ScenarioIndex, Timestep, Output
-from pywr.parameters import ConstantParameter, DailyProfileParameter, load_parameter
-from pywr.parameters.control_curves import ControlCurveParameter, ControlCurveInterpolatedParameter, \
-    PiecewiseLinearControlCurve, ControlCurvePiecewiseInterpolatedParameter
-from pywr.parameters._control_curves import _interpolate
-from pywr.recorders import NumpyArrayNodeRecorder, NumpyArrayStorageRecorder, assert_rec
-import numpy as np
-import pandas as pd
-from numpy.testing import assert_allclose
-import pytest
-import datetime
-import os
-from fixtures import simple_linear_model, simple_storage_model
-from helpers import load_model
-
-@pytest.fixture
-def model(simple_storage_model):
-    """ Modified simple_storage_model to be steady-state. """
-    i = simple_storage_model.nodes['Input']
-    i.max_flow = 0
-    o = simple_storage_model.nodes['Output']
-    o.max_flow = 0
-    s = simple_storage_model.nodes['Storage']
-    s.max_volume = 100.0
-    return simple_storage_model
-
-
-class TestPiecewiseLinearControlCurve:
-    def test_run(self, simple_storage_model):
-        model = simple_storage_model
-        storage_node = model.nodes["Storage"]
-        input_node = model.nodes["Input"]
-        output_node = model.nodes["Output"]
-        control_curve = ConstantParameter(model, 0.5)
-        parameter = PiecewiseLinearControlCurve(model, storage_node, control_curve, values=[(50, 100), (200, 500)], name="PLCC")
-        assert parameter.minimum == 0.0
-        assert parameter.maximum == 1.0
-
-        input_node.max_flow = 1.0
-        input_node.cost = 0
-        output_node.max_flow = 0.0
-        storage_node.initial_volume = 0.0
-        storage_node.max_volume = 100.0
-        storage_node.cost = -10
-
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.delta = 1
-        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
-
-        @assert_rec(model, parameter)
-        def expected_func(timestep, scenario_index):
-            volume = timestep.index
-            control_curve = 0.5
-            current_position = volume / storage_node.max_volume
-            if current_position > control_curve:
-                factor = (volume - 50) / 50
-                value = 200 + factor * (500 - 200)
-            else:
-                factor = volume / 50
-                value = 50 + factor * (100 - 50)
-            return value
-
-        model.run()
-
-    @pytest.mark.parametrize("configuration, expected_value", [
-        ((0.0, 0.0, 1.0, 50.0, 100.0), 50.0),
-        ((1.0, 0.0, 1.0, 50.0, 100.0), 100.0),
-        ((0.5, 0.0, 1.0, 50.0, 100.0), 75.0),
-        ((0.0, 0.5, 1.0, 50.0, 100.0), 50.0),
-        ((0.75, 0.5, 1.0, 50.0, 100.0), 75.0),
-    ])
-    def test_interpolation(self, configuration, expected_value):
-        current_position, lower_bound, upper_bound, lower_value, upper_value = configuration
-        assert _interpolate(current_position, lower_bound, upper_bound, lower_value, upper_value) == expected_value
-
-    def test_json(self, simple_storage_model):
-        model = simple_storage_model
-        control_curve = ConstantParameter(model, 0.5, name="cc")
-        parameter_data = {
-            "type": "piecewiselinearcontrolcurve",
-            "storage_node": "Storage",
-            "control_curve": "cc",
-            "minimum": 0.2,
-            "maximum": 0.7,
-            "values": [[5, 10], [100, 200]]
-        }
-        parameter = load_parameter(model, parameter_data)
-        assert parameter.minimum == 0.2
-        assert parameter.maximum == 0.7
-        assert parameter.below_lower == 5
-        assert parameter.below_upper == 10
-        assert parameter.above_lower == 100
-        assert parameter.above_upper == 200
-        assert parameter.control_curve is control_curve
-        assert parameter.storage_node is model.nodes["Storage"]
-
-
-class TestPiecewiseControlCurveParameter:
-    """Tests for ControlCurveParameter """
-
-    @staticmethod
-    def _assert_results(m, s):
-        """ Correct results for the following tests """
-
-        @assert_rec(m, s.cost)
-        def expected_func(timestep, scenario_index):
-            v = s.initial_volume
-            if v >= 80.0:
-                expected = 1.0
-            elif v >= 60:
-                expected = 0.7
-            else:
-                expected = 0.4
-            return expected
-
-        for initial_volume in (90, 70, 30):
-            s.initial_volume = initial_volume
-            m.run()
-
-
-    def test_with_values(self, model):
-        """Test with `values` keyword argument"""
-        m = model
-        s = m.nodes['Storage']
-
-        # Return 10.0 when above 0.0 when below
-        s.cost = ControlCurveParameter(m, s, [0.8, 0.6], [1.0, 0.7, 0.4])
-        self._assert_results(m, s)
-
-    def test_with_parameters(self, model):
-        """ Test with `parameters` keyword argument. """
-        m = model
-        s = m.nodes['Storage']
-
-        # Two different control curves
-        cc = [ConstantParameter(model, 0.8), ConstantParameter(model, 0.6)]
-        # Three different parameters to return
-        params = [
-            ConstantParameter(model, 1.0), ConstantParameter(model, 0.7), ConstantParameter(model, 0.4)
-        ]
-        s.cost = ControlCurveParameter(model, s, cc, parameters=params)
-
-        self._assert_results(m, s)
-
-    def test_values_load(self, model):
-        """ Test load of float lists. """
-
-        m = model
-        s = m.nodes['Storage']
-
-        data = {
-            "type": "controlcurve",
-            "control_curves": [0.8, 0.6],
-            "values": [1.0, 0.7, 0.4],
-            "storage_node": "Storage"
-        }
-
-        s.cost = p = load_parameter(model, data)
-        assert isinstance(p, ControlCurveParameter)
-        self._assert_results(m, s)
-
-    def test_parameters_load(self, model):
-        """ Test load of parameter lists for 'control_curves' and 'parameters' keys. """
-
-        m = model
-        s = m.nodes['Storage']
-
-        data = {
-            "type": "controlcurve",
-            "storage_node": "Storage",
-            "control_curves": [
-                {
-                    "type": "constant",
-                    "value": 0.8
-                },
-                {
-                    "type": "monthlyprofile",
-                    "values": [0.6]*12
-                }
-            ],
-            "parameters": [
-                {
-                    "type": "constant",
-                    "value": 1.0,
-                },
-                {
-                    "type": "constant",
-                    "value": 0.7
-                },
-                {
-                    "type": "constant",
-                    "value": 0.4
-                }
-            ]
-        }
-
-        s.cost = p = load_parameter(model, data)
-        assert isinstance(p, ControlCurveParameter)
-        self._assert_results(m, s)
-
-    def test_single_cc_load(self, model):
-        """ Test load from dict with 'control_curve' key
-
-        This is different to the above test by using singular 'control_curve' key in the dict
-        """
-        m = model
-        s = m.nodes['Storage']
-
-        data = {
-            "type": "controlcurve",
-            "storage_node": "Storage",
-            "control_curve": 0.8,
-        }
-
-        s.cost = p = load_parameter(model, data)
-        assert isinstance(p, ControlCurveParameter)
-
-        @assert_rec(m, p)
-        def expected_func(timestep, scenario_index):
-            v = s.initial_volume
-            if v >= 80.0:
-                expected = 0
-            else:
-                expected = 1
-            return expected
-
-        for initial_volume in (90, 70):
-            s.initial_volume = initial_volume
-            m.run()
-
-    def test_with_nonstorage(self, model):
-        """ Test usage on non-`Storage` node. """
-        # Now test if the parameter is used on a non storage node
-        m = model
-        s = m.nodes['Storage']
-
-        l = Link(m, 'Link')
-        cc = ConstantParameter(model, 0.8)
-        l.cost = ControlCurveParameter(model, s, cc, [10.0, 0.0])
-
-        @assert_rec(m, l.cost)
-        def expected_func(timestep, scenario_index):
-            v = s.initial_volume
-            if v >= 80.0:
-                expected = 10.0
-            else:
-                expected = 0.0
-            return expected
-
-        for initial_volume in (90, 70):
-            s.initial_volume = initial_volume
-            m.run()
-
-
-    def test_with_nonstorage_load(self, model):
-        """ Test load from dict with 'storage_node' key. """
-        m = model
-        s = m.nodes['Storage']
-        l = Link(m, 'Link')
-
-        data = {
-            "type": "controlcurve",
-            "control_curve": 0.8,
-            "values": [10.0, 0.0],
-            "storage_node": "Storage"
-        }
-
-        l.cost = p = load_parameter(model, data)
-        assert isinstance(p, ControlCurveParameter)
-
-        @assert_rec(m, l.cost)
-        def expected_func(timestep, scenario_index):
-            v = s.initial_volume
-            if v >= 80.0:
-                expected = 10.0
-            else:
-                expected = 0.0
-            return expected
-
-        for initial_volume in (90, 70):
-            s.initial_volume = initial_volume
-            m.run()
-
-
-@pytest.mark.parametrize("use_parameters", [False, True])
-def test_control_curve_interpolated(model, use_parameters):
-    m = model
-    m.timestepper.delta = 200
-
-    s = m.nodes['Storage']
-    o = m.nodes['Output']
-    s.connect(o)
-
-    cc = ConstantParameter(model, 0.8)
-    values = [20.0, 5.0, 0.0]
-
-    if use_parameters:
-        # Create the parameter using parameters for the values
-        parameters = [ConstantParameter(model, v) for v in values]
-        s.cost = p = ControlCurveInterpolatedParameter(model, s, cc, parameters=parameters)
-    else:
-        # Create the parameter using a list of values
-        s.cost = p = ControlCurveInterpolatedParameter(model, s, cc, values)
-
-    @assert_rec(model, p)
-    def expected_func(timestep, scenario_index):
-        v = s.initial_volume
-        c = cc.value(timestep, scenario_index)
-        if c == 1.0 and v == 100.0:
-            expected = values[1]
-        elif c == 0.0 and v == 0.0:
-            expected = values[1]
-        else:
-            expected = np.interp(v/100.0, [0.0, c, 1.0], values[::-1])
-        return expected
-
-    for control_curve in (0.0, 0.8, 1.0):
-        cc.set_double_variables(np.array([control_curve,]))
-        for initial_volume in (0.0, 10.0, 50.0, 80.0, 90.0, 100.0):
-            s.initial_volume = initial_volume
-            model.run()
-
-
-@pytest.mark.parametrize("use_parameters", [False, True])
-def test_control_curve_interpolated_json(use_parameters):
-    # this is a little hack-y, as the parameters don't provide access to their
-    # data once they've been initalised
-    if use_parameters:
-        model = load_model("reservoir_with_cc_param_values.json")
-    else:
-        model = load_model("reservoir_with_cc.json")
-    reservoir1 = model.nodes["reservoir1"]
-    model.setup()
-    path = os.path.join(os.path.dirname(__file__), "models", "control_curve.csv")
-    control_curve = pd.read_csv(path)["Control Curve"].values
-    values = [-8, -6, -4]
-
-    @assert_rec(model, reservoir1.cost)
-    def expected_cost(timestep, si):
-        # calculate expected cost manually and compare to parameter output
-        volume_factor = reservoir1._current_pc[si.global_id]
-        cc = control_curve[timestep.index]
-        return np.interp(volume_factor, [0.0, cc, 1.0], values[::-1])
-    model.run()
-
-
-@pytest.mark.xfail(reason="Circular dependency in the JSON definition. "
-                          "See GitHub issue #380: https://github.com/pywr/pywr/issues/380")
-def test_circular_control_curve_interpolated_json():
-    # this is a little hack-y, as the parameters don't provide access to their
-    # data once they've been initalised
-    model = load_model("reservoir_with_circular_cc.json")
-    reservoir1 = model.nodes["reservoir1"]
-    model.setup()
-    path = os.path.join(os.path.dirname(__file__), "models", "control_curve.csv")
-    control_curve = pd.read_csv(path)["Control Curve"].values
-    values = [-8, -6, -4]
-
-    @assert_rec(model, reservoir1.cost)
-    def expected_cost(timestep, si):
-        # calculate expected cost manually and compare to parameter output
-        volume_factor = reservoir1._current_pc[si.global_id]
-        cc = control_curve[timestep.index]
-        return np.interp(volume_factor, [0.0, cc, 1.0], values[::-1])
-    model.run()
-
-
-def test_demand_saving_with_indexed_array():
-    """Test demand saving based on reservoir control curves
-
-    This is a relatively complex test to pass due to the large number of
-    dependencies of the parameters actually being tested. The test is an
-    example of how demand savings can be applied in times of drought based
-    on the state of a reservoir.
-    """
-
-    model = load_model("demand_saving2.json")
-
-    model.timestepper.end = pd.Timestamp("2016-01-31")
-
-    rec_demand = NumpyArrayNodeRecorder(model, model.nodes["Demand"])
-    rec_storage = NumpyArrayStorageRecorder(model, model.nodes["Reservoir"])
-
-    model.check()
-    model.run()
-
-    max_volume = model.nodes["Reservoir"].max_volume
-
-    # model starts with no demand saving
-    demand_baseline = 50.0
-    demand_factor = 0.9  # jan-apr
-    demand_saving = 1.0
-    assert_allclose(rec_demand.data[0, 0], demand_baseline * demand_factor * demand_saving)
-
-    # first control curve breached
-    demand_saving = 0.95
-    assert(rec_storage.data[4, 0] < (0.8 * max_volume) )
-    assert_allclose(rec_demand.data[5, 0], demand_baseline * demand_factor * demand_saving)
-
-    # second control curve breached
-    demand_saving = 0.5
-    assert(rec_storage.data[11, 0] < (0.5 * max_volume) )
-    assert_allclose(rec_demand.data[12, 0], demand_baseline * demand_factor * demand_saving)
-
-
-def test_demand_saving_with_indexed_array_from_hdf():
-    """Test demand saving based on a predefined demand saving level in a HDF file."""
-    model = load_model("demand_saving_hdf.json")
-
-    model.timestepper.end = pd.Timestamp("2016-01-31")
-
-    rec_demand = NumpyArrayNodeRecorder(model, model.nodes["Demand"])
-    rec_storage = NumpyArrayStorageRecorder(model, model.nodes["Reservoir"])
-
-    model.check()
-    model.run()
-
-    max_volume = model.nodes["Reservoir"].max_volume
-
-    # model starts with no demand saving
-    demand_baseline = 50.0
-    demand_saving = 1.0
-    assert_allclose(rec_demand.data[0, 0], demand_baseline * demand_saving)
-
-    # first control curve breached
-    demand_saving = 0.8
-    assert_allclose(rec_demand.data[11, 0], demand_baseline * demand_saving)
-
-    # second control curve breached
-    demand_saving = 0.5
-    assert_allclose(rec_demand.data[12, 0], demand_baseline * demand_saving)
-
-    # second control curve breached
-    demand_saving = 0.25
-    assert_allclose(rec_demand.data[13, 0], demand_baseline * demand_saving)
-
-
-class TestControlCurvePiecewiseInterpolatedParameter:
-    """Tests for `ControlCurvePiecewiseInterpolatedParameter` """
-    def test_single_control_curve(self, simple_storage_model):
-        """Test `ControlCurvePiecewiseInterpolatedParameter` with one control curve. """
-        model = simple_storage_model
-        storage_node = model.nodes["Storage"]
-        input_node = model.nodes["Input"]
-        output_node = model.nodes["Output"]
-
-        control_curves = [
-            ConstantParameter(model, 0.5),
-        ]
-
-        parameter = ControlCurvePiecewiseInterpolatedParameter(model, storage_node, control_curves,
-                                                               [(500, 200), (100, 50)], name="CCPIP")
-        assert parameter.minimum == 0.0
-        assert parameter.maximum == 1.0
-
-        input_node.max_flow = 1.0
-        input_node.cost = 0
-        output_node.max_flow = 0.0
-        storage_node.initial_volume = 0.0
-        storage_node.max_volume = 100.0
-        storage_node.cost = -10
-
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.delta = 1
-        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
-
-        @assert_rec(model, parameter)
-        def expected_func(timestep, scenario_index):
-            volume = timestep.index
-            control_curve = 0.5
-            current_position = volume / storage_node.max_volume
-            if current_position >= control_curve:
-                factor = (volume - 50) / 50
-                value = 200 + factor * (500 - 200)
-            else:
-                factor = volume / 50
-                value = 50 + factor * (100 - 50)
-            return value
-
-        model.run()
-
-    def test_two_control_curves(self, simple_storage_model):
-        """Test `ControlCurvePiecewiseInterpolatedParameter` with two control curves. """
-        model = simple_storage_model
-        storage_node = model.nodes["Storage"]
-        input_node = model.nodes["Input"]
-        output_node = model.nodes["Output"]
-
-        control_curves = [
-            ConstantParameter(model, 0.75),
-            ConstantParameter(model, 0.25),
-        ]
-
-        parameter = ControlCurvePiecewiseInterpolatedParameter(model, storage_node, control_curves,
-                                                               [(500, 200), (100, 50), (0, -100)], name="CCPIP")
-        assert parameter.minimum == 0.0
-        assert parameter.maximum == 1.0
-
-        input_node.max_flow = 1.0
-        input_node.cost = 0
-        output_node.max_flow = 0.0
-        storage_node.initial_volume = 0.0
-        storage_node.max_volume = 100.0
-        storage_node.cost = -10
-
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.delta = 1
-        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
-
-        @assert_rec(model, parameter)
-        def expected_func(timestep, scenario_index):
-            volume = timestep.index
-
-            current_position = volume / storage_node.max_volume
-            if current_position >= 0.75:
-                factor = (volume - 75) / 25
-                value = 200 + factor * (500 - 200)
-            elif current_position >= 0.25:
-                factor = (volume - 25) / 50
-                value = 50 + factor * (100 - 50)
-            else:
-                factor = volume / 25
-                value = -100 + factor * 100
-            return value
-
-        model.run()
-
-    def test_json(self, simple_storage_model):
-        """Test loading from JSON data."""
-        model = simple_storage_model
-        control_curve1 = ConstantParameter(model, 0.5, name="cc1")
-        control_curve2 = ConstantParameter(model, 0.25, name="cc2")
-        parameter_data = {
-            "type": "controlcurvepiecewiseinterpolated",
-            "storage_node": "Storage",
-            "control_curves": ["cc1", "cc2"],
-            "minimum": 0.2,
-            "maximum": 0.7,
-            "values": [[200, 100], [10, 5], [0, -10]]
-        }
-        parameter = load_parameter(model, parameter_data)
-        assert parameter.minimum == 0.2
-        assert parameter.maximum == 0.7
-        np.testing.assert_allclose(parameter.values, [[200, 100], [10, 5], [0, -10]])
-        assert parameter.control_curves == [control_curve1, control_curve2]
-        assert parameter.storage_node is model.nodes["Storage"]
+from pywr.core import Model, Storage, Link, ScenarioIndex, Timestep, Output
+from pywr.parameters import ConstantParameter, DailyProfileParameter, load_parameter
+from pywr.parameters.control_curves import ControlCurveParameter, ControlCurveInterpolatedParameter, \
+    PiecewiseLinearControlCurve, ControlCurvePiecewiseInterpolatedParameter
+from pywr.parameters._control_curves import _interpolate
+from pywr.recorders import NumpyArrayNodeRecorder, NumpyArrayStorageRecorder, assert_rec
+import numpy as np
+import pandas as pd
+from numpy.testing import assert_allclose
+import pytest
+import datetime
+import os
+from fixtures import simple_linear_model, simple_storage_model
+from helpers import load_model
+
+@pytest.fixture
+def model(simple_storage_model):
+    """ Modified simple_storage_model to be steady-state. """
+    i = simple_storage_model.nodes['Input']
+    i.max_flow = 0
+    o = simple_storage_model.nodes['Output']
+    o.max_flow = 0
+    s = simple_storage_model.nodes['Storage']
+    s.max_volume = 100.0
+    return simple_storage_model
+
+
+class TestPiecewiseLinearControlCurve:
+    def test_run(self, simple_storage_model):
+        model = simple_storage_model
+        storage_node = model.nodes["Storage"]
+        input_node = model.nodes["Input"]
+        output_node = model.nodes["Output"]
+        control_curve = ConstantParameter(model, 0.5)
+        parameter = PiecewiseLinearControlCurve(model, storage_node, control_curve, values=[(50, 100), (200, 500)], name="PLCC")
+        assert parameter.minimum == 0.0
+        assert parameter.maximum == 1.0
+
+        input_node.max_flow = 1.0
+        input_node.cost = 0
+        output_node.max_flow = 0.0
+        storage_node.initial_volume = 0.0
+        storage_node.max_volume = 100.0
+        storage_node.cost = -10
+
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.delta = 1
+        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
+
+        @assert_rec(model, parameter)
+        def expected_func(timestep, scenario_index):
+            volume = timestep.index
+            control_curve = 0.5
+            current_position = volume / storage_node.max_volume
+            if current_position > control_curve:
+                factor = (volume - 50) / 50
+                value = 200 + factor * (500 - 200)
+            else:
+                factor = volume / 50
+                value = 50 + factor * (100 - 50)
+            return value
+
+        model.run()
+
+    @pytest.mark.parametrize("configuration, expected_value", [
+        ((0.0, 0.0, 1.0, 50.0, 100.0), 50.0),
+        ((1.0, 0.0, 1.0, 50.0, 100.0), 100.0),
+        ((0.5, 0.0, 1.0, 50.0, 100.0), 75.0),
+        ((0.0, 0.5, 1.0, 50.0, 100.0), 50.0),
+        ((0.75, 0.5, 1.0, 50.0, 100.0), 75.0),
+    ])
+    def test_interpolation(self, configuration, expected_value):
+        current_position, lower_bound, upper_bound, lower_value, upper_value = configuration
+        assert _interpolate(current_position, lower_bound, upper_bound, lower_value, upper_value) == expected_value
+
+    def test_json(self, simple_storage_model):
+        model = simple_storage_model
+        control_curve = ConstantParameter(model, 0.5, name="cc")
+        parameter_data = {
+            "type": "piecewiselinearcontrolcurve",
+            "storage_node": "Storage",
+            "control_curve": "cc",
+            "minimum": 0.2,
+            "maximum": 0.7,
+            "values": [[5, 10], [100, 200]]
+        }
+        parameter = load_parameter(model, parameter_data)
+        assert parameter.minimum == 0.2
+        assert parameter.maximum == 0.7
+        assert parameter.below_lower == 5
+        assert parameter.below_upper == 10
+        assert parameter.above_lower == 100
+        assert parameter.above_upper == 200
+        assert parameter.control_curve is control_curve
+        assert parameter.storage_node is model.nodes["Storage"]
+
+
+class TestPiecewiseControlCurveParameter:
+    """Tests for ControlCurveParameter """
+
+    @staticmethod
+    def _assert_results(m, s):
+        """ Correct results for the following tests """
+
+        @assert_rec(m, s.cost)
+        def expected_func(timestep, scenario_index):
+            v = s.initial_volume
+            if v >= 80.0:
+                expected = 1.0
+            elif v >= 60:
+                expected = 0.7
+            else:
+                expected = 0.4
+            return expected
+
+        for initial_volume in (90, 70, 30):
+            s.initial_volume = initial_volume
+            m.run()
+
+
+    def test_with_values(self, model):
+        """Test with `values` keyword argument"""
+        m = model
+        s = m.nodes['Storage']
+
+        # Return 10.0 when above 0.0 when below
+        s.cost = ControlCurveParameter(m, s, [0.8, 0.6], [1.0, 0.7, 0.4])
+        self._assert_results(m, s)
+
+    def test_with_parameters(self, model):
+        """ Test with `parameters` keyword argument. """
+        m = model
+        s = m.nodes['Storage']
+
+        # Two different control curves
+        cc = [ConstantParameter(model, 0.8), ConstantParameter(model, 0.6)]
+        # Three different parameters to return
+        params = [
+            ConstantParameter(model, 1.0), ConstantParameter(model, 0.7), ConstantParameter(model, 0.4)
+        ]
+        s.cost = ControlCurveParameter(model, s, cc, parameters=params)
+
+        self._assert_results(m, s)
+
+    def test_values_load(self, model):
+        """ Test load of float lists. """
+
+        m = model
+        s = m.nodes['Storage']
+
+        data = {
+            "type": "controlcurve",
+            "control_curves": [0.8, 0.6],
+            "values": [1.0, 0.7, 0.4],
+            "storage_node": "Storage"
+        }
+
+        s.cost = p = load_parameter(model, data)
+        assert isinstance(p, ControlCurveParameter)
+        self._assert_results(m, s)
+
+    def test_parameters_load(self, model):
+        """ Test load of parameter lists for 'control_curves' and 'parameters' keys. """
+
+        m = model
+        s = m.nodes['Storage']
+
+        data = {
+            "type": "controlcurve",
+            "storage_node": "Storage",
+            "control_curves": [
+                {
+                    "type": "constant",
+                    "value": 0.8
+                },
+                {
+                    "type": "monthlyprofile",
+                    "values": [0.6]*12
+                }
+            ],
+            "parameters": [
+                {
+                    "type": "constant",
+                    "value": 1.0,
+                },
+                {
+                    "type": "constant",
+                    "value": 0.7
+                },
+                {
+                    "type": "constant",
+                    "value": 0.4
+                }
+            ]
+        }
+
+        s.cost = p = load_parameter(model, data)
+        assert isinstance(p, ControlCurveParameter)
+        self._assert_results(m, s)
+
+    def test_single_cc_load(self, model):
+        """ Test load from dict with 'control_curve' key
+
+        This is different to the above test by using singular 'control_curve' key in the dict
+        """
+        m = model
+        s = m.nodes['Storage']
+
+        data = {
+            "type": "controlcurve",
+            "storage_node": "Storage",
+            "control_curve": 0.8,
+        }
+
+        s.cost = p = load_parameter(model, data)
+        assert isinstance(p, ControlCurveParameter)
+
+        @assert_rec(m, p)
+        def expected_func(timestep, scenario_index):
+            v = s.initial_volume
+            if v >= 80.0:
+                expected = 0
+            else:
+                expected = 1
+            return expected
+
+        for initial_volume in (90, 70):
+            s.initial_volume = initial_volume
+            m.run()
+
+    def test_with_nonstorage(self, model):
+        """ Test usage on non-`Storage` node. """
+        # Now test if the parameter is used on a non storage node
+        m = model
+        s = m.nodes['Storage']
+
+        l = Link(m, 'Link')
+        cc = ConstantParameter(model, 0.8)
+        l.cost = ControlCurveParameter(model, s, cc, [10.0, 0.0])
+
+        @assert_rec(m, l.cost)
+        def expected_func(timestep, scenario_index):
+            v = s.initial_volume
+            if v >= 80.0:
+                expected = 10.0
+            else:
+                expected = 0.0
+            return expected
+
+        for initial_volume in (90, 70):
+            s.initial_volume = initial_volume
+            m.run()
+
+
+    def test_with_nonstorage_load(self, model):
+        """ Test load from dict with 'storage_node' key. """
+        m = model
+        s = m.nodes['Storage']
+        l = Link(m, 'Link')
+
+        data = {
+            "type": "controlcurve",
+            "control_curve": 0.8,
+            "values": [10.0, 0.0],
+            "storage_node": "Storage"
+        }
+
+        l.cost = p = load_parameter(model, data)
+        assert isinstance(p, ControlCurveParameter)
+
+        @assert_rec(m, l.cost)
+        def expected_func(timestep, scenario_index):
+            v = s.initial_volume
+            if v >= 80.0:
+                expected = 10.0
+            else:
+                expected = 0.0
+            return expected
+
+        for initial_volume in (90, 70):
+            s.initial_volume = initial_volume
+            m.run()
+
+
+@pytest.mark.parametrize("use_parameters", [False, True])
+def test_control_curve_interpolated(model, use_parameters):
+    m = model
+    m.timestepper.delta = 200
+
+    s = m.nodes['Storage']
+    o = m.nodes['Output']
+    s.connect(o)
+
+    cc = ConstantParameter(model, 0.8)
+    values = [20.0, 5.0, 0.0]
+
+    if use_parameters:
+        # Create the parameter using parameters for the values
+        parameters = [ConstantParameter(model, v) for v in values]
+        s.cost = p = ControlCurveInterpolatedParameter(model, s, cc, parameters=parameters)
+    else:
+        # Create the parameter using a list of values
+        s.cost = p = ControlCurveInterpolatedParameter(model, s, cc, values)
+
+    @assert_rec(model, p)
+    def expected_func(timestep, scenario_index):
+        v = s.initial_volume
+        c = cc.value(timestep, scenario_index)
+        if c == 1.0 and v == 100.0:
+            expected = values[1]
+        elif c == 0.0 and v == 0.0:
+            expected = values[1]
+        else:
+            expected = np.interp(v/100.0, [0.0, c, 1.0], values[::-1])
+        return expected
+
+    for control_curve in (0.0, 0.8, 1.0):
+        cc.set_double_variables(np.array([control_curve,]))
+        for initial_volume in (0.0, 10.0, 50.0, 80.0, 90.0, 100.0):
+            s.initial_volume = initial_volume
+            model.run()
+
+
+@pytest.mark.parametrize("use_parameters", [False, True])
+def test_control_curve_interpolated_json(use_parameters):
+    # this is a little hack-y, as the parameters don't provide access to their
+    # data once they've been initalised
+    if use_parameters:
+        model = load_model("reservoir_with_cc_param_values.json")
+    else:
+        model = load_model("reservoir_with_cc.json")
+    reservoir1 = model.nodes["reservoir1"]
+    model.setup()
+    path = os.path.join(os.path.dirname(__file__), "models", "control_curve.csv")
+    control_curve = pd.read_csv(path)["Control Curve"].values
+    values = [-8, -6, -4]
+
+    @assert_rec(model, reservoir1.cost)
+    def expected_cost(timestep, si):
+        # calculate expected cost manually and compare to parameter output
+        volume_factor = reservoir1._current_pc[si.global_id]
+        cc = control_curve[timestep.index]
+        return np.interp(volume_factor, [0.0, cc, 1.0], values[::-1])
+    model.run()
+
+
+@pytest.mark.xfail(reason="Circular dependency in the JSON definition. "
+                          "See GitHub issue #380: https://github.com/pywr/pywr/issues/380")
+def test_circular_control_curve_interpolated_json():
+    # this is a little hack-y, as the parameters don't provide access to their
+    # data once they've been initalised
+    model = load_model("reservoir_with_circular_cc.json")
+    reservoir1 = model.nodes["reservoir1"]
+    model.setup()
+    path = os.path.join(os.path.dirname(__file__), "models", "control_curve.csv")
+    control_curve = pd.read_csv(path)["Control Curve"].values
+    values = [-8, -6, -4]
+
+    @assert_rec(model, reservoir1.cost)
+    def expected_cost(timestep, si):
+        # calculate expected cost manually and compare to parameter output
+        volume_factor = reservoir1._current_pc[si.global_id]
+        cc = control_curve[timestep.index]
+        return np.interp(volume_factor, [0.0, cc, 1.0], values[::-1])
+    model.run()
+
+
+def test_demand_saving_with_indexed_array():
+    """Test demand saving based on reservoir control curves
+
+    This is a relatively complex test to pass due to the large number of
+    dependencies of the parameters actually being tested. The test is an
+    example of how demand savings can be applied in times of drought based
+    on the state of a reservoir.
+    """
+
+    model = load_model("demand_saving2.json")
+
+    model.timestepper.end = pd.Timestamp("2016-01-31")
+
+    rec_demand = NumpyArrayNodeRecorder(model, model.nodes["Demand"])
+    rec_storage = NumpyArrayStorageRecorder(model, model.nodes["Reservoir"])
+
+    model.check()
+    model.run()
+
+    max_volume = model.nodes["Reservoir"].max_volume
+
+    # model starts with no demand saving
+    demand_baseline = 50.0
+    demand_factor = 0.9  # jan-apr
+    demand_saving = 1.0
+    assert_allclose(rec_demand.data[0, 0], demand_baseline * demand_factor * demand_saving)
+
+    # first control curve breached
+    demand_saving = 0.95
+    assert(rec_storage.data[4, 0] < (0.8 * max_volume) )
+    assert_allclose(rec_demand.data[5, 0], demand_baseline * demand_factor * demand_saving)
+
+    # second control curve breached
+    demand_saving = 0.5
+    assert(rec_storage.data[11, 0] < (0.5 * max_volume) )
+    assert_allclose(rec_demand.data[12, 0], demand_baseline * demand_factor * demand_saving)
+
+
+def test_demand_saving_with_indexed_array_from_hdf():
+    """Test demand saving based on a predefined demand saving level in a HDF file."""
+    model = load_model("demand_saving_hdf.json")
+
+    model.timestepper.end = pd.Timestamp("2016-01-31")
+
+    rec_demand = NumpyArrayNodeRecorder(model, model.nodes["Demand"])
+    rec_storage = NumpyArrayStorageRecorder(model, model.nodes["Reservoir"])
+
+    model.check()
+    model.run()
+
+    max_volume = model.nodes["Reservoir"].max_volume
+
+    # model starts with no demand saving
+    demand_baseline = 50.0
+    demand_saving = 1.0
+    assert_allclose(rec_demand.data[0, 0], demand_baseline * demand_saving)
+
+    # first control curve breached
+    demand_saving = 0.8
+    assert_allclose(rec_demand.data[11, 0], demand_baseline * demand_saving)
+
+    # second control curve breached
+    demand_saving = 0.5
+    assert_allclose(rec_demand.data[12, 0], demand_baseline * demand_saving)
+
+    # second control curve breached
+    demand_saving = 0.25
+    assert_allclose(rec_demand.data[13, 0], demand_baseline * demand_saving)
+
+
+class TestControlCurvePiecewiseInterpolatedParameter:
+    """Tests for `ControlCurvePiecewiseInterpolatedParameter` """
+    def test_single_control_curve(self, simple_storage_model):
+        """Test `ControlCurvePiecewiseInterpolatedParameter` with one control curve. """
+        model = simple_storage_model
+        storage_node = model.nodes["Storage"]
+        input_node = model.nodes["Input"]
+        output_node = model.nodes["Output"]
+
+        control_curves = [
+            ConstantParameter(model, 0.5),
+        ]
+
+        parameter = ControlCurvePiecewiseInterpolatedParameter(model, storage_node, control_curves,
+                                                               [(500, 200), (100, 50)], name="CCPIP")
+        assert parameter.minimum == 0.0
+        assert parameter.maximum == 1.0
+
+        input_node.max_flow = 1.0
+        input_node.cost = 0
+        output_node.max_flow = 0.0
+        storage_node.initial_volume = 0.0
+        storage_node.max_volume = 100.0
+        storage_node.cost = -10
+
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.delta = 1
+        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
+
+        @assert_rec(model, parameter)
+        def expected_func(timestep, scenario_index):
+            volume = timestep.index
+            control_curve = 0.5
+            current_position = volume / storage_node.max_volume
+            if current_position >= control_curve:
+                factor = (volume - 50) / 50
+                value = 200 + factor * (500 - 200)
+            else:
+                factor = volume / 50
+                value = 50 + factor * (100 - 50)
+            return value
+
+        model.run()
+
+    def test_two_control_curves(self, simple_storage_model):
+        """Test `ControlCurvePiecewiseInterpolatedParameter` with two control curves. """
+        model = simple_storage_model
+        storage_node = model.nodes["Storage"]
+        input_node = model.nodes["Input"]
+        output_node = model.nodes["Output"]
+
+        control_curves = [
+            ConstantParameter(model, 0.75),
+            ConstantParameter(model, 0.25),
+        ]
+
+        parameter = ControlCurvePiecewiseInterpolatedParameter(model, storage_node, control_curves,
+                                                               [(500, 200), (100, 50), (0, -100)], name="CCPIP")
+        assert parameter.minimum == 0.0
+        assert parameter.maximum == 1.0
+
+        input_node.max_flow = 1.0
+        input_node.cost = 0
+        output_node.max_flow = 0.0
+        storage_node.initial_volume = 0.0
+        storage_node.max_volume = 100.0
+        storage_node.cost = -10
+
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.delta = 1
+        model.timestepper.end = model.timestepper.start + model.timestepper.offset*100
+
+        @assert_rec(model, parameter)
+        def expected_func(timestep, scenario_index):
+            volume = timestep.index
+
+            current_position = volume / storage_node.max_volume
+            if current_position >= 0.75:
+                factor = (volume - 75) / 25
+                value = 200 + factor * (500 - 200)
+            elif current_position >= 0.25:
+                factor = (volume - 25) / 50
+                value = 50 + factor * (100 - 50)
+            else:
+                factor = volume / 25
+                value = -100 + factor * 100
+            return value
+
+        model.run()
+
+    def test_json(self, simple_storage_model):
+        """Test loading from JSON data."""
+        model = simple_storage_model
+        control_curve1 = ConstantParameter(model, 0.5, name="cc1")
+        control_curve2 = ConstantParameter(model, 0.25, name="cc2")
+        parameter_data = {
+            "type": "controlcurvepiecewiseinterpolated",
+            "storage_node": "Storage",
+            "control_curves": ["cc1", "cc2"],
+            "minimum": 0.2,
+            "maximum": 0.7,
+            "values": [[200, 100], [10, 5], [0, -10]]
+        }
+        parameter = load_parameter(model, parameter_data)
+        assert parameter.minimum == 0.2
+        assert parameter.maximum == 0.7
+        np.testing.assert_allclose(parameter.values, [[200, 100], [10, 5], [0, -10]])
+        assert parameter.control_curves == [control_curve1, control_curve2]
+        assert parameter.storage_node is model.nodes["Storage"]
```

### Comparing `pywr-1.8.0/tests/test_core.py` & `pywr-1.9.0/tests/test_core.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,566 +1,566 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-
-import datetime
-import pytest
-from fixtures import *
-from helpers import *
-from pywr._core import Timestep, ScenarioIndex
-import pandas
-from pywr.core import *
-from pywr.domains.river import *
-from pywr.parameters import Parameter, ConstantParameter, DataFrameParameter, AggregatedParameter
-from pywr.recorders import assert_rec, AssertionRecorder
-
-TEST_FOLDER = os.path.dirname(__file__)
-
-def test_names():
-    '''Test node names'''
-    model = Model()
-
-    node1 = Input(model, name='A')
-    node2 = Output(model, name='B')
-    assert(model.nodes['A'] is node1)
-    assert(model.nodes['B'] is node2)
-
-    nodes = sorted(model.nodes, key=lambda node: node.name)
-    assert(nodes == [node1, node2])
-
-    # rename node
-    node1.name = 'C'
-    assert(model.nodes['C'] is node1)
-    assert('A' not in model.nodes)
-
-    # attempt name collision (via rename)
-    with pytest.raises(ValueError):
-        node2.name = 'C'
-
-    # attempt name collision (via new)
-    with pytest.raises(ValueError):
-        node3 = Input(model, name='C')
-    assert(len(model.nodes) == 2)  # node3 not added to graph
-
-    # attempt to create a node without a name
-    with pytest.raises(TypeError):
-        node4 = Input(model)
-
-
-def test_model_nodes(model):
-    """Test Model.nodes API"""
-    node = Input(model, 'test')
-
-    # test node by index
-    assert(model.nodes['test'] is node)
-
-    with pytest.raises(KeyError):
-        model.nodes['invalid']
-
-    # test node iterator
-    all_nodes = [node for node in model.nodes]
-    assert(all_nodes == [node])
-
-    # support for item deletion
-    del(model.nodes['test'])
-    all_nodes = [node for node in model.nodes]
-    assert(all_nodes == [])
-
-
-def test_unexpected_kwarg_node():
-    model = Model()
-
-    with pytest.raises(TypeError):
-        node = Node(model, 'test_node', invalid=True)
-    with pytest.raises(TypeError):
-        inpt = Input(model, 'test_input', invalid=True)
-    with pytest.raises(TypeError):
-        storage = Storage(model, 'test_storage', invalid=True)
-    # none of the nodes should have been added to the model as they all
-    # raised exceptions during __init__
-    assert(not model.nodes)
-
-
-def test_unexpected_kwarg_model():
-    with pytest.raises(TypeError):
-        model = Model(thisisgoingtofail=True)
-    model = Model()
-
-def test_slots_connect_disconnect():
-    """Test connection and disconnection to storage node slots
-    """
-    model = Model()
-
-    supply1 = Input(model, name='supply1')
-    supply2 = Input(model, name='supply2')
-    storage = Storage(model, name='storage', num_inputs=2, num_outputs=2)
-
-    storage_inputs = [[x for x in storage.iter_slots(slot_name=n, is_connector=True)][0] for n in (0, 1)]
-    storage_outputs = [[x for x in storage.iter_slots(slot_name=n, is_connector=False)][0] for n in (0, 1)]
-
-    # attempt to connect to invalid slot
-    # an error is raised, and no connection is made
-    with pytest.raises(IndexError):
-        supply1.connect(storage, to_slot=3)
-    assert((supply1, storage_inputs[0]) not in model.edges())
-    assert((supply1, storage_inputs[1]) not in model.edges())
-
-    # connect node to storage slot 0
-    supply1.connect(storage, to_slot=0)
-    assert((supply1, storage_outputs[0]) in model.edges())
-    assert((supply1, storage_inputs[1]) not in model.edges())
-
-    # the same node can be connected to multiple slots
-    # connect node to storage slot 1
-    supply1.connect(storage, to_slot=1)
-    assert((supply1, storage_outputs[0]) in model.edges())
-    assert((supply1, storage_outputs[1]) in model.edges())
-
-    # disconnect the node from a particular slot (recommended method)
-    supply1.disconnect(storage, slot_name=0)
-    assert((supply1, storage_outputs[0]) not in model.edges())
-    assert((supply1, storage_outputs[1]) in model.edges())
-
-    # disconnect the node from a particular slot (direct, not recommended)
-    supply1.connect(storage, to_slot=0)
-    supply1.disconnect(storage_outputs[0])
-    assert((supply1, storage_outputs[0]) not in model.edges())
-    assert((supply1, storage_outputs[1]) in model.edges())
-
-    # specifying the storage in general removes the connection from all slots
-    supply1.connect(storage, to_slot=0)
-    supply1.disconnect(storage)
-    assert((supply1, storage_outputs[0]) not in model.edges())
-    assert((supply1, storage_outputs[1]) not in model.edges())
-
-    # it's an error to attempt to disconnect if nodes aren't connected
-    with pytest.raises(Exception):
-        supply1.disconnect(storage)
-
-
-def test_node_position():
-    model = Model()
-
-    # node position, from kwargs
-
-    node1 = Input(model, "input", position={"schematic": (10, 20), "geographic": (-1, 52)})
-
-    # node position, from JSON
-
-    data = {
-        "name": "output",
-        "type": "output",
-        "position": {
-            "schematic": (30, 40),
-            "geographic": (-1.5, 52.2),
-        }
-    }
-    node2 = Node.load(data, model)
-
-    assert(node1.position["schematic"] == (10, 20))
-    assert(node1.position["geographic"] == (-1, 52))
-    assert(node2.position["schematic"] == (30, 40))
-    assert(node2.position["geographic"] == (-1.5, 52.2))
-
-    node1.position["schematic"] = (50, 60)
-    assert(node1.position["schematic"] == (50, 60))
-
-    # node without position
-
-    node3 = Node(model, "node3")
-    assert(node3.position == {})
-
-    # reservoir position, from JSON
-
-    data = {
-        "name": "reservoir",
-        "type": "storage",
-        "position": {
-            "schematic": (99, 70),
-            "geographic": (-2.5, 55.6),
-        },
-        "max_volume": 1000,
-        "initial_volume": 500
-    }
-
-    storage = Storage.load(data, model)
-
-    assert(storage.position["schematic"] == (99, 70))
-    assert(storage.position["geographic"] == (-2.5, 55.6))
-
-
-test_data = ["timeseries1.xlsx", os.path.join("models", "timeseries1.csv")]
-@pytest.mark.parametrize("filename", test_data)
-def test_timeseries_excel(simple_linear_model, filename):
-    """Test creation of a DataFrameParameter from external data (e.g. CSV)"""
-    model = simple_linear_model
-
-    # create DataFrameParameter from external data
-    filename = os.path.join(TEST_FOLDER, filename)
-    data = {"url": filename, "column": "Data", "index_col": "Timestamp"}
-    if filename.endswith(".csv"):
-        data.update({"parse_dates": True, "dayfirst":True})
-    ts = DataFrameParameter.load(model, data)
-
-    # model (intentionally not aligned)
-    index = ts.dataframe.index
-    model.timestepper.start = index[0] + (5 * index.freq)
-    model.timestepper.end = index[-1] - (12 * index.freq)
-
-    # need to assign parameter for it's setup method to be called
-    model.nodes["Input"].max_flow = ts
-
-    @assert_rec(model, ts)
-    def expected(timestep, scenario_index):
-        return ts.dataframe.loc[timestep.datetime]
-
-    model.run()
-
-def test_dirty_model():
-    """Test that the LP is updated when the model structure is redefined"""
-    # start dirty
-    model = Model()
-    assert(model.dirty)
-
-    # add some nodes, still dirty
-    supply1 = Input(model, 'supply1')
-    demand1 = Output(model, 'demand1')
-    supply1.connect(demand1)
-    assert(model.dirty)
-
-    # run the model, clean
-    result = model.step()
-    assert(not model.dirty)
-
-    # add a new node, dirty
-    supply2 = Input(model, 'supply2')
-
-    # run the model, clean
-    result = model.step()
-    assert(not model.dirty)
-
-    # add a new connection, dirty
-    supply2.connect(demand1)
-    assert(model.dirty)
-
-    # run the model, clean
-    result = model.step()
-    assert(not model.dirty)
-
-    # remove a connection, dirty
-    supply2.disconnect()
-    assert(model.dirty)
-
-def test_reset_initial_volume():
-    """
-    If the model doesn't reset correctly changing the initial volume and
-    re-running will cause an exception.
-    """
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01')
-    )
-
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0)
-    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
-    storage.connect(otpt)
-
-    model.check()
-
-    for initial_volume in (50, 100):
-        storage.max_volume = initial_volume
-        storage.initial_volume = initial_volume
-        model.run()
-        assert(otpt.flow == initial_volume)
-
-def test_shorthand_property():
-    # test shorthand assignment of constant properties
-    model = Model()
-    node = Node(model, 'node')
-    for attr in ('min_flow', 'max_flow', 'cost', 'conversion_factor'):
-        # should except int, float or Paramter
-        setattr(node, attr, 123)
-        if attr == 'conversion_factor':
-            with pytest.raises(ValueError):
-                setattr(node, attr, Parameter(model))
-        else:
-            setattr(node, attr, Parameter(model))
-
-        with pytest.raises(TypeError):
-            setattr(node, attr, '123')
-            setattr(node, attr, None)
-
-
-def test_shorthand_property_storage():
-    # test shorthand assignment of constant properties
-    model = Model()
-    node = Storage(model, 'node')
-    for attr in ('min_volume', 'max_volume', 'cost', 'level'):
-        # should except int, float or Paramter
-        setattr(node, attr, 123)
-        if attr == 'conversion_factor':
-            with pytest.raises(ValueError):
-                setattr(node, attr, Parameter(model))
-        else:
-            setattr(node, attr, Parameter(model))
-
-        with pytest.raises(TypeError):
-            setattr(node, attr, '123')
-            setattr(node, attr, None)
-
-
-def test_reset_before_run():
-    # See issue #82. Previously this would raise:
-    #    AttributeError: Memoryview is not initialized
-    model = Model()
-    node = Node(model, 'node')
-    model.reset()
-
-
-def test_check_isolated_nodes(simple_linear_model):
-    """Test model storage checker"""
-    # the simple model shouldn't have any isolated nodes
-    model = simple_linear_model
-    model.check()
-
-    # add a node, but don't connect it to the network
-    isolated_node = Input(model, 'isolated')
-    with pytest.raises(ModelStructureError):
-        model.check()
-
-def test_check_isolated_nodes_storage():
-    """Test model structure checker with Storage
-
-    The Storage node itself doesn't have any connections, but it's child
-    nodes do need to be connected.
-    """
-    model = Model()
-
-    # add a storage, but don't connect it's outflow to anything
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume=0.0)
-    with pytest.raises(ModelStructureError):
-        model.check()
-
-    # add a demand node and connect it to the storage outflow
-    demand = Output(model, 'demand')
-    storage.connect(demand, from_slot=0)
-    model.check()
-
-def test_storage_max_volume_zero():
-    """Test a that an max_volume of zero results in a NaN for current_pc and no exception
-
-    """
-
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01')
-    )
-
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume=0.0)
-    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
-    storage.connect(otpt)
-
-    storage.max_volume = 0
-
-    model.run()
-    assert np.isnan(storage.current_pc)
-
-
-def test_storage_max_volume_param():
-    """Test a that an max_volume with a Parameter results in the correct current_pc
-
-    """
-
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01')
-    )
-
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0)
-    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
-    storage.connect(otpt)
-
-    p = ConstantParameter(model, 20.0)
-    storage.max_volume = p
-    storage.initial_volume = 10.0
-    storage.initial_volume_pc = 0.5
-
-    model.setup()
-    np.testing.assert_allclose(storage.current_pc, 0.5)
-
-    model.run()
-
-    p.set_double_variables(np.asarray([40.0, ]))
-    model.reset()
-
-    # This is a demonstration of the issue describe in #470
-    #   https://github.com/pywr/pywr/issues/470
-    # The initial storage is defined in both absolute and relative terms
-    # but these are now not consistent with one another and the updated max_volume
-
-    np.testing.assert_allclose(storage.volume, 10.0)
-    np.testing.assert_allclose(storage.current_pc, 0.5)
-
-
-def test_storage_initial_volume_pc():
-    """Test that setting initial volume as a percentage works as expected.
-    """
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01')
-    )
-
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume_pc=0.5, max_volume=20.0)
-    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
-    storage.connect(otpt)
-
-    model.setup()
-    np.testing.assert_allclose(storage.current_pc, 0.5)
-    np.testing.assert_allclose(storage.volume, 10.0)
-
-    model.run()
-
-    storage.max_volume = 40.0
-    model.reset()
-    np.testing.assert_allclose(storage.current_pc, 0.5)
-    np.testing.assert_allclose(storage.volume, 20.0)
-
-
-def test_storage_initial_missing_raises():
-    """Test that a RuntimeError is raised if no initial volume is specified.
-    """
-    model = Model()
-
-    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, max_volume=20.0)
-    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
-    storage.connect(otpt)
-
-    with pytest.raises(RuntimeError):
-        model.run()
-
-
-def test_storage_initial_volume_table():
-    """ Test loading initial storage volume from a table """
-    filename = os.path.join(TEST_FOLDER, "models", "reservoir_initial_vol_from_table.json")
-    model = Model.load(filename)
-
-    np.testing.assert_allclose(model.nodes["supply1"].initial_volume, 0.0)
-    np.testing.assert_allclose(model.nodes["supply2"].initial_volume, 35.0)
-
-
-def test_recursive_delete():
-    """Test recursive deletion of child nodes for compound nodes"""
-    model = Model()
-    n1 = Input(model, "n1")
-    n2 = Output(model, "n2")
-    s = Storage(model, "s", num_outputs=2)
-    assert len(model.nodes) == 3
-    assert len(model.graph.nodes()) == 6
-    del(model.nodes["n1"])
-    assert len(model.nodes) == 2
-    assert len(model.graph.nodes()) == 5
-    del(model.nodes["s"])
-    assert len(model.nodes) == 1
-    assert len(model.graph.nodes()) == 1
-
-
-def test_json_include():
-    """Test include in JSON document"""
-    filename = os.path.join(TEST_FOLDER, "models", "extra1.json")
-    model = Model.load(filename)
-
-    supply1 = model.nodes["supply1"]
-    supply2 = model.nodes["supply2"]
-    assert(isinstance(supply2.max_flow, ConstantParameter))
-
-
-def test_py_include():
-    """Test include in Python document"""
-    filename = os.path.join(TEST_FOLDER, "models", "python_include.json")
-    model = Model.load(filename)
-
-    model.run()
-
-
-def test_json_min_version():
-    """Test warning is raised if document minimum version is more than we have"""
-    filename = os.path.join(TEST_FOLDER, "models", "version1.json")
-    with pytest.warns(RuntimeWarning):
-        model = Model.load(filename)
-
-def test_initial_timestep():
-    """Current timestep before model has started is undefined"""
-    filename = os.path.join(TEST_FOLDER, "models", "extra1.json")
-    model = Model.load(filename)
-    assert(model.timestepper.current is None)
-    model.run()
-    assert(isinstance(model.timestepper.current, Timestep))
-
-def test_timestepper_repr(model):
-    timestepper = model.timestepper
-    print(timestepper)
-
-def test_timestep_repr():
-    filename = os.path.join(TEST_FOLDER, "models", "simple1.json")
-    model = Model.load(filename)
-    model.timestepper.end = "2015-01-05"
-    res = model.run()
-    assert(isinstance(res.timestep, Timestep))
-    assert("2015-01-05" in str(res.timestep))
-
-def test_virtual_storage_cost():
-    """VirtualStorage doesn't (currently) implement its cost attribute"""
-    model = Model()
-    A = Input(model, "A")
-    B = Output(model, "B")
-    A.connect(B)
-    node = VirtualStorage(model, "storage", [A, B])
-    node.check()
-    node.cost = 5.0
-    with pytest.raises(NotImplementedError):
-        model.check()
-
-def test_json_invalid():
-    """JSON exceptions should report file name"""
-    filename = os.path.join(TEST_FOLDER, "models", "invalid"+".json")
-    with pytest.raises(ValueError) as excinfo:
-        model = Model.load(filename)
-    assert("invalid.json" in str(excinfo.value))
-
-def test_json_invalid_include():
-    """JSON exceptions should report file name, even for includes"""
-    filename = os.path.join(TEST_FOLDER, "models", "invalid_include"+".json")
-    with pytest.raises(ValueError) as excinfo:
-        model = Model.load(filename)
-    assert("invalid.json" in str(excinfo.value))
-
-
-def test_variable_load():
-    """Current timestep before model has started is undefined"""
-    filename = os.path.join(TEST_FOLDER, "models", "demand_saving2_with_variables.json")
-    model = Model.load(filename)
-
-    # Test the correct number of each component is loaded
-    assert len(model.variables) == 3
-    assert len(model.objectives) == 1
-    assert len(model.constraints) == 1
-    # Test the names are as expected
-    assert sorted([c.name for c in model.variables]) == ["demand_profile", "level1", "level2"]
-    assert sorted([c.name for c in model.objectives]) == ["total_deficit", ]
-    assert sorted([c.is_objective for c in model.objectives]) == ["minimise", ]
-    assert sorted([c.name for c in model.constraints]) == ["min_volume", ]
-
-
-def test_delta_greater_than_zero_days(simple_linear_model):
-    """Test trying to use zero length timestep raises an error."""
-    model = simple_linear_model
-    model.timestepper.delta = 0
-
-    with pytest.raises(ValueError):
-        model.run()
-
-
-def test_timestep_greater_than_zero_days():
-    """Test trying to create zero length Timestep."""
-
-    with pytest.raises(ValueError):
-        # Test setting days <= 0 raises an error
-        Timestep(pandas.Period('2019-01-01', freq='D'), 0, 0)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import datetime
+import pytest
+from fixtures import *
+from helpers import *
+from pywr._core import Timestep, ScenarioIndex
+import pandas
+from pywr.core import *
+from pywr.domains.river import *
+from pywr.parameters import Parameter, ConstantParameter, DataFrameParameter, AggregatedParameter
+from pywr.recorders import assert_rec, AssertionRecorder
+
+TEST_FOLDER = os.path.dirname(__file__)
+
+def test_names():
+    '''Test node names'''
+    model = Model()
+
+    node1 = Input(model, name='A')
+    node2 = Output(model, name='B')
+    assert(model.nodes['A'] is node1)
+    assert(model.nodes['B'] is node2)
+
+    nodes = sorted(model.nodes, key=lambda node: node.name)
+    assert(nodes == [node1, node2])
+
+    # rename node
+    node1.name = 'C'
+    assert(model.nodes['C'] is node1)
+    assert('A' not in model.nodes)
+
+    # attempt name collision (via rename)
+    with pytest.raises(ValueError):
+        node2.name = 'C'
+
+    # attempt name collision (via new)
+    with pytest.raises(ValueError):
+        node3 = Input(model, name='C')
+    assert(len(model.nodes) == 2)  # node3 not added to graph
+
+    # attempt to create a node without a name
+    with pytest.raises(TypeError):
+        node4 = Input(model)
+
+
+def test_model_nodes(model):
+    """Test Model.nodes API"""
+    node = Input(model, 'test')
+
+    # test node by index
+    assert(model.nodes['test'] is node)
+
+    with pytest.raises(KeyError):
+        model.nodes['invalid']
+
+    # test node iterator
+    all_nodes = [node for node in model.nodes]
+    assert(all_nodes == [node])
+
+    # support for item deletion
+    del(model.nodes['test'])
+    all_nodes = [node for node in model.nodes]
+    assert(all_nodes == [])
+
+
+def test_unexpected_kwarg_node():
+    model = Model()
+
+    with pytest.raises(TypeError):
+        node = Node(model, 'test_node', invalid=True)
+    with pytest.raises(TypeError):
+        inpt = Input(model, 'test_input', invalid=True)
+    with pytest.raises(TypeError):
+        storage = Storage(model, 'test_storage', invalid=True)
+    # none of the nodes should have been added to the model as they all
+    # raised exceptions during __init__
+    assert(not model.nodes)
+
+
+def test_unexpected_kwarg_model():
+    with pytest.raises(TypeError):
+        model = Model(thisisgoingtofail=True)
+    model = Model()
+
+def test_slots_connect_disconnect():
+    """Test connection and disconnection to storage node slots
+    """
+    model = Model()
+
+    supply1 = Input(model, name='supply1')
+    supply2 = Input(model, name='supply2')
+    storage = Storage(model, name='storage', num_inputs=2, num_outputs=2)
+
+    storage_inputs = [[x for x in storage.iter_slots(slot_name=n, is_connector=True)][0] for n in (0, 1)]
+    storage_outputs = [[x for x in storage.iter_slots(slot_name=n, is_connector=False)][0] for n in (0, 1)]
+
+    # attempt to connect to invalid slot
+    # an error is raised, and no connection is made
+    with pytest.raises(IndexError):
+        supply1.connect(storage, to_slot=3)
+    assert((supply1, storage_inputs[0]) not in model.edges())
+    assert((supply1, storage_inputs[1]) not in model.edges())
+
+    # connect node to storage slot 0
+    supply1.connect(storage, to_slot=0)
+    assert((supply1, storage_outputs[0]) in model.edges())
+    assert((supply1, storage_inputs[1]) not in model.edges())
+
+    # the same node can be connected to multiple slots
+    # connect node to storage slot 1
+    supply1.connect(storage, to_slot=1)
+    assert((supply1, storage_outputs[0]) in model.edges())
+    assert((supply1, storage_outputs[1]) in model.edges())
+
+    # disconnect the node from a particular slot (recommended method)
+    supply1.disconnect(storage, slot_name=0)
+    assert((supply1, storage_outputs[0]) not in model.edges())
+    assert((supply1, storage_outputs[1]) in model.edges())
+
+    # disconnect the node from a particular slot (direct, not recommended)
+    supply1.connect(storage, to_slot=0)
+    supply1.disconnect(storage_outputs[0])
+    assert((supply1, storage_outputs[0]) not in model.edges())
+    assert((supply1, storage_outputs[1]) in model.edges())
+
+    # specifying the storage in general removes the connection from all slots
+    supply1.connect(storage, to_slot=0)
+    supply1.disconnect(storage)
+    assert((supply1, storage_outputs[0]) not in model.edges())
+    assert((supply1, storage_outputs[1]) not in model.edges())
+
+    # it's an error to attempt to disconnect if nodes aren't connected
+    with pytest.raises(Exception):
+        supply1.disconnect(storage)
+
+
+def test_node_position():
+    model = Model()
+
+    # node position, from kwargs
+
+    node1 = Input(model, "input", position={"schematic": (10, 20), "geographic": (-1, 52)})
+
+    # node position, from JSON
+
+    data = {
+        "name": "output",
+        "type": "output",
+        "position": {
+            "schematic": (30, 40),
+            "geographic": (-1.5, 52.2),
+        }
+    }
+    node2 = Node.load(data, model)
+
+    assert(node1.position["schematic"] == (10, 20))
+    assert(node1.position["geographic"] == (-1, 52))
+    assert(node2.position["schematic"] == (30, 40))
+    assert(node2.position["geographic"] == (-1.5, 52.2))
+
+    node1.position["schematic"] = (50, 60)
+    assert(node1.position["schematic"] == (50, 60))
+
+    # node without position
+
+    node3 = Node(model, "node3")
+    assert(node3.position == {})
+
+    # reservoir position, from JSON
+
+    data = {
+        "name": "reservoir",
+        "type": "storage",
+        "position": {
+            "schematic": (99, 70),
+            "geographic": (-2.5, 55.6),
+        },
+        "max_volume": 1000,
+        "initial_volume": 500
+    }
+
+    storage = Storage.load(data, model)
+
+    assert(storage.position["schematic"] == (99, 70))
+    assert(storage.position["geographic"] == (-2.5, 55.6))
+
+
+test_data = ["timeseries1.xlsx", os.path.join("models", "timeseries1.csv")]
+@pytest.mark.parametrize("filename", test_data)
+def test_timeseries_excel(simple_linear_model, filename):
+    """Test creation of a DataFrameParameter from external data (e.g. CSV)"""
+    model = simple_linear_model
+
+    # create DataFrameParameter from external data
+    filename = os.path.join(TEST_FOLDER, filename)
+    data = {"url": filename, "column": "Data", "index_col": "Timestamp"}
+    if filename.endswith(".csv"):
+        data.update({"parse_dates": True, "dayfirst":True})
+    ts = DataFrameParameter.load(model, data)
+
+    # model (intentionally not aligned)
+    index = ts.dataframe.index
+    model.timestepper.start = index[0] + (5 * index.freq)
+    model.timestepper.end = index[-1] - (12 * index.freq)
+
+    # need to assign parameter for it's setup method to be called
+    model.nodes["Input"].max_flow = ts
+
+    @assert_rec(model, ts)
+    def expected(timestep, scenario_index):
+        return ts.dataframe.loc[timestep.datetime]
+
+    model.run()
+
+def test_dirty_model():
+    """Test that the LP is updated when the model structure is redefined"""
+    # start dirty
+    model = Model()
+    assert(model.dirty)
+
+    # add some nodes, still dirty
+    supply1 = Input(model, 'supply1')
+    demand1 = Output(model, 'demand1')
+    supply1.connect(demand1)
+    assert(model.dirty)
+
+    # run the model, clean
+    result = model.step()
+    assert(not model.dirty)
+
+    # add a new node, dirty
+    supply2 = Input(model, 'supply2')
+
+    # run the model, clean
+    result = model.step()
+    assert(not model.dirty)
+
+    # add a new connection, dirty
+    supply2.connect(demand1)
+    assert(model.dirty)
+
+    # run the model, clean
+    result = model.step()
+    assert(not model.dirty)
+
+    # remove a connection, dirty
+    supply2.disconnect()
+    assert(model.dirty)
+
+def test_reset_initial_volume():
+    """
+    If the model doesn't reset correctly changing the initial volume and
+    re-running will cause an exception.
+    """
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01')
+    )
+
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0)
+    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
+    storage.connect(otpt)
+
+    model.check()
+
+    for initial_volume in (50, 100):
+        storage.max_volume = initial_volume
+        storage.initial_volume = initial_volume
+        model.run()
+        assert(otpt.flow == initial_volume)
+
+def test_shorthand_property():
+    # test shorthand assignment of constant properties
+    model = Model()
+    node = Node(model, 'node')
+    for attr in ('min_flow', 'max_flow', 'cost', 'conversion_factor'):
+        # should except int, float or Paramter
+        setattr(node, attr, 123)
+        if attr == 'conversion_factor':
+            with pytest.raises(ValueError):
+                setattr(node, attr, Parameter(model))
+        else:
+            setattr(node, attr, Parameter(model))
+
+        with pytest.raises(TypeError):
+            setattr(node, attr, '123')
+            setattr(node, attr, None)
+
+
+def test_shorthand_property_storage():
+    # test shorthand assignment of constant properties
+    model = Model()
+    node = Storage(model, 'node')
+    for attr in ('min_volume', 'max_volume', 'cost', 'level'):
+        # should except int, float or Paramter
+        setattr(node, attr, 123)
+        if attr == 'conversion_factor':
+            with pytest.raises(ValueError):
+                setattr(node, attr, Parameter(model))
+        else:
+            setattr(node, attr, Parameter(model))
+
+        with pytest.raises(TypeError):
+            setattr(node, attr, '123')
+            setattr(node, attr, None)
+
+
+def test_reset_before_run():
+    # See issue #82. Previously this would raise:
+    #    AttributeError: Memoryview is not initialized
+    model = Model()
+    node = Node(model, 'node')
+    model.reset()
+
+
+def test_check_isolated_nodes(simple_linear_model):
+    """Test model storage checker"""
+    # the simple model shouldn't have any isolated nodes
+    model = simple_linear_model
+    model.check()
+
+    # add a node, but don't connect it to the network
+    isolated_node = Input(model, 'isolated')
+    with pytest.raises(ModelStructureError):
+        model.check()
+
+def test_check_isolated_nodes_storage():
+    """Test model structure checker with Storage
+
+    The Storage node itself doesn't have any connections, but it's child
+    nodes do need to be connected.
+    """
+    model = Model()
+
+    # add a storage, but don't connect it's outflow to anything
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume=0.0)
+    with pytest.raises(ModelStructureError):
+        model.check()
+
+    # add a demand node and connect it to the storage outflow
+    demand = Output(model, 'demand')
+    storage.connect(demand, from_slot=0)
+    model.check()
+
+def test_storage_max_volume_zero():
+    """Test a that an max_volume of zero results in a NaN for current_pc and no exception
+
+    """
+
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01')
+    )
+
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume=0.0)
+    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
+    storage.connect(otpt)
+
+    storage.max_volume = 0
+
+    model.run()
+    assert np.isnan(storage.current_pc)
+
+
+def test_storage_max_volume_param():
+    """Test a that an max_volume with a Parameter results in the correct current_pc
+
+    """
+
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01')
+    )
+
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0)
+    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
+    storage.connect(otpt)
+
+    p = ConstantParameter(model, 20.0)
+    storage.max_volume = p
+    storage.initial_volume = 10.0
+    storage.initial_volume_pc = 0.5
+
+    model.setup()
+    np.testing.assert_allclose(storage.current_pc, 0.5)
+
+    model.run()
+
+    p.set_double_variables(np.asarray([40.0, ]))
+    model.reset()
+
+    # This is a demonstration of the issue describe in #470
+    #   https://github.com/pywr/pywr/issues/470
+    # The initial storage is defined in both absolute and relative terms
+    # but these are now not consistent with one another and the updated max_volume
+
+    np.testing.assert_allclose(storage.volume, 10.0)
+    np.testing.assert_allclose(storage.current_pc, 0.5)
+
+
+def test_storage_initial_volume_pc():
+    """Test that setting initial volume as a percentage works as expected.
+    """
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01')
+    )
+
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, initial_volume_pc=0.5, max_volume=20.0)
+    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
+    storage.connect(otpt)
+
+    model.setup()
+    np.testing.assert_allclose(storage.current_pc, 0.5)
+    np.testing.assert_allclose(storage.volume, 10.0)
+
+    model.run()
+
+    storage.max_volume = 40.0
+    model.reset()
+    np.testing.assert_allclose(storage.current_pc, 0.5)
+    np.testing.assert_allclose(storage.volume, 20.0)
+
+
+def test_storage_initial_missing_raises():
+    """Test that a RuntimeError is raised if no initial volume is specified.
+    """
+    model = Model()
+
+    storage = Storage(model, 'storage', num_inputs=1, num_outputs=0, max_volume=20.0)
+    otpt = Output(model, 'output', max_flow=99999, cost=-99999)
+    storage.connect(otpt)
+
+    with pytest.raises(RuntimeError):
+        model.run()
+
+
+def test_storage_initial_volume_table():
+    """ Test loading initial storage volume from a table """
+    filename = os.path.join(TEST_FOLDER, "models", "reservoir_initial_vol_from_table.json")
+    model = Model.load(filename)
+
+    np.testing.assert_allclose(model.nodes["supply1"].initial_volume, 0.0)
+    np.testing.assert_allclose(model.nodes["supply2"].initial_volume, 35.0)
+
+
+def test_recursive_delete():
+    """Test recursive deletion of child nodes for compound nodes"""
+    model = Model()
+    n1 = Input(model, "n1")
+    n2 = Output(model, "n2")
+    s = Storage(model, "s", num_outputs=2)
+    assert len(model.nodes) == 3
+    assert len(model.graph.nodes()) == 6
+    del(model.nodes["n1"])
+    assert len(model.nodes) == 2
+    assert len(model.graph.nodes()) == 5
+    del(model.nodes["s"])
+    assert len(model.nodes) == 1
+    assert len(model.graph.nodes()) == 1
+
+
+def test_json_include():
+    """Test include in JSON document"""
+    filename = os.path.join(TEST_FOLDER, "models", "extra1.json")
+    model = Model.load(filename)
+
+    supply1 = model.nodes["supply1"]
+    supply2 = model.nodes["supply2"]
+    assert(isinstance(supply2.max_flow, ConstantParameter))
+
+
+def test_py_include():
+    """Test include in Python document"""
+    filename = os.path.join(TEST_FOLDER, "models", "python_include.json")
+    model = Model.load(filename)
+
+    model.run()
+
+
+def test_json_min_version():
+    """Test warning is raised if document minimum version is more than we have"""
+    filename = os.path.join(TEST_FOLDER, "models", "version1.json")
+    with pytest.warns(RuntimeWarning):
+        model = Model.load(filename)
+
+def test_initial_timestep():
+    """Current timestep before model has started is undefined"""
+    filename = os.path.join(TEST_FOLDER, "models", "extra1.json")
+    model = Model.load(filename)
+    assert(model.timestepper.current is None)
+    model.run()
+    assert(isinstance(model.timestepper.current, Timestep))
+
+def test_timestepper_repr(model):
+    timestepper = model.timestepper
+    print(timestepper)
+
+def test_timestep_repr():
+    filename = os.path.join(TEST_FOLDER, "models", "simple1.json")
+    model = Model.load(filename)
+    model.timestepper.end = "2015-01-05"
+    res = model.run()
+    assert(isinstance(res.timestep, Timestep))
+    assert("2015-01-05" in str(res.timestep))
+
+def test_virtual_storage_cost():
+    """VirtualStorage doesn't (currently) implement its cost attribute"""
+    model = Model()
+    A = Input(model, "A")
+    B = Output(model, "B")
+    A.connect(B)
+    node = VirtualStorage(model, "storage", [A, B])
+    node.check()
+    node.cost = 5.0
+    with pytest.raises(NotImplementedError):
+        model.check()
+
+def test_json_invalid():
+    """JSON exceptions should report file name"""
+    filename = os.path.join(TEST_FOLDER, "models", "invalid"+".json")
+    with pytest.raises(ValueError) as excinfo:
+        model = Model.load(filename)
+    assert("invalid.json" in str(excinfo.value))
+
+def test_json_invalid_include():
+    """JSON exceptions should report file name, even for includes"""
+    filename = os.path.join(TEST_FOLDER, "models", "invalid_include"+".json")
+    with pytest.raises(ValueError) as excinfo:
+        model = Model.load(filename)
+    assert("invalid.json" in str(excinfo.value))
+
+
+def test_variable_load():
+    """Current timestep before model has started is undefined"""
+    filename = os.path.join(TEST_FOLDER, "models", "demand_saving2_with_variables.json")
+    model = Model.load(filename)
+
+    # Test the correct number of each component is loaded
+    assert len(model.variables) == 3
+    assert len(model.objectives) == 1
+    assert len(model.constraints) == 1
+    # Test the names are as expected
+    assert sorted([c.name for c in model.variables]) == ["demand_profile", "level1", "level2"]
+    assert sorted([c.name for c in model.objectives]) == ["total_deficit", ]
+    assert sorted([c.is_objective for c in model.objectives]) == ["minimise", ]
+    assert sorted([c.name for c in model.constraints]) == ["min_volume", ]
+
+
+def test_delta_greater_than_zero_days(simple_linear_model):
+    """Test trying to use zero length timestep raises an error."""
+    model = simple_linear_model
+    model.timestepper.delta = 0
+
+    with pytest.raises(ValueError):
+        model.run()
+
+
+def test_timestep_greater_than_zero_days():
+    """Test trying to create zero length Timestep."""
+
+    with pytest.raises(ValueError):
+        # Test setting days <= 0 raises an error
+        Timestep(pandas.Period('2019-01-01', freq='D'), 0, 0)
```

### Comparing `pywr-1.8.0/tests/test_delay.py` & `pywr-1.9.0/tests/test_delay.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,103 +1,103 @@
-from pywr.model import Model
-from pywr.nodes import Catchment, Output, DelayNode
-from pywr.recorders import NumpyArrayNodeRecorder, AssertionRecorder
-from pywr.parameters import ArrayIndexedScenarioParameter, FlowDelayParameter, load_parameter
-from pywr.core import Scenario
-from numpy.testing import assert_array_almost_equal
-import numpy as np
-import pytest
-
-
-@pytest.mark.parametrize("key, delay, initial_flow", [("days", 1, None),
-                                                      ("timesteps", 1, None),
-                                                      ("days", 1, 5.0),
-                                                      ("days", 3, 5.0),
-                                                      ("timesteps", 10, 5.0)])
-def test_delay_node(key, delay, initial_flow):
-    """ Test that the `DelayNode` and the `FlowDelayParameter` internal to it correctly delay node for a range of inputs and
-    across scenarios"""
-    model = Model()
-
-    model.timestepper.start = "2015/01/01"
-    model.timestepper.end = "2015/01/31"
-
-    scen = Scenario(model, name="scenario", size=2)
-    flow_vals = np.arange(1, 63).reshape((31, 2), order="F")
-    flow = ArrayIndexedScenarioParameter(model, scen, flow_vals)
-
-    catchment = Catchment(model, name="input", flow=flow)
-    kwargs = {key: delay}
-    if initial_flow:
-        kwargs["initial_flow"] = initial_flow
-    delaynode = DelayNode(model, name="delaynode", **kwargs)
-    output = Output(model, name="output")
-
-    catchment.connect(delaynode)
-    delaynode.connect(output)
-
-    rec = NumpyArrayNodeRecorder(model, output)
-
-    model.run()
-    if initial_flow:
-        expected = np.concatenate([np.full((delay, 2), initial_flow), flow_vals[:-delay, :]])
-    else:
-        expected = np.concatenate([np.zeros((delay, 2)), flow_vals[:-delay, :]])
-
-    assert_array_almost_equal(rec.data, expected)
-
-
-def test_delay_param_load():
-    """Test that the `.load` method of `FlowDelayParameter` works correctly"""
-
-    model = Model()
-    model.timestepper.start = "2015/01/01"
-    model.timestepper.end = "2015/01/31"
-    catchment = Catchment(model, name="input", flow=1)
-    output = Output(model, name="output")
-    catchment.connect(output)
-
-    data = {"name": "delay",
-            "node": "input",
-            "days": 2}
-
-    param = FlowDelayParameter.load(model, data)
-
-    assert param.days == 2
-
-    data = {"name": "delay2",
-            "node": "input",
-            "timesteps": 2}
-
-    param2 = FlowDelayParameter.load(model, data)
-
-    assert param2.timesteps == 2
-
-    expected = np.concatenate([np.zeros(2), np.ones(29)]).reshape(31, 1)
-
-    AssertionRecorder(model, param, name="rec1", expected_data=expected)
-    AssertionRecorder(model, param2, name="rec2", expected_data=expected)
-
-    model.setup()
-    model.run()
-
-
-@pytest.mark.parametrize("key, delay", [("days", 5),
-                                        ("timesteps", 0.5)])
-def test_delay_failure(key, delay):
-    """Test the FlowDelayParameter returns a ValueError when the input value of the `days` attribute is not
-    divisible exactly by the model timestep delta and when the `timesteps` attribute is less than 1
-    """
-
-    model = Model()
-    model.timestepper.start = "2015/01/01"
-    model.timestepper.end = "2015/01/31"
-    model.timestepper.delta = 3
-
-    catchment = Catchment(model, name="input", flow=1)
-    output = Output(model, name="output")
-    catchment.connect(output)
-
-    FlowDelayParameter(model, catchment, **{key: delay})
-
-    with pytest.raises(ValueError):
-        model.setup()
+from pywr.model import Model
+from pywr.nodes import Catchment, Output, DelayNode
+from pywr.recorders import NumpyArrayNodeRecorder, AssertionRecorder
+from pywr.parameters import ArrayIndexedScenarioParameter, FlowDelayParameter, load_parameter
+from pywr.core import Scenario
+from numpy.testing import assert_array_almost_equal
+import numpy as np
+import pytest
+
+
+@pytest.mark.parametrize("key, delay, initial_flow", [("days", 1, None),
+                                                      ("timesteps", 1, None),
+                                                      ("days", 1, 5.0),
+                                                      ("days", 3, 5.0),
+                                                      ("timesteps", 10, 5.0)])
+def test_delay_node(key, delay, initial_flow):
+    """ Test that the `DelayNode` and the `FlowDelayParameter` internal to it correctly delay node for a range of inputs and
+    across scenarios"""
+    model = Model()
+
+    model.timestepper.start = "2015/01/01"
+    model.timestepper.end = "2015/01/31"
+
+    scen = Scenario(model, name="scenario", size=2)
+    flow_vals = np.arange(1, 63).reshape((31, 2), order="F")
+    flow = ArrayIndexedScenarioParameter(model, scen, flow_vals)
+
+    catchment = Catchment(model, name="input", flow=flow)
+    kwargs = {key: delay}
+    if initial_flow:
+        kwargs["initial_flow"] = initial_flow
+    delaynode = DelayNode(model, name="delaynode", **kwargs)
+    output = Output(model, name="output")
+
+    catchment.connect(delaynode)
+    delaynode.connect(output)
+
+    rec = NumpyArrayNodeRecorder(model, output)
+
+    model.run()
+    if initial_flow:
+        expected = np.concatenate([np.full((delay, 2), initial_flow), flow_vals[:-delay, :]])
+    else:
+        expected = np.concatenate([np.zeros((delay, 2)), flow_vals[:-delay, :]])
+
+    assert_array_almost_equal(rec.data, expected)
+
+
+def test_delay_param_load():
+    """Test that the `.load` method of `FlowDelayParameter` works correctly"""
+
+    model = Model()
+    model.timestepper.start = "2015/01/01"
+    model.timestepper.end = "2015/01/31"
+    catchment = Catchment(model, name="input", flow=1)
+    output = Output(model, name="output")
+    catchment.connect(output)
+
+    data = {"name": "delay",
+            "node": "input",
+            "days": 2}
+
+    param = FlowDelayParameter.load(model, data)
+
+    assert param.days == 2
+
+    data = {"name": "delay2",
+            "node": "input",
+            "timesteps": 2}
+
+    param2 = FlowDelayParameter.load(model, data)
+
+    assert param2.timesteps == 2
+
+    expected = np.concatenate([np.zeros(2), np.ones(29)]).reshape(31, 1)
+
+    AssertionRecorder(model, param, name="rec1", expected_data=expected)
+    AssertionRecorder(model, param2, name="rec2", expected_data=expected)
+
+    model.setup()
+    model.run()
+
+
+@pytest.mark.parametrize("key, delay", [("days", 5),
+                                        ("timesteps", 0.5)])
+def test_delay_failure(key, delay):
+    """Test the FlowDelayParameter returns a ValueError when the input value of the `days` attribute is not
+    divisible exactly by the model timestep delta and when the `timesteps` attribute is less than 1
+    """
+
+    model = Model()
+    model.timestepper.start = "2015/01/01"
+    model.timestepper.end = "2015/01/31"
+    model.timestepper.delta = 3
+
+    catchment = Catchment(model, name="input", flow=1)
+    output = Output(model, name="output")
+    catchment.connect(output)
+
+    FlowDelayParameter(model, catchment, **{key: delay})
+
+    with pytest.raises(ValueError):
+        model.setup()
```

### Comparing `pywr-1.8.0/tests/test_df_resampling.py` & `pywr-1.9.0/tests/test_df_resampling.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,213 +1,213 @@
-from pywr.dataframe_tools import align_and_resample_dataframe, ResamplingError
-import numpy as np
-import pandas as pd
-import pytest
-
-
-def make_df(freq, start='2015-01-01', end='2015-12-31'):
-    # Daily time-step
-    index = pd.period_range(start, end, freq=freq)
-    series = pd.DataFrame(np.arange(len(index), dtype=np.float64), index=index)
-    return series
-
-
-def make_model_index(freq, start='2015-01-01', end='2015-12-31'):
-    return pd.period_range(start, end, freq=freq)
-
-
-class TestDownSampling:
-    """Test for down-sampling a dataframe to lower frequency model time-step."""
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_daily_to_monthly(self, resample_func):
-        """Test daily data to monthly model time-step."""
-        input_df = make_df('D')
-        model_index = make_model_index('M')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('M').agg(resample_func))
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_daily_to_weekly(self, resample_func):
-        """Test daily to weekly model time-step."""
-        input_df = make_df('D')
-        model_index = make_model_index('W')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('W').agg(resample_func))
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_daily_to_7daily(self, resample_func):
-        """Test daily to 7-day model time-step."""
-        input_df = make_df('D')
-        model_index = make_model_index('7D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('7D').agg(resample_func))
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_misaligned_daily_to_7daily(self, resample_func):
-        """Test daily to 7-day model time-step."""
-        input_df = make_df('D', start='2014-12-20')
-        model_index = make_model_index('7D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('7D').agg(resample_func))
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_weekly_to_monthly(self, resample_func):
-        """Test weekly to monthly model time-step."""
-        input_df = make_df('W')
-        model_index = make_model_index('M')
-
-        with pytest.raises(ResamplingError):
-            input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_weekly_to_two_weekly(self, resample_func):
-        """Test weekly to two-weekly model time-step."""
-        input_df = make_df('W')
-        model_index = make_model_index('2W')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('2W').agg(resample_func))
-
-
-class TestUpSampling:
-    """Test for up-sampling a dataframe to higher frequency model time-step."""
-
-    def test_monthly_to_daily(self):
-        """Test monthly data to daily model time-step."""
-        input_df = make_df('M')
-        model_index = make_model_index('D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_weekly_to_daily(self):
-        """Test weekly aligned to daily model time-step."""
-        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
-        input_df = make_df('W-WED')
-        model_index = make_model_index('D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_7daily_to_daily(self):
-        """Test weekly aligned to daily model time-step."""
-        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
-        input_df = make_df('7D')
-        model_index = make_model_index('D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_nonaligned_weekly_to_daily(self):
-        """Test weekly non-aligned data to daily model time-step."""
-        input_df = make_df('W')
-        model_index = make_model_index('D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_weekly_to_7daily(self, resample_func):
-        """Test weekly to 7-day model time-step."""
-        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
-        input_df = make_df('W-WED')
-        model_index = make_model_index('7D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-        expected_df = expected_df.resample('7D').agg(resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
-    def test_nonaligned_weekly_to_7daily(self, resample_func):
-        """Test invalid weekly data to 7-day model time-step."""
-        # Because the weeks do not align with the 7-day time-step it is not possible to align this data.
-        input_df = make_df('W')
-        model_index = make_model_index('7D')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-        expected_df = expected_df.resample('7D').agg(resample_func)
-
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_annual_to_daily(self):
-
-        input_df = make_df('A', '2010-01-01', '2020-01-01')
-        model_index = make_model_index('D', '2010-01-01', '2020-01-01')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index)
-
-        expected_df = input_df.resample('D').ffill()
-        expected_df = expected_df['2010-01-01':'2020-01-01']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_annual_to_monthly(self):
-
-        input_df = make_df('A', '2010-01-01', '2020-01-01')
-        model_index = make_model_index('M', '2010-01-01', '2020-01-01')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func='ffill')
-
-        expected_df = input_df.resample('M').ffill()
-        expected_df = expected_df['2010-01-01':'2020-01-01']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
-
-    def test_two_weekly_to_weekly(self):
-        """Test weekly to two-weekly model time-step."""
-        input_df = make_df('2W')
-        model_index = make_model_index('W')
-
-        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func='ffill')
-
-        expected_df = input_df.resample('W').ffill()
-        expected_df = expected_df['2015-01-01':'2015-12-31']
-
-        pd.testing.assert_index_equal(input_resampled.index, model_index)
-        pd.testing.assert_frame_equal(input_resampled, expected_df)
+from pywr.dataframe_tools import align_and_resample_dataframe, ResamplingError
+import numpy as np
+import pandas as pd
+import pytest
+
+
+def make_df(freq, start='2015-01-01', end='2015-12-31'):
+    # Daily time-step
+    index = pd.period_range(start, end, freq=freq)
+    series = pd.DataFrame(np.arange(len(index), dtype=np.float64), index=index)
+    return series
+
+
+def make_model_index(freq, start='2015-01-01', end='2015-12-31'):
+    return pd.period_range(start, end, freq=freq)
+
+
+class TestDownSampling:
+    """Test for down-sampling a dataframe to lower frequency model time-step."""
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_daily_to_monthly(self, resample_func):
+        """Test daily data to monthly model time-step."""
+        input_df = make_df('D')
+        model_index = make_model_index('M')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('M').agg(resample_func))
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_daily_to_weekly(self, resample_func):
+        """Test daily to weekly model time-step."""
+        input_df = make_df('D')
+        model_index = make_model_index('W')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('W').agg(resample_func))
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_daily_to_7daily(self, resample_func):
+        """Test daily to 7-day model time-step."""
+        input_df = make_df('D')
+        model_index = make_model_index('7D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('7D').agg(resample_func))
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_misaligned_daily_to_7daily(self, resample_func):
+        """Test daily to 7-day model time-step."""
+        input_df = make_df('D', start='2014-12-20')
+        model_index = make_model_index('7D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('7D').agg(resample_func))
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_weekly_to_monthly(self, resample_func):
+        """Test weekly to monthly model time-step."""
+        input_df = make_df('W')
+        model_index = make_model_index('M')
+
+        with pytest.raises(ResamplingError):
+            input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_weekly_to_two_weekly(self, resample_func):
+        """Test weekly to two-weekly model time-step."""
+        input_df = make_df('W')
+        model_index = make_model_index('2W')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, input_resampled.resample('2W').agg(resample_func))
+
+
+class TestUpSampling:
+    """Test for up-sampling a dataframe to higher frequency model time-step."""
+
+    def test_monthly_to_daily(self):
+        """Test monthly data to daily model time-step."""
+        input_df = make_df('M')
+        model_index = make_model_index('D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_weekly_to_daily(self):
+        """Test weekly aligned to daily model time-step."""
+        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
+        input_df = make_df('W-WED')
+        model_index = make_model_index('D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_7daily_to_daily(self):
+        """Test weekly aligned to daily model time-step."""
+        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
+        input_df = make_df('7D')
+        model_index = make_model_index('D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_nonaligned_weekly_to_daily(self):
+        """Test weekly non-aligned data to daily model time-step."""
+        input_df = make_df('W')
+        model_index = make_model_index('D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_weekly_to_7daily(self, resample_func):
+        """Test weekly to 7-day model time-step."""
+        # Note that we have to choose a week starting on Wednesday because 2015-01-01 is a Wednesday.
+        input_df = make_df('W-WED')
+        model_index = make_model_index('7D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+        expected_df = expected_df.resample('7D').agg(resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    @pytest.mark.parametrize('resample_func', ['mean', 'max'])
+    def test_nonaligned_weekly_to_7daily(self, resample_func):
+        """Test invalid weekly data to 7-day model time-step."""
+        # Because the weeks do not align with the 7-day time-step it is not possible to align this data.
+        input_df = make_df('W')
+        model_index = make_model_index('7D')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func=resample_func)
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+        expected_df = expected_df.resample('7D').agg(resample_func)
+
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_annual_to_daily(self):
+
+        input_df = make_df('A', '2010-01-01', '2020-01-01')
+        model_index = make_model_index('D', '2010-01-01', '2020-01-01')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index)
+
+        expected_df = input_df.resample('D').ffill()
+        expected_df = expected_df['2010-01-01':'2020-01-01']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_annual_to_monthly(self):
+
+        input_df = make_df('A', '2010-01-01', '2020-01-01')
+        model_index = make_model_index('M', '2010-01-01', '2020-01-01')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func='ffill')
+
+        expected_df = input_df.resample('M').ffill()
+        expected_df = expected_df['2010-01-01':'2020-01-01']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
+
+    def test_two_weekly_to_weekly(self):
+        """Test weekly to two-weekly model time-step."""
+        input_df = make_df('2W')
+        model_index = make_model_index('W')
+
+        input_resampled = align_and_resample_dataframe(input_df, model_index, resample_func='ffill')
+
+        expected_df = input_df.resample('W').ffill()
+        expected_df = expected_df['2015-01-01':'2015-12-31']
+
+        pd.testing.assert_index_equal(input_resampled.index, model_index)
+        pd.testing.assert_frame_equal(input_resampled, expected_df)
```

### Comparing `pywr-1.8.0/tests/test_groundwater.py` & `pywr-1.9.0/tests/test_groundwater.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,91 +1,91 @@
-from pywr.domains.groundwater import KeatingAquifer
-from pywr.parameters.groundwater import KeatingStreamFlowParameter
-from pywr.core import Model, Input, Output, Link
-from pywr.recorders import (NumpyArrayNodeRecorder, NumpyArrayLevelRecorder,
-                            NumpyArrayStorageRecorder)
-import pandas
-import numpy as np
-import pytest
-
-num_streams = 1
-num_additional_inputs = 1
-stream_flow_levels = [[100.0, 125.0]] # m
-transmissivity = [1000, 20000] # m2/d
-transmissivity = [t * 0.001 for t in transmissivity] # m3 to Ml
-coefficient = 1 # no units
-storativity = [0.05] # %
-levels = [0.0, 1000.0] # m
-area = 50000 * 50000 # m2
-
-def test_keating_aquifer():
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01'),
-    )
-
-    aqfer = KeatingAquifer(
-        model,
-        'keating',
-        num_streams,
-        num_additional_inputs,
-        stream_flow_levels,
-        transmissivity,
-        coefficient,
-        levels,
-        area=area,
-        storativity=storativity,
-    )
-
-    catchment = Input(model, 'catchment', max_flow=0)
-    stream = Output(model, 'stream', max_flow=np.inf, cost=0)
-    abstraction = Output(model, 'abstraction', max_flow=15, cost=-999)
-
-    catchment.connect(aqfer)
-    aqfer.connect(stream, from_slot=0)
-    aqfer.connect(abstraction, from_slot=1)
-
-    rec_level = NumpyArrayLevelRecorder(model, aqfer)
-    rec_volume = NumpyArrayStorageRecorder(model, aqfer)
-    rec_stream = NumpyArrayNodeRecorder(model, stream)
-    rec_abstraction = NumpyArrayNodeRecorder(model, abstraction)
-
-    model.check()
-
-    assert(len(aqfer.inputs) == (num_streams + num_additional_inputs))
-
-    for initial_level in (50, 100, 110, 150):
-        # set the inital aquifer level and therefor the initial volume
-        aqfer.initial_level = initial_level
-        initial_volume = aqfer.initial_volume
-        assert(initial_volume == (area * storativity[0] * initial_level * 0.001))
-        # run the model (for one timestep only)
-        model.run()
-        # manually calculate keating streamflow and check model flows are OK
-        Qp = 2 * transmissivity[0] * max(initial_level - stream_flow_levels[0][0], 0) * coefficient
-        Qe = 2 * transmissivity[1] * max(initial_level - stream_flow_levels[0][1], 0) * coefficient
-        delta_storage = initial_volume - rec_volume.data[0, 0]
-        abs_flow = rec_abstraction.data[0, 0]
-        stream_flow = rec_stream.data[0, 0]
-        assert(delta_storage == (stream_flow + abs_flow))
-        assert(stream_flow == (Qp+Qe))
-
-    A_VERY_LARGE_NUMBER = 9999999999999
-    model.timestepper.end = pandas.to_datetime('2016-01-02')
-
-    # fill the aquifer completely
-    # there is no spill for the storage so it should find no feasible solution
-    with pytest.raises(RuntimeError):
-        catchment.max_flow = A_VERY_LARGE_NUMBER
-        catchment.min_flow = A_VERY_LARGE_NUMBER
-        model.run()
-
-    # drain the aquifer completely
-    catchment.min_flow = 0
-    catchment.max_flow = 0
-    abstraction.max_flow = A_VERY_LARGE_NUMBER
-    model.run()
-    assert(rec_volume.data[1, 0] == 0)
-    abs_flow = rec_abstraction.data[1, 0]
-    stream_flow = rec_stream.data[1, 0]
-    assert(stream_flow == 0)
-    assert(abs_flow == 0)
+from pywr.domains.groundwater import KeatingAquifer
+from pywr.parameters.groundwater import KeatingStreamFlowParameter
+from pywr.core import Model, Input, Output, Link
+from pywr.recorders import (NumpyArrayNodeRecorder, NumpyArrayLevelRecorder,
+                            NumpyArrayStorageRecorder)
+import pandas
+import numpy as np
+import pytest
+
+num_streams = 1
+num_additional_inputs = 1
+stream_flow_levels = [[100.0, 125.0]] # m
+transmissivity = [1000, 20000] # m2/d
+transmissivity = [t * 0.001 for t in transmissivity] # m3 to Ml
+coefficient = 1 # no units
+storativity = [0.05] # %
+levels = [0.0, 1000.0] # m
+area = 50000 * 50000 # m2
+
+def test_keating_aquifer():
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01'),
+    )
+
+    aqfer = KeatingAquifer(
+        model,
+        'keating',
+        num_streams,
+        num_additional_inputs,
+        stream_flow_levels,
+        transmissivity,
+        coefficient,
+        levels,
+        area=area,
+        storativity=storativity,
+    )
+
+    catchment = Input(model, 'catchment', max_flow=0)
+    stream = Output(model, 'stream', max_flow=np.inf, cost=0)
+    abstraction = Output(model, 'abstraction', max_flow=15, cost=-999)
+
+    catchment.connect(aqfer)
+    aqfer.connect(stream, from_slot=0)
+    aqfer.connect(abstraction, from_slot=1)
+
+    rec_level = NumpyArrayLevelRecorder(model, aqfer)
+    rec_volume = NumpyArrayStorageRecorder(model, aqfer)
+    rec_stream = NumpyArrayNodeRecorder(model, stream)
+    rec_abstraction = NumpyArrayNodeRecorder(model, abstraction)
+
+    model.check()
+
+    assert(len(aqfer.inputs) == (num_streams + num_additional_inputs))
+
+    for initial_level in (50, 100, 110, 150):
+        # set the inital aquifer level and therefor the initial volume
+        aqfer.initial_level = initial_level
+        initial_volume = aqfer.initial_volume
+        assert(initial_volume == (area * storativity[0] * initial_level * 0.001))
+        # run the model (for one timestep only)
+        model.run()
+        # manually calculate keating streamflow and check model flows are OK
+        Qp = 2 * transmissivity[0] * max(initial_level - stream_flow_levels[0][0], 0) * coefficient
+        Qe = 2 * transmissivity[1] * max(initial_level - stream_flow_levels[0][1], 0) * coefficient
+        delta_storage = initial_volume - rec_volume.data[0, 0]
+        abs_flow = rec_abstraction.data[0, 0]
+        stream_flow = rec_stream.data[0, 0]
+        assert(delta_storage == (stream_flow + abs_flow))
+        assert(stream_flow == (Qp+Qe))
+
+    A_VERY_LARGE_NUMBER = 9999999999999
+    model.timestepper.end = pandas.to_datetime('2016-01-02')
+
+    # fill the aquifer completely
+    # there is no spill for the storage so it should find no feasible solution
+    with pytest.raises(RuntimeError):
+        catchment.max_flow = A_VERY_LARGE_NUMBER
+        catchment.min_flow = A_VERY_LARGE_NUMBER
+        model.run()
+
+    # drain the aquifer completely
+    catchment.min_flow = 0
+    catchment.max_flow = 0
+    abstraction.max_flow = A_VERY_LARGE_NUMBER
+    model.run()
+    assert(rec_volume.data[1, 0] == 0)
+    abs_flow = rec_abstraction.data[1, 0]
+    stream_flow = rec_stream.data[1, 0]
+    assert(stream_flow == 0)
+    assert(abs_flow == 0)
```

### Comparing `pywr-1.8.0/tests/test_hashes.py` & `pywr-1.9.0/tests/test_hashes.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-import os
-import pytest
-from pywr import hashes
-
-
-TEST_FOLDER = os.path.dirname(__file__)
-
-@pytest.mark.parametrize("filename, algorithm, hash, correct",[
-    ('timeseries2.csv', 'md5', 'a5c4032e2d8f5205ca99dedcfa4cd18e', True),
-    ('timeseries2.csv', 'sha256', '0f75b3cee325d37112687d3d10596f44e0add374f4e40a1b6687912c05e65366', True),
-    ('timeseries2.h5', 'md5', '0f6c65a36851c89c7c4e63ab1893554b', True),
-    # This next one is the sha256 hash, but is given as md5. Therefore it should fail.
-    ('timeseries2.h5', 'md5', '1272702d60694f3417b910fb158e717de4fccdbf6aa10aa37f1c95cd78f8075e', False),
-                         ])
-def test_hash_timeseries2_(filename, algorithm, hash, correct):
-    """ Test the hash value of files in the models directory """
-
-    fullname = os.path.join(TEST_FOLDER, 'models', filename)
-
-    if correct:
-        hashes.check_hash(fullname, hash, algorithm=algorithm)
-    else:
-        with pytest.raises(hashes.HashMismatchError):
-            hashes.check_hash(fullname, hash, algorithm=algorithm)
+import os
+import pytest
+from pywr import hashes
+
+
+TEST_FOLDER = os.path.dirname(__file__)
+
+@pytest.mark.parametrize("filename, algorithm, hash, correct",[
+    ('timeseries2.csv', 'md5', 'a5c4032e2d8f5205ca99dedcfa4cd18e', True),
+    ('timeseries2.csv', 'sha256', '0f75b3cee325d37112687d3d10596f44e0add374f4e40a1b6687912c05e65366', True),
+    ('timeseries2.h5', 'md5', '0f6c65a36851c89c7c4e63ab1893554b', True),
+    # This next one is the sha256 hash, but is given as md5. Therefore it should fail.
+    ('timeseries2.h5', 'md5', '1272702d60694f3417b910fb158e717de4fccdbf6aa10aa37f1c95cd78f8075e', False),
+                         ])
+def test_hash_timeseries2_(filename, algorithm, hash, correct):
+    """ Test the hash value of files in the models directory """
+
+    fullname = os.path.join(TEST_FOLDER, 'models', filename)
+
+    if correct:
+        hashes.check_hash(fullname, hash, algorithm=algorithm)
+    else:
+        with pytest.raises(hashes.HashMismatchError):
+            hashes.check_hash(fullname, hash, algorithm=algorithm)
```

### Comparing `pywr-1.8.0/tests/test_license.py` & `pywr-1.9.0/tests/test_license.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,206 +1,206 @@
-#!/usr/bin/env python
-
-import pytest
-from datetime import datetime
-from pywr.core import Timestep, ScenarioIndex
-from pywr.parameters.licenses import License, TimestepLicense, AnnualLicense, AnnualExponentialLicense, AnnualHyperbolaLicense
-from pywr.recorders import NumpyArrayNodeRecorder
-from fixtures import simple_linear_model
-from helpers import load_model
-from numpy.testing import assert_allclose
-import numpy as np
-import pandas
-
-def test_base_license():
-    with pytest.raises(TypeError):
-        lic = License()
-
-
-def test_daily_license(simple_linear_model):
-    '''Test daily licence'''
-    m = simple_linear_model
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-    lic = TimestepLicense(m, None, 42.0)
-    assert(isinstance(lic, License))
-    assert(lic.value(Timestep(pandas.Period('2015-1-1'), 0, 1), si) == 42.0)
-
-    # daily licences don't have resource state
-    assert(lic.resource_state(Timestep(pandas.Period('2015-1-1'), 0, 1)) is None)
-
-
-def test_simple_model_with_annual_licence(simple_linear_model):
-    m = simple_linear_model
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    annual_total = 365
-    lic = AnnualLicense(m, m.nodes["Input"], annual_total)
-    # Apply licence to the model
-    m.nodes["Input"].max_flow = lic
-    m.nodes["Output"].max_flow = 10.0
-    m.nodes["Output"].cost = -10.0
-    m.setup()
-
-    m.step()
-
-    # Licence is a hard constraint of 1.0
-    # timestepper.current is now end of the first day
-    assert_allclose(m.nodes["Output"].flow, 1.0)
-    # Check the constraint for the next timestep.
-    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
-
-    # Now constrain the demand so that licence is not fully used
-    m.nodes["Output"].max_flow = 0.5
-    m.step()
-
-    assert_allclose(m.nodes["Output"].flow, 0.5)
-    # Check the constraint for the next timestep. The available amount should now be larger
-    # due to the reduced use
-    remaining = (annual_total-1.5)
-    assert_allclose(lic.value(m.timestepper._next, si), remaining / (365 - 2))
-
-    # Unconstrain the demand
-    m.nodes["Output"].max_flow = 10.0
-    m.step()
-    assert_allclose(m.nodes["Output"].flow, remaining / (365 - 2))
-    # Licence should now be on track for an expected value of 1.0
-    remaining -= remaining / (365 - 2)
-    assert_allclose(lic.value(m.timestepper._next, si), remaining / (365 - 3))
-
-def test_annual_license_json():
-    """
-    This test demonstrates how an annual licence can be forceably distributed
-    evenly across a year. The licence must build up a surplus before it can
-    use more than the average.
-    """
-    model = load_model("annual_license.json")
-
-    model.timestepper.start = "2001-01-01"
-    model.timestepper.end = "2001-01-31"
-    model.timestepper.delta = 5
-
-    rec = NumpyArrayNodeRecorder(model, model.nodes["supply1"])
-
-    model.run()
-
-    initial_amount = 200.0
-    # first day evenly apportions initial amount for each day of year
-    first_day = initial_amount / 365
-    assert_allclose(rec.data[0], first_day)
-    # second day does the same, minus yesterday and with less days remaining
-    remaining_days = 365 - 5
-    second_day = (initial_amount - first_day * 5) / remaining_days
-    assert_allclose(rec.data[1], second_day)
-    # actual amount is the same as maximum was taken
-    assert_allclose(first_day, second_day)
-    # third day nothing is taken (no demand), so licence is saved
-    assert_allclose(rec.data[2], 0.0)
-    # fourth day more can be supplied as we've built up a surplus
-    remaining_days = 365 - 5 * 3
-    fourth_day = (initial_amount - (first_day+second_day)*5) / remaining_days
-    assert_allclose(rec.data[3], fourth_day)
-    assert fourth_day > first_day
-
-
-def test_simple_model_with_annual_licence_multi_year(simple_linear_model):
-    """ Test the AnnualLicense over multiple years
-    """
-    import pandas as pd
-    import datetime, calendar
-    m = simple_linear_model
-    # Modify model to run for 3 years of non-leap years at 30 day time-step.
-    m.timestepper.start = pd.to_datetime('2017-1-1')
-    m.timestepper.end = pd.to_datetime('2020-1-1')
-    m.timestepper.delta = datetime.timedelta(30)
-
-    annual_total = 365.0
-    lic = AnnualLicense(m, m.nodes["Input"], annual_total)
-    # Apply licence to the model
-    m.nodes["Input"].max_flow = lic
-    m.nodes["Output"].max_flow = 10.0
-    m.nodes["Output"].cost = -10.0
-    m.setup()
-
-    for i in range(len(m.timestepper)):
-        m.step()
-        days_in_year = 365 + int(calendar.isleap(m.timestepper.current.datetime.year))
-        assert_allclose(m.nodes["Output"].flow, annual_total/days_in_year)
-
-
-def test_simple_model_with_exponential_license(simple_linear_model):
-    m = simple_linear_model
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    annual_total = 365
-    # Expoential licence with max_value of e should give a hard constraint of 1.0 when on track
-    lic = AnnualExponentialLicense(m, m.nodes["Input"], annual_total, np.e)
-    # Apply licence to the model
-    m.nodes["Input"].max_flow = lic
-    m.nodes["Output"].max_flow = 10.0
-    m.nodes["Output"].cost = -10.0
-    m.setup()
-
-    m.step()
-
-    # Licence is a hard constraint of 1.0
-    # timestepper.current is now end of the first day
-    assert_allclose(m.nodes["Output"].flow, 1.0)
-    # Check the constraint for the next timestep.
-    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
-
-    # Now constrain the demand so that licence is not fully used
-    m.nodes["Output"].max_flow = 0.5
-    m.step()
-
-    assert_allclose(m.nodes["Output"].flow, 0.5)
-    # Check the constraint for the next timestep. The available amount should now be larger
-    # due to the reduced use
-    remaining = (annual_total-1.5)
-    assert_allclose(lic.value(m.timestepper._next, si), np.exp(-remaining / (365 - 2) + 1))
-
-    # Unconstrain the demand
-    m.nodes["Output"].max_flow = 10.0
-    m.step()
-    assert_allclose(m.nodes["Output"].flow, np.exp(-remaining / (365 - 2) + 1))
-    # Licence should now be on track for an expected value of 1.0
-    remaining -= np.exp(-remaining / (365 - 2) + 1)
-    assert_allclose(lic.value(m.timestepper._next, si), np.exp(-remaining / (365 - 3) + 1))
-
-
-def test_simple_model_with_hyperbola_license(simple_linear_model):
-    m = simple_linear_model
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    annual_total = 365
-    # Expoential licence with max_value of e should give a hard constraint of 1.0 when on track
-    lic = AnnualHyperbolaLicense(m, m.nodes["Input"], annual_total, 1.0)
-    # Apply licence to the model
-    m.nodes["Input"].max_flow = lic
-    m.nodes["Output"].max_flow = 10.0
-    m.nodes["Output"].cost = -10.0
-    m.setup()
-
-    m.step()
-
-    # Licence is a hard constraint of 1.0
-    # timestepper.current is now end of the first day
-    assert_allclose(m.nodes["Output"].flow, 1.0)
-    # Check the constraint for the next timestep.
-    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
-
-    # Now constrain the demand so that licence is not fully used
-    m.nodes["Output"].max_flow = 0.5
-    m.step()
-
-    assert_allclose(m.nodes["Output"].flow, 0.5)
-    # Check the constraint for the next timestep. The available amount should now be larger
-    # due to the reduced use
-    remaining = (annual_total-1.5)
-    assert_allclose(lic.value(m.timestepper._next, si), (365 - 2) / remaining)
-
-    # Unconstrain the demand
-    m.nodes["Output"].max_flow = 10.0
-    m.step()
-    assert_allclose(m.nodes["Output"].flow, (365 - 2) / remaining)
-    # Licence should now be on track for an expected value of 1.0
-    remaining -= (365 - 2) / remaining
-    assert_allclose(lic.value(m.timestepper._next, si), (365 - 3) / remaining)
+#!/usr/bin/env python
+
+import pytest
+from datetime import datetime
+from pywr.core import Timestep, ScenarioIndex
+from pywr.parameters.licenses import License, TimestepLicense, AnnualLicense, AnnualExponentialLicense, AnnualHyperbolaLicense
+from pywr.recorders import NumpyArrayNodeRecorder
+from fixtures import simple_linear_model
+from helpers import load_model
+from numpy.testing import assert_allclose
+import numpy as np
+import pandas
+
+def test_base_license():
+    with pytest.raises(TypeError):
+        lic = License()
+
+
+def test_daily_license(simple_linear_model):
+    '''Test daily licence'''
+    m = simple_linear_model
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+    lic = TimestepLicense(m, None, 42.0)
+    assert(isinstance(lic, License))
+    assert(lic.value(Timestep(pandas.Period('2015-1-1'), 0, 1), si) == 42.0)
+
+    # daily licences don't have resource state
+    assert(lic.resource_state(Timestep(pandas.Period('2015-1-1'), 0, 1)) is None)
+
+
+def test_simple_model_with_annual_licence(simple_linear_model):
+    m = simple_linear_model
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    annual_total = 365
+    lic = AnnualLicense(m, m.nodes["Input"], annual_total)
+    # Apply licence to the model
+    m.nodes["Input"].max_flow = lic
+    m.nodes["Output"].max_flow = 10.0
+    m.nodes["Output"].cost = -10.0
+    m.setup()
+
+    m.step()
+
+    # Licence is a hard constraint of 1.0
+    # timestepper.current is now end of the first day
+    assert_allclose(m.nodes["Output"].flow, 1.0)
+    # Check the constraint for the next timestep.
+    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
+
+    # Now constrain the demand so that licence is not fully used
+    m.nodes["Output"].max_flow = 0.5
+    m.step()
+
+    assert_allclose(m.nodes["Output"].flow, 0.5)
+    # Check the constraint for the next timestep. The available amount should now be larger
+    # due to the reduced use
+    remaining = (annual_total-1.5)
+    assert_allclose(lic.value(m.timestepper._next, si), remaining / (365 - 2))
+
+    # Unconstrain the demand
+    m.nodes["Output"].max_flow = 10.0
+    m.step()
+    assert_allclose(m.nodes["Output"].flow, remaining / (365 - 2))
+    # Licence should now be on track for an expected value of 1.0
+    remaining -= remaining / (365 - 2)
+    assert_allclose(lic.value(m.timestepper._next, si), remaining / (365 - 3))
+
+def test_annual_license_json():
+    """
+    This test demonstrates how an annual licence can be forceably distributed
+    evenly across a year. The licence must build up a surplus before it can
+    use more than the average.
+    """
+    model = load_model("annual_license.json")
+
+    model.timestepper.start = "2001-01-01"
+    model.timestepper.end = "2001-01-31"
+    model.timestepper.delta = 5
+
+    rec = NumpyArrayNodeRecorder(model, model.nodes["supply1"])
+
+    model.run()
+
+    initial_amount = 200.0
+    # first day evenly apportions initial amount for each day of year
+    first_day = initial_amount / 365
+    assert_allclose(rec.data[0], first_day)
+    # second day does the same, minus yesterday and with less days remaining
+    remaining_days = 365 - 5
+    second_day = (initial_amount - first_day * 5) / remaining_days
+    assert_allclose(rec.data[1], second_day)
+    # actual amount is the same as maximum was taken
+    assert_allclose(first_day, second_day)
+    # third day nothing is taken (no demand), so licence is saved
+    assert_allclose(rec.data[2], 0.0)
+    # fourth day more can be supplied as we've built up a surplus
+    remaining_days = 365 - 5 * 3
+    fourth_day = (initial_amount - (first_day+second_day)*5) / remaining_days
+    assert_allclose(rec.data[3], fourth_day)
+    assert fourth_day > first_day
+
+
+def test_simple_model_with_annual_licence_multi_year(simple_linear_model):
+    """ Test the AnnualLicense over multiple years
+    """
+    import pandas as pd
+    import datetime, calendar
+    m = simple_linear_model
+    # Modify model to run for 3 years of non-leap years at 30 day time-step.
+    m.timestepper.start = pd.to_datetime('2017-1-1')
+    m.timestepper.end = pd.to_datetime('2020-1-1')
+    m.timestepper.delta = datetime.timedelta(30)
+
+    annual_total = 365.0
+    lic = AnnualLicense(m, m.nodes["Input"], annual_total)
+    # Apply licence to the model
+    m.nodes["Input"].max_flow = lic
+    m.nodes["Output"].max_flow = 10.0
+    m.nodes["Output"].cost = -10.0
+    m.setup()
+
+    for i in range(len(m.timestepper)):
+        m.step()
+        days_in_year = 365 + int(calendar.isleap(m.timestepper.current.datetime.year))
+        assert_allclose(m.nodes["Output"].flow, annual_total/days_in_year)
+
+
+def test_simple_model_with_exponential_license(simple_linear_model):
+    m = simple_linear_model
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    annual_total = 365
+    # Expoential licence with max_value of e should give a hard constraint of 1.0 when on track
+    lic = AnnualExponentialLicense(m, m.nodes["Input"], annual_total, np.e)
+    # Apply licence to the model
+    m.nodes["Input"].max_flow = lic
+    m.nodes["Output"].max_flow = 10.0
+    m.nodes["Output"].cost = -10.0
+    m.setup()
+
+    m.step()
+
+    # Licence is a hard constraint of 1.0
+    # timestepper.current is now end of the first day
+    assert_allclose(m.nodes["Output"].flow, 1.0)
+    # Check the constraint for the next timestep.
+    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
+
+    # Now constrain the demand so that licence is not fully used
+    m.nodes["Output"].max_flow = 0.5
+    m.step()
+
+    assert_allclose(m.nodes["Output"].flow, 0.5)
+    # Check the constraint for the next timestep. The available amount should now be larger
+    # due to the reduced use
+    remaining = (annual_total-1.5)
+    assert_allclose(lic.value(m.timestepper._next, si), np.exp(-remaining / (365 - 2) + 1))
+
+    # Unconstrain the demand
+    m.nodes["Output"].max_flow = 10.0
+    m.step()
+    assert_allclose(m.nodes["Output"].flow, np.exp(-remaining / (365 - 2) + 1))
+    # Licence should now be on track for an expected value of 1.0
+    remaining -= np.exp(-remaining / (365 - 2) + 1)
+    assert_allclose(lic.value(m.timestepper._next, si), np.exp(-remaining / (365 - 3) + 1))
+
+
+def test_simple_model_with_hyperbola_license(simple_linear_model):
+    m = simple_linear_model
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    annual_total = 365
+    # Expoential licence with max_value of e should give a hard constraint of 1.0 when on track
+    lic = AnnualHyperbolaLicense(m, m.nodes["Input"], annual_total, 1.0)
+    # Apply licence to the model
+    m.nodes["Input"].max_flow = lic
+    m.nodes["Output"].max_flow = 10.0
+    m.nodes["Output"].cost = -10.0
+    m.setup()
+
+    m.step()
+
+    # Licence is a hard constraint of 1.0
+    # timestepper.current is now end of the first day
+    assert_allclose(m.nodes["Output"].flow, 1.0)
+    # Check the constraint for the next timestep.
+    assert_allclose(lic.value(m.timestepper._next, si), 1.0)
+
+    # Now constrain the demand so that licence is not fully used
+    m.nodes["Output"].max_flow = 0.5
+    m.step()
+
+    assert_allclose(m.nodes["Output"].flow, 0.5)
+    # Check the constraint for the next timestep. The available amount should now be larger
+    # due to the reduced use
+    remaining = (annual_total-1.5)
+    assert_allclose(lic.value(m.timestepper._next, si), (365 - 2) / remaining)
+
+    # Unconstrain the demand
+    m.nodes["Output"].max_flow = 10.0
+    m.step()
+    assert_allclose(m.nodes["Output"].flow, (365 - 2) / remaining)
+    # Licence should now be on track for an expected value of 1.0
+    remaining -= (365 - 2) / remaining
+    assert_allclose(lic.value(m.timestepper._next, si), (365 - 3) / remaining)
```

### Comparing `pywr-1.8.0/tests/test_named_iterator.py` & `pywr-1.9.0/tests/test_named_iterator.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-import pytest
-
-from pywr.model import NamedIterator
-
-class Item:
-    def __init__(self, name):
-        self.name = name
-
-@pytest.fixture
-def items():
-    return [
-        Item("Hello"),
-        Item("World"),
-        Item("River Thames"),
-    ]
-
-@pytest.fixture
-def iterator(items):
-    i = NamedIterator()
-    for item in items:
-        i[item.name] = item
-    return i
-
-
-def test_iterator_keys_and_values(items, iterator):
-    expected_names = {"Hello", "World", "River Thames"}
-    assert set(iterator.keys()) == expected_names
-    assert set(iterator.values()) == set(items)
-
-
-def test_iterator_contains(items, iterator):
-    assert "Hello" in iterator
-    assert "River Thames" in iterator
-    assert items[0] in iterator
-    assert items[-1] in iterator
-
-
-def test_iterator_length(iterator):
-    assert len(iterator) == 3
-
-
-def test_iterator_delete(iterator):
-    del iterator["World"]
-    assert {i.name for i in iterator} == {"Hello", "River Thames"}
-
-
-def test_iterator_append(iterator):
-    new_item = Item("New Item")
-    iterator.append(new_item)
-    assert len(iterator) == 4
-    assert new_item in iterator
+import pytest
+
+from pywr.model import NamedIterator
+
+class Item:
+    def __init__(self, name):
+        self.name = name
+
+@pytest.fixture
+def items():
+    return [
+        Item("Hello"),
+        Item("World"),
+        Item("River Thames"),
+    ]
+
+@pytest.fixture
+def iterator(items):
+    i = NamedIterator()
+    for item in items:
+        i[item.name] = item
+    return i
+
+
+def test_iterator_keys_and_values(items, iterator):
+    expected_names = {"Hello", "World", "River Thames"}
+    assert set(iterator.keys()) == expected_names
+    assert set(iterator.values()) == set(items)
+
+
+def test_iterator_contains(items, iterator):
+    assert "Hello" in iterator
+    assert "River Thames" in iterator
+    assert items[0] in iterator
+    assert items[-1] in iterator
+
+
+def test_iterator_length(iterator):
+    assert len(iterator) == 3
+
+
+def test_iterator_delete(iterator):
+    del iterator["World"]
+    assert {i.name for i in iterator} == {"Hello", "River Thames"}
+
+
+def test_iterator_append(iterator):
+    new_item = Item("New Item")
+    iterator.append(new_item)
+    assert len(iterator) == 4
+    assert new_item in iterator
```

### Comparing `pywr-1.8.0/tests/test_node_iterator.py` & `pywr-1.9.0/tests/test_node_iterator.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-from pywr.nodes import Node
-
-from fixtures import simple_linear_model, simple_storage_model
-
-model = simple_linear_model
-
-
-def test_keys_and_values(model):
-    expected_names = {"Input", "Link", "Output"}
-    assert set(model.nodes.keys()) == expected_names
-    nodes = {node for node in model.nodes}
-    assert len(nodes) == 3
-    assert set(model.nodes.values()) == nodes
-
-
-def test_contains(model):
-    assert "Input" in model.nodes
-    assert "Output" in model.nodes
-    output = model.nodes["Output"]
-    assert isinstance(output, Node)
-    assert output in model.nodes
-
-
-def test_delete(model):
-    del(model.nodes["Output"])
-    assert len(model.nodes) == 2
-    assert {node.name for node in model.nodes} == {"Input", "Link"}
-
-
-def test_delete_compound_node(simple_storage_model):
-    """Removal of a compound node removes child nodes also"""
-    assert len(list(simple_storage_model.nodes._nodes(hide_children=False))) == 5
-    del(simple_storage_model.nodes["Storage"])
-    assert len(list(simple_storage_model.nodes._nodes(hide_children=False))) == 2
-    assert {node.name for node in simple_storage_model.nodes} == {"Input", "Output"}
+from pywr.nodes import Node
+
+from fixtures import simple_linear_model, simple_storage_model
+
+model = simple_linear_model
+
+
+def test_keys_and_values(model):
+    expected_names = {"Input", "Link", "Output"}
+    assert set(model.nodes.keys()) == expected_names
+    nodes = {node for node in model.nodes}
+    assert len(nodes) == 3
+    assert set(model.nodes.values()) == nodes
+
+
+def test_contains(model):
+    assert "Input" in model.nodes
+    assert "Output" in model.nodes
+    output = model.nodes["Output"]
+    assert isinstance(output, Node)
+    assert output in model.nodes
+
+
+def test_delete(model):
+    del(model.nodes["Output"])
+    assert len(model.nodes) == 2
+    assert {node.name for node in model.nodes} == {"Input", "Link"}
+
+
+def test_delete_compound_node(simple_storage_model):
+    """Removal of a compound node removes child nodes also"""
+    assert len(list(simple_storage_model.nodes._nodes(hide_children=False))) == 5
+    del(simple_storage_model.nodes["Storage"])
+    assert len(list(simple_storage_model.nodes._nodes(hide_children=False))) == 2
+    assert {node.name for node in simple_storage_model.nodes} == {"Input", "Output"}
```

### Comparing `pywr-1.8.0/tests/test_notebook.py` & `pywr-1.9.0/tests/test_notebook.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-import pytest
-from numpy.testing import assert_array_equal
-from pywr.notebook import pywr_model_to_d3_json, pywr_json_to_d3_json, PywrSchematic
-import json
-import os
-
-from helpers import load_model
-
-
-def get_node(nodes, name):
-    for node in nodes:
-        if node["name"] == name:
-            return node
-
-
-def get_node_attribute(node, attr_name):
-    for attr in node["attributes"]:
-        if attr["attribute"] == attr_name:
-            return attr
-
-
-class TestPywrSchematic:
-    def test_from_json(self):
-        json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
-        schematic = PywrSchematic(json_path)
-        assert "nodes" in schematic.graph.keys()
-        assert "links" in schematic.graph.keys()
-
-    def test_from_dict(self):
-        json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
-        with open(json_path) as fh:
-            json_data = json.load(fh)
-        schematic = PywrSchematic(json_data)
-        assert "nodes" in schematic.graph.keys()
-        assert "links" in schematic.graph.keys()
-
-    def test_from_model(self):
-        model = load_model("demand_saving2_with_variables.json")
-        schematic = PywrSchematic(model)
-        assert "nodes" in schematic.graph.keys()
-        assert "links" in schematic.graph.keys()
-
-
-@pytest.mark.parametrize("from_json", [True, False])
-def test_from_json(from_json):
-    json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
-
-    if from_json:
-        json_dict = pywr_json_to_d3_json(json_path, attributes=True)
-    else:
-        model = load_model("demand_saving2_with_variables.json")
-        json_dict = pywr_model_to_d3_json(model, attributes=True)
-
-    assert "nodes" in json_dict.keys()
-    assert "links" in json_dict.keys()
-
-    node_names = ["Inflow", "Reservoir", "Demand", "Spill"]
-    for node in json_dict["nodes"]:
-        assert node["name"] in node_names
-
-        if node["name"] == "Reservoir":
-            assert_array_equal(node["position"], [1, 1])
-
-    demand = get_node(json_dict["nodes"], "Demand")
-    demand_max_flow = get_node_attribute(demand, "max_flow")
-
-    assert demand_max_flow["value"] == "demand_max_flow - AggregatedParameter"
-
-
-def test_d3_data():
-    """Test returned by `pywr_json_to_d3_json` and `pywr_model_to_d3_json` is similar.
-
-    These return graph data from a JSON file and Model instance respectively. Here we test that each returns the
-    same node names and number on links. The data won't match exactly due to differences in node ordering.
-    """
-    json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
-    model = load_model("demand_saving2_with_variables.json")
-
-    d3_data_from_json = pywr_json_to_d3_json(json_path)
-    d3_data_from_model = pywr_model_to_d3_json(model)
-
-    json_nodes = {n["name"]: n for n in d3_data_from_json["nodes"]}
-    model_nodes = {n["name"]: n for n in d3_data_from_model["nodes"]}
-
-    assert json_nodes == model_nodes
-    assert len(d3_data_from_json["links"]) == len(d3_data_from_model["links"])
+import pytest
+from numpy.testing import assert_array_equal
+from pywr.notebook import pywr_model_to_d3_json, pywr_json_to_d3_json, PywrSchematic
+import json
+import os
+
+from helpers import load_model
+
+
+def get_node(nodes, name):
+    for node in nodes:
+        if node["name"] == name:
+            return node
+
+
+def get_node_attribute(node, attr_name):
+    for attr in node["attributes"]:
+        if attr["attribute"] == attr_name:
+            return attr
+
+
+class TestPywrSchematic:
+    def test_from_json(self):
+        json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
+        schematic = PywrSchematic(json_path)
+        assert "nodes" in schematic.graph.keys()
+        assert "links" in schematic.graph.keys()
+
+    def test_from_dict(self):
+        json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
+        with open(json_path) as fh:
+            json_data = json.load(fh)
+        schematic = PywrSchematic(json_data)
+        assert "nodes" in schematic.graph.keys()
+        assert "links" in schematic.graph.keys()
+
+    def test_from_model(self):
+        model = load_model("demand_saving2_with_variables.json")
+        schematic = PywrSchematic(model)
+        assert "nodes" in schematic.graph.keys()
+        assert "links" in schematic.graph.keys()
+
+
+@pytest.mark.parametrize("from_json", [True, False])
+def test_from_json(from_json):
+    json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
+
+    if from_json:
+        json_dict = pywr_json_to_d3_json(json_path, attributes=True)
+    else:
+        model = load_model("demand_saving2_with_variables.json")
+        json_dict = pywr_model_to_d3_json(model, attributes=True)
+
+    assert "nodes" in json_dict.keys()
+    assert "links" in json_dict.keys()
+
+    node_names = ["Inflow", "Reservoir", "Demand", "Spill"]
+    for node in json_dict["nodes"]:
+        assert node["name"] in node_names
+
+        if node["name"] == "Reservoir":
+            assert_array_equal(node["position"], [1, 1])
+
+    demand = get_node(json_dict["nodes"], "Demand")
+    demand_max_flow = get_node_attribute(demand, "max_flow")
+
+    assert demand_max_flow["value"] == "demand_max_flow - AggregatedParameter"
+
+
+def test_d3_data():
+    """Test returned by `pywr_json_to_d3_json` and `pywr_model_to_d3_json` is similar.
+
+    These return graph data from a JSON file and Model instance respectively. Here we test that each returns the
+    same node names and number on links. The data won't match exactly due to differences in node ordering.
+    """
+    json_path = os.path.join(os.path.dirname(__file__), "models", "demand_saving2_with_variables.json")
+    model = load_model("demand_saving2_with_variables.json")
+
+    d3_data_from_json = pywr_json_to_d3_json(json_path)
+    d3_data_from_model = pywr_model_to_d3_json(model)
+
+    json_nodes = {n["name"]: n for n in d3_data_from_json["nodes"]}
+    model_nodes = {n["name"]: n for n in d3_data_from_model["nodes"]}
+
+    assert json_nodes == model_nodes
+    assert len(d3_data_from_json["links"]) == len(d3_data_from_model["links"])
```

### Comparing `pywr-1.8.0/tests/test_parameters.py` & `pywr-1.9.0/tests/test_parameters.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,1866 +1,1866 @@
-"""
-Test for individual Parameter classes
-"""
-from pywr.core import Model, Timestep, Scenario, ScenarioIndex, Storage, Link, Input, Output
-from pywr.parameters import (Parameter, ArrayIndexedParameter, ConstantScenarioParameter,
-    ArrayIndexedScenarioMonthlyFactorsParameter, MonthlyProfileParameter, DailyProfileParameter,
-    DataFrameParameter, AggregatedParameter, ConstantParameter, ConstantScenarioIndexParameter,
-    IndexParameter, AggregatedIndexParameter, RecorderThresholdParameter, ScenarioMonthlyProfileParameter,
-    ScenarioWeeklyProfileParameter, Polynomial1DParameter, Polynomial2DStorageParameter, ArrayIndexedScenarioParameter,
-    InterpolatedParameter, WeeklyProfileParameter, InterpolatedQuadratureParameter, PiecewiseIntegralParameter,
-    FunctionParameter, AnnualHarmonicSeriesParameter, load_parameter, InterpolatedFlowParameter,
-    ScenarioDailyProfileParameter)
-from pywr.recorders import AssertionRecorder, assert_rec
-from pywr.model import OrphanedParameterWarning
-from pywr.dataframe_tools import ResamplingError
-from pywr.recorders import Recorder
-from fixtures import simple_linear_model, simple_storage_model
-from helpers import load_model
-import json
-import os
-import datetime
-import numpy as np
-import pandas as pd
-import pytest
-import itertools
-import calendar
-from numpy.testing import assert_allclose
-from scipy.interpolate import Rbf
-
-TEST_DIR = os.path.dirname(__file__)
-
-@pytest.fixture
-def model():
-    return Model()
-
-
-class TestConstantParameter:
-    """ Tests for `ConstantParameter` """
-    def test_basic_use(self, simple_linear_model):
-        """ Test the basic use of `ConstantParameter` using the Python API """
-        model = simple_linear_model
-        # Add two scenarios
-        scA = Scenario(model, 'Scenario A', size=2)
-        scB = Scenario(model, 'Scenario B', size=5)
-
-        p = ConstantParameter(model, np.pi, name='pi', comment='Mmmmm Pi!')
-
-        assert not p.is_variable
-        assert p.double_size == 1
-        assert p.integer_size == 0
-
-        model.setup()
-        ts = model.timestepper.current
-        # Now ensure the appropriate value is returned for all scenarios
-        for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
-            si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
-            np.testing.assert_allclose(p.value(ts, si), np.pi)
-
-    def test_being_a_variable(self, simple_linear_model):
-        """ Test the basic use of `ConstantParameter` when `is_variable=True` """
-        model = simple_linear_model
-        p = ConstantParameter(model, np.pi, name='pi', comment='Mmmmm Pi!', is_variable=True,
-                              lower_bounds=np.pi/2, upper_bounds=2*np.pi)
-        model.setup()
-
-        assert p.is_variable
-        assert p.double_size == 1
-        assert p.integer_size == 0
-
-        np.testing.assert_allclose(p.get_double_lower_bounds(), np.array([np.pi/2]))
-        np.testing.assert_allclose(p.get_double_upper_bounds(), np.array([2*np.pi]))
-
-        np.testing.assert_allclose(p.get_double_variables(), np.array([np.pi]))
-
-        # No test updating the variables
-        p.set_double_variables(np.array([1.5*np.pi, ]))
-        np.testing.assert_allclose(p.get_double_variables(), np.array([1.5*np.pi]))
-
-        # None of the integer functions should be implemented because this parameter
-        # has no integer variables
-        with pytest.raises(NotImplementedError):
-            p.get_integer_lower_bounds()
-
-        with pytest.raises(NotImplementedError):
-            p.get_integer_upper_bounds()
-
-        with pytest.raises(NotImplementedError):
-            p.get_integer_variables()
-
-
-def test_parameter_array_indexed(simple_linear_model):
-    """
-    Test ArrayIndexedParameter
-
-    """
-    model = simple_linear_model
-    A = np.arange(len(model.timestepper), dtype=np.float64)
-    p = ArrayIndexedParameter(model, A)
-    model.setup()
-    # scenario indices (not used for this test)
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-    for v, ts in zip(A, model.timestepper):
-        np.testing.assert_allclose(p.value(ts, si), v)
-
-    # Now check that IndexError is raised if an out of bounds Timestep is given.
-    ts = Timestep(pd.Period('2016-01-01', freq='1D'), 366, 1)
-    with pytest.raises(IndexError):
-        p.value(ts, si)
-
-
-def test_parameter_array_indexed_json_load(simple_linear_model, tmpdir):
-    """Test ArrayIndexedParameter can be loaded from json dict"""
-    model = simple_linear_model
-    # Daily time-step
-    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
-    df = pd.DataFrame(np.arange(365), index=index, columns=['data'])
-    df_path = tmpdir.join('df.csv')
-    df.to_csv(str(df_path))
-
-    data = {
-        'type': 'arrayindexed',
-        'url': str(df_path),
-        'index_col': 'date',
-        'parse_dates': True,
-        'column': 'data',
-    }
-
-    p = load_parameter(model, data)
-    model.setup()
-
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-    for v, ts in enumerate(model.timestepper):
-        np.testing.assert_allclose(p.value(ts, si), v)
-
-def test_parameter_constant_scenario(simple_linear_model):
-    """
-    Test ConstantScenarioParameter
-
-    """
-    model = simple_linear_model
-    # Add two scenarios
-    scA = Scenario(model, 'Scenario A', size=2)
-    scB = Scenario(model, 'Scenario B', size=5)
-
-    p = ConstantScenarioParameter(model, scB, np.arange(scB.size, dtype=np.float64))
-    model.setup()
-    ts = model.timestepper.current
-    # Now ensure the appropriate value is returned for the Scenario B indices.
-    for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
-        si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
-        np.testing.assert_allclose(p.value(ts, si), float(b))
-
-
-def test_parameter_constant_scenario(simple_linear_model):
-    """
-    Test ConstantScenarioIndexParameter
-
-    """
-    model = simple_linear_model
-    # Add two scenarios
-    scA = Scenario(model, 'Scenario A', size=2)
-    scB = Scenario(model, 'Scenario B', size=5)
-
-    p = ConstantScenarioIndexParameter(model, scB, np.arange(scB.size, dtype=np.int32))
-    model.setup()
-    ts = model.timestepper.current
-    # Now ensure the appropriate value is returned for the Scenario B indices.
-    for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
-        si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
-        np.testing.assert_allclose(p.index(ts, si), b)
-
-
-def test_parameter_array_indexed_scenario_monthly_factors(simple_linear_model):
-    """
-    Test ArrayIndexedParameterScenarioMonthlyFactors
-
-    """
-    model = simple_linear_model
-    # Baseline timeseries data
-    values = np.arange(len(model.timestepper), dtype=np.float64)
-
-    # Add two scenarios
-    scA = Scenario(model, 'Scenario A', size=2)
-    scB = Scenario(model, 'Scenario B', size=5)
-
-    # Random factors for each Scenario B value per month
-    factors = np.random.rand(scB.size, 12)
-
-    p = ArrayIndexedScenarioMonthlyFactorsParameter(model, scB, values, factors)
-    model.setup()
-
-    # Iterate in time
-    for v, ts in zip(values, model.timestepper):
-        imth = ts.datetime.month - 1
-        # Now ensure the appropriate value is returned for the Scenario B indices.
-        for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
-            f = factors[b, imth]
-            si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
-            np.testing.assert_allclose(p.value(ts, si), v*f)
-
-def test_parameter_array_indexed_scenario_monthly_factors_json(model):
-    model.path = os.path.join(TEST_DIR, "models")
-    scA = Scenario(model, 'Scenario A', size=2)
-    scB = Scenario(model, 'Scenario B', size=3)
-
-    p1 = ArrayIndexedScenarioMonthlyFactorsParameter.load(model, {
-        "scenario": "Scenario A",
-        "values": list(range(32)),
-        "factors": [list(range(1, 13)),list(range(13, 25))],
-    })
-
-    p2 = ArrayIndexedScenarioMonthlyFactorsParameter.load(model, {
-        "scenario": "Scenario B",
-        "values": {
-            "url": "timeseries1.csv",
-            "index_col": "Timestamp",
-            "column": "Data",
-        },
-        "factors": {
-            "url": "monthly_profiles.csv",
-            "index_col": "scenario",
-        },
-    })
-
-    node1 = Input(model, "node1", max_flow=p1)
-    node2 = Input(model, "node2", max_flow=p2)
-    nodeN = Output(model, "nodeN", max_flow=None, cost=-1)
-    node1.connect(nodeN)
-    node2.connect(nodeN)
-
-    model.timestepper.start = "2015-01-01"
-    model.timestepper.end = "2015-01-31"
-    model.run()
-
-
-class TestMonthlyProfileParameter:
-
-    def test_no_interpolation(self, simple_linear_model):
-        """Test no-interpolation. """
-        model = simple_linear_model
-        values = np.arange(12, dtype=np.float64)
-        p = MonthlyProfileParameter(model, values)
-        model.setup()
-
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            imth = timestep.month - 1
-            return values[imth]
-
-        model.run()
-
-    def test_interpolation_month_start(self, simple_linear_model):
-        """Test interpolating monthly values from first day of the month."""
-        model = simple_linear_model
-        values = np.arange(12, dtype=np.float64)
-        p = MonthlyProfileParameter(model, values, interp_day='first')
-        model.setup()
-
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            imth = timestep.month - 1
-            days_in_month = calendar.monthrange(timestep.year, timestep.month)[1]
-            day = timestep.day
-
-            # Perform linear interpolation
-            x = (day - 1) / (days_in_month - 1)
-            return values[imth] * (1 - x) + values[(imth+1) % 12] * x
-        model.run()
-
-    def test_interpolation_month_end(self, simple_linear_model):
-        """Test interpolating monthly values from last day of the month."""
-        model = simple_linear_model
-        values = np.arange(12, dtype=np.float64)
-        p = MonthlyProfileParameter(model, values, interp_day='last')
-        model.setup()
-
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            imth = timestep.month - 1
-            days_in_month = calendar.monthrange(timestep.year, timestep.month)[1]
-            day = timestep.day
-
-            # Perform linear interpolation
-            x = day / days_in_month
-            return values[(imth - 1) % 12] * (1 - x) + values[imth] * x
-        model.run()
-
-
-class TestScenarioMonthlyProfileParameter:
-
-    def test_init(self, simple_linear_model):
-        model = simple_linear_model
-        scenario = Scenario(model, 'A', 10)
-        values = np.random.rand(10, 12)
-
-        p = ScenarioMonthlyProfileParameter(model, scenario, values)
-
-        model.setup()
-        # Iterate in time
-        for ts in model.timestepper:
-            imth = ts.datetime.month - 1
-            for i in range(scenario.size):
-                si = ScenarioIndex(i, np.array([i], dtype=np.int32))
-                np.testing.assert_allclose(p.value(ts, si), values[i, imth])
-
-    def test_json(self):
-        model = load_model('scenario_monthly_profile.json')
-
-        # check first day initalised
-        assert (model.timestepper.start == datetime.datetime(2015, 1, 1))
-
-        # check results
-        supply1 = model.nodes['supply1']
-
-        # Multiplication factors
-        factors = np.array([
-            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
-            [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],
-            [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22],
-        ])
-
-        for expected in (23.92, 22.14, 22.57, 24.97, 27.59):
-            model.step()
-            imth = model.timestepper.current.month - 1
-            assert_allclose(supply1.flow, expected*factors[:, imth], atol=1e-7)
-
-def test_parameter_daily_profile(simple_linear_model):
-    """
-    Test DailyProfileParameter
-
-    """
-    model = simple_linear_model
-    values = np.arange(366, dtype=np.float64)
-    p = DailyProfileParameter(model, values)
-    model.setup()
-
-    # Iterate in time
-    for ts in model.timestepper:
-        month = ts.datetime.month
-        day = ts.datetime.day
-        iday = int((datetime.datetime(2016, month, day) - datetime.datetime(2016, 1, 1)).days)
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-        np.testing.assert_allclose(p.value(ts, si), values[iday])
-
-def test_daily_profile_leap_day(model):
-    """Test behaviour of daily profile parameter for leap years
-    """
-    inpt = Input(model, "input")
-    otpt = Output(model, "otpt", max_flow=None, cost=-999)
-    inpt.connect(otpt)
-    inpt.max_flow = DailyProfileParameter(model, np.arange(0, 366, dtype=np.float64))
-
-    # non-leap year
-    model.timestepper.start = pd.to_datetime("2015-01-01")
-    model.timestepper.end = pd.to_datetime("2015-12-31")
-    model.run()
-    assert_allclose(inpt.flow, 365) # NOT 364
-
-    # leap year
-    model.timestepper.start = pd.to_datetime("2016-01-01")
-    model.timestepper.end = pd.to_datetime("2016-12-31")
-    model.run()
-    assert_allclose(inpt.flow, 365)
-
-class TestScenarioDailyProfileParameter:
-
-    def test_scenario_daily_profile(self, simple_linear_model):
-
-        model = simple_linear_model
-        scenario = Scenario(model, 'A', 2)
-        values = np.array([np.arange(366, dtype=np.float64), np.arange(366, 0, -1, dtype=np.float64)])
-
-        # Remove values for 29th feb as not testing leap year in this func
-        expected_values = np.delete(values.T, 59, 0)
-
-        p = ScenarioDailyProfileParameter.load(model, {"scenario": "A", "values": values})
-
-        AssertionRecorder(model, p, expected_data=expected_values)
-
-        model.setup()
-        model.run()
-
-    def test_scenario_daily_profile_leap_day(self, simple_linear_model):
-        """Test behaviour of daily profile parameter for leap years
-        """
-
-        model = simple_linear_model
-        model.timestepper.start = pd.to_datetime("2016-01-01")
-        model.timestepper.end = pd.to_datetime("2016-12-31")
-
-        scenario = Scenario(model, 'A', 2)
-        values = np.array([np.arange(366, dtype=np.float64), np.arange(366, 0, -1, dtype=np.float64)])
-
-        expected_values = values.T
-
-        p = ScenarioDailyProfileParameter(model, scenario, values)
-        AssertionRecorder(model, p, expected_data=expected_values)
-
-        model.setup()
-        model.run()
-
-def test_scenario_weekly_profile(simple_linear_model):
-
-    model = simple_linear_model
-    scenario = Scenario(model, 'A', 2)
-
-    v = np.arange(1, 53, dtype=np.float64)
-    values = np.array([v, v * 2])
-
-    p = ScenarioWeeklyProfileParameter(model, scenario, values)
-
-    @assert_rec(model, p)
-    def expected_func(timestep, scenario_index):
-        day = timestep.dayofyear - 1
-        if day > 58:  # 28th Feb
-            day += 1
-        week = min(day // 7, 51)
-        value = week + 1
-        if scenario_index.global_id == 1:
-            value *= 2
-        return value
-
-    model.setup()
-    model.run()
-
-
-
-def test_weekly_profile(simple_linear_model):
-    model = simple_linear_model
-
-    model.timestepper.start = "2004-01-01"
-    model.timestepper.end = "2005-05-01"
-    model.timestepper.delta = 7
-
-    values = np.arange(0, 52) ** 2 + 27.5
-
-    p = WeeklyProfileParameter.load(model, {"values": values})
-
-    @assert_rec(model, p)
-    def expected_func(timestep, scenario_index):
-        week = int(min((timestep.dayofyear - 1) // 7, 51))
-        value = week ** 2 + 27.5
-        return value
-
-    model.run()
-
-class TestAnnualHarmonicSeriesParameter:
-    """ Tests for `AnnualHarmonicSeriesParameter` """
-    def test_single_harmonic(self, model):
-
-        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25], [np.pi/4])
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        for ts in model.timestepper:
-            doy = (ts.datetime.dayofyear - 1)/365
-            np.testing.assert_allclose(p1.value(ts, si), 0.5 + 0.25*np.cos(doy*2*np.pi + np.pi/4))
-
-    def test_double_harmonic(self, model):
-        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25, 0.3], [np.pi/4, np.pi/3])
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        for ts in model.timestepper:
-            doy = (ts.datetime.dayofyear - 1) /365
-            expected = 0.5 + 0.25*np.cos(doy*2*np.pi + np.pi / 4) + 0.3*np.cos(doy*4*np.pi + np.pi/3)
-            np.testing.assert_allclose(p1.value(ts, si), expected)
-
-    def test_load(self, model):
-
-        data = {
-            "type": "annualharmonicseries",
-            "mean": 0.5,
-            "amplitudes": [0.25],
-            "phases": [np.pi/4]
-        }
-
-        p1 = load_parameter(model, data)
-
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-        for ts in model.timestepper:
-            doy = (ts.datetime.dayofyear - 1) / 365
-            np.testing.assert_allclose(p1.value(ts, si), 0.5 + 0.25 * np.cos(doy * 2 * np.pi + np.pi / 4))
-
-    def test_variable(self, model):
-        """ Test that variable updating works. """
-        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25], [np.pi/4], is_variable=True)
-
-        assert p1.double_size == 3
-        assert p1.integer_size == 0
-
-        new_var = np.array([0.6, 0.1, np.pi/2])
-        p1.set_double_variables(new_var)
-        np.testing.assert_allclose(p1.get_double_variables(), new_var)
-
-        with pytest.raises(NotImplementedError):
-            p1.set_integer_variables(np.arange(3, dtype=np.int32))
-
-        with pytest.raises(NotImplementedError):
-            p1.get_integer_variables()
-
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        for ts in model.timestepper:
-            doy = (ts.datetime.dayofyear - 1)/365
-            np.testing.assert_allclose(p1.value(ts, si), 0.6 + 0.1*np.cos(doy*2*np.pi + np.pi/2))
-
-
-def custom_test_func(array, axis=None):
-    return np.sum(array**2, axis=axis)
-
-
-class TestAggregatedParameter:
-    """Tests for AggregatedParameter"""
-    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "median": np.median, "sum": np.sum}
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "median", "sum"])
-    def test_agg(self, simple_linear_model, agg_func):
-        model = simple_linear_model
-        model.timestepper.delta = 15
-
-        scenarioA = Scenario(model, "Scenario A", size=2)
-        scenarioB = Scenario(model, "Scenario B", size=5)
-
-        values = np.arange(366, dtype=np.float64)
-        p1 = DailyProfileParameter(model, values)
-        p2 = ConstantScenarioParameter(model, scenarioB, np.arange(scenarioB.size, dtype=np.float64))
-
-        p = AggregatedParameter(model, [p1, p2], agg_func=agg_func)
-
-        func = TestAggregatedParameter.funcs[agg_func]
-
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            x = p1.get_value(scenario_index)
-            y = p2.get_value(scenario_index)
-            return func(np.array([x,y]))
-
-        model.run()
-
-    def test_load(self, simple_linear_model):
-        """ Test load from JSON dict"""
-        model = simple_linear_model
-        data = {
-            "type": "aggregated",
-            "agg_func": "product",
-            "parameters": [
-                0.8,
-                {
-                    "type": "monthlyprofile",
-                    "values": list(range(12))
-                }
-            ]
-        }
-
-        p = load_parameter(model, data)
-        # Correct instance is loaded
-        assert isinstance(p, AggregatedParameter)
-
-        @assert_rec(model, p)
-        def expected(timestep, scenario_index):
-            return (timestep.month - 1) * 0.8
-
-        model.run()
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
-    def test_agg_func_get_set(self, model, agg_func):
-        if agg_func == "custom":
-            agg_func = custom_test_func
-        p = AggregatedParameter(model, [], agg_func=agg_func)
-        assert p.agg_func == agg_func
-        p.agg_func = "product"
-        assert p.agg_func == "product"
-
-
-class DummyIndexParameter(IndexParameter):
-    """A simple IndexParameter which returns a constant value"""
-    def __init__(self, model, index, **kwargs):
-        super(DummyIndexParameter, self).__init__(model, **kwargs)
-        self._index = index
-    def index(self, timestep, scenario_index):
-        return self._index
-    def __repr__(self):
-        return "<DummyIndexParameter \"{}\">".format(self.name)
-
-class TestAggregatedIndexParameter:
-    """Tests for AggregatedIndexParameter"""
-    funcs = {"min": np.min, "max": np.max, "sum": np.sum, "product": np.product}
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "sum", "product"])
-    def test_agg(self, simple_linear_model, agg_func):
-        model = simple_linear_model
-        model.timestepper.delta = 1
-        model.timestepper.start = "2017-01-01"
-        model.timestepper.end = "2017-01-03"
-
-        scenarioA = Scenario(model, "Scenario A", size=2)
-        scenarioB = Scenario(model, "Scenario B", size=5)
-
-        p1 = DummyIndexParameter(model, 2)
-        p2 = DummyIndexParameter(model, 3)
-
-        p = AggregatedIndexParameter(model, [p1, p2], agg_func=agg_func)
-
-        func = TestAggregatedIndexParameter.funcs[agg_func]
-
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            x = p1.get_index(scenario_index)
-            y = p2.get_index(scenario_index)
-            return func(np.array([x,y], np.int32))
-
-        model.run()
-
-    def test_agg_anyall(self, simple_linear_model):
-        """Test the "any" and "all" aggregation functions"""
-        model = simple_linear_model
-        model.timestepper.delta = 1
-        model.timestepper.start = "2017-01-01"
-        model.timestepper.end = "2017-01-03"
-
-        scenarioA = Scenario(model, "Scenario A", size=2)
-        scenarioB = Scenario(model, "Scenario B", size=5)
-        num_comb = len(model.scenarios.get_combinations())
-
-        parameters = {
-            0: DummyIndexParameter(model, 0, name="p0"),
-            1: DummyIndexParameter(model, 1, name="p1"),
-            2: DummyIndexParameter(model, 2, name="p2"),
-        }
-
-        data = [(0, 0), (1, 0), (0, 1), (1, 1), (1, 1, 1), (0, 2)]
-        data_parameters = [[parameters[i] for i in d] for d in data]
-        expected = [(np.any(d), np.all(d)) for d in data]
-
-        for n, params in enumerate(data_parameters):
-            for m, agg_func in enumerate(["any", "all"]):
-                p = AggregatedIndexParameter(model, params, agg_func=agg_func)
-                e = np.ones([len(model.timestepper), num_comb]) * expected[n][m]
-                r = AssertionRecorder(model, p, expected_data=e, name="assertion {}-{}".format(n, agg_func))
-
-        model.run()
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
-    def test_agg_func_get_set(self, model, agg_func):
-        if agg_func == "custom":
-            agg_func = custom_test_func
-        p = AggregatedIndexParameter(model, [], agg_func=agg_func)
-        assert p.agg_func == agg_func
-        p.agg_func = "product"
-        assert p.agg_func == "product"
-
-
-def test_parameter_child_variables(model):
-
-    p1 = Parameter(model)
-    # Default parameter
-    assert len(p1.parents) == 0
-    assert len(p1.children) == 0
-
-    c1 = Parameter(model)
-    c1.parents.add(p1)
-    assert len(p1.children) == 1
-    assert c1 in p1.children
-    assert p1 in c1.parents
-
-    # Test third level
-    c2 = Parameter(model)
-    c2.parents.add(c1)
-
-    # Disable parent
-    c1.parents.clear()
-
-    assert len(p1.children) == 0
-
-
-def test_scaled_profile_nested_load(model):
-    """ Test `ScaledProfileParameter` loading with `AggregatedParameter` """
-    model.timestepper.delta = 15
-
-    s = Storage(model, 'Storage', max_volume=100.0, initial_volume=50.0, num_outputs=0)
-    d = Output(model, 'Link')
-    data = {
-        'type': 'scaledprofile',
-        'scale': 50.0,
-        'profile': {
-            'type': 'aggregated',
-            'agg_func': 'product',
-            'parameters': [
-                {
-                    'type': 'monthlyprofile',
-                    'values': [0.5]*12
-                },
-                {
-                    'type': 'constant',
-                    'value': 1.5,
-                }
-            ]
-        }
-    }
-
-    s.connect(d)
-
-    d.max_flow = p = load_parameter(model, data)
-
-    @assert_rec(model, p)
-    def expected_func(timestep, scenario_index):
-        return 50.0 * 0.5 * 1.5
-
-    model.run()
-
-
-def test_parameter_df_upsampling(model):
-    """ Test that the `DataFrameParameter` can upsample data from a `pandas.DataFrame` and return that correctly
-    """
-    # scenario indices (not used for this test)
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    # Use a 7 day timestep for this test and run 2015
-    model.timestepper.delta = datetime.timedelta(7)
-    model.timestepper.start = pd.to_datetime('2015-01-01')
-    model.timestepper.end = pd.to_datetime('2015-12-31')
-    model.timestepper.setup()
-
-    # Daily time-step
-    index = pd.period_range('2015-01-01', periods=365, freq='D')
-    series = pd.Series(np.arange(365), index=index)
-
-    p = DataFrameParameter(model, series)
-    p.setup()
-
-    A = series.resample('7D').mean()
-    for v, ts in zip(A, model.timestepper):
-        np.testing.assert_allclose(p.value(ts, si), v)
-
-    model.reset()
-    # Daily time-step that requires aligning
-    index = pd.date_range('2014-12-31', periods=366, freq='D')
-    series = pd.Series(np.arange(366), index=index)
-
-    p = DataFrameParameter(model, series)
-    p.setup()
-
-    # offset the resample appropriately for the test
-    A = series[1:].resample('7D').mean()
-    for v, ts in zip(A, model.timestepper):
-        np.testing.assert_allclose(p.value(ts, si), v)
-
-    model.reset()
-    # Daily time-step that is not covering the require range
-    index = pd.date_range('2015-02-01', periods=365, freq='D')
-    series = pd.Series(np.arange(365), index=index)
-
-    p = DataFrameParameter(model, series)
-    with pytest.raises(ResamplingError):
-        p.setup()
-
-    model.reset()
-    # Daily time-step that is not covering the require range
-    index = pd.date_range('2014-11-01', periods=365, freq='D')
-    series = pd.Series(np.arange(365), index=index)
-
-    p = DataFrameParameter(model, series)
-    with pytest.raises(ResamplingError):
-        p.setup()
-
-
-def test_parameter_df_upsampling_multiple_columns(model):
-    """ Test that the `DataFrameParameter` works with multiple columns that map to a `Scenario`
-    """
-    scA = Scenario(model, 'A', size=20)
-    scB = Scenario(model, 'B', size=2)
-    # scenario indices (not used for this test)
-
-    # Use a 7 day timestep for this test and run 2015
-    model.timestepper.delta = datetime.timedelta(7)
-    model.timestepper.start = pd.to_datetime('2015-01-01')
-    model.timestepper.end = pd.to_datetime('2015-12-31')
-    model.timestepper.setup()
-
-    # Daily time-step
-    index = pd.date_range('2015-01-01', periods=365, freq='D')
-    df = pd.DataFrame(np.random.rand(365, 20), index=index)
-
-    p = DataFrameParameter(model, df, scenario=scA)
-    p.setup()
-
-    A = df.resample('7D', axis=0).mean()
-    for v, ts in zip(A.values, model.timestepper):
-        np.testing.assert_allclose([p.value(ts, ScenarioIndex(i, np.array([i], dtype=np.int32))) for i in range(20)], v)
-
-    p = DataFrameParameter(model, df, scenario=scB)
-    with pytest.raises(ValueError):
-        p.setup()
-
-
-def test_parameter_df_json_load(model, tmpdir):
-
-    # Daily time-step
-    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
-    df = pd.DataFrame(np.random.rand(365), index=index, columns=['data'])
-    df_path = tmpdir.join('df.csv')
-    df.to_csv(str(df_path))
-
-    data = {
-        'type': 'dataframe',
-        'url': str(df_path),
-        'index_col': 'date',
-        'parse_dates': True,
-    }
-
-    p = load_parameter(model, data)
-    p.setup()
-
-
-def test_parameter_df_embed_load(model):
-
-    # Daily time-step
-    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
-    df = pd.DataFrame(np.random.rand(365), index=index, columns=['data'])
-
-    # Save to JSON and load. This is the format we support loading as embedded data
-    df_data = df.to_json(date_format="iso")
-    # Removing the time information from the dataset for testing purposes
-    df_data = df_data.replace('T00:00:00.000Z', '')
-    df_data = json.loads(df_data)
-
-    data = {
-        'type': 'dataframe',
-        'data': df_data,
-        'parse_dates': True,
-    }
-
-    p = load_parameter(model, data)
-    p.setup()
-
-
-def test_simple_json_parameter_reference():
-    # note that parameters in the "parameters" section cannot be literals
-    model = load_model("parameter_reference.json")
-    max_flow = model.nodes["supply1"].max_flow
-    assert(isinstance(max_flow, ConstantParameter))
-    assert(max_flow.value(None, None) == 125.0)
-    cost = model.nodes["demand1"].cost
-    assert(isinstance(cost, ConstantParameter))
-    assert(cost.value(None, None) == -10.0)
-
-    assert(len(model.parameters) == 4)  # 4 parameters defined
-
-
-def test_threshold_parameter(simple_linear_model):
-    model = simple_linear_model
-    model.timestepper.delta = 150
-
-    scenario = Scenario(model, "Scenario", size=2)
-
-    class DummyRecorder(Recorder):
-        def __init__(self, model, value, *args, **kwargs):
-            super(DummyRecorder, self).__init__(model, *args, **kwargs)
-            self.val = value
-        def setup(self):
-            super(DummyRecorder, self).setup()
-            num_comb = len(model.scenarios.combinations)
-            self.data = np.empty([len(model.timestepper), num_comb], dtype=np.float64)
-        def after(self):
-            timestep = model.timestepper.current
-            self.data[timestep.index, :] = self.val
-
-    threshold = 10.0
-    values = [50.0, 60.0]
-
-    rec1 = DummyRecorder(model, threshold-5, name="rec1")  # below
-    rec2 = DummyRecorder(model, threshold, name="rec2")    # equal
-    rec3 = DummyRecorder(model, threshold+5, name="rec3")  # above
-
-    expected = [
-        ("LT", (1, 0, 0)),
-        ("GT", (0, 0, 1)),
-        ("EQ", (0, 1, 0)),
-        ("LE", (1, 1, 0)),
-        ("GE", (0, 1, 1)),
-    ]
-
-    for predicate, (value_lt, value_eq, value_gt) in expected:
-        for rec in (rec1, rec2, rec3):
-            param = RecorderThresholdParameter(model, rec, threshold, values=values, predicate=predicate)
-            e_val = values[getattr(rec.val, "__{}__".format(predicate.lower()))(threshold)]
-            e = np.ones([len(model.timestepper), len(model.scenarios.get_combinations())]) * e_val
-            e[0, :] = values[1] # first timestep is always "on"
-            r = AssertionRecorder(model, param, expected_data=e)
-            r.name = "assert {} {} {}".format(rec.val, predicate, threshold)
-
-    model.run()
-
-
-def test_constant_from_df():
-    """
-    Test that a dataframe can be used to provide data to ConstantParameter (single values).
-    """
-    model = load_model('simple_df.json')
-
-    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
-    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
-
-    ts = model.timestepper.next()
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
-    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -10.0)
-
-
-def test_constant_from_shared_df():
-    """
-    Test that a shared dataframe can be used to provide data to ConstantParameter (single values).
-    """
-    model = load_model('simple_df_shared.json')
-
-    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
-    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
-
-    ts = model.timestepper.next()
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
-    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -10.0)
-
-
-def test_constant_from_multiindex_df():
-    """
-    Test that a dataframe can be used to provide data to ConstantParameter (single values).
-    """
-    model = load_model('multiindex_df.json')
-
-
-    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
-    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
-
-    ts = model.timestepper.next()
-    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
-    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -100.0)
-
-def test_parameter_registry_overwrite(model):
-    # define a parameter
-    class NewParameter(Parameter):
-        DATA = 42
-        def __init__(self, model, values, *args, **kwargs):
-            super(NewParameter, self).__init__(model, *args, **kwargs)
-            self.values = values
-    NewParameter.register()
-
-    # re-define a parameter
-    class NewParameter(IndexParameter):
-        DATA = 43
-        def __init__(self, model, values, *args, **kwargs):
-            super(NewParameter, self).__init__(model, *args, **kwargs)
-            self.values = values
-    NewParameter.register()
-
-    data = {
-        "type": "new",
-        "values": 0
-    }
-    parameter = load_parameter(model, data)
-
-    # parameter is instance of new class, not old class
-    assert(isinstance(parameter, NewParameter))
-    assert(parameter.DATA == 43)
-
-
-def test_invalid_parameter_values():
-    """
-    Test that `load_parameter_values` returns a ValueError rather than KeyError.
-
-    This is useful to catch and give useful messages when no valid reference to
-    a data location is given.
-
-    Regression test for Issue #247 (https://github.com/pywr/pywr/issues/247)
-    """
-
-    from pywr.parameters._parameters import load_parameter_values
-
-    m = Model()
-    data = {'name': 'my_parameter', 'type': 'AParameterThatShouldHaveValues'}
-    with pytest.raises(ValueError):
-        load_parameter_values(model, data)
-
-
-class Test1DPolynomialParameter:
-    """ Tests for `Polynomial1DParameter` """
-    def test_init(self, simple_storage_model):
-        """ Test initialisation raises error with too many keywords """
-        stg = simple_storage_model.nodes['Storage']
-        param = ConstantParameter(simple_storage_model, 2.0)
-        with pytest.raises(ValueError):
-            # Passing both "parameter" and "storage_node" is invalid
-            Polynomial1DParameter(simple_storage_model, [0.5, np.pi], parameter=param, storage_node=stg)
-
-    def test_1st_order_with_parameter(self, simple_linear_model):
-        """ Test 1st order with a `Parameter` """
-        model = simple_linear_model
-
-        x = 2.0
-        p1 = Polynomial1DParameter(model, [0.5, np.pi], parameter=ConstantParameter(model, x))
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi * x
-        model.run()
-
-    def test_2nd_order_with_parameter(self, simple_linear_model):
-        """ Test 2nd order with a `Parameter` """
-        model = simple_linear_model
-
-        x = 2.0
-        px = ConstantParameter(model, x)
-        p1 = Polynomial1DParameter(model, [0.5, np.pi, 3.0], parameter=px)
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi*x + 3.0*x**2
-        model.run()
-
-    def test_1st_order_with_storage(self, simple_storage_model):
-        """ Test with a `Storage` node """
-        model = simple_storage_model
-        stg = model.nodes['Storage']
-        x = stg.initial_volume
-        p1 = Polynomial1DParameter(model, [0.5, np.pi], storage_node=stg)
-        p2 = Polynomial1DParameter(model, [0.5, np.pi], storage_node=stg, use_proportional_volume=True)
-
-        # Test with absolute storage
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi*x
-
-        # Test with proportional storage
-        @assert_rec(model, p2, name="proportionalassertion")
-        def expected_func(timestep, scenario_index):
-
-            return 0.5 + np.pi * x/stg.max_volume
-
-        model.setup()
-        model.step()
-
-
-    def test_load(self, simple_linear_model):
-        model = simple_linear_model
-
-        x = 1.5
-        data = {
-            "type": "polynomial1d",
-            "coefficients": [0.5, 2.5],
-            "parameter": {
-                "type": "constant",
-                "value": x
-            }
-        }
-
-        p1 = load_parameter(model, data)
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + 2.5*x
-        model.run()
-
-    def test_load_with_scaling(self, simple_linear_model):
-        model = simple_linear_model
-        x = 1.5
-        data = {
-            "type": "polynomial1d",
-            "coefficients": [0.5, 2.5],
-            "parameter": {
-                "type": "constant",
-                "value": x
-            },
-            "scale": 1.25,
-            "offset": 0.75
-        }
-        xscaled = x*1.25 + 0.75
-        p1 = load_parameter(model, data)
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + 2.5*xscaled
-        model.run()
-
-
-def test_interpolated_parameter(simple_linear_model):
-    model = simple_linear_model
-    model.timestepper.start = "1920-01-01"
-    model.timestepper.end = "1920-01-12"
-
-    p1 = ArrayIndexedParameter(model, [0,1,2,3,4,5,6,7,8,9,10,11])
-    p2 = InterpolatedParameter(model, p1, [0, 5, 10, 11], [0, 5*2, 10*3, 2])
-
-    @assert_rec(model, p2)
-    def expected_func(timestep, scenario_index):
-        values = [0, 2, 4, 6, 8, 10, 14, 18, 22, 26, 30, 2]
-        return values[timestep.index]
-    model.run()
-
-
-class TestInterpolatedQuadratureParameter:
-
-    @pytest.mark.parametrize("lower_interval", [None, 0, 1])
-    def test_calc(self, simple_linear_model, lower_interval):
-        model = simple_linear_model
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.end = "1920-01-12"
-
-        b = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
-        a = None
-        if lower_interval is not None:
-            a = ConstantParameter(model, lower_interval)
-
-        p2 = InterpolatedQuadratureParameter(model, b, [-5, 0, 5, 10, 11], [0, 0, 5 * 2, 10 * 3, 2],
-                                             lower_parameter=a)
-
-        def area(i):
-            if i < 0:
-                value = 0
-            elif i < 6:
-                value = 2*i**2 / 2
-            elif i < 11:
-                value = 25 + 4*(i - 5)**2 / 2 + (i - 5) * 10
-            else:
-                value = 25 + 50 + 50 + 28 / 2 + 2
-            return value
-
-        @assert_rec(model, p2)
-        def expected_func(timestep, scenario_index):
-            i = timestep.index
-            value = area(i)
-            if lower_interval is not None:
-                value -= area(lower_interval)
-            return value
-
-        model.run()
-
-    def test_load(self, simple_linear_model):
-        model = simple_linear_model
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.end = "1920-01-12"
-
-        p1 = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], name='p1')
-
-        p2 = {
-            'type': 'interpolatedquadrature',
-            'upper_parameter': 'p1',
-            'x': [0, 5, 10, 11],
-            'y': [0, 5 * 2, 10 * 3, 2]
-        }
-
-        p2 = load_parameter(model, p2)
-
-        @assert_rec(model, p2)
-        def expected_func(timestep, scenario_index):
-            i = timestep.index
-            if i < 6:
-                value = 2 * i ** 2 / 2
-            elif i < 11:
-                value = 25 + 4 * (i - 5) ** 2 / 2 + (i - 5) * 10
-            else:
-                value = 25 + 50 + 50 + 28 / 2 + 2
-            return value
-
-        model.run()
-
-
-class TestPiecewiseIntegralParameter:
-    X = [3, 8, 11]
-    Y = [5, 10, 2]
-
-    @staticmethod
-    def area(i):
-        if i < 0:
-            value = 0
-        elif i <= 3:
-            value = 5 * i
-        elif i <= 8:
-            value = 5 * 3 + 10 * (i - 3)
-        else:
-            value = 5 * 3 + 10 * (8 - 3) + 2 * (i - 8)
-        return value
-
-    def test_calc(self, simple_linear_model):
-        """Test the piecewise integral calculaiton."""
-        model = simple_linear_model
-
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.end = "1920-01-12"
-
-        x = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
-        p2 = PiecewiseIntegralParameter(model, x, self.X, self.Y)
-
-        @assert_rec(model, p2)
-        def expected_func(timestep, scenario_index):
-            i = timestep.index
-            return self.area(i)
-
-        model.run()
-
-    def test_load(self, simple_linear_model):
-        """Test loading from JSON."""
-        model = simple_linear_model
-
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.end = "1920-01-12"
-
-        x = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], name='x')
-
-        p2 = {
-            'type': 'piecewiseintegralparameter',
-            'parameter': 'x',
-            'x': self.X,
-            'y': self.Y,
-        }
-
-        p2 = load_parameter(model, p2)
-
-        @assert_rec(model, p2)
-        def expected_func(timestep, scenario_index):
-            i = timestep.index
-            return self.area(i)
-
-        model.run()
-
-
-class Test2DStoragePolynomialParameter:
-
-    def test_1st(self, simple_storage_model):
-        """ Test 1st order """
-        model = simple_storage_model
-        stg = model.nodes['Storage']
-
-        x = 2.0
-        y = stg.initial_volume
-        coefs = [[0.5, np.pi], [2.5, 0.3]]
-
-        p1 = Polynomial2DStorageParameter(model, coefs, stg, ConstantParameter(model, x))
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
-        model.setup()
-        model.step()
-
-    def test_load(self, simple_storage_model):
-        model = simple_storage_model
-        stg = model.nodes['Storage']
-
-        x = 2.0
-        y = stg.initial_volume/stg.max_volume
-        data = {
-            "type": "polynomial2dstorage",
-            "coefficients": [[0.5, np.pi], [2.5, 0.3]],
-            "use_proportional_volume": True,
-            "parameter": {
-                "type": "constant",
-                "value": x
-            },
-            "storage_node": "Storage"
-        }
-
-        p1 = load_parameter(model, data)
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
-        model.setup()
-        model.step()
-
-    def test_load_wth_scaling(self, simple_storage_model):
-        model = simple_storage_model
-        stg = model.nodes['Storage']
-
-        x = 2.0
-        y = stg.initial_volume/stg.max_volume
-        data = {
-            "type": "polynomial2dstorage",
-            "coefficients": [[0.5, np.pi], [2.5, 0.3]],
-            "use_proportional_volume": True,
-            "parameter": {
-                "type": "constant",
-                "value": x
-            },
-            "storage_node": "Storage",
-            "storage_scale": 1.3,
-            "storage_offset": 0.75,
-            "parameter_scale": 1.25,
-            "parameter_offset": -0.5
-        }
-
-        p1 = load_parameter(model, data)
-
-        # Scaled parameters
-        x = x*1.25 - 0.5
-        y = y*1.3 + 0.75
-
-        @assert_rec(model, p1)
-        def expected_func(timestep, scenario_index):
-            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
-        model.setup()
-        model.step()
-
-
-class TestDivisionParameter:
-
-    def test_divsion(self, simple_linear_model):
-        model = simple_linear_model
-        model.timestepper.start = "2017-01-01"
-        model.timestepper.end = "2017-01-15"
-
-        profile = list(range(1, 367))
-
-        data = {
-            "type": "division",
-            "numerator": {
-                "name": "raw",
-                "type": "dailyprofile",
-                "values": profile,
-            },
-            "denominator": {
-                "type": "constant",
-                "value": 123.456
-            }
-        }
-
-        model.nodes["Input"].max_flow = parameter = load_parameter(model, data)
-        model.nodes["Output"].max_flow = 9999
-        model.nodes["Output"].cost = -100
-
-        daily_profile = model.parameters["raw"]
-
-        @assert_rec(model, parameter)
-        def expected(timestep, scenario_index):
-            value = daily_profile.get_value(scenario_index)
-            return value / 123.456
-        model.run()
-
-
-class TestMinMaxNegativeOffsetParameter:
-    @pytest.mark.parametrize("ptype,profile", [
-        ("max", list(range(-10, 356))),
-        ("min", list(range(0, 366))),
-        ("negative", list(range(-366, 0))),
-        ("negativemax", list(range(-366, 0))),
-        ("negativemin", list(range(-366, 0))),
-        ("offset", list(range(0, 366))),
-    ])
-    def test_parameter(cls, simple_linear_model, ptype,profile):
-        model = simple_linear_model
-        model.timestepper.start = "2017-01-01"
-        model.timestepper.end = "2017-12-31"
-
-        data = {
-            "type": ptype,
-            "parameter": {
-                "name": "raw",
-                "type": "dailyprofile",
-                "values": profile,
-            }
-        }
-
-        if ptype in ("max", "min", "negativemax", "negativemin"):
-            data["threshold"] = 3
-        elif ptype == 'offset':
-            data["offset"] = 3
-
-        func = {
-            "min": min,
-            "max": max,
-            "negative": lambda t, x: -x,
-            "negativemax": lambda t, x: max(t, -x),
-            "negativemin": lambda t, x: min(t, -x),
-            "offset": lambda o, x: x + o
-        }[ptype]
-
-        model.nodes["Input"].max_flow = parameter = load_parameter(model, data)
-        model.nodes["Output"].max_flow = 9999
-        model.nodes["Output"].cost = -100
-
-        daily_profile = model.parameters["raw"]
-
-        @assert_rec(model, parameter)
-        def expected(timestep, scenario_index):
-            value = daily_profile.get_value(scenario_index)
-            return func(3, value)
-        model.run()
-
-    def test_offset_parameter_variable(self, simple_linear_model):
-        """Test OffsetParameter's variable API."""
-
-        data = {
-            "type": "offset",
-            "parameter": {
-                "name": "raw",
-                "type": "dailyprofile",
-                "values": list(range(366)),
-            },
-            "offset": 10,
-            "lower_bounds": -100,
-            "upper_bounds": 100,
-        }
-        parameter = load_parameter(simple_linear_model, data)
-        np.testing.assert_allclose(parameter.offset, 10)
-        np.testing.assert_allclose(parameter.get_double_variables(), [10.0])
-        np.testing.assert_allclose(parameter.get_double_lower_bounds(), [-100.0])
-        np.testing.assert_allclose(parameter.get_double_upper_bounds(), [100.0])
-        # Update value using variable API
-        parameter.set_double_variables(np.array([20.0]))
-        np.testing.assert_allclose(parameter.offset, 20)
-        np.testing.assert_allclose(parameter.get_double_variables(), [20.0])
-
-
-def test_ocptt(simple_linear_model):
-    model = simple_linear_model
-    inpt = model.nodes["Input"]
-    s1 = Scenario(model, "scenario 1", size=3)
-    s2 = Scenario(model, "scenario 1", size=2)
-    x = np.arange(len(model.timestepper)).reshape([len(model.timestepper), 1]) + 5
-    y = np.arange(s1.size).reshape([1, s1.size])
-    z = x * y ** 2
-    p = ArrayIndexedScenarioParameter(model, s1, z)
-    inpt.max_flow = p
-    model.setup()
-    model.reset()
-    model.step()
-
-    values1 = [p.get_value(scenario_index) for scenario_index in model.scenarios.combinations]
-    values2 = list(p.get_all_values())
-    assert_allclose(values1, [0, 0, 5, 5, 20, 20])
-    assert_allclose(values2, [0, 0, 5, 5, 20, 20])
-
-
-class TestThresholdParameters:
-
-    def test_storage_threshold_parameter(self, simple_storage_model):
-        """ Test StorageThresholdParameter """
-        m = simple_storage_model
-
-        data = {
-            "type": "storagethreshold",
-            "storage_node": "Storage",
-            "threshold": 10.0,
-            "predicate": ">"
-        }
-
-        p1 = load_parameter(m, data)
-
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        m.nodes['Storage'].initial_volume = 15.0
-        m.setup()
-        # Storage > 10
-        assert p1.index(m.timestepper.current, si) == 1
-
-        m.nodes['Storage'].initial_volume = 5.0
-        m.setup()
-        # Storage < 10
-        assert p1.index(m.timestepper.current, si) == 0
-
-    def test_node_threshold_parameter2(self, simple_linear_model):
-        model = simple_linear_model
-        model.nodes["Input"].max_flow = ArrayIndexedParameter(model, np.arange(0, 20))
-        model.nodes["Output"].cost = -10.0
-        model.timestepper.start = "1920-01-01"
-        model.timestepper.end = "1920-01-15"
-        model.timestepper.delta = 1
-
-        threshold = 5.0
-
-        parameters = {}
-        for predicate in (">", "<", "="):
-            data = {
-                "type": "nodethreshold",
-                "node": "Output",
-                "threshold": 5.0,
-                "predicate": predicate,
-                # we need to define values so AssertionRecorder can be used
-                "values": [0.0, 1.0],
-            }
-            parameter = load_parameter(model, data)
-            parameter.name = "nodethresold {}".format(predicate)
-            parameters[predicate] = parameter
-
-            if predicate == ">":
-                expected_data = (np.arange(-1, 20) > threshold).astype(int)
-            elif predicate == "<":
-                expected_data = (np.arange(-1, 20) < threshold).astype(int)
-            else:
-                expected_data = (np.arange(-1, 20) == threshold).astype(int)
-            expected_data[0] = 0 # previous flow in initial timestep is undefined
-            expected_data = expected_data[:, np.newaxis]
-
-            rec = AssertionRecorder(model, parameter, expected_data=expected_data, name="assertion recorder {}".format(predicate))
-
-        model.run()
-
-    @pytest.mark.parametrize("threshold, ratchet", [
-        [5.0, False],
-        [{"type": "constant", "value": 5.0}, False],
-        [{"type": "constant", "value": 5.0}, True],
-    ], ids=["double", "parameter", "parameter-ratchet"])
-    def test_parameter_threshold_parameter(self, simple_linear_model, threshold, ratchet):
-        """ Test ParameterThresholdParameter """
-        m = simple_linear_model
-        m.nodes['Input'].max_flow = 10.0
-        m.nodes['Output'].cost = -10.0
-
-        data = {
-            "type": "parameterthreshold",
-            "parameter": {
-                "type": "constant",
-                "value": 3.0
-            },
-            "threshold": threshold,
-            "predicate": "<",
-            "ratchet": ratchet
-        }
-
-        p1 = load_parameter(m, data)
-
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        # Triggered initial 3 < 5
-        m.setup()
-        m.step()
-        assert p1.index(m.timestepper.current, si) == 1
-
-        # Update parameter, now 8 > 5; not triggered.
-        p1.param.set_double_variables(np.array([8.0,]))
-        m.step()
-        # If using a ratchet the trigger remains on.
-        assert p1.index(m.timestepper.current, si) == (1 if ratchet else 0)
-
-        # Resetting the model resets the ratchet too.
-        m.reset()
-        m.step()
-        # flow < 5
-        assert p1.index(m.timestepper.current, si) == 0
-
-    def test_current_year_threshold_parameter(self, simple_linear_model):
-        """Test CurrentYearThresholdParameter"""
-        m = simple_linear_model
-
-        m.timestepper.start = '2020-01-01'
-        m.timestepper.end = '2030-01-01'
-
-        data = {
-            'type': 'currentyearthreshold',
-            'threshold': 2025,
-            "predicate": ">=",
-        }
-
-        p = load_parameter(m, data)
-
-        @assert_rec(m, p, get_index=True)
-        def expected_func(timestep, scenario_index):
-            current_year = timestep.year
-            value = 1 if current_year >= 2025 else 0
-            return value
-
-        m.run()
-
-    def test_current_ordinal_threshold_parameter(self, simple_linear_model):
-        """Test CurrentYearThresholdParameter"""
-        m = simple_linear_model
-
-        m.timestepper.start = '2020-01-01'
-        m.timestepper.end = '2030-01-01'
-
-        threshold = datetime.date(2025, 6, 15).toordinal()
-
-        data = {
-            'type': 'currentordinaldaythreshold',
-            'threshold': threshold,
-            "predicate": ">=",
-        }
-
-        p = load_parameter(m, data)
-
-        @assert_rec(m, p, get_index=True)
-        def expected_func(timestep, scenario_index):
-            o = timestep.datetime.toordinal()
-            value = 1 if o >= threshold else 0
-            return value
-
-        m.run()
-
-
-def test_orphaned_components(simple_linear_model):
-    model = simple_linear_model
-    model.nodes["Input"].max_flow = ConstantParameter(model, 10.0)
-
-    result = model.find_orphaned_parameters()
-    assert(not result)
-    # assert that warning not raised by check
-    with pytest.warns(None) as record:
-        model.check()
-    for w in record:
-        if isinstance(w, OrphanedParameterWarning):
-            pytest.fail("OrphanedParameterWarning raised unexpectedly!")
-
-    # add some orphans
-    orphan1 = ConstantParameter(model, 5.0)
-    orphan2 = ConstantParameter(model, 10.0)
-    orphans = {orphan1, orphan2}
-    result = model.find_orphaned_parameters()
-    assert(orphans == result)
-
-    with pytest.warns(OrphanedParameterWarning):
-        model.check()
-
-def test_deficit_parameter():
-    """Test DeficitParameter
-
-    Here we test both uses of the DeficitParameter:
-      1) Recording the deficit for a node each timestep
-      2) Using yesterday's deficit to control today's flow
-    """
-    model = load_model("deficit.json")
-
-    model.run()
-
-    max_flow = np.array([5, 6, 7, 8, 9, 10, 11, 12, 11, 10, 9, 8])
-    demand = 10.0
-    supplied = np.minimum(max_flow, demand)
-    expected = demand - supplied
-    actual = model.recorders["deficit_recorder"].data
-    assert_allclose(expected, actual[:,0])
-
-    expected_yesterday = [0]+list(expected[0:-1])
-    actual_yesterday = model.recorders["yesterday_recorder"].data
-    assert_allclose(expected_yesterday, actual_yesterday[:,0])
-
-
-def test_flow_parameter():
-    """test FlowParameter
-
-    """
-    model = load_model("flow_parameter.json")
-
-    model.run()
-
-    max_flow = np.array([5, 6, 7, 8, 9, 10, 11, 12, 11, 10, 9, 8])
-    demand = 10.0
-    supplied = np.minimum(max_flow, demand)
-
-    actual = model.recorders["flow_recorder"].data
-    assert_allclose(supplied, actual[:,0])
-
-    expected_yesterday = [3.1415]+list(supplied[0:-1])
-    actual_yesterday = model.recorders["yesterday_flow_recorder"].data
-    assert_allclose(expected_yesterday, actual_yesterday[:,0])
-
-
-class TestHydroPowerTargets:
-    def test_target_json(self):
-        """ Test loading a HydropowerTargetParameter from JSON. """
-        model = load_model("hydropower_target_example.json")
-        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
-
-        # 30 time-steps are run such that the head gets so flow to hit the max_flow
-        # constraint. The first few time-steps are also bound by the min_flow constraint.
-        for i in range(30):
-            model.step()
-
-            rec = model.recorders["turbine1_energy"]
-            param = model.parameters["turbine1_discharge"]
-
-            turbine1 = model.nodes["turbine1"]
-            assert turbine1.flow[0] > 0
-
-            if np.allclose(turbine1.flow[0], 500.0):
-                # If flow is bounded by min_flow then more HP is produced.
-                assert rec.data[i, 0] > param.target.get_value(si)
-            elif np.allclose(turbine1.flow[0], 1000.0):
-                # If flow is bounded by max_flow then less HP is produced.
-                assert rec.data[i, 0] < param.target.get_value(si)
-            else:
-                # If flow is within the bounds target is met exactly.
-                assert_allclose(rec.data[i, 0], param.target.get_value(si))
-
-
-class TestFlowInterpolation:
-
-    def test_flow_interpolation_parameter(self):
-        """The test includes interpolation of river water level based on flow"""
-
-        model = load_model("flow_interpolation.json")
-
-        model.run()
-
-        water_levels1 = np.array([3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])
-
-        modelled_levels = model.recorders["water_level_value"].data
-        assert_allclose(water_levels1, modelled_levels[:, 0])
-
-
-class TestUniformDrawdownProfileParameter:
-
-    def test_uniform_drawdown_profile(self, simple_linear_model):
-        """Test the uniform drawn profile over a leap year and non-leap year."""
-
-        m = simple_linear_model
-        m.timestepper.start = '2015-04-01'
-        m.timestepper.end = '2017-04-01'
-
-        expected_values = np.r_[
-            np.linspace(1, 1/366, 366),  # This period covers Apr-2015 to Apr-2016 (i.e. 366 days)
-            np.linspace(1, 1/365, 365),  # This period covers Apr-2016 to Apr-2017 (i.e. 365 days)
-            np.linspace(1, 1/365, 365),  # This period covers Apr-2017 to Apr-2018 (i.e. 365 days)
-        ]
-
-        data = {
-            'type': 'uniformdrawdownprofile',
-            "reset_day": 1,
-            "reset_month": 4
-        }
-
-        p = load_parameter(m, data)
-
-        @assert_rec(m, p)
-        def expected_func(timestep, scenario_index):
-            return expected_values[timestep.index]
-
-        m.run()
-
-
-class TestRbfProfileParameter:
-    """Tests for RbfParameter."""
-
-    @pytest.mark.parametrize(['min_value', 'max_value'], [(None, None), (0.3, None), (None, 0.6)])
-    def test_rbf_profile(self, simple_linear_model, min_value, max_value):
-        """Test the Rbf profile parameter."""
-
-        m = simple_linear_model
-        m.timestepper.start = '2015-01-01'
-        m.timestepper.end = '2015-12-31'
-
-        # The Rbf parameter should mirror the input data at the start and end to create roughly
-        # consistent gradients across the end of year boundary.
-        interp_days_of_year = [-65, 1, 100, 200, 300, 366, 465]
-        interp_values = [0.2, 0.5, 0.7, 0.5, 0.2, 0.5, 0.7]
-
-        expected_values = Rbf(interp_days_of_year, interp_values)(np.arange(365) + 1)
-
-        data = {
-            'type': 'rbfprofile',
-            "days_of_year": [1, 100, 200, 300],
-            "values": [0.5, 0.7, 0.5, 0.2],
-        }
-        if min_value is not None:
-            data["min_value"] = min_value
-        if max_value is not None:
-            data["max_value"] = max_value
-
-        p = load_parameter(m, data)
-
-        @assert_rec(m, p)
-        def expected_func(timestep, scenario_index):
-            ev = expected_values[timestep.index]
-            if min_value is not None:
-                ev = max(min_value, ev)
-            if max_value is not None:
-                ev = min(max_value, ev)
-            return ev
-
-        m.run()
-
-    @pytest.mark.parametrize('wrong_doys', [
-        [2, 100, 300],  # Incorrect first day
-        [1, 180],  # Too few values
-        [1, 100, 366],  # Incorrect last day
-        [1, 200, 140],  # Not monotonic
-        [1, 140, 140],  # Not strictly monotonic
-    ])
-    def test_incorrect_inputs(self, simple_linear_model, wrong_doys):
-        """Test initialising RbfParameter with incorrect days of the year."""
-
-        data = {
-            'type': 'rbfprofile',
-            "days_of_year": wrong_doys,
-            "values": np.random.rand(len(wrong_doys)).tolist()
-        }
-        with pytest.raises(ValueError):
-            load_parameter(simple_linear_model, data)
-
-    def test_variable_api(self, simple_linear_model):
-        """Test using variable API implementation on RbfParameter."""
-
-        data = {
-            'type': 'rbfprofile',
-            "days_of_year": [1, 100, 200, 300],
-            "values": [0.5, 0.7, 0.5, 0.2]
-        }
-
-        p = load_parameter(simple_linear_model, data)
-        assert p.double_size == 4
-        assert p.integer_size == 0
-
-        new_values = np.random.rand(p.double_size)
-        p.set_double_variables(new_values)
-        np.testing.assert_allclose(p.get_double_variables(), new_values)
-        
-    def test_variable_doys_api(self, simple_linear_model):
-        """Test using the variable API when optimising the days of the year. """
-
-        data = {
-            'type': 'rbfprofile',
-            "days_of_year": [1, 100, 200, 300],
-            "values": [0.5, 0.7, 0.5, 0.2],
-            "lower_bounds": 0.1,
-            "upper_bounds": 0.8,
-            "variable_days_of_year_range": 20,
-            "is_variable": True
-        }
-
-        p = load_parameter(simple_linear_model, data)
-        assert p.double_size == 4
-        assert p.integer_size == 3
-
-        new_values = np.random.rand(p.double_size)
-        p.set_double_variables(new_values)
-        np.testing.assert_allclose(p.get_double_variables(), new_values)
-
-        new_doys = np.array([90, 190, 290], dtype=np.int32)
-        p.set_integer_variables(new_doys)
-        np.testing.assert_allclose(p.get_integer_variables(), new_doys)
-
-        lb = np.array([80, 180, 280], dtype=np.int32)
-        np.testing.assert_allclose(p.get_integer_lower_bounds(), lb)
-
-        ub = np.array([120, 220, 320], dtype=np.int32)
-        np.testing.assert_allclose(p.get_integer_upper_bounds(), ub)
-
-    def test_too_close_doys_error(self, simple_linear_model):
-        """Test that setting days of the year too close together for optimisation raises an error."""
-
-        data = {
-            'type': 'rbfprofile',
-            "days_of_year": [1, 140, 200],   # Closest distance is 60 days
-            "values": [0.5, 0.7, 0.5],
-            "lower_bounds": 0.1,
-            "upper_bounds": 0.8,
-            "variable_days_of_year_range": 30,  # A range of 30 could cause overlap (140 + 30, 200 - 30)
-            "is_variable": True
-        }
-
-        with pytest.raises(ValueError):
-            load_parameter(simple_linear_model, data)
-
-            
-class TestDiscountFactorParameter:
-    def test_discount_json(self):
-        """ Test loading a DiscountFactorParameter from JSON. """
-        model = load_model("discount.json")
-        # run model for period 2015-2020, with base year 2015 and discount rate of 0.035 (3.5%)
-        p = model.parameters['discount_factor']
-        @assert_rec(model, p)
-        def expected_func(timestep, scenario_index):
-            year = timestep.year
-            return 1/pow(1.035, year - 2015)
-
-        model.run()
+"""
+Test for individual Parameter classes
+"""
+from pywr.core import Model, Timestep, Scenario, ScenarioIndex, Storage, Link, Input, Output
+from pywr.parameters import (Parameter, ArrayIndexedParameter, ConstantScenarioParameter,
+    ArrayIndexedScenarioMonthlyFactorsParameter, MonthlyProfileParameter, DailyProfileParameter,
+    DataFrameParameter, AggregatedParameter, ConstantParameter, ConstantScenarioIndexParameter,
+    IndexParameter, AggregatedIndexParameter, RecorderThresholdParameter, ScenarioMonthlyProfileParameter,
+    ScenarioWeeklyProfileParameter, Polynomial1DParameter, Polynomial2DStorageParameter, ArrayIndexedScenarioParameter,
+    InterpolatedParameter, WeeklyProfileParameter, InterpolatedQuadratureParameter, PiecewiseIntegralParameter,
+    FunctionParameter, AnnualHarmonicSeriesParameter, load_parameter, InterpolatedFlowParameter,
+    ScenarioDailyProfileParameter)
+from pywr.recorders import AssertionRecorder, assert_rec
+from pywr.model import OrphanedParameterWarning
+from pywr.dataframe_tools import ResamplingError
+from pywr.recorders import Recorder
+from fixtures import simple_linear_model, simple_storage_model
+from helpers import load_model
+import json
+import os
+import datetime
+import numpy as np
+import pandas as pd
+import pytest
+import itertools
+import calendar
+from numpy.testing import assert_allclose
+from scipy.interpolate import Rbf
+
+TEST_DIR = os.path.dirname(__file__)
+
+@pytest.fixture
+def model():
+    return Model()
+
+
+class TestConstantParameter:
+    """ Tests for `ConstantParameter` """
+    def test_basic_use(self, simple_linear_model):
+        """ Test the basic use of `ConstantParameter` using the Python API """
+        model = simple_linear_model
+        # Add two scenarios
+        scA = Scenario(model, 'Scenario A', size=2)
+        scB = Scenario(model, 'Scenario B', size=5)
+
+        p = ConstantParameter(model, np.pi, name='pi', comment='Mmmmm Pi!')
+
+        assert not p.is_variable
+        assert p.double_size == 1
+        assert p.integer_size == 0
+
+        model.setup()
+        ts = model.timestepper.current
+        # Now ensure the appropriate value is returned for all scenarios
+        for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
+            si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
+            np.testing.assert_allclose(p.value(ts, si), np.pi)
+
+    def test_being_a_variable(self, simple_linear_model):
+        """ Test the basic use of `ConstantParameter` when `is_variable=True` """
+        model = simple_linear_model
+        p = ConstantParameter(model, np.pi, name='pi', comment='Mmmmm Pi!', is_variable=True,
+                              lower_bounds=np.pi/2, upper_bounds=2*np.pi)
+        model.setup()
+
+        assert p.is_variable
+        assert p.double_size == 1
+        assert p.integer_size == 0
+
+        np.testing.assert_allclose(p.get_double_lower_bounds(), np.array([np.pi/2]))
+        np.testing.assert_allclose(p.get_double_upper_bounds(), np.array([2*np.pi]))
+
+        np.testing.assert_allclose(p.get_double_variables(), np.array([np.pi]))
+
+        # No test updating the variables
+        p.set_double_variables(np.array([1.5*np.pi, ]))
+        np.testing.assert_allclose(p.get_double_variables(), np.array([1.5*np.pi]))
+
+        # None of the integer functions should be implemented because this parameter
+        # has no integer variables
+        with pytest.raises(NotImplementedError):
+            p.get_integer_lower_bounds()
+
+        with pytest.raises(NotImplementedError):
+            p.get_integer_upper_bounds()
+
+        with pytest.raises(NotImplementedError):
+            p.get_integer_variables()
+
+
+def test_parameter_array_indexed(simple_linear_model):
+    """
+    Test ArrayIndexedParameter
+
+    """
+    model = simple_linear_model
+    A = np.arange(len(model.timestepper), dtype=np.float64)
+    p = ArrayIndexedParameter(model, A)
+    model.setup()
+    # scenario indices (not used for this test)
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+    for v, ts in zip(A, model.timestepper):
+        np.testing.assert_allclose(p.value(ts, si), v)
+
+    # Now check that IndexError is raised if an out of bounds Timestep is given.
+    ts = Timestep(pd.Period('2016-01-01', freq='1D'), 366, 1)
+    with pytest.raises(IndexError):
+        p.value(ts, si)
+
+
+def test_parameter_array_indexed_json_load(simple_linear_model, tmpdir):
+    """Test ArrayIndexedParameter can be loaded from json dict"""
+    model = simple_linear_model
+    # Daily time-step
+    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
+    df = pd.DataFrame(np.arange(365), index=index, columns=['data'])
+    df_path = tmpdir.join('df.csv')
+    df.to_csv(str(df_path))
+
+    data = {
+        'type': 'arrayindexed',
+        'url': str(df_path),
+        'index_col': 'date',
+        'parse_dates': True,
+        'column': 'data',
+    }
+
+    p = load_parameter(model, data)
+    model.setup()
+
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+    for v, ts in enumerate(model.timestepper):
+        np.testing.assert_allclose(p.value(ts, si), v)
+
+def test_parameter_constant_scenario(simple_linear_model):
+    """
+    Test ConstantScenarioParameter
+
+    """
+    model = simple_linear_model
+    # Add two scenarios
+    scA = Scenario(model, 'Scenario A', size=2)
+    scB = Scenario(model, 'Scenario B', size=5)
+
+    p = ConstantScenarioParameter(model, scB, np.arange(scB.size, dtype=np.float64))
+    model.setup()
+    ts = model.timestepper.current
+    # Now ensure the appropriate value is returned for the Scenario B indices.
+    for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
+        si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
+        np.testing.assert_allclose(p.value(ts, si), float(b))
+
+
+def test_parameter_constant_scenario(simple_linear_model):
+    """
+    Test ConstantScenarioIndexParameter
+
+    """
+    model = simple_linear_model
+    # Add two scenarios
+    scA = Scenario(model, 'Scenario A', size=2)
+    scB = Scenario(model, 'Scenario B', size=5)
+
+    p = ConstantScenarioIndexParameter(model, scB, np.arange(scB.size, dtype=np.int32))
+    model.setup()
+    ts = model.timestepper.current
+    # Now ensure the appropriate value is returned for the Scenario B indices.
+    for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
+        si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
+        np.testing.assert_allclose(p.index(ts, si), b)
+
+
+def test_parameter_array_indexed_scenario_monthly_factors(simple_linear_model):
+    """
+    Test ArrayIndexedParameterScenarioMonthlyFactors
+
+    """
+    model = simple_linear_model
+    # Baseline timeseries data
+    values = np.arange(len(model.timestepper), dtype=np.float64)
+
+    # Add two scenarios
+    scA = Scenario(model, 'Scenario A', size=2)
+    scB = Scenario(model, 'Scenario B', size=5)
+
+    # Random factors for each Scenario B value per month
+    factors = np.random.rand(scB.size, 12)
+
+    p = ArrayIndexedScenarioMonthlyFactorsParameter(model, scB, values, factors)
+    model.setup()
+
+    # Iterate in time
+    for v, ts in zip(values, model.timestepper):
+        imth = ts.datetime.month - 1
+        # Now ensure the appropriate value is returned for the Scenario B indices.
+        for i, (a, b) in enumerate(itertools.product(range(scA.size), range(scB.size))):
+            f = factors[b, imth]
+            si = ScenarioIndex(i, np.array([a, b], dtype=np.int32))
+            np.testing.assert_allclose(p.value(ts, si), v*f)
+
+def test_parameter_array_indexed_scenario_monthly_factors_json(model):
+    model.path = os.path.join(TEST_DIR, "models")
+    scA = Scenario(model, 'Scenario A', size=2)
+    scB = Scenario(model, 'Scenario B', size=3)
+
+    p1 = ArrayIndexedScenarioMonthlyFactorsParameter.load(model, {
+        "scenario": "Scenario A",
+        "values": list(range(32)),
+        "factors": [list(range(1, 13)),list(range(13, 25))],
+    })
+
+    p2 = ArrayIndexedScenarioMonthlyFactorsParameter.load(model, {
+        "scenario": "Scenario B",
+        "values": {
+            "url": "timeseries1.csv",
+            "index_col": "Timestamp",
+            "column": "Data",
+        },
+        "factors": {
+            "url": "monthly_profiles.csv",
+            "index_col": "scenario",
+        },
+    })
+
+    node1 = Input(model, "node1", max_flow=p1)
+    node2 = Input(model, "node2", max_flow=p2)
+    nodeN = Output(model, "nodeN", max_flow=None, cost=-1)
+    node1.connect(nodeN)
+    node2.connect(nodeN)
+
+    model.timestepper.start = "2015-01-01"
+    model.timestepper.end = "2015-01-31"
+    model.run()
+
+
+class TestMonthlyProfileParameter:
+
+    def test_no_interpolation(self, simple_linear_model):
+        """Test no-interpolation. """
+        model = simple_linear_model
+        values = np.arange(12, dtype=np.float64)
+        p = MonthlyProfileParameter(model, values)
+        model.setup()
+
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            imth = timestep.month - 1
+            return values[imth]
+
+        model.run()
+
+    def test_interpolation_month_start(self, simple_linear_model):
+        """Test interpolating monthly values from first day of the month."""
+        model = simple_linear_model
+        values = np.arange(12, dtype=np.float64)
+        p = MonthlyProfileParameter(model, values, interp_day='first')
+        model.setup()
+
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            imth = timestep.month - 1
+            days_in_month = calendar.monthrange(timestep.year, timestep.month)[1]
+            day = timestep.day
+
+            # Perform linear interpolation
+            x = (day - 1) / (days_in_month - 1)
+            return values[imth] * (1 - x) + values[(imth+1) % 12] * x
+        model.run()
+
+    def test_interpolation_month_end(self, simple_linear_model):
+        """Test interpolating monthly values from last day of the month."""
+        model = simple_linear_model
+        values = np.arange(12, dtype=np.float64)
+        p = MonthlyProfileParameter(model, values, interp_day='last')
+        model.setup()
+
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            imth = timestep.month - 1
+            days_in_month = calendar.monthrange(timestep.year, timestep.month)[1]
+            day = timestep.day
+
+            # Perform linear interpolation
+            x = day / days_in_month
+            return values[(imth - 1) % 12] * (1 - x) + values[imth] * x
+        model.run()
+
+
+class TestScenarioMonthlyProfileParameter:
+
+    def test_init(self, simple_linear_model):
+        model = simple_linear_model
+        scenario = Scenario(model, 'A', 10)
+        values = np.random.rand(10, 12)
+
+        p = ScenarioMonthlyProfileParameter(model, scenario, values)
+
+        model.setup()
+        # Iterate in time
+        for ts in model.timestepper:
+            imth = ts.datetime.month - 1
+            for i in range(scenario.size):
+                si = ScenarioIndex(i, np.array([i], dtype=np.int32))
+                np.testing.assert_allclose(p.value(ts, si), values[i, imth])
+
+    def test_json(self):
+        model = load_model('scenario_monthly_profile.json')
+
+        # check first day initalised
+        assert (model.timestepper.start == datetime.datetime(2015, 1, 1))
+
+        # check results
+        supply1 = model.nodes['supply1']
+
+        # Multiplication factors
+        factors = np.array([
+            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
+            [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],
+            [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22],
+        ])
+
+        for expected in (23.92, 22.14, 22.57, 24.97, 27.59):
+            model.step()
+            imth = model.timestepper.current.month - 1
+            assert_allclose(supply1.flow, expected*factors[:, imth], atol=1e-7)
+
+def test_parameter_daily_profile(simple_linear_model):
+    """
+    Test DailyProfileParameter
+
+    """
+    model = simple_linear_model
+    values = np.arange(366, dtype=np.float64)
+    p = DailyProfileParameter(model, values)
+    model.setup()
+
+    # Iterate in time
+    for ts in model.timestepper:
+        month = ts.datetime.month
+        day = ts.datetime.day
+        iday = int((datetime.datetime(2016, month, day) - datetime.datetime(2016, 1, 1)).days)
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+        np.testing.assert_allclose(p.value(ts, si), values[iday])
+
+def test_daily_profile_leap_day(model):
+    """Test behaviour of daily profile parameter for leap years
+    """
+    inpt = Input(model, "input")
+    otpt = Output(model, "otpt", max_flow=None, cost=-999)
+    inpt.connect(otpt)
+    inpt.max_flow = DailyProfileParameter(model, np.arange(0, 366, dtype=np.float64))
+
+    # non-leap year
+    model.timestepper.start = pd.to_datetime("2015-01-01")
+    model.timestepper.end = pd.to_datetime("2015-12-31")
+    model.run()
+    assert_allclose(inpt.flow, 365) # NOT 364
+
+    # leap year
+    model.timestepper.start = pd.to_datetime("2016-01-01")
+    model.timestepper.end = pd.to_datetime("2016-12-31")
+    model.run()
+    assert_allclose(inpt.flow, 365)
+
+class TestScenarioDailyProfileParameter:
+
+    def test_scenario_daily_profile(self, simple_linear_model):
+
+        model = simple_linear_model
+        scenario = Scenario(model, 'A', 2)
+        values = np.array([np.arange(366, dtype=np.float64), np.arange(366, 0, -1, dtype=np.float64)])
+
+        # Remove values for 29th feb as not testing leap year in this func
+        expected_values = np.delete(values.T, 59, 0)
+
+        p = ScenarioDailyProfileParameter.load(model, {"scenario": "A", "values": values})
+
+        AssertionRecorder(model, p, expected_data=expected_values)
+
+        model.setup()
+        model.run()
+
+    def test_scenario_daily_profile_leap_day(self, simple_linear_model):
+        """Test behaviour of daily profile parameter for leap years
+        """
+
+        model = simple_linear_model
+        model.timestepper.start = pd.to_datetime("2016-01-01")
+        model.timestepper.end = pd.to_datetime("2016-12-31")
+
+        scenario = Scenario(model, 'A', 2)
+        values = np.array([np.arange(366, dtype=np.float64), np.arange(366, 0, -1, dtype=np.float64)])
+
+        expected_values = values.T
+
+        p = ScenarioDailyProfileParameter(model, scenario, values)
+        AssertionRecorder(model, p, expected_data=expected_values)
+
+        model.setup()
+        model.run()
+
+def test_scenario_weekly_profile(simple_linear_model):
+
+    model = simple_linear_model
+    scenario = Scenario(model, 'A', 2)
+
+    v = np.arange(1, 53, dtype=np.float64)
+    values = np.array([v, v * 2])
+
+    p = ScenarioWeeklyProfileParameter(model, scenario, values)
+
+    @assert_rec(model, p)
+    def expected_func(timestep, scenario_index):
+        day = timestep.dayofyear - 1
+        if day > 58:  # 28th Feb
+            day += 1
+        week = min(day // 7, 51)
+        value = week + 1
+        if scenario_index.global_id == 1:
+            value *= 2
+        return value
+
+    model.setup()
+    model.run()
+
+
+
+def test_weekly_profile(simple_linear_model):
+    model = simple_linear_model
+
+    model.timestepper.start = "2004-01-01"
+    model.timestepper.end = "2005-05-01"
+    model.timestepper.delta = 7
+
+    values = np.arange(0, 52) ** 2 + 27.5
+
+    p = WeeklyProfileParameter.load(model, {"values": values})
+
+    @assert_rec(model, p)
+    def expected_func(timestep, scenario_index):
+        week = int(min((timestep.dayofyear - 1) // 7, 51))
+        value = week ** 2 + 27.5
+        return value
+
+    model.run()
+
+class TestAnnualHarmonicSeriesParameter:
+    """ Tests for `AnnualHarmonicSeriesParameter` """
+    def test_single_harmonic(self, model):
+
+        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25], [np.pi/4])
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        for ts in model.timestepper:
+            doy = (ts.datetime.dayofyear - 1)/365
+            np.testing.assert_allclose(p1.value(ts, si), 0.5 + 0.25*np.cos(doy*2*np.pi + np.pi/4))
+
+    def test_double_harmonic(self, model):
+        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25, 0.3], [np.pi/4, np.pi/3])
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        for ts in model.timestepper:
+            doy = (ts.datetime.dayofyear - 1) /365
+            expected = 0.5 + 0.25*np.cos(doy*2*np.pi + np.pi / 4) + 0.3*np.cos(doy*4*np.pi + np.pi/3)
+            np.testing.assert_allclose(p1.value(ts, si), expected)
+
+    def test_load(self, model):
+
+        data = {
+            "type": "annualharmonicseries",
+            "mean": 0.5,
+            "amplitudes": [0.25],
+            "phases": [np.pi/4]
+        }
+
+        p1 = load_parameter(model, data)
+
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+        for ts in model.timestepper:
+            doy = (ts.datetime.dayofyear - 1) / 365
+            np.testing.assert_allclose(p1.value(ts, si), 0.5 + 0.25 * np.cos(doy * 2 * np.pi + np.pi / 4))
+
+    def test_variable(self, model):
+        """ Test that variable updating works. """
+        p1 = AnnualHarmonicSeriesParameter(model, 0.5, [0.25], [np.pi/4], is_variable=True)
+
+        assert p1.double_size == 3
+        assert p1.integer_size == 0
+
+        new_var = np.array([0.6, 0.1, np.pi/2])
+        p1.set_double_variables(new_var)
+        np.testing.assert_allclose(p1.get_double_variables(), new_var)
+
+        with pytest.raises(NotImplementedError):
+            p1.set_integer_variables(np.arange(3, dtype=np.int32))
+
+        with pytest.raises(NotImplementedError):
+            p1.get_integer_variables()
+
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        for ts in model.timestepper:
+            doy = (ts.datetime.dayofyear - 1)/365
+            np.testing.assert_allclose(p1.value(ts, si), 0.6 + 0.1*np.cos(doy*2*np.pi + np.pi/2))
+
+
+def custom_test_func(array, axis=None):
+    return np.sum(array**2, axis=axis)
+
+
+class TestAggregatedParameter:
+    """Tests for AggregatedParameter"""
+    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "median": np.median, "sum": np.sum}
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "median", "sum"])
+    def test_agg(self, simple_linear_model, agg_func):
+        model = simple_linear_model
+        model.timestepper.delta = 15
+
+        scenarioA = Scenario(model, "Scenario A", size=2)
+        scenarioB = Scenario(model, "Scenario B", size=5)
+
+        values = np.arange(366, dtype=np.float64)
+        p1 = DailyProfileParameter(model, values)
+        p2 = ConstantScenarioParameter(model, scenarioB, np.arange(scenarioB.size, dtype=np.float64))
+
+        p = AggregatedParameter(model, [p1, p2], agg_func=agg_func)
+
+        func = TestAggregatedParameter.funcs[agg_func]
+
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            x = p1.get_value(scenario_index)
+            y = p2.get_value(scenario_index)
+            return func(np.array([x,y]))
+
+        model.run()
+
+    def test_load(self, simple_linear_model):
+        """ Test load from JSON dict"""
+        model = simple_linear_model
+        data = {
+            "type": "aggregated",
+            "agg_func": "product",
+            "parameters": [
+                0.8,
+                {
+                    "type": "monthlyprofile",
+                    "values": list(range(12))
+                }
+            ]
+        }
+
+        p = load_parameter(model, data)
+        # Correct instance is loaded
+        assert isinstance(p, AggregatedParameter)
+
+        @assert_rec(model, p)
+        def expected(timestep, scenario_index):
+            return (timestep.month - 1) * 0.8
+
+        model.run()
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
+    def test_agg_func_get_set(self, model, agg_func):
+        if agg_func == "custom":
+            agg_func = custom_test_func
+        p = AggregatedParameter(model, [], agg_func=agg_func)
+        assert p.agg_func == agg_func
+        p.agg_func = "product"
+        assert p.agg_func == "product"
+
+
+class DummyIndexParameter(IndexParameter):
+    """A simple IndexParameter which returns a constant value"""
+    def __init__(self, model, index, **kwargs):
+        super(DummyIndexParameter, self).__init__(model, **kwargs)
+        self._index = index
+    def index(self, timestep, scenario_index):
+        return self._index
+    def __repr__(self):
+        return "<DummyIndexParameter \"{}\">".format(self.name)
+
+class TestAggregatedIndexParameter:
+    """Tests for AggregatedIndexParameter"""
+    funcs = {"min": np.min, "max": np.max, "sum": np.sum, "product": np.product}
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "sum", "product"])
+    def test_agg(self, simple_linear_model, agg_func):
+        model = simple_linear_model
+        model.timestepper.delta = 1
+        model.timestepper.start = "2017-01-01"
+        model.timestepper.end = "2017-01-03"
+
+        scenarioA = Scenario(model, "Scenario A", size=2)
+        scenarioB = Scenario(model, "Scenario B", size=5)
+
+        p1 = DummyIndexParameter(model, 2)
+        p2 = DummyIndexParameter(model, 3)
+
+        p = AggregatedIndexParameter(model, [p1, p2], agg_func=agg_func)
+
+        func = TestAggregatedIndexParameter.funcs[agg_func]
+
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            x = p1.get_index(scenario_index)
+            y = p2.get_index(scenario_index)
+            return func(np.array([x,y], np.int32))
+
+        model.run()
+
+    def test_agg_anyall(self, simple_linear_model):
+        """Test the "any" and "all" aggregation functions"""
+        model = simple_linear_model
+        model.timestepper.delta = 1
+        model.timestepper.start = "2017-01-01"
+        model.timestepper.end = "2017-01-03"
+
+        scenarioA = Scenario(model, "Scenario A", size=2)
+        scenarioB = Scenario(model, "Scenario B", size=5)
+        num_comb = len(model.scenarios.get_combinations())
+
+        parameters = {
+            0: DummyIndexParameter(model, 0, name="p0"),
+            1: DummyIndexParameter(model, 1, name="p1"),
+            2: DummyIndexParameter(model, 2, name="p2"),
+        }
+
+        data = [(0, 0), (1, 0), (0, 1), (1, 1), (1, 1, 1), (0, 2)]
+        data_parameters = [[parameters[i] for i in d] for d in data]
+        expected = [(np.any(d), np.all(d)) for d in data]
+
+        for n, params in enumerate(data_parameters):
+            for m, agg_func in enumerate(["any", "all"]):
+                p = AggregatedIndexParameter(model, params, agg_func=agg_func)
+                e = np.ones([len(model.timestepper), num_comb]) * expected[n][m]
+                r = AssertionRecorder(model, p, expected_data=e, name="assertion {}-{}".format(n, agg_func))
+
+        model.run()
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
+    def test_agg_func_get_set(self, model, agg_func):
+        if agg_func == "custom":
+            agg_func = custom_test_func
+        p = AggregatedIndexParameter(model, [], agg_func=agg_func)
+        assert p.agg_func == agg_func
+        p.agg_func = "product"
+        assert p.agg_func == "product"
+
+
+def test_parameter_child_variables(model):
+
+    p1 = Parameter(model)
+    # Default parameter
+    assert len(p1.parents) == 0
+    assert len(p1.children) == 0
+
+    c1 = Parameter(model)
+    c1.parents.add(p1)
+    assert len(p1.children) == 1
+    assert c1 in p1.children
+    assert p1 in c1.parents
+
+    # Test third level
+    c2 = Parameter(model)
+    c2.parents.add(c1)
+
+    # Disable parent
+    c1.parents.clear()
+
+    assert len(p1.children) == 0
+
+
+def test_scaled_profile_nested_load(model):
+    """ Test `ScaledProfileParameter` loading with `AggregatedParameter` """
+    model.timestepper.delta = 15
+
+    s = Storage(model, 'Storage', max_volume=100.0, initial_volume=50.0, num_outputs=0)
+    d = Output(model, 'Link')
+    data = {
+        'type': 'scaledprofile',
+        'scale': 50.0,
+        'profile': {
+            'type': 'aggregated',
+            'agg_func': 'product',
+            'parameters': [
+                {
+                    'type': 'monthlyprofile',
+                    'values': [0.5]*12
+                },
+                {
+                    'type': 'constant',
+                    'value': 1.5,
+                }
+            ]
+        }
+    }
+
+    s.connect(d)
+
+    d.max_flow = p = load_parameter(model, data)
+
+    @assert_rec(model, p)
+    def expected_func(timestep, scenario_index):
+        return 50.0 * 0.5 * 1.5
+
+    model.run()
+
+
+def test_parameter_df_upsampling(model):
+    """ Test that the `DataFrameParameter` can upsample data from a `pandas.DataFrame` and return that correctly
+    """
+    # scenario indices (not used for this test)
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    # Use a 7 day timestep for this test and run 2015
+    model.timestepper.delta = datetime.timedelta(7)
+    model.timestepper.start = pd.to_datetime('2015-01-01')
+    model.timestepper.end = pd.to_datetime('2015-12-31')
+    model.timestepper.setup()
+
+    # Daily time-step
+    index = pd.period_range('2015-01-01', periods=365, freq='D')
+    series = pd.Series(np.arange(365), index=index)
+
+    p = DataFrameParameter(model, series)
+    p.setup()
+
+    A = series.resample('7D').mean()
+    for v, ts in zip(A, model.timestepper):
+        np.testing.assert_allclose(p.value(ts, si), v)
+
+    model.reset()
+    # Daily time-step that requires aligning
+    index = pd.date_range('2014-12-31', periods=366, freq='D')
+    series = pd.Series(np.arange(366), index=index)
+
+    p = DataFrameParameter(model, series)
+    p.setup()
+
+    # offset the resample appropriately for the test
+    A = series[1:].resample('7D').mean()
+    for v, ts in zip(A, model.timestepper):
+        np.testing.assert_allclose(p.value(ts, si), v)
+
+    model.reset()
+    # Daily time-step that is not covering the require range
+    index = pd.date_range('2015-02-01', periods=365, freq='D')
+    series = pd.Series(np.arange(365), index=index)
+
+    p = DataFrameParameter(model, series)
+    with pytest.raises(ResamplingError):
+        p.setup()
+
+    model.reset()
+    # Daily time-step that is not covering the require range
+    index = pd.date_range('2014-11-01', periods=365, freq='D')
+    series = pd.Series(np.arange(365), index=index)
+
+    p = DataFrameParameter(model, series)
+    with pytest.raises(ResamplingError):
+        p.setup()
+
+
+def test_parameter_df_upsampling_multiple_columns(model):
+    """ Test that the `DataFrameParameter` works with multiple columns that map to a `Scenario`
+    """
+    scA = Scenario(model, 'A', size=20)
+    scB = Scenario(model, 'B', size=2)
+    # scenario indices (not used for this test)
+
+    # Use a 7 day timestep for this test and run 2015
+    model.timestepper.delta = datetime.timedelta(7)
+    model.timestepper.start = pd.to_datetime('2015-01-01')
+    model.timestepper.end = pd.to_datetime('2015-12-31')
+    model.timestepper.setup()
+
+    # Daily time-step
+    index = pd.date_range('2015-01-01', periods=365, freq='D')
+    df = pd.DataFrame(np.random.rand(365, 20), index=index)
+
+    p = DataFrameParameter(model, df, scenario=scA)
+    p.setup()
+
+    A = df.resample('7D', axis=0).mean()
+    for v, ts in zip(A.values, model.timestepper):
+        np.testing.assert_allclose([p.value(ts, ScenarioIndex(i, np.array([i], dtype=np.int32))) for i in range(20)], v)
+
+    p = DataFrameParameter(model, df, scenario=scB)
+    with pytest.raises(ValueError):
+        p.setup()
+
+
+def test_parameter_df_json_load(model, tmpdir):
+
+    # Daily time-step
+    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
+    df = pd.DataFrame(np.random.rand(365), index=index, columns=['data'])
+    df_path = tmpdir.join('df.csv')
+    df.to_csv(str(df_path))
+
+    data = {
+        'type': 'dataframe',
+        'url': str(df_path),
+        'index_col': 'date',
+        'parse_dates': True,
+    }
+
+    p = load_parameter(model, data)
+    p.setup()
+
+
+def test_parameter_df_embed_load(model):
+
+    # Daily time-step
+    index = pd.date_range('2015-01-01', periods=365, freq='D', name='date')
+    df = pd.DataFrame(np.random.rand(365), index=index, columns=['data'])
+
+    # Save to JSON and load. This is the format we support loading as embedded data
+    df_data = df.to_json(date_format="iso")
+    # Removing the time information from the dataset for testing purposes
+    df_data = df_data.replace('T00:00:00.000Z', '')
+    df_data = json.loads(df_data)
+
+    data = {
+        'type': 'dataframe',
+        'data': df_data,
+        'parse_dates': True,
+    }
+
+    p = load_parameter(model, data)
+    p.setup()
+
+
+def test_simple_json_parameter_reference():
+    # note that parameters in the "parameters" section cannot be literals
+    model = load_model("parameter_reference.json")
+    max_flow = model.nodes["supply1"].max_flow
+    assert(isinstance(max_flow, ConstantParameter))
+    assert(max_flow.value(None, None) == 125.0)
+    cost = model.nodes["demand1"].cost
+    assert(isinstance(cost, ConstantParameter))
+    assert(cost.value(None, None) == -10.0)
+
+    assert(len(model.parameters) == 4)  # 4 parameters defined
+
+
+def test_threshold_parameter(simple_linear_model):
+    model = simple_linear_model
+    model.timestepper.delta = 150
+
+    scenario = Scenario(model, "Scenario", size=2)
+
+    class DummyRecorder(Recorder):
+        def __init__(self, model, value, *args, **kwargs):
+            super(DummyRecorder, self).__init__(model, *args, **kwargs)
+            self.val = value
+        def setup(self):
+            super(DummyRecorder, self).setup()
+            num_comb = len(model.scenarios.combinations)
+            self.data = np.empty([len(model.timestepper), num_comb], dtype=np.float64)
+        def after(self):
+            timestep = model.timestepper.current
+            self.data[timestep.index, :] = self.val
+
+    threshold = 10.0
+    values = [50.0, 60.0]
+
+    rec1 = DummyRecorder(model, threshold-5, name="rec1")  # below
+    rec2 = DummyRecorder(model, threshold, name="rec2")    # equal
+    rec3 = DummyRecorder(model, threshold+5, name="rec3")  # above
+
+    expected = [
+        ("LT", (1, 0, 0)),
+        ("GT", (0, 0, 1)),
+        ("EQ", (0, 1, 0)),
+        ("LE", (1, 1, 0)),
+        ("GE", (0, 1, 1)),
+    ]
+
+    for predicate, (value_lt, value_eq, value_gt) in expected:
+        for rec in (rec1, rec2, rec3):
+            param = RecorderThresholdParameter(model, rec, threshold, values=values, predicate=predicate)
+            e_val = values[getattr(rec.val, "__{}__".format(predicate.lower()))(threshold)]
+            e = np.ones([len(model.timestepper), len(model.scenarios.get_combinations())]) * e_val
+            e[0, :] = values[1] # first timestep is always "on"
+            r = AssertionRecorder(model, param, expected_data=e)
+            r.name = "assert {} {} {}".format(rec.val, predicate, threshold)
+
+    model.run()
+
+
+def test_constant_from_df():
+    """
+    Test that a dataframe can be used to provide data to ConstantParameter (single values).
+    """
+    model = load_model('simple_df.json')
+
+    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
+    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
+
+    ts = model.timestepper.next()
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
+    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -10.0)
+
+
+def test_constant_from_shared_df():
+    """
+    Test that a shared dataframe can be used to provide data to ConstantParameter (single values).
+    """
+    model = load_model('simple_df_shared.json')
+
+    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
+    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
+
+    ts = model.timestepper.next()
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
+    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -10.0)
+
+
+def test_constant_from_multiindex_df():
+    """
+    Test that a dataframe can be used to provide data to ConstantParameter (single values).
+    """
+    model = load_model('multiindex_df.json')
+
+
+    assert isinstance(model.nodes['demand1'].max_flow, ConstantParameter)
+    assert isinstance(model.nodes['demand1'].cost, ConstantParameter)
+
+    ts = model.timestepper.next()
+    si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+    np.testing.assert_allclose(model.nodes['demand1'].max_flow.value(ts, si), 10.0)
+    np.testing.assert_allclose(model.nodes['demand1'].cost.value(ts, si), -100.0)
+
+def test_parameter_registry_overwrite(model):
+    # define a parameter
+    class NewParameter(Parameter):
+        DATA = 42
+        def __init__(self, model, values, *args, **kwargs):
+            super(NewParameter, self).__init__(model, *args, **kwargs)
+            self.values = values
+    NewParameter.register()
+
+    # re-define a parameter
+    class NewParameter(IndexParameter):
+        DATA = 43
+        def __init__(self, model, values, *args, **kwargs):
+            super(NewParameter, self).__init__(model, *args, **kwargs)
+            self.values = values
+    NewParameter.register()
+
+    data = {
+        "type": "new",
+        "values": 0
+    }
+    parameter = load_parameter(model, data)
+
+    # parameter is instance of new class, not old class
+    assert(isinstance(parameter, NewParameter))
+    assert(parameter.DATA == 43)
+
+
+def test_invalid_parameter_values():
+    """
+    Test that `load_parameter_values` returns a ValueError rather than KeyError.
+
+    This is useful to catch and give useful messages when no valid reference to
+    a data location is given.
+
+    Regression test for Issue #247 (https://github.com/pywr/pywr/issues/247)
+    """
+
+    from pywr.parameters._parameters import load_parameter_values
+
+    m = Model()
+    data = {'name': 'my_parameter', 'type': 'AParameterThatShouldHaveValues'}
+    with pytest.raises(ValueError):
+        load_parameter_values(model, data)
+
+
+class Test1DPolynomialParameter:
+    """ Tests for `Polynomial1DParameter` """
+    def test_init(self, simple_storage_model):
+        """ Test initialisation raises error with too many keywords """
+        stg = simple_storage_model.nodes['Storage']
+        param = ConstantParameter(simple_storage_model, 2.0)
+        with pytest.raises(ValueError):
+            # Passing both "parameter" and "storage_node" is invalid
+            Polynomial1DParameter(simple_storage_model, [0.5, np.pi], parameter=param, storage_node=stg)
+
+    def test_1st_order_with_parameter(self, simple_linear_model):
+        """ Test 1st order with a `Parameter` """
+        model = simple_linear_model
+
+        x = 2.0
+        p1 = Polynomial1DParameter(model, [0.5, np.pi], parameter=ConstantParameter(model, x))
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi * x
+        model.run()
+
+    def test_2nd_order_with_parameter(self, simple_linear_model):
+        """ Test 2nd order with a `Parameter` """
+        model = simple_linear_model
+
+        x = 2.0
+        px = ConstantParameter(model, x)
+        p1 = Polynomial1DParameter(model, [0.5, np.pi, 3.0], parameter=px)
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi*x + 3.0*x**2
+        model.run()
+
+    def test_1st_order_with_storage(self, simple_storage_model):
+        """ Test with a `Storage` node """
+        model = simple_storage_model
+        stg = model.nodes['Storage']
+        x = stg.initial_volume
+        p1 = Polynomial1DParameter(model, [0.5, np.pi], storage_node=stg)
+        p2 = Polynomial1DParameter(model, [0.5, np.pi], storage_node=stg, use_proportional_volume=True)
+
+        # Test with absolute storage
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi*x
+
+        # Test with proportional storage
+        @assert_rec(model, p2, name="proportionalassertion")
+        def expected_func(timestep, scenario_index):
+
+            return 0.5 + np.pi * x/stg.max_volume
+
+        model.setup()
+        model.step()
+
+
+    def test_load(self, simple_linear_model):
+        model = simple_linear_model
+
+        x = 1.5
+        data = {
+            "type": "polynomial1d",
+            "coefficients": [0.5, 2.5],
+            "parameter": {
+                "type": "constant",
+                "value": x
+            }
+        }
+
+        p1 = load_parameter(model, data)
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + 2.5*x
+        model.run()
+
+    def test_load_with_scaling(self, simple_linear_model):
+        model = simple_linear_model
+        x = 1.5
+        data = {
+            "type": "polynomial1d",
+            "coefficients": [0.5, 2.5],
+            "parameter": {
+                "type": "constant",
+                "value": x
+            },
+            "scale": 1.25,
+            "offset": 0.75
+        }
+        xscaled = x*1.25 + 0.75
+        p1 = load_parameter(model, data)
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + 2.5*xscaled
+        model.run()
+
+
+def test_interpolated_parameter(simple_linear_model):
+    model = simple_linear_model
+    model.timestepper.start = "1920-01-01"
+    model.timestepper.end = "1920-01-12"
+
+    p1 = ArrayIndexedParameter(model, [0,1,2,3,4,5,6,7,8,9,10,11])
+    p2 = InterpolatedParameter(model, p1, [0, 5, 10, 11], [0, 5*2, 10*3, 2])
+
+    @assert_rec(model, p2)
+    def expected_func(timestep, scenario_index):
+        values = [0, 2, 4, 6, 8, 10, 14, 18, 22, 26, 30, 2]
+        return values[timestep.index]
+    model.run()
+
+
+class TestInterpolatedQuadratureParameter:
+
+    @pytest.mark.parametrize("lower_interval", [None, 0, 1])
+    def test_calc(self, simple_linear_model, lower_interval):
+        model = simple_linear_model
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.end = "1920-01-12"
+
+        b = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
+        a = None
+        if lower_interval is not None:
+            a = ConstantParameter(model, lower_interval)
+
+        p2 = InterpolatedQuadratureParameter(model, b, [-5, 0, 5, 10, 11], [0, 0, 5 * 2, 10 * 3, 2],
+                                             lower_parameter=a)
+
+        def area(i):
+            if i < 0:
+                value = 0
+            elif i < 6:
+                value = 2*i**2 / 2
+            elif i < 11:
+                value = 25 + 4*(i - 5)**2 / 2 + (i - 5) * 10
+            else:
+                value = 25 + 50 + 50 + 28 / 2 + 2
+            return value
+
+        @assert_rec(model, p2)
+        def expected_func(timestep, scenario_index):
+            i = timestep.index
+            value = area(i)
+            if lower_interval is not None:
+                value -= area(lower_interval)
+            return value
+
+        model.run()
+
+    def test_load(self, simple_linear_model):
+        model = simple_linear_model
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.end = "1920-01-12"
+
+        p1 = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], name='p1')
+
+        p2 = {
+            'type': 'interpolatedquadrature',
+            'upper_parameter': 'p1',
+            'x': [0, 5, 10, 11],
+            'y': [0, 5 * 2, 10 * 3, 2]
+        }
+
+        p2 = load_parameter(model, p2)
+
+        @assert_rec(model, p2)
+        def expected_func(timestep, scenario_index):
+            i = timestep.index
+            if i < 6:
+                value = 2 * i ** 2 / 2
+            elif i < 11:
+                value = 25 + 4 * (i - 5) ** 2 / 2 + (i - 5) * 10
+            else:
+                value = 25 + 50 + 50 + 28 / 2 + 2
+            return value
+
+        model.run()
+
+
+class TestPiecewiseIntegralParameter:
+    X = [3, 8, 11]
+    Y = [5, 10, 2]
+
+    @staticmethod
+    def area(i):
+        if i < 0:
+            value = 0
+        elif i <= 3:
+            value = 5 * i
+        elif i <= 8:
+            value = 5 * 3 + 10 * (i - 3)
+        else:
+            value = 5 * 3 + 10 * (8 - 3) + 2 * (i - 8)
+        return value
+
+    def test_calc(self, simple_linear_model):
+        """Test the piecewise integral calculaiton."""
+        model = simple_linear_model
+
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.end = "1920-01-12"
+
+        x = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
+        p2 = PiecewiseIntegralParameter(model, x, self.X, self.Y)
+
+        @assert_rec(model, p2)
+        def expected_func(timestep, scenario_index):
+            i = timestep.index
+            return self.area(i)
+
+        model.run()
+
+    def test_load(self, simple_linear_model):
+        """Test loading from JSON."""
+        model = simple_linear_model
+
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.end = "1920-01-12"
+
+        x = ArrayIndexedParameter(model, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], name='x')
+
+        p2 = {
+            'type': 'piecewiseintegralparameter',
+            'parameter': 'x',
+            'x': self.X,
+            'y': self.Y,
+        }
+
+        p2 = load_parameter(model, p2)
+
+        @assert_rec(model, p2)
+        def expected_func(timestep, scenario_index):
+            i = timestep.index
+            return self.area(i)
+
+        model.run()
+
+
+class Test2DStoragePolynomialParameter:
+
+    def test_1st(self, simple_storage_model):
+        """ Test 1st order """
+        model = simple_storage_model
+        stg = model.nodes['Storage']
+
+        x = 2.0
+        y = stg.initial_volume
+        coefs = [[0.5, np.pi], [2.5, 0.3]]
+
+        p1 = Polynomial2DStorageParameter(model, coefs, stg, ConstantParameter(model, x))
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
+        model.setup()
+        model.step()
+
+    def test_load(self, simple_storage_model):
+        model = simple_storage_model
+        stg = model.nodes['Storage']
+
+        x = 2.0
+        y = stg.initial_volume/stg.max_volume
+        data = {
+            "type": "polynomial2dstorage",
+            "coefficients": [[0.5, np.pi], [2.5, 0.3]],
+            "use_proportional_volume": True,
+            "parameter": {
+                "type": "constant",
+                "value": x
+            },
+            "storage_node": "Storage"
+        }
+
+        p1 = load_parameter(model, data)
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
+        model.setup()
+        model.step()
+
+    def test_load_wth_scaling(self, simple_storage_model):
+        model = simple_storage_model
+        stg = model.nodes['Storage']
+
+        x = 2.0
+        y = stg.initial_volume/stg.max_volume
+        data = {
+            "type": "polynomial2dstorage",
+            "coefficients": [[0.5, np.pi], [2.5, 0.3]],
+            "use_proportional_volume": True,
+            "parameter": {
+                "type": "constant",
+                "value": x
+            },
+            "storage_node": "Storage",
+            "storage_scale": 1.3,
+            "storage_offset": 0.75,
+            "parameter_scale": 1.25,
+            "parameter_offset": -0.5
+        }
+
+        p1 = load_parameter(model, data)
+
+        # Scaled parameters
+        x = x*1.25 - 0.5
+        y = y*1.3 + 0.75
+
+        @assert_rec(model, p1)
+        def expected_func(timestep, scenario_index):
+            return 0.5 + np.pi*x + 2.5*y+ 0.3*x*y
+        model.setup()
+        model.step()
+
+
+class TestDivisionParameter:
+
+    def test_divsion(self, simple_linear_model):
+        model = simple_linear_model
+        model.timestepper.start = "2017-01-01"
+        model.timestepper.end = "2017-01-15"
+
+        profile = list(range(1, 367))
+
+        data = {
+            "type": "division",
+            "numerator": {
+                "name": "raw",
+                "type": "dailyprofile",
+                "values": profile,
+            },
+            "denominator": {
+                "type": "constant",
+                "value": 123.456
+            }
+        }
+
+        model.nodes["Input"].max_flow = parameter = load_parameter(model, data)
+        model.nodes["Output"].max_flow = 9999
+        model.nodes["Output"].cost = -100
+
+        daily_profile = model.parameters["raw"]
+
+        @assert_rec(model, parameter)
+        def expected(timestep, scenario_index):
+            value = daily_profile.get_value(scenario_index)
+            return value / 123.456
+        model.run()
+
+
+class TestMinMaxNegativeOffsetParameter:
+    @pytest.mark.parametrize("ptype,profile", [
+        ("max", list(range(-10, 356))),
+        ("min", list(range(0, 366))),
+        ("negative", list(range(-366, 0))),
+        ("negativemax", list(range(-366, 0))),
+        ("negativemin", list(range(-366, 0))),
+        ("offset", list(range(0, 366))),
+    ])
+    def test_parameter(cls, simple_linear_model, ptype,profile):
+        model = simple_linear_model
+        model.timestepper.start = "2017-01-01"
+        model.timestepper.end = "2017-12-31"
+
+        data = {
+            "type": ptype,
+            "parameter": {
+                "name": "raw",
+                "type": "dailyprofile",
+                "values": profile,
+            }
+        }
+
+        if ptype in ("max", "min", "negativemax", "negativemin"):
+            data["threshold"] = 3
+        elif ptype == 'offset':
+            data["offset"] = 3
+
+        func = {
+            "min": min,
+            "max": max,
+            "negative": lambda t, x: -x,
+            "negativemax": lambda t, x: max(t, -x),
+            "negativemin": lambda t, x: min(t, -x),
+            "offset": lambda o, x: x + o
+        }[ptype]
+
+        model.nodes["Input"].max_flow = parameter = load_parameter(model, data)
+        model.nodes["Output"].max_flow = 9999
+        model.nodes["Output"].cost = -100
+
+        daily_profile = model.parameters["raw"]
+
+        @assert_rec(model, parameter)
+        def expected(timestep, scenario_index):
+            value = daily_profile.get_value(scenario_index)
+            return func(3, value)
+        model.run()
+
+    def test_offset_parameter_variable(self, simple_linear_model):
+        """Test OffsetParameter's variable API."""
+
+        data = {
+            "type": "offset",
+            "parameter": {
+                "name": "raw",
+                "type": "dailyprofile",
+                "values": list(range(366)),
+            },
+            "offset": 10,
+            "lower_bounds": -100,
+            "upper_bounds": 100,
+        }
+        parameter = load_parameter(simple_linear_model, data)
+        np.testing.assert_allclose(parameter.offset, 10)
+        np.testing.assert_allclose(parameter.get_double_variables(), [10.0])
+        np.testing.assert_allclose(parameter.get_double_lower_bounds(), [-100.0])
+        np.testing.assert_allclose(parameter.get_double_upper_bounds(), [100.0])
+        # Update value using variable API
+        parameter.set_double_variables(np.array([20.0]))
+        np.testing.assert_allclose(parameter.offset, 20)
+        np.testing.assert_allclose(parameter.get_double_variables(), [20.0])
+
+
+def test_ocptt(simple_linear_model):
+    model = simple_linear_model
+    inpt = model.nodes["Input"]
+    s1 = Scenario(model, "scenario 1", size=3)
+    s2 = Scenario(model, "scenario 1", size=2)
+    x = np.arange(len(model.timestepper)).reshape([len(model.timestepper), 1]) + 5
+    y = np.arange(s1.size).reshape([1, s1.size])
+    z = x * y ** 2
+    p = ArrayIndexedScenarioParameter(model, s1, z)
+    inpt.max_flow = p
+    model.setup()
+    model.reset()
+    model.step()
+
+    values1 = [p.get_value(scenario_index) for scenario_index in model.scenarios.combinations]
+    values2 = list(p.get_all_values())
+    assert_allclose(values1, [0, 0, 5, 5, 20, 20])
+    assert_allclose(values2, [0, 0, 5, 5, 20, 20])
+
+
+class TestThresholdParameters:
+
+    def test_storage_threshold_parameter(self, simple_storage_model):
+        """ Test StorageThresholdParameter """
+        m = simple_storage_model
+
+        data = {
+            "type": "storagethreshold",
+            "storage_node": "Storage",
+            "threshold": 10.0,
+            "predicate": ">"
+        }
+
+        p1 = load_parameter(m, data)
+
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        m.nodes['Storage'].initial_volume = 15.0
+        m.setup()
+        # Storage > 10
+        assert p1.index(m.timestepper.current, si) == 1
+
+        m.nodes['Storage'].initial_volume = 5.0
+        m.setup()
+        # Storage < 10
+        assert p1.index(m.timestepper.current, si) == 0
+
+    def test_node_threshold_parameter2(self, simple_linear_model):
+        model = simple_linear_model
+        model.nodes["Input"].max_flow = ArrayIndexedParameter(model, np.arange(0, 20))
+        model.nodes["Output"].cost = -10.0
+        model.timestepper.start = "1920-01-01"
+        model.timestepper.end = "1920-01-15"
+        model.timestepper.delta = 1
+
+        threshold = 5.0
+
+        parameters = {}
+        for predicate in (">", "<", "="):
+            data = {
+                "type": "nodethreshold",
+                "node": "Output",
+                "threshold": 5.0,
+                "predicate": predicate,
+                # we need to define values so AssertionRecorder can be used
+                "values": [0.0, 1.0],
+            }
+            parameter = load_parameter(model, data)
+            parameter.name = "nodethresold {}".format(predicate)
+            parameters[predicate] = parameter
+
+            if predicate == ">":
+                expected_data = (np.arange(-1, 20) > threshold).astype(int)
+            elif predicate == "<":
+                expected_data = (np.arange(-1, 20) < threshold).astype(int)
+            else:
+                expected_data = (np.arange(-1, 20) == threshold).astype(int)
+            expected_data[0] = 0 # previous flow in initial timestep is undefined
+            expected_data = expected_data[:, np.newaxis]
+
+            rec = AssertionRecorder(model, parameter, expected_data=expected_data, name="assertion recorder {}".format(predicate))
+
+        model.run()
+
+    @pytest.mark.parametrize("threshold, ratchet", [
+        [5.0, False],
+        [{"type": "constant", "value": 5.0}, False],
+        [{"type": "constant", "value": 5.0}, True],
+    ], ids=["double", "parameter", "parameter-ratchet"])
+    def test_parameter_threshold_parameter(self, simple_linear_model, threshold, ratchet):
+        """ Test ParameterThresholdParameter """
+        m = simple_linear_model
+        m.nodes['Input'].max_flow = 10.0
+        m.nodes['Output'].cost = -10.0
+
+        data = {
+            "type": "parameterthreshold",
+            "parameter": {
+                "type": "constant",
+                "value": 3.0
+            },
+            "threshold": threshold,
+            "predicate": "<",
+            "ratchet": ratchet
+        }
+
+        p1 = load_parameter(m, data)
+
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        # Triggered initial 3 < 5
+        m.setup()
+        m.step()
+        assert p1.index(m.timestepper.current, si) == 1
+
+        # Update parameter, now 8 > 5; not triggered.
+        p1.param.set_double_variables(np.array([8.0,]))
+        m.step()
+        # If using a ratchet the trigger remains on.
+        assert p1.index(m.timestepper.current, si) == (1 if ratchet else 0)
+
+        # Resetting the model resets the ratchet too.
+        m.reset()
+        m.step()
+        # flow < 5
+        assert p1.index(m.timestepper.current, si) == 0
+
+    def test_current_year_threshold_parameter(self, simple_linear_model):
+        """Test CurrentYearThresholdParameter"""
+        m = simple_linear_model
+
+        m.timestepper.start = '2020-01-01'
+        m.timestepper.end = '2030-01-01'
+
+        data = {
+            'type': 'currentyearthreshold',
+            'threshold': 2025,
+            "predicate": ">=",
+        }
+
+        p = load_parameter(m, data)
+
+        @assert_rec(m, p, get_index=True)
+        def expected_func(timestep, scenario_index):
+            current_year = timestep.year
+            value = 1 if current_year >= 2025 else 0
+            return value
+
+        m.run()
+
+    def test_current_ordinal_threshold_parameter(self, simple_linear_model):
+        """Test CurrentYearThresholdParameter"""
+        m = simple_linear_model
+
+        m.timestepper.start = '2020-01-01'
+        m.timestepper.end = '2030-01-01'
+
+        threshold = datetime.date(2025, 6, 15).toordinal()
+
+        data = {
+            'type': 'currentordinaldaythreshold',
+            'threshold': threshold,
+            "predicate": ">=",
+        }
+
+        p = load_parameter(m, data)
+
+        @assert_rec(m, p, get_index=True)
+        def expected_func(timestep, scenario_index):
+            o = timestep.datetime.toordinal()
+            value = 1 if o >= threshold else 0
+            return value
+
+        m.run()
+
+
+def test_orphaned_components(simple_linear_model):
+    model = simple_linear_model
+    model.nodes["Input"].max_flow = ConstantParameter(model, 10.0)
+
+    result = model.find_orphaned_parameters()
+    assert(not result)
+    # assert that warning not raised by check
+    with pytest.warns(None) as record:
+        model.check()
+    for w in record:
+        if isinstance(w, OrphanedParameterWarning):
+            pytest.fail("OrphanedParameterWarning raised unexpectedly!")
+
+    # add some orphans
+    orphan1 = ConstantParameter(model, 5.0)
+    orphan2 = ConstantParameter(model, 10.0)
+    orphans = {orphan1, orphan2}
+    result = model.find_orphaned_parameters()
+    assert(orphans == result)
+
+    with pytest.warns(OrphanedParameterWarning):
+        model.check()
+
+def test_deficit_parameter():
+    """Test DeficitParameter
+
+    Here we test both uses of the DeficitParameter:
+      1) Recording the deficit for a node each timestep
+      2) Using yesterday's deficit to control today's flow
+    """
+    model = load_model("deficit.json")
+
+    model.run()
+
+    max_flow = np.array([5, 6, 7, 8, 9, 10, 11, 12, 11, 10, 9, 8])
+    demand = 10.0
+    supplied = np.minimum(max_flow, demand)
+    expected = demand - supplied
+    actual = model.recorders["deficit_recorder"].data
+    assert_allclose(expected, actual[:,0])
+
+    expected_yesterday = [0]+list(expected[0:-1])
+    actual_yesterday = model.recorders["yesterday_recorder"].data
+    assert_allclose(expected_yesterday, actual_yesterday[:,0])
+
+
+def test_flow_parameter():
+    """test FlowParameter
+
+    """
+    model = load_model("flow_parameter.json")
+
+    model.run()
+
+    max_flow = np.array([5, 6, 7, 8, 9, 10, 11, 12, 11, 10, 9, 8])
+    demand = 10.0
+    supplied = np.minimum(max_flow, demand)
+
+    actual = model.recorders["flow_recorder"].data
+    assert_allclose(supplied, actual[:,0])
+
+    expected_yesterday = [3.1415]+list(supplied[0:-1])
+    actual_yesterday = model.recorders["yesterday_flow_recorder"].data
+    assert_allclose(expected_yesterday, actual_yesterday[:,0])
+
+
+class TestHydroPowerTargets:
+    def test_target_json(self):
+        """ Test loading a HydropowerTargetParameter from JSON. """
+        model = load_model("hydropower_target_example.json")
+        si = ScenarioIndex(0, np.array([0], dtype=np.int32))
+
+        # 30 time-steps are run such that the head gets so flow to hit the max_flow
+        # constraint. The first few time-steps are also bound by the min_flow constraint.
+        for i in range(30):
+            model.step()
+
+            rec = model.recorders["turbine1_energy"]
+            param = model.parameters["turbine1_discharge"]
+
+            turbine1 = model.nodes["turbine1"]
+            assert turbine1.flow[0] > 0
+
+            if np.allclose(turbine1.flow[0], 500.0):
+                # If flow is bounded by min_flow then more HP is produced.
+                assert rec.data[i, 0] > param.target.get_value(si)
+            elif np.allclose(turbine1.flow[0], 1000.0):
+                # If flow is bounded by max_flow then less HP is produced.
+                assert rec.data[i, 0] < param.target.get_value(si)
+            else:
+                # If flow is within the bounds target is met exactly.
+                assert_allclose(rec.data[i, 0], param.target.get_value(si))
+
+
+class TestFlowInterpolation:
+
+    def test_flow_interpolation_parameter(self):
+        """The test includes interpolation of river water level based on flow"""
+
+        model = load_model("flow_interpolation.json")
+
+        model.run()
+
+        water_levels1 = np.array([3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])
+
+        modelled_levels = model.recorders["water_level_value"].data
+        assert_allclose(water_levels1, modelled_levels[:, 0])
+
+
+class TestUniformDrawdownProfileParameter:
+
+    def test_uniform_drawdown_profile(self, simple_linear_model):
+        """Test the uniform drawn profile over a leap year and non-leap year."""
+
+        m = simple_linear_model
+        m.timestepper.start = '2015-04-01'
+        m.timestepper.end = '2017-04-01'
+
+        expected_values = np.r_[
+            np.linspace(1, 1/366, 366),  # This period covers Apr-2015 to Apr-2016 (i.e. 366 days)
+            np.linspace(1, 1/365, 365),  # This period covers Apr-2016 to Apr-2017 (i.e. 365 days)
+            np.linspace(1, 1/365, 365),  # This period covers Apr-2017 to Apr-2018 (i.e. 365 days)
+        ]
+
+        data = {
+            'type': 'uniformdrawdownprofile',
+            "reset_day": 1,
+            "reset_month": 4
+        }
+
+        p = load_parameter(m, data)
+
+        @assert_rec(m, p)
+        def expected_func(timestep, scenario_index):
+            return expected_values[timestep.index]
+
+        m.run()
+
+
+class TestRbfProfileParameter:
+    """Tests for RbfParameter."""
+
+    @pytest.mark.parametrize(['min_value', 'max_value'], [(None, None), (0.3, None), (None, 0.6)])
+    def test_rbf_profile(self, simple_linear_model, min_value, max_value):
+        """Test the Rbf profile parameter."""
+
+        m = simple_linear_model
+        m.timestepper.start = '2015-01-01'
+        m.timestepper.end = '2015-12-31'
+
+        # The Rbf parameter should mirror the input data at the start and end to create roughly
+        # consistent gradients across the end of year boundary.
+        interp_days_of_year = [-65, 1, 100, 200, 300, 366, 465]
+        interp_values = [0.2, 0.5, 0.7, 0.5, 0.2, 0.5, 0.7]
+
+        expected_values = Rbf(interp_days_of_year, interp_values)(np.arange(365) + 1)
+
+        data = {
+            'type': 'rbfprofile',
+            "days_of_year": [1, 100, 200, 300],
+            "values": [0.5, 0.7, 0.5, 0.2],
+        }
+        if min_value is not None:
+            data["min_value"] = min_value
+        if max_value is not None:
+            data["max_value"] = max_value
+
+        p = load_parameter(m, data)
+
+        @assert_rec(m, p)
+        def expected_func(timestep, scenario_index):
+            ev = expected_values[timestep.index]
+            if min_value is not None:
+                ev = max(min_value, ev)
+            if max_value is not None:
+                ev = min(max_value, ev)
+            return ev
+
+        m.run()
+
+    @pytest.mark.parametrize('wrong_doys', [
+        [2, 100, 300],  # Incorrect first day
+        [1, 180],  # Too few values
+        [1, 100, 366],  # Incorrect last day
+        [1, 200, 140],  # Not monotonic
+        [1, 140, 140],  # Not strictly monotonic
+    ])
+    def test_incorrect_inputs(self, simple_linear_model, wrong_doys):
+        """Test initialising RbfParameter with incorrect days of the year."""
+
+        data = {
+            'type': 'rbfprofile',
+            "days_of_year": wrong_doys,
+            "values": np.random.rand(len(wrong_doys)).tolist()
+        }
+        with pytest.raises(ValueError):
+            load_parameter(simple_linear_model, data)
+
+    def test_variable_api(self, simple_linear_model):
+        """Test using variable API implementation on RbfParameter."""
+
+        data = {
+            'type': 'rbfprofile',
+            "days_of_year": [1, 100, 200, 300],
+            "values": [0.5, 0.7, 0.5, 0.2]
+        }
+
+        p = load_parameter(simple_linear_model, data)
+        assert p.double_size == 4
+        assert p.integer_size == 0
+
+        new_values = np.random.rand(p.double_size)
+        p.set_double_variables(new_values)
+        np.testing.assert_allclose(p.get_double_variables(), new_values)
+        
+    def test_variable_doys_api(self, simple_linear_model):
+        """Test using the variable API when optimising the days of the year. """
+
+        data = {
+            'type': 'rbfprofile',
+            "days_of_year": [1, 100, 200, 300],
+            "values": [0.5, 0.7, 0.5, 0.2],
+            "lower_bounds": 0.1,
+            "upper_bounds": 0.8,
+            "variable_days_of_year_range": 20,
+            "is_variable": True
+        }
+
+        p = load_parameter(simple_linear_model, data)
+        assert p.double_size == 4
+        assert p.integer_size == 3
+
+        new_values = np.random.rand(p.double_size)
+        p.set_double_variables(new_values)
+        np.testing.assert_allclose(p.get_double_variables(), new_values)
+
+        new_doys = np.array([90, 190, 290], dtype=np.int32)
+        p.set_integer_variables(new_doys)
+        np.testing.assert_allclose(p.get_integer_variables(), new_doys)
+
+        lb = np.array([80, 180, 280], dtype=np.int32)
+        np.testing.assert_allclose(p.get_integer_lower_bounds(), lb)
+
+        ub = np.array([120, 220, 320], dtype=np.int32)
+        np.testing.assert_allclose(p.get_integer_upper_bounds(), ub)
+
+    def test_too_close_doys_error(self, simple_linear_model):
+        """Test that setting days of the year too close together for optimisation raises an error."""
+
+        data = {
+            'type': 'rbfprofile',
+            "days_of_year": [1, 140, 200],   # Closest distance is 60 days
+            "values": [0.5, 0.7, 0.5],
+            "lower_bounds": 0.1,
+            "upper_bounds": 0.8,
+            "variable_days_of_year_range": 30,  # A range of 30 could cause overlap (140 + 30, 200 - 30)
+            "is_variable": True
+        }
+
+        with pytest.raises(ValueError):
+            load_parameter(simple_linear_model, data)
+
+            
+class TestDiscountFactorParameter:
+    def test_discount_json(self):
+        """ Test loading a DiscountFactorParameter from JSON. """
+        model = load_model("discount.json")
+        # run model for period 2015-2020, with base year 2015 and discount rate of 0.035 (3.5%)
+        p = model.parameters['discount_factor']
+        @assert_rec(model, p)
+        def expected_func(timestep, scenario_index):
+            year = timestep.year
+            return 1/pow(1.035, year - 2015)
+
+        model.run()
```

### Comparing `pywr-1.8.0/tests/test_piecewise.py` & `pywr-1.9.0/tests/test_piecewise.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-import pywr.core
-from pywr.parameters import ConstantParameter
-import numpy as np
-from numpy.testing import assert_allclose
-import pytest
-from helpers import assert_model, load_model
-
-
-@pytest.fixture(params=[(10.0, 10.0, 10.0), (5.0, 5.0, 1.0)])
-def simple_piecewise_model(request):
-    """
-    Make a simple model with a single Input and Output and PiecewiseLink
-
-    Input -> PiecewiseLink -> Output
-
-    """
-    in_flow, out_flow, benefit = request.param
-    min_flow_req = 5.0
-
-    model = pywr.core.Model()
-    inpt = pywr.core.Input(model, name="Input", max_flow=in_flow)
-    lnk = pywr.core.PiecewiseLink(model, name="Link", cost=[-1.0, 0.0], max_flow=[min_flow_req, None])
-
-    inpt.connect(lnk)
-    otpt = pywr.core.Output(model, name="Output", min_flow=out_flow, cost=-benefit)
-    lnk.connect(otpt)
-
-    expected_sent = in_flow if benefit > 1.0 else out_flow
-
-    expected_node_results = {
-        "Input": expected_sent,
-        "Link": expected_sent,
-        "Link Sublink 0": min(min_flow_req, expected_sent),
-        "Link Sublink 1": expected_sent - min(min_flow_req, expected_sent),
-        "Output": expected_sent,
-    }
-    return model, expected_node_results
-
-
-def test_piecewise_model(simple_piecewise_model):
-    assert_model(*simple_piecewise_model)
-
-
-def test_piecewise_json():
-    """Test loading of a piecewise link from JSON"""
-    model = load_model("piecewise1.json")
-    sublinks = model.nodes["link1"].sublinks
-    max_flows = [sublink.max_flow for sublink in sublinks]
-    costs = [sublink.cost for sublink in sublinks]
-    assert_allclose(max_flows, [20, np.inf])
-    assert_allclose(costs, [-10, 5])
-    model.run()
-    assert_allclose(model.nodes["demand1"].flow, 20)
-
-
-def test_piecewise_with_parameters_json():
-    """Test using parameters with piecewise link."""
-    model = load_model("piecewise1_with_parameters.json")
-    sublinks = model.nodes["link1"].sublinks
-
-    assert isinstance(sublinks[0].max_flow, ConstantParameter)
-    assert np.isinf(sublinks[1].max_flow)
-    assert isinstance(sublinks[0].cost, ConstantParameter)
-    assert isinstance(sublinks[1].cost, ConstantParameter)
-
-    model.run()
-
-    assert_allclose(model.nodes["demand1"].flow, 20)
+import pywr.core
+from pywr.parameters import ConstantParameter
+import numpy as np
+from numpy.testing import assert_allclose
+import pytest
+from helpers import assert_model, load_model
+
+
+@pytest.fixture(params=[(10.0, 10.0, 10.0), (5.0, 5.0, 1.0)])
+def simple_piecewise_model(request):
+    """
+    Make a simple model with a single Input and Output and PiecewiseLink
+
+    Input -> PiecewiseLink -> Output
+
+    """
+    in_flow, out_flow, benefit = request.param
+    min_flow_req = 5.0
+
+    model = pywr.core.Model()
+    inpt = pywr.core.Input(model, name="Input", max_flow=in_flow)
+    lnk = pywr.core.PiecewiseLink(model, name="Link", cost=[-1.0, 0.0], max_flow=[min_flow_req, None])
+
+    inpt.connect(lnk)
+    otpt = pywr.core.Output(model, name="Output", min_flow=out_flow, cost=-benefit)
+    lnk.connect(otpt)
+
+    expected_sent = in_flow if benefit > 1.0 else out_flow
+
+    expected_node_results = {
+        "Input": expected_sent,
+        "Link": expected_sent,
+        "Link Sublink 0": min(min_flow_req, expected_sent),
+        "Link Sublink 1": expected_sent - min(min_flow_req, expected_sent),
+        "Output": expected_sent,
+    }
+    return model, expected_node_results
+
+
+def test_piecewise_model(simple_piecewise_model):
+    assert_model(*simple_piecewise_model)
+
+
+def test_piecewise_json():
+    """Test loading of a piecewise link from JSON"""
+    model = load_model("piecewise1.json")
+    sublinks = model.nodes["link1"].sublinks
+    max_flows = [sublink.max_flow for sublink in sublinks]
+    costs = [sublink.cost for sublink in sublinks]
+    assert_allclose(max_flows, [20, np.inf])
+    assert_allclose(costs, [-10, 5])
+    model.run()
+    assert_allclose(model.nodes["demand1"].flow, 20)
+
+
+def test_piecewise_with_parameters_json():
+    """Test using parameters with piecewise link."""
+    model = load_model("piecewise1_with_parameters.json")
+    sublinks = model.nodes["link1"].sublinks
+
+    assert isinstance(sublinks[0].max_flow, ConstantParameter)
+    assert np.isinf(sublinks[1].max_flow)
+    assert isinstance(sublinks[0].cost, ConstantParameter)
+    assert isinstance(sublinks[1].cost, ConstantParameter)
+
+    model.run()
+
+    assert_allclose(model.nodes["demand1"].flow, 20)
```

### Comparing `pywr-1.8.0/tests/test_platypus.py` & `pywr-1.9.0/tests/test_platypus.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,117 +1,117 @@
-import pytest
-platypus = pytest.importorskip("platypus")
-from pywr.optimisation.platypus import PlatypusWrapper, PywrRandomGenerator
-from pywr.optimisation import clear_global_model_cache
-from platypus import NSGAII, ProcessPoolEvaluator
-import os
-import numpy as np
-
-
-TEST_FOLDER = os.path.dirname(__file__)
-
-
-class TwoReservoirWrapper(PlatypusWrapper):
-    def customise_model(self, model):
-        self.the_model_has_been_customised = True
-
-
-@pytest.fixture()
-def two_reservoir_problem():
-    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir.json')
-    yield TwoReservoirWrapper(filename)
-    # Clean up the
-    clear_global_model_cache()
-    # We force deallocation the cache here to prevent problems using process pools
-    # with pytest.
-    import gc
-    gc.collect()
-
-
-@pytest.fixture()
-def two_reservoir_constrained_problem():
-    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir_constrained.json')
-    yield TwoReservoirWrapper(filename)
-    # Clean up the
-    clear_global_model_cache()
-    # We force deallocation the cache here to prevent problems using process pools
-    # with pytest.
-    import gc
-    gc.collect()
-
-
-def test_platypus_init(two_reservoir_problem):
-    """ Test the initialisation of the platypus problem. """
-    p = two_reservoir_problem
-
-    assert p.problem.nvars == 12
-    assert p.problem.nobjs == 2
-    assert p.problem.nconstrs == 0
-    # Check the `customise_model` method has been called.
-    assert p.the_model_has_been_customised
-
-
-def test_platypus_constrained_init(two_reservoir_constrained_problem):
-    """ Test the initialisation of a constrained platypus problem. """
-    p = two_reservoir_constrained_problem
-
-    assert p.problem.nvars == 12
-    assert p.problem.nobjs == 2
-    assert p.problem.nconstrs == 2
-    # Check the `customise_model` method has been called.
-    assert p.the_model_has_been_customised
-
-
-def test_platypus_nsgaii_step(two_reservoir_problem):
-    """ Undertake a single step of the NSGAII algorithm with a small population. """
-    algorithm = NSGAII(two_reservoir_problem.problem, population_size=10)
-    algorithm.step()
-
-
-def test_platypus_nsgaii_step(two_reservoir_constrained_problem):
-    """ Undertake a single step of the NSGAII algorithm with a small population. """
-    algorithm = NSGAII(two_reservoir_constrained_problem.problem, population_size=10)
-    algorithm.step()
-
-
-def test_platypus_nsgaii_process_pool(two_reservoir_problem):
-    """ Undertake a single step of the NSGAII algorithm with a ProcessPool. """
-    with ProcessPoolEvaluator(2) as evaluator:
-        algorithm = NSGAII(two_reservoir_problem.problem, population_size=50, evaluator=evaluator)
-        algorithm.run(10)
-
-
-class TestPywrRandomGenerator:
-    def test_current_model(self, two_reservoir_problem):
-        """ Test PywrRandomGenerator inserts the current model configuration in to the population. """
-        generator = PywrRandomGenerator(wrapper=two_reservoir_problem)
-        algorithm = NSGAII(two_reservoir_problem.problem, population_size=10, generator=generator)
-        algorithm.initialize()
-        # Ensure the first solution in the population has variable values from the model
-        solution = algorithm.population[0]
-        np.testing.assert_allclose(solution.variables, np.zeros(12))
-
-    @pytest.mark.parametrize('use_current', [True, False])
-    def test_other_solutions(self, two_reservoir_problem, use_current):
-        """Test PywrRandomGenerator inserts other solutions into the population. """
-
-        # Create some alternative initial solutions
-        solutions = [
-            {
-                'control_curve': {'doubles': [1] * 12}
-            },
-            {
-                'control_curve': {'doubles': [2] * 12}
-            },
-        ]
-
-        generator = PywrRandomGenerator(wrapper=two_reservoir_problem, solutions=solutions, use_current=use_current)
-        algorithm = NSGAII(two_reservoir_problem.problem, population_size=10, generator=generator)
-        algorithm.initialize()
-        # Ensure the first solution in the population has variable values from the model
-        if use_current:
-            np.testing.assert_allclose(algorithm.population[0].variables, np.zeros(12))
-            np.testing.assert_allclose(algorithm.population[1].variables, np.ones(12))
-            np.testing.assert_allclose(algorithm.population[2].variables, np.ones(12) * 2)
-        else:
-            np.testing.assert_allclose(algorithm.population[0].variables, np.ones(12))
-            np.testing.assert_allclose(algorithm.population[1].variables, np.ones(12) * 2)
+import pytest
+platypus = pytest.importorskip("platypus")
+from pywr.optimisation.platypus import PlatypusWrapper, PywrRandomGenerator
+from pywr.optimisation import clear_global_model_cache
+from platypus import NSGAII, ProcessPoolEvaluator
+import os
+import numpy as np
+
+
+TEST_FOLDER = os.path.dirname(__file__)
+
+
+class TwoReservoirWrapper(PlatypusWrapper):
+    def customise_model(self, model):
+        self.the_model_has_been_customised = True
+
+
+@pytest.fixture()
+def two_reservoir_problem():
+    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir.json')
+    yield TwoReservoirWrapper(filename)
+    # Clean up the
+    clear_global_model_cache()
+    # We force deallocation the cache here to prevent problems using process pools
+    # with pytest.
+    import gc
+    gc.collect()
+
+
+@pytest.fixture()
+def two_reservoir_constrained_problem():
+    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir_constrained.json')
+    yield TwoReservoirWrapper(filename)
+    # Clean up the
+    clear_global_model_cache()
+    # We force deallocation the cache here to prevent problems using process pools
+    # with pytest.
+    import gc
+    gc.collect()
+
+
+def test_platypus_init(two_reservoir_problem):
+    """ Test the initialisation of the platypus problem. """
+    p = two_reservoir_problem
+
+    assert p.problem.nvars == 12
+    assert p.problem.nobjs == 2
+    assert p.problem.nconstrs == 0
+    # Check the `customise_model` method has been called.
+    assert p.the_model_has_been_customised
+
+
+def test_platypus_constrained_init(two_reservoir_constrained_problem):
+    """ Test the initialisation of a constrained platypus problem. """
+    p = two_reservoir_constrained_problem
+
+    assert p.problem.nvars == 12
+    assert p.problem.nobjs == 2
+    assert p.problem.nconstrs == 2
+    # Check the `customise_model` method has been called.
+    assert p.the_model_has_been_customised
+
+
+def test_platypus_nsgaii_step(two_reservoir_problem):
+    """ Undertake a single step of the NSGAII algorithm with a small population. """
+    algorithm = NSGAII(two_reservoir_problem.problem, population_size=10)
+    algorithm.step()
+
+
+def test_platypus_nsgaii_step(two_reservoir_constrained_problem):
+    """ Undertake a single step of the NSGAII algorithm with a small population. """
+    algorithm = NSGAII(two_reservoir_constrained_problem.problem, population_size=10)
+    algorithm.step()
+
+
+def test_platypus_nsgaii_process_pool(two_reservoir_problem):
+    """ Undertake a single step of the NSGAII algorithm with a ProcessPool. """
+    with ProcessPoolEvaluator(2) as evaluator:
+        algorithm = NSGAII(two_reservoir_problem.problem, population_size=50, evaluator=evaluator)
+        algorithm.run(10)
+
+
+class TestPywrRandomGenerator:
+    def test_current_model(self, two_reservoir_problem):
+        """ Test PywrRandomGenerator inserts the current model configuration in to the population. """
+        generator = PywrRandomGenerator(wrapper=two_reservoir_problem)
+        algorithm = NSGAII(two_reservoir_problem.problem, population_size=10, generator=generator)
+        algorithm.initialize()
+        # Ensure the first solution in the population has variable values from the model
+        solution = algorithm.population[0]
+        np.testing.assert_allclose(solution.variables, np.zeros(12))
+
+    @pytest.mark.parametrize('use_current', [True, False])
+    def test_other_solutions(self, two_reservoir_problem, use_current):
+        """Test PywrRandomGenerator inserts other solutions into the population. """
+
+        # Create some alternative initial solutions
+        solutions = [
+            {
+                'control_curve': {'doubles': [1] * 12}
+            },
+            {
+                'control_curve': {'doubles': [2] * 12}
+            },
+        ]
+
+        generator = PywrRandomGenerator(wrapper=two_reservoir_problem, solutions=solutions, use_current=use_current)
+        algorithm = NSGAII(two_reservoir_problem.problem, population_size=10, generator=generator)
+        algorithm.initialize()
+        # Ensure the first solution in the population has variable values from the model
+        if use_current:
+            np.testing.assert_allclose(algorithm.population[0].variables, np.zeros(12))
+            np.testing.assert_allclose(algorithm.population[1].variables, np.ones(12))
+            np.testing.assert_allclose(algorithm.population[2].variables, np.ones(12) * 2)
+        else:
+            np.testing.assert_allclose(algorithm.population[0].variables, np.ones(12))
+            np.testing.assert_allclose(algorithm.population[1].variables, np.ones(12) * 2)
```

### Comparing `pywr-1.8.0/tests/test_pygmo.py` & `pywr-1.9.0/tests/test_pygmo.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-import pytest
-pygmo = pytest.importorskip("pygmo")
-
-import pygmo as pg
-from pywr.optimisation.pygmo import PygmoWrapper
-from pywr.optimisation import clear_global_model_cache
-import os
-
-
-TEST_FOLDER = os.path.dirname(__file__)
-
-
-@pytest.fixture()
-def two_reservoir_wrapper():
-    """ Two reservoir test optimisation problem in PygmoWrapper. """
-    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir.json')
-    yield PygmoWrapper(filename)
-    # Clean up the
-    clear_global_model_cache()
-
-
-@pytest.fixture()
-def two_reservoir_constrained_wrapper():
-    """ Two reservoir test optimisation problem in PygmoWrapper. """
-    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir_constrained.json')
-    yield PygmoWrapper(filename)
-    # Clean up the
-    clear_global_model_cache()
-
-
-def test_pygmo_single_generation(two_reservoir_wrapper):
-    """ Simple pygmo wrapper test. """
-    wrapper = two_reservoir_wrapper
-    prob = pg.problem(wrapper)
-    algo = pg.algorithm(pg.moead(gen=1))
-
-    pg.mp_island.init_pool(2)
-    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
-    isl.evolve(1)
-
-
-def test_pygmo_single_generation_constrained(two_reservoir_constrained_wrapper):
-    """ Simple pygmo wrapper test of constrained problem. """
-    wrapper = two_reservoir_constrained_wrapper
-    prob = pg.problem(wrapper)
-    algo = pg.algorithm(pg.moead(gen=1))
-
-    pg.mp_island.init_pool(2)
-    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
-    isl.evolve(1)
+import pytest
+pygmo = pytest.importorskip("pygmo")
+
+import pygmo as pg
+from pywr.optimisation.pygmo import PygmoWrapper
+from pywr.optimisation import clear_global_model_cache
+import os
+
+
+TEST_FOLDER = os.path.dirname(__file__)
+
+
+@pytest.fixture()
+def two_reservoir_wrapper():
+    """ Two reservoir test optimisation problem in PygmoWrapper. """
+    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir.json')
+    yield PygmoWrapper(filename)
+    # Clean up the
+    clear_global_model_cache()
+
+
+@pytest.fixture()
+def two_reservoir_constrained_wrapper():
+    """ Two reservoir test optimisation problem in PygmoWrapper. """
+    filename = os.path.join(TEST_FOLDER, 'models', 'two_reservoir_constrained.json')
+    yield PygmoWrapper(filename)
+    # Clean up the
+    clear_global_model_cache()
+
+
+def test_pygmo_single_generation(two_reservoir_wrapper):
+    """ Simple pygmo wrapper test. """
+    wrapper = two_reservoir_wrapper
+    prob = pg.problem(wrapper)
+    algo = pg.algorithm(pg.moead(gen=1))
+
+    pg.mp_island.init_pool(2)
+    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
+    isl.evolve(1)
+
+
+def test_pygmo_single_generation_constrained(two_reservoir_constrained_wrapper):
+    """ Simple pygmo wrapper test of constrained problem. """
+    wrapper = two_reservoir_constrained_wrapper
+    prob = pg.problem(wrapper)
+    algo = pg.algorithm(pg.moead(gen=1))
+
+    pg.mp_island.init_pool(2)
+    isl = pg.island(algo=algo, prob=prob, size=50, udi=pg.mp_island())
+    isl.evolve(1)
```

### Comparing `pywr-1.8.0/tests/test_recorders.py` & `pywr-1.9.0/tests/test_recorders.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,2055 +1,2055 @@
-# -*- coding: utf-8 -*-
-"""
-Test the Recorder object API
-
-"""
-import pywr.core
-from pywr.core import Model, Input, Output, Scenario, AggregatedNode
-import numpy as np
-import pandas
-import pytest
-import tables
-import json
-from numpy.testing import assert_allclose, assert_equal
-from fixtures import simple_linear_model, simple_storage_model
-from pywr.recorders import (Recorder, NumpyArrayNodeRecorder, NumpyArrayStorageRecorder, NumpyArrayAreaRecorder, NumpyArrayLevelRecorder,
-                            AggregatedRecorder, CSVRecorder, TablesRecorder, TotalDeficitNodeRecorder,
-                            TotalFlowNodeRecorder, RollingMeanFlowNodeRecorder, MeanFlowNodeRecorder, NumpyArrayParameterRecorder,
-                            NumpyArrayIndexParameterRecorder, RollingWindowParameterRecorder, AnnualCountIndexParameterRecorder,
-                            RootMeanSquaredErrorNodeRecorder, MeanAbsoluteErrorNodeRecorder, MeanSquareErrorNodeRecorder,
-                            PercentBiasNodeRecorder, RMSEStandardDeviationRatioNodeRecorder, NashSutcliffeEfficiencyNodeRecorder,
-                            EventRecorder, Event, StorageThresholdRecorder, NodeThresholdRecorder, EventDurationRecorder, EventStatisticRecorder,
-                            FlowDurationCurveRecorder, FlowDurationCurveDeviationRecorder, StorageDurationCurveRecorder,
-                            HydropowerRecorder, TotalHydroEnergyRecorder,
-                            TotalParameterRecorder, MeanParameterRecorder,
-                            NumpyArrayNodeDeficitRecorder, NumpyArrayNodeSuppliedRatioRecorder, NumpyArrayNodeCurtailmentRatioRecorder,
-                            SeasonalFlowDurationCurveRecorder, load_recorder, ParameterNameWarning, NumpyArrayDailyProfileParameterRecorder,
-                            AnnualTotalFlowRecorder, AnnualCountIndexThresholdRecorder, TimestepCountIndexParameterRecorder)
-
-from pywr.recorders.progress import ProgressRecorder
-
-from pywr.parameters import (DailyProfileParameter, FunctionParameter, ArrayIndexedParameter, ConstantParameter,
-                             InterpolatedVolumeParameter)
-from helpers import load_model
-import os
-import sys
-
-
-class TestRecorder:
-    """Tests for Recorder base class."""
-
-    def test_default_no_constraint(self, simple_linear_model):
-        """Test the constraint properties in the default instance (i.e. not a constraint)."""
-        r = Recorder(simple_linear_model)
-        assert r.constraint_lower_bounds is None
-        assert r.constraint_upper_bounds is None
-        assert not r.is_constraint
-        assert not r.is_lower_bounded_constraint
-        assert not r.is_double_bounded_constraint
-        assert not r.is_upper_bounded_constraint
-        assert not r.is_equality_constraint
-
-    def test_equality_constraint(self, simple_linear_model):
-        """Test equality constraint identification. """
-        r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=10.0)
-        assert r.is_constraint
-        assert not r.is_lower_bounded_constraint
-        assert not r.is_double_bounded_constraint
-        assert not r.is_upper_bounded_constraint
-        assert r.is_equality_constraint
-
-    def test_lower_bounded_constraint(self, simple_linear_model):
-        """Test lower bounded constraint identification. """
-        r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=None)
-        assert r.is_constraint
-        assert r.is_lower_bounded_constraint
-        assert not r.is_double_bounded_constraint
-        assert not r.is_upper_bounded_constraint
-        assert not r.is_equality_constraint
-
-    def test_upper_bounded_constraint(self, simple_linear_model):
-        """Test upper bounded constraint identification. """
-        r = Recorder(simple_linear_model, constraint_lower_bounds=None, constraint_upper_bounds=10.0)
-        assert r.is_constraint
-        assert not r.is_lower_bounded_constraint
-        assert not r.is_double_bounded_constraint
-        assert r.is_upper_bounded_constraint
-        assert not r.is_equality_constraint
-
-    def test_double_bounded_constraint(self, simple_linear_model):
-        """Test upper bounds constraint identification. """
-        r = Recorder(simple_linear_model, constraint_lower_bounds=2.0, constraint_upper_bounds=10.0)
-        assert r.is_constraint
-        assert not r.is_lower_bounded_constraint
-        assert r.is_double_bounded_constraint
-        assert not r.is_upper_bounded_constraint
-        assert not r.is_equality_constraint
-
-    def test_invalid_bounds_constraint(self, simple_linear_model):
-        """Test lower bounds greater than upper bounds."""
-        with pytest.raises(ValueError):
-            r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=2.0)
-
-    @pytest.mark.parametrize('lb, ub', (
-            (None, None),
-            (5.0, 10.0),  # Feasible double bounds
-            (15.0, 20.0),  # Infeasible double bounds
-            (0.0, 2.0),  # Infeasible double bounds
-            (0.0, None),  # Feasible lower bounds
-            (15.0, None),  # Infeasible lower bounds
-            (None, 15.0),  # Feasible upper bounds
-            (None, 5.0),  # Infeasible upper bounds
-    ))
-    def test_is_constraint_violated(self, simple_linear_model, lb, ub):
-        """Test the calculation of a violated constraint and model feasibility."""
-        m = simple_linear_model
-
-        class TestRecorder(Recorder):
-            def aggregated_value(self):
-                return 10.0
-
-        r = TestRecorder(m, constraint_lower_bounds=lb, constraint_upper_bounds=ub)
-
-        if lb is None and ub is None:
-            assert not r.is_constraint
-            with pytest.raises(ValueError):
-                r.is_constraint_violated()
-        elif lb is None and ub is not None:
-            # Upper bounded only
-            if ub >= 10.0:
-                assert not r.is_constraint_violated()
-                assert m.is_feasible()
-            else:
-                assert r.is_constraint_violated()
-                assert not m.is_feasible()
-        elif lb is not None and ub is None:
-            # Lower bounded only
-            if lb <= 10.0:
-                assert not r.is_constraint_violated()
-                assert m.is_feasible()
-            else:
-                assert r.is_constraint_violated()
-                assert not m.is_feasible()
-        else:
-            # Double bounds
-            if lb <= 10.0 <= ub:
-                assert not r.is_constraint_violated()
-                assert m.is_feasible()
-            else:
-                assert r.is_constraint_violated()
-                assert not m.is_feasible()
-
-
-def test_numpy_recorder(simple_linear_model):
-    """
-    Test the NumpyArrayNodeRecorder
-    """
-    model = simple_linear_model
-    otpt = model.nodes['Output']
-
-    model.nodes['Input'].max_flow = 10.0
-    otpt.cost = -2.0
-    rec = NumpyArrayNodeRecorder(model, otpt)
-
-    # test retrieval of recorder
-    assert model.recorders['numpyarraynoderecorder.Output'] == rec
-    # test changing name of recorder
-    rec.name = 'timeseries.Output'
-    assert model.recorders['timeseries.Output'] == rec
-    with pytest.raises(KeyError):
-        model.recorders['numpyarraynoderecorder.Output']
-
-    model.run()
-
-    assert rec.data.shape == (365, 1)
-    assert np.all((rec.data - 10.0) < 1e-12)
-
-    df = rec.to_dataframe()
-    assert df.shape == (365, 1)
-    assert np.all((df.values - 10.0) < 1e-12)
-
-
-def test_numpy_recorder_from_json(simple_linear_model):
-    """ Test loading NumpyArrayNodeRecorder from JSON style data """
-
-    model = simple_linear_model
-
-    data = {
-        "type": "numpyarraynode",
-        "node": "Output"
-    }
-
-    rec = load_recorder(model, data)
-    assert isinstance(rec, NumpyArrayNodeRecorder)
-
-def test_numpy_recorder_factored(simple_linear_model):
-    """Test the optional factor applies correctly """
-
-    model = simple_linear_model
-    otpt = model.nodes['Output']
-    otpt.max_flow = 30.0
-    model.nodes['Input'].max_flow = 10.0
-    otpt.cost = -2
-
-    factor = 2.0
-    rec_fact = NumpyArrayNodeRecorder(model, otpt, factor=factor)
-
-    model.run()
-
-    assert rec_fact.data.shape == (365, 1)
-    assert_allclose(20, rec_fact.data, atol=1e-7)
-
-class TestFlowDurationCurveRecorders:
-    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "sum": np.sum}
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum"])
-    def test_fdc_recorder(self, agg_func):
-        """
-        Test the FlowDurationCurveRecorder
-        """
-        model = load_model("timeseries2.json")
-        input = model.nodes['catchment1']
-
-        percentiles = np.linspace(20., 100., 5)
-        rec = FlowDurationCurveRecorder(model, input, percentiles, temporal_agg_func=agg_func, agg_func="min")
-
-        # test retrieval of recorder
-        assert model.recorders['flowdurationcurverecorder.catchment1'] == rec
-        # test changing name of recorder
-        rec.name = 'timeseries.Input'
-        assert model.recorders['timeseries.Input'] == rec
-        with pytest.raises(KeyError):
-            model.recorders['flowdurationcurverecorder.catchment1']
-
-        model.run()
-
-        func = TestAggregatedRecorder.funcs[agg_func]
-
-        assert_allclose(rec.fdc[:, 0], [20.42,  21.78,  23.22,  26.47,  29.31])
-        assert_allclose(func(rec.fdc, axis=0), rec.values())
-        assert_allclose(np.min(func(rec.fdc, axis=0)), rec.aggregated_value())
-
-        assert rec.fdc.shape == (len(percentiles), len(model.scenarios.combinations))
-        df = rec.to_dataframe()
-        assert df.shape == (len(percentiles), len(model.scenarios.combinations))
-
-    def test_seasonal_fdc_recorder(self):
-        """
-        Test the FlowDurationCurveRecorder
-        """
-        model = load_model("timeseries4.json")
-
-        df = pandas.read_csv(os.path.join(os.path.dirname(__file__), 'models', 'timeseries3.csv'),
-                             parse_dates=True, dayfirst=True, index_col=0)
-
-        percentiles = np.linspace(20., 100., 5)
-
-        summer_flows = df.loc[pandas.Timestamp("2014-06-01"):pandas.Timestamp("2014-08-31"), :]
-        summer_fdc = np.percentile(summer_flows, percentiles, axis=0)
-
-        model.run()
-
-        rec = model.recorders["seasonal_fdc"]
-        assert_allclose(rec.fdc, summer_fdc)
-
-    @pytest.mark.parametrize("agg_func, aggregate", [
-        ("min", False), ("max", False), ("mean", False), ("mean", True), ("sum", False)])
-    def test_fdc_dev_recorder(self, agg_func, aggregate):
-        """
-        Test the FlowDurationCurveDeviationRecorder
-        """
-        model = load_model("timeseries2.json")
-        input = model.nodes['catchment1']
-        term = model.nodes['term1']
-        scenarioA = model.scenarios['scenario A']
-
-        natural_flow = pandas.read_csv(os.path.join(os.path.dirname(__file__), 'models', 'timeseries2.csv'),
-                                       parse_dates=True, dayfirst=True, index_col=0)
-        percentiles = np.linspace(20., 100., 5)
-
-        natural_fdc = np.percentile(natural_flow, percentiles, axis=0)
-
-        # Lower target is 20% below natural
-        lower_input_fdc = natural_fdc * 0.8
-        # Upper is 10% above
-        upper_input_fdc = natural_fdc * 1.1
-
-        if aggregate:
-            # Setup only a single target for all scenarios.
-            lower_input_fdc = lower_input_fdc.mean(axis=1)
-            upper_input_fdc = upper_input_fdc.mean(axis=1)
-            scenarioA = None
-
-        rec = FlowDurationCurveDeviationRecorder(model, term, percentiles, lower_input_fdc, upper_input_fdc,
-                                                 temporal_agg_func=agg_func, agg_func="mean", scenario=scenarioA)
-
-        # test retrieval of recorder
-        assert model.recorders['flowdurationcurvedeviationrecorder.term1'] == rec
-        # test changing name of recorder
-        rec.name = 'timeseries.Input'
-        assert model.recorders['timeseries.Input'] == rec
-        with pytest.raises(KeyError):
-            model.recorders['flowdurationcurvedeviationrecorder.term1']
-
-        model.run()
-
-        actual_fdc = np.maximum(natural_fdc - 23, 0.0)
-
-        if aggregate:
-            lower_input_fdc = lower_input_fdc[:, np.newaxis]
-            upper_input_fdc = upper_input_fdc[:, np.newaxis]
-
-        # Compute deviation
-        lower_deviation = (lower_input_fdc - actual_fdc) / lower_input_fdc
-        upper_deviation = (actual_fdc - upper_input_fdc) / upper_input_fdc
-        deviation = np.maximum(np.maximum(lower_deviation, upper_deviation), np.zeros_like(lower_deviation))
-
-        func = TestAggregatedRecorder.funcs[agg_func]
-
-        assert_allclose(rec.fdc_deviations[:, 0], deviation[:, 0])
-        assert_allclose(func(rec.fdc_deviations, axis=0), rec.values())
-        assert_allclose(np.mean(func(rec.fdc_deviations, axis=0)), rec.aggregated_value())
-
-        assert rec.fdc_deviations.shape == (len(percentiles), len(model.scenarios.combinations))
-        df = rec.to_dataframe()
-        assert df.shape == (len(percentiles), len(model.scenarios.combinations))
-
-    def test_fdc_dev_from_json(self):
-
-        model = load_model("timeseries2_with_fdc.json")
-        model.run()
-
-        rec = model.recorders['fdc_dev1']
-        df = rec.to_dataframe()
-        assert df.shape == (5, len(model.scenarios.combinations))
-
-        rec = model.recorders['fdc_dev2']
-        df = rec.to_dataframe()
-        assert df.shape == (5, len(model.scenarios.combinations))
-
-
-def test_sdc_recorder():
-    """
-    Test the StorageDurationCurveRecorder
-    """
-    model = load_model("timeseries3.json")
-    inpt = model.nodes['catchment1']
-    strg = model.nodes['reservoir1']
-
-    percentiles = np.linspace(20., 100., 5)
-    flow_rec = NumpyArrayNodeRecorder(model, inpt)
-    rec = StorageDurationCurveRecorder(model, strg, percentiles, temporal_agg_func="max", agg_func="min")
-
-    # test retrieval of recorder
-    assert model.recorders['storagedurationcurverecorder.reservoir1'] == rec
-
-    model.run()
-
-    # Manually calculate expected storage and percentiles
-    strg_volume = strg.initial_volume + np.cumsum(flow_rec.data - 23.0, axis=0)
-    strg_pciles = np.percentile(strg_volume, percentiles, axis=0)
-
-    assert_allclose(rec.sdc, strg_pciles)
-    assert_allclose(np.max(rec.sdc, axis=0), rec.values())
-    assert_allclose(np.min(np.max(rec.sdc, axis=0)), rec.aggregated_value())
-
-    assert rec.sdc.shape == (len(percentiles), len(model.scenarios.combinations))
-    df = rec.to_dataframe()
-    assert df.shape == (len(percentiles), len(model.scenarios.combinations))
-
-
-@pytest.mark.parametrize('proportional', [True, False])
-def test_numpy_storage_recorder(simple_storage_model, proportional):
-    """
-    Test the NumpyArrayStorageRecorder
-    """
-    model = simple_storage_model
-
-    res = model.nodes['Storage']
-
-    rec = NumpyArrayStorageRecorder(model, res, proportional=proportional)
-
-    model.run()
-
-    expected = np.array([[7, 4, 1, 0, 0]]).T
-    if proportional:
-        expected = expected / 20
-
-    assert(rec.data.shape == (5, 1))
-    assert_allclose(rec.data, expected, atol=1e-7)
-
-    df = rec.to_dataframe()
-    assert df.shape == (5, 1)
-    assert_allclose(df.values, expected, atol=1e-7)
-
-
-def test_numpy_array_level_recorder(simple_storage_model):
-    model = simple_storage_model
-
-    storage = model.nodes["Storage"]
-    level_param = InterpolatedVolumeParameter(model, storage, [0, 20], [0, 100])
-    storage.level = level_param
-    level_rec = NumpyArrayLevelRecorder(model, storage, temporal_agg_func='min')
-
-    model.run()
-
-    expected = np.array([[50, 35, 20, 5, 0]]).T
-    assert_allclose(level_rec.data, expected, atol=1e-7)
-
-    df = level_rec.to_dataframe()
-    assert df.shape == (5, 1)
-    assert_allclose(df.values, expected, atol=1e-7)
-
-    assert_allclose(level_rec.aggregated_value(), np.min(expected))
-
-
-def test_numpy_array_area_recorder(simple_storage_model):
-
-    model = simple_storage_model
-
-    storage = model.nodes["Storage"]
-    area_param = InterpolatedVolumeParameter(model, storage, [0, 20], [0, 100])
-    storage.area = area_param
-    area_rec = NumpyArrayAreaRecorder(model, storage, temporal_agg_func='min')
-
-    model.run()
-
-    expected = np.array([[50, 35, 20, 5, 0]]).T
-    assert_allclose(area_rec.data, expected, atol=1e-7)
-
-    df = area_rec.to_dataframe()
-    assert df.shape == (5, 1)
-    assert_allclose(df.values, expected, atol=1e-7)
-
-    assert_allclose(area_rec.aggregated_value(), np.min(expected))
-
-
-def test_numpy_parameter_recorder(simple_linear_model):
-    """
-    Test the NumpyArrayParameterRecorder
-    """
-    from pywr.parameters import DailyProfileParameter
-
-    model = simple_linear_model
-    # using leap year simplifies tests
-    model.timestepper.start = pandas.to_datetime("2016-01-01")
-    model.timestepper.end = pandas.to_datetime("2016-12-31")
-    otpt = model.nodes['Output']
-
-    p = DailyProfileParameter(model, np.arange(366, dtype=np.float64), )
-    p.name = 'daily profile'
-    model.nodes['Input'].max_flow = p
-    otpt.cost = -2.0
-    rec = NumpyArrayParameterRecorder(model, model.nodes['Input'].max_flow)
-
-    # test retrieval of recorder
-    assert model.recorders['numpyarrayparameterrecorder.daily profile'] == rec
-
-    model.run()
-
-    assert rec.data.shape == (366, 1)
-    assert_allclose(rec.data, np.arange(366, dtype=np.float64)[:, np.newaxis])
-
-    df = rec.to_dataframe()
-    assert df.shape == (366, 1)
-    assert_allclose(df.values, np.arange(366, dtype=np.float64)[:, np.newaxis])
-
-
-def test_numpy_daily_profile_parameter_recorder(simple_linear_model):
-    """
-    Test the NumpyArrayDailyProfileParameterRecorder
-    """
-    from pywr.parameters import DailyProfileParameter
-
-    model = simple_linear_model
-    # using leap year simplifies tests
-    model.timestepper.start = pandas.to_datetime("2016-01-01")
-    model.timestepper.end = pandas.to_datetime("2017-12-31")
-    otpt = model.nodes['Output']
-
-    p = DailyProfileParameter(model, np.arange(366, dtype=np.float64), )
-    p.name = 'daily profile'
-    model.nodes['Input'].max_flow = p
-    otpt.cost = -2.0
-    rec = NumpyArrayDailyProfileParameterRecorder(model, model.nodes['Input'].max_flow)
-
-    # test retrieval of recorder
-    assert model.recorders['numpyarraydailyprofileparameterrecorder.daily profile'] == rec
-
-    model.run()
-
-    assert rec.data.shape == (366, 1)
-    assert_allclose(rec.data, np.arange(366, dtype=np.float64)[:, np.newaxis])
-
-    df = rec.to_dataframe()
-    assert df.shape == (366, 1)
-    assert_allclose(df.values, np.arange(366, dtype=np.float64)[:, np.newaxis])
-
-
-def test_numpy_index_parameter_recorder(simple_storage_model):
-    """
-    Test the NumpyArrayIndexParameterRecorder
-
-    Note the parameter is recorded at the start of the timestep, while the
-    storage is recorded at the end of the timestep.
-    """
-    from pywr.parameters.control_curves import ControlCurveIndexParameter
-
-    model = simple_storage_model
-
-    res = model.nodes['Storage']
-
-    p = ControlCurveIndexParameter(model, res, [5.0/20.0, 2.5/20.0])
-
-    res_rec = NumpyArrayStorageRecorder(model, res)
-    lvl_rec = NumpyArrayIndexParameterRecorder(model, p)
-
-    model.run()
-
-    assert(res_rec.data.shape == (5, 1))
-    assert_allclose(res_rec.data, np.array([[7, 4, 1, 0, 0]]).T, atol=1e-7)
-    assert (lvl_rec.data.shape == (5, 1))
-    assert_allclose(lvl_rec.data, np.array([[0, 0, 1, 2, 2]]).T, atol=1e-7)
-
-
-    df = lvl_rec.to_dataframe()
-    assert df.shape == (5, 1)
-    assert_allclose(df.values, np.array([[0, 0, 1, 2, 2]]).T, atol=1e-7)
-
-
-def test_parameter_recorder_json():
-    model = load_model("parameter_recorder.json")
-    rec_demand = model.recorders["demand_max_recorder"]
-    rec_supply = model.recorders["supply_max_recorder"]
-    model.run()
-    assert_allclose(rec_demand.data, 10)
-    assert_allclose(rec_supply.data, 15)
-
-
-def test_nested_recorder_json():
-    model = load_model("agg_recorder_nesting.json")
-    rec_demand = model.recorders["demand_max_recorder"]
-    rec_supply = model.recorders["supply_max_recorder"]
-    rec_total = model.recorders["max_recorder"]
-    model.run()
-    assert_allclose(rec_demand.aggregated_value(), 10)
-    assert_allclose(rec_supply.aggregated_value(), 15)
-    assert_allclose(rec_total.aggregated_value(), 25)
-
-
-@pytest.fixture()
-def daily_profile_model(simple_linear_model):
-    model = simple_linear_model
-    # using leap year simplifies test
-    model.timestepper.start = pandas.to_datetime("2016-01-01")
-    model.timestepper.end = pandas.to_datetime("2016-12-31")
-
-    node = model.nodes["Input"]
-    values = np.arange(0, 366, dtype=np.float64)
-    node.max_flow = DailyProfileParameter(model, values, name='profile')
-    return model
-
-
-def test_parameter_mean_recorder(daily_profile_model):
-    model = daily_profile_model
-    node = model.nodes["Input"]
-    scenario = Scenario(model, "dummy", size=3)
-
-    timesteps = 3
-    rec_mean = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
-                                              temporal_agg_func="mean", name="rec_mean")
-    rec_sum = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
-                                             temporal_agg_func="sum", name="rec_sum")
-    rec_min = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
-                                             temporal_agg_func="min", name="rec_min")
-    rec_max = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
-                                             temporal_agg_func="max", name="rec_max")
-
-    model.run()
-
-    assert_allclose(rec_mean.data[[0, 1, 2, 3, 364], 0], [0, 0.5, 1, 2, 363])
-    assert_allclose(rec_max.data[[0, 1, 2, 3, 364], 0], [0, 1, 2, 3, 364])
-    assert_allclose(rec_min.data[[0, 1, 2, 3, 364], 0], [0, 0, 0, 1, 362])
-    assert_allclose(rec_sum.data[[0, 1, 2, 3, 364], 0], [0, 1, 3, 6, 1089])
-
-def test_parameter_mean_recorder_json(simple_linear_model):
-    model = simple_linear_model
-    node = model.nodes["Input"]
-    values = np.arange(0, 366, dtype=np.float64)
-    parameter = DailyProfileParameter(model, values, name="input_max_flow")
-
-    node.max_flow = parameter
-
-    data = {
-        "type": "rollingwindowparameter",
-        "parameter": "input_max_flow",
-        "window": 3,
-        "temporal_agg_func": "mean",
-    }
-
-    rec = load_recorder(model, data)
-
-
-class TestTotalParameterRecorder:
-
-    @pytest.mark.parametrize('factor, integrate',
-                             [[1.0, False], [1.0, True], [2.0, False], [0.5, True]])
-    def test_values(self, daily_profile_model, factor, integrate):
-        model = daily_profile_model
-        model.timestepper.delta = 2
-        param = model.parameters['profile']
-        rec = TotalParameterRecorder(model, param, name="total", factor=factor, integrate=integrate)
-        model.run()
-
-        expected = np.arange(0, 366, dtype=np.float64)[::2].sum()*factor
-        if integrate:
-            expected *= 2
-        assert_allclose(rec.values(), expected)
-
-    @pytest.mark.parametrize('integrate', [None, True, False])
-    def test_from_json(self, daily_profile_model, integrate):
-
-        model = daily_profile_model
-
-        data = {
-            "type": "totalparameter",
-            "parameter": "profile",
-            "agg_func": "mean",
-            "factor": 2.0,
-        }
-
-        if integrate is not None:
-            data['integrate'] = integrate
-
-        rec = load_recorder(model, data)
-        assert rec.factor == 2.0
-
-        if integrate is not None:
-            assert rec.integrate == integrate
-        else:
-            assert not rec.integrate
-
-
-class TestMeanParameterRecorder:
-    @pytest.mark.parametrize('factor', [1.0, 2.0, 0.5])
-    def test_values(self, daily_profile_model, factor):
-        model = daily_profile_model
-        model.timestepper.delta = 2
-        param = model.parameters['profile']
-        rec = MeanParameterRecorder(model, param, name="mean", factor=factor)
-        model.run()
-
-        expected = np.arange(0, 366, dtype=np.float64)[::2].mean()*factor
-        assert_allclose(rec.values(), expected)
-
-    def test_from_json(self, daily_profile_model):
-
-        model = daily_profile_model
-
-        data = {
-            "type": "meanparameter",
-            "parameter": "profile",
-            "agg_func": "mean",
-            "factor": 2.0,
-        }
-
-        rec = load_recorder(model, data)
-        assert rec.factor == 2.0
-
-
-def test_concatenated_dataframes(simple_storage_model):
-    """
-    Test that Model.to_dataframe returns something sensible.
-
-    """
-    model = simple_storage_model
-
-    scA = Scenario(model, 'A', size=2)
-    scB = Scenario(model, 'B', size=3)
-
-    res = model.nodes['Storage']
-    rec1 = NumpyArrayStorageRecorder(model, res)
-    otpt = model.nodes['Output']
-    rec2 = NumpyArrayNodeRecorder(model, otpt)
-    # The following can't return a DataFrame; is included to check
-    # it doesn't cause any issues
-    rec3 = TotalDeficitNodeRecorder(model, otpt)
-
-    model.run()
-
-    df = model.to_dataframe()
-    assert df.shape == (5, 2*2*3)
-    assert df.columns.names == ['Recorder', 'A', 'B']
-
-
-@pytest.mark.parametrize("complib", [None, "gzip", "bz2"])
-def test_csv_recorder(simple_storage_model, tmpdir, complib):
-    """Test the CSV Recorder
-    """
-    model = simple_storage_model
-    otpt = model.nodes['Output']
-    otpt.cost = -2.0
-
-    # Rename output to a unicode character to check encoding to files
-    otpt.name = u"\u03A9"
-    expected_header = ['Datetime', 'Input', 'Storage', u"\u03A9"]
-
-    csvfile = tmpdir.join('output.csv')
-    # By default the CSVRecorder saves all nodes in alphabetical order
-    # and scenario index 0.
-    rec = CSVRecorder(model, str(csvfile), complib=complib, complevel=5)
-
-    model.run()
-
-    import csv
-    kwargs = {"encoding": "utf-8"}
-    mode = "rt"
-
-    if complib == "gzip":
-        import gzip
-        fh = gzip.open(str(csvfile), mode, **kwargs)
-    elif complib in ("bz2", "bzip2"):
-        import bz2
-        fh = bz2.open(str(csvfile), mode, **kwargs)
-    else:
-        fh = open(str(csvfile), mode, **kwargs)
-
-    expected = [
-        expected_header,
-        ['2016-01-01T00:00:00', 5.0, 7.0, 8.0],
-        ['2016-01-02T00:00:00', 5.0, 4.0, 8.0],
-        ['2016-01-03T00:00:00', 5.0, 1.0, 8.0],
-        ['2016-01-04T00:00:00', 5.0, 0.0, 6.0],
-        ['2016-01-05T00:00:00', 5.0, 0.0, 5.0],
-    ]
-
-    data = fh.read(1024)
-    dialect = csv.Sniffer().sniff(data)
-    fh.seek(0)
-    reader = csv.reader(fh, dialect)
-
-    for irow, row in enumerate(reader):
-        expected_row = expected[irow]
-        if irow == 0:
-            assert expected_row == row
-        else:
-            assert expected_row[0] == row[0]  # Check datetime
-            # Check values
-            np.testing.assert_allclose([float(v) for v in row[1:]], expected_row[1:])
-    fh.close()
-
-
-def test_loading_csv_recorder_from_json(tmpdir):
-    """
-    Test the CSV Recorder which is loaded from json
-    """
-
-    filename = 'csv_recorder.json'
-
-    # This is a bit horrible, but need to edit the JSON dynamically
-    # so that the output.h5 is written in the temporary directory
-    path = os.path.join(os.path.dirname(__file__), 'models')
-    with open(os.path.join(path, filename), 'r') as f:
-        data = f.read()
-    data = json.loads(data)
-
-    # Make an absolute, but temporary, path for the recorder
-    url = data['recorders']['model_out']['url']
-    data['recorders']['model_out']['url'] = str(tmpdir.join(url))
-
-    model = Model.load(data, path=path)
-
-    csvfile = tmpdir.join('output.csv')
-    model.run()
-
-    periods = model.timestepper.datetime_index
-
-    import csv
-    with open(str(csvfile), 'r') as fh:
-        dialect = csv.Sniffer().sniff(fh.read(1024))
-        fh.seek(0)
-        reader = csv.reader(fh, dialect)
-        for irow, row in enumerate(reader):
-            if irow == 0:
-                expected = ['Datetime', 'inpt', 'otpt']
-                actual = row
-            else:
-                dt = periods[irow-1].to_timestamp()
-                expected = [dt.isoformat()]
-                actual = [row[0]]
-                assert np.all((np.array([float(v) for v in row[1:]]) - 10.0) < 1e-12)
-            assert expected == actual
-
-class TestTablesRecorder:
-
-    def test_create_directory(self, simple_linear_model, tmpdir):
-        """ Test TablesRecorder to create a new directory """
-
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
-
-        inpt.max_flow = 10.0
-        otpt.cost = -2.0
-        # Make a path with a new directory
-        folder = tmpdir.join('outputs')
-        h5file = folder.join('output.h5')
-        assert(not folder.exists())
-        rec = TablesRecorder(model, str(h5file), create_directories=True)
-        model.run()
-        assert(folder.exists())
-        assert(h5file.exists())
-
-    def test_nodes(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
-
-        inpt.max_flow = 10.0
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f)
-
-            model.run()
-
-            for node_name in model.nodes.keys():
-                ca = h5f.get_node('/', node_name)
-                assert ca.shape == (365, 1)
-                if node_name == 'Sum':
-                    np.testing.assert_allclose(ca, 20.0)
-                else:
-                    np.testing.assert_allclose(ca, 10.0)
-
-            from datetime import date, timedelta
-            d = date(2015, 1, 1)
-            time = h5f.get_node('/time')
-            for i in range(len(model.timestepper)):
-                row = time[i]
-                assert row['year'] == d.year
-                assert row['month'] == d.month
-                assert row['day'] == d.day
-
-                d += timedelta(1)
-
-            scenarios = h5f.get_node('/scenarios')
-            for i, s in enumerate(model.scenarios.scenarios):
-                row = scenarios[i]
-                assert row['name'] == s.name.encode('utf-8')
-                assert row['size'] == s.size
-
-            model.reset()
-            model.run()
-
-            time = h5f.get_node('/time')
-            assert len(time) == len(model.timestepper)
-
-    def test_multiple_scenarios(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        from pywr.parameters import ConstantScenarioParameter
-        model = simple_linear_model
-        scA = Scenario(model, name='A', size=4)
-        scB = Scenario(model, name='B', size=2)
-
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
-        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f)
-
-            model.run()
-
-            for node_name in model.nodes.keys():
-                ca = h5f.get_node('/', node_name)
-                assert ca.shape == (365, 4, 2)
-                np.testing.assert_allclose(ca[0, ...], [[10, 10], [20, 20], [20, 30], [20, 40]])
-
-            scenarios = h5f.get_node('/scenarios')
-            for i, s in enumerate(model.scenarios.scenarios):
-                row = scenarios[i]
-                assert row['name'] == s.name.encode('utf-8')
-                assert row['size'] == s.size
-
-    def test_user_scenarios(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder with user defined scenario subset
-
-        """
-        from pywr.parameters import ConstantScenarioParameter
-        model = simple_linear_model
-        scA = Scenario(model, name='A', size=4)
-        scB = Scenario(model, name='B', size=2)
-
-        # Use first and last combinations
-        model.scenarios.user_combinations = [[0, 0], [3, 1]]
-
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
-        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f)
-
-            model.run()
-
-            for node_name in model.nodes.keys():
-                ca = h5f.get_node('/', node_name)
-                assert ca.shape == (365, 2)
-                np.testing.assert_allclose(ca[0, ...], [10, 40])
-
-            # check combinations table exists
-            combinations = h5f.get_node('/scenario_combinations')
-            for i, comb in enumerate(model.scenarios.user_combinations):
-                row = combinations[i]
-                assert row['A'] == comb[0]
-                assert row['B'] == comb[1]
-
-    def test_parameters(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        from pywr.parameters import ConstantParameter
-
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        p = ConstantParameter(model, 10.0, name='max_flow')
-        inpt.max_flow = p
-
-        # ensure TablesRecorder can handle parameters with a / in the name
-        p_slash = ConstantParameter(model, 0.0, name='name with a / in it')
-        inpt.min_flow = p_slash
-
-        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
-
-        inpt.max_flow = 10.0
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            with pytest.warns(ParameterNameWarning):
-                rec = TablesRecorder(model, h5f, parameters=[p, p_slash])
-
-            # check parameters have been added to the component tree
-            # this is particularly important for parameters which update their
-            # values in `after`, e.g. DeficitParameter (see #465)
-            assert(not model.find_orphaned_parameters())
-            assert(p in rec.children)
-            assert(p_slash in rec.children)
-
-            with pytest.warns(tables.NaturalNameWarning):
-                model.run()
-
-            for node_name in model.nodes.keys():
-                ca = h5f.get_node('/', node_name)
-                assert ca.shape == (365, 1)
-                if node_name == 'Sum':
-                    np.testing.assert_allclose(ca, 20.0)
-                elif "name with a" in node_name:
-                    assert(node_name == "name with a _ in it")
-                    np.testing.assert_allclose(ca, 0.0)
-                else:
-                    np.testing.assert_allclose(ca, 10.0)
-
-    def test_nodes_with_str(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        from pywr.parameters import ConstantParameter
-
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
-        p = ConstantParameter(model, 10.0, name='max_flow')
-        inpt.max_flow = p
-
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            nodes = ['Output', 'Input', 'Sum']
-            where = "/agroup"
-            rec = TablesRecorder(model, h5f, nodes=nodes,
-                                 parameters=[p, ], where=where)
-
-            model.run()
-
-            for node_name in ['Output', 'Input', 'Sum', 'max_flow']:
-                ca = h5f.get_node("/agroup/" + node_name)
-                assert ca.shape == (365, 1)
-                if node_name == 'Sum':
-                    np.testing.assert_allclose(ca, 20.0)
-                else:
-                    np.testing.assert_allclose(ca, 10.0)
-
-    def test_demand_saving_with_indexed_array(self, tmpdir):
-        """Test recording various items from demand saving example
-
-        """
-        model = load_model("demand_saving2.json")
-
-        model.timestepper.end = "2016-01-31"
-
-        model.check()
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-
-            nodes = [
-                ('/outputs/demand', 'Demand'),
-                ('/storage/reservoir', 'Reservoir'),
-            ]
-
-            parameters = [
-                ('/parameters/demand_saving_level', 'demand_saving_level'),
-            ]
-
-            rec = TablesRecorder(model, h5f, nodes=nodes, parameters=parameters)
-
-            model.run()
-
-            max_volume = model.nodes["Reservoir"].max_volume
-            rec_demand = h5f.get_node('/outputs/demand', 'Demand').read()
-            rec_storage = h5f.get_node('/storage/reservoir', 'Reservoir').read()
-
-            # model starts with no demand saving
-            demand_baseline = 50.0
-            demand_factor = 0.9  # jan-apr
-            demand_saving = 1.0
-            assert_allclose(rec_demand[0, 0], demand_baseline * demand_factor * demand_saving)
-
-            # first control curve breached
-            demand_saving = 0.95
-            assert (rec_storage[4, 0] < (0.8 * max_volume))
-            assert_allclose(rec_demand[5, 0], demand_baseline * demand_factor * demand_saving)
-
-            # second control curve breached
-            demand_saving = 0.5
-            assert (rec_storage[11, 0] < (0.5 * max_volume))
-            assert_allclose(rec_demand[12, 0], demand_baseline * demand_factor * demand_saving)
-
-    def test_demand_saving_with_indexed_array(self, tmpdir):
-        """Test recording various items from demand saving example.
-
-        This time the TablesRecorder is defined in JSON.
-        """
-        filename = "demand_saving_with_tables_recorder.json"
-        # This is a bit horrible, but need to edit the JSON dynamically
-        # so that the output.h5 is written in the temporary directory
-        path = os.path.join(os.path.dirname(__file__), 'models')
-        with open(os.path.join(path, filename), 'r') as f:
-            data = f.read()
-        data = json.loads(data)
-
-        # Make an absolute, but temporary, path for the recorder
-        url = data['recorders']['database']['url']
-        data['recorders']['database']['url'] = str(tmpdir.join(url))
-
-        model = Model.load(data, path=path)
-
-        model.timestepper.end = "2016-01-31"
-        model.check()
-
-        # run model
-        model.run()
-
-        # run model again (to test reset behaviour)
-        model.run()
-        max_volume = model.nodes["Reservoir"].max_volume
-
-        h5file = tmpdir.join('output.h5')
-        with tables.open_file(str(h5file), 'r') as h5f:
-            assert model.metadata['title'] == h5f.title
-            # Check metadata on root node
-            assert h5f.root._v_attrs.author == 'pytest'
-            assert h5f.root._v_attrs.run_number == 0
-
-            rec_demand = h5f.get_node('/outputs/demand').read()
-            rec_storage = h5f.get_node('/storage/reservoir').read()
-
-            # model starts with no demand saving
-            demand_baseline = 50.0
-            demand_factor = 0.9  # jan-apr
-            demand_saving = 1.0
-            assert_allclose(rec_demand[0, 0], demand_baseline * demand_factor * demand_saving)
-
-            # first control curve breached
-            demand_saving = 0.95
-            assert (rec_storage[4, 0] < (0.8 * max_volume))
-            assert_allclose(rec_demand[5, 0], demand_baseline * demand_factor * demand_saving)
-
-            # second control curve breached
-            demand_saving = 0.5
-            assert (rec_storage[11, 0] < (0.5 * max_volume))
-            assert_allclose(rec_demand[12, 0], demand_baseline * demand_factor * demand_saving)
-
-    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-    def test_routes(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
-
-        inpt.max_flow = 10.0
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f, routes_flows='flows')
-
-            model.run()
-
-            flows = h5f.get_node('/flows')
-            assert flows.shape == (365, 1, 1)
-            np.testing.assert_allclose(flows.read(), np.ones((365, 1, 1))*10)
-
-            routes = h5f.get_node('/routes')
-            assert routes.shape[0] == 1
-            row = routes[0]
-            row['start'] = "Input"
-            row['end'] = "Output"
-
-            from datetime import date, timedelta
-            d = date(2015, 1, 1)
-            time = h5f.get_node('/time')
-            for i in range(len(model.timestepper)):
-                row = time[i]
-                assert row['year'] == d.year
-                assert row['month'] == d.month
-                assert row['day'] == d.day
-
-                d += timedelta(1)
-
-            scenarios = h5f.get_node('/scenarios')
-            for s in model.scenarios.scenarios:
-                row = scenarios[i]
-                assert row['name'] == s.name
-                assert row['size'] == s.size
-
-            model.reset()
-            model.run()
-
-            time = h5f.get_node('/time')
-            assert len(time) == len(model.timestepper)
-
-    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-    def test_routes_multiple_scenarios(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder
-
-        """
-        from pywr.parameters import ConstantScenarioParameter
-        model = simple_linear_model
-        scA = Scenario(model, name='A', size=4)
-        scB = Scenario(model, name='B', size=2)
-
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
-        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f, routes_flows='flows')
-
-            model.run()
-
-            flows = h5f.get_node('/flows')
-            assert flows.shape == (365, 1, 4, 2)
-            np.testing.assert_allclose(flows[0, 0], [[10, 10], [20, 20], [20, 30], [20, 40]])
-
-    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-    def test_routes_user_scenarios(self, simple_linear_model, tmpdir):
-        """
-        Test the TablesRecorder with user defined scenario subset
-
-        """
-        from pywr.parameters import ConstantScenarioParameter
-        model = simple_linear_model
-        scA = Scenario(model, name='A', size=4)
-        scB = Scenario(model, name='B', size=2)
-
-        # Use first and last combinations
-        model.scenarios.user_combinations = [[0, 0], [3, 1]]
-
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
-        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        import tables
-        with tables.open_file(str(h5file), 'w') as h5f:
-            rec = TablesRecorder(model, h5f, routes_flows='flows')
-
-            model.run()
-
-            flows = h5f.get_node('/flows')
-            assert flows.shape == (365, 1, 2)
-            np.testing.assert_allclose(flows[0, 0], [10, 40])
-
-            # check combinations table exists
-            combinations = h5f.get_node('/scenario_combinations')
-            for i, comb in enumerate(model.scenarios.user_combinations):
-                row = combinations[i]
-                assert row['A'] == comb[0]
-                assert row['B'] == comb[1]
-
-        # This part of the test requires IPython (see `pywr.notebook`)
-        pytest.importorskip("IPython")  # triggers a skip of the test if IPython not found.
-        from pywr.notebook.sankey import routes_to_sankey_links
-
-        links = routes_to_sankey_links(str(h5file), 'flows')
-        # Value is mean of 10 and 40
-
-        link = links[0]
-        assert link['source'] == 'Input'
-        assert link['target'] == 'Output'
-        np.testing.assert_allclose(link['value'], 25.0)
-
-        links = routes_to_sankey_links(str(h5file), 'flows', scenario_slice=0)
-        link = links[0]
-        assert link['source'] == 'Input'
-        assert link['target'] == 'Output'
-        np.testing.assert_allclose(link['value'], 10.0)
-
-        links = routes_to_sankey_links(str(h5file), 'flows', scenario_slice=1, time_slice=0)
-        link = links[0]
-        assert link['source'] == 'Input'
-        assert link['target'] == 'Output'
-        np.testing.assert_allclose(link['value'], 40.0)
-
-    def test_generate_dataframes(self, simple_linear_model, tmpdir):
-        """Test TablesRecorder.generate_dataframes """
-        from pywr.parameters import ConstantScenarioParameter
-        model = simple_linear_model
-        scA = Scenario(model, name='A', size=4)
-        scB = Scenario(model, name='B', size=2)
-
-        otpt = model.nodes['Output']
-        inpt = model.nodes['Input']
-
-        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
-        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
-        otpt.cost = -2.0
-
-        h5file = tmpdir.join('output.h5')
-        TablesRecorder(model, h5file)
-        model.run()
-
-        dfs = {}
-        for node, df in TablesRecorder.generate_dataframes(h5file):
-            dfs[node] = df
-
-        for node_name in model.nodes.keys():
-            df = dfs[node_name]
-            assert df.shape == (365, 8)
-            np.testing.assert_allclose(df.iloc[0, :], [10, 10, 20, 20, 20, 30, 20, 40])
-
-
-class TestDeficitRecorders:
-
-    def test_total_deficit_node_recorder(self, simple_linear_model):
-        """
-        Test TotalDeficitNodeRecorder
-        """
-        model = simple_linear_model
-        model.timestepper.delta = 5
-        otpt = model.nodes['Output']
-        otpt.max_flow = 30.0
-        model.nodes['Input'].max_flow = 10.0
-        otpt.cost = -2.0
-        rec = TotalDeficitNodeRecorder(model, otpt)
-
-        model.step()
-        assert_allclose(20.0*5, rec.aggregated_value(), atol=1e-7)
-
-        model.step()
-        assert_allclose(40.0*5, rec.aggregated_value(), atol=1e-7)
-
-    def test_array_deficit_recoder(self, simple_linear_model):
-        """Test `NumpyArrayNodeDeficitRecorder` """
-        model = simple_linear_model
-        model.timestepper.delta = 1
-        otpt = model.nodes['Output']
-
-        inflow = np.arange(365) * 0.1
-        demand = np.ones_like(inflow) * 30.0
-
-        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
-        otpt.max_flow = ArrayIndexedParameter(model, demand)
-        otpt.cost = -2.0
-
-        expected_supply = np.minimum(inflow, demand)
-        expected_deficit = demand - expected_supply
-
-        rec = NumpyArrayNodeDeficitRecorder(model, otpt)
-
-        model.run()
-
-        assert rec.data.shape == (365, 1)
-        np.testing.assert_allclose(expected_deficit[:, np.newaxis], rec.data)
-
-        df = rec.to_dataframe()
-        assert df.shape == (365, 1)
-        np.testing.assert_allclose(expected_deficit[:, np.newaxis], df.values)
-
-    def test_array_supplied_ratio_recoder(self, simple_linear_model):
-        """Test `NumpyArrayNodeSuppliedRatioRecorder` """
-        model = simple_linear_model
-        model.timestepper.delta = 1
-        otpt = model.nodes['Output']
-
-        inflow = np.arange(365) * 0.1
-        demand = np.ones_like(inflow) * 30.0
-
-        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
-        otpt.max_flow = ArrayIndexedParameter(model, demand)
-        otpt.cost = -2.0
-
-        expected_supply = np.minimum(inflow, demand)
-        expected_ratio = expected_supply / demand
-
-        rec = NumpyArrayNodeSuppliedRatioRecorder(model, otpt)
-
-        model.run()
-
-        assert rec.data.shape == (365, 1)
-        np.testing.assert_allclose(expected_ratio[:, np.newaxis], rec.data)
-
-        df = rec.to_dataframe()
-        assert df.shape == (365, 1)
-        np.testing.assert_allclose(expected_ratio[:, np.newaxis], df.values)
-
-    def test_array_curtailment_ratio_recoder(self, simple_linear_model):
-        """Test `NumpyArrayNodeCurtailmentRatioRecorder` """
-        model = simple_linear_model
-        model.timestepper.delta = 1
-        otpt = model.nodes['Output']
-
-        inflow = np.arange(365) * 0.1
-        demand = np.ones_like(inflow) * 30.0
-
-        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
-        otpt.max_flow = ArrayIndexedParameter(model, demand)
-        otpt.cost = -2.0
-
-        expected_supply = np.minimum(inflow, demand)
-        expected_curtailment_ratio = 1 - expected_supply / demand
-
-        rec = NumpyArrayNodeCurtailmentRatioRecorder(model, otpt)
-
-        model.run()
-
-        assert rec.data.shape == (365, 1)
-        np.testing.assert_allclose(expected_curtailment_ratio[:, np.newaxis], rec.data)
-
-        df = rec.to_dataframe()
-        assert df.shape == (365, 1)
-        np.testing.assert_allclose(expected_curtailment_ratio[:, np.newaxis], df.values)
-
-
-def test_timestep_count_index_parameter_recorder(simple_storage_model):
-    """
-    The test uses a simple reservoir model with different inputs that
-    trigger a control curve failure after a different number of years.
-    """
-    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
-    from pywr.parameters.control_curves import ControlCurveIndexParameter
-    model = simple_storage_model
-    scenario = Scenario(model, 'A', size=2)
-    # Simulate 5 years
-    model.timestepper.start = '2015-01-01'
-    model.timestepper.end = '2019-12-31'
-    # Control curve parameter
-    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
-
-    # Storage model has a capacity of 20, but starts at 10 Ml
-    # Demand is roughly 2 Ml/d per year
-    #  First ensemble balances the demand
-    #  Second ensemble should fail during 3rd year
-    demand = 2.0 / 365
-    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
-    model.nodes['Output'].max_flow = demand
-
-    # Create the recorder with a threshold of 1
-    rec = TimestepCountIndexParameterRecorder(model, param, 1)
-
-    model.run()
-
-    assert_allclose([0, 183 + 365 + 365], rec.values(), atol=1e-7)
-
-
-@pytest.mark.parametrize("params", [1, 2])
-def test_annual_count_index_threshold_recorder(simple_storage_model, params):
-    """
-    The test sets uses a simple reservoir model with different inputs that
-    trigger a control curve failure after different numbers of years.
-    """
-    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
-    from pywr.parameters.control_curves import ControlCurveIndexParameter
-    model = simple_storage_model
-    scenario = Scenario(model, 'A', size=2)
-    # Simulate 5 years
-    model.timestepper.start = '2015-01-01'
-    model.timestepper.end = '2019-12-31'
-    # Control curve parameter
-    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
-
-    # Storage model has a capacity of 20, but starts at 10 Ml
-    # Demand is roughly 2 Ml/d per year
-    #  First ensemble balances the demand
-    #  Second ensemble should fail during 3rd year
-    demand = 2.0 / 365
-    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
-    model.nodes['Output'].max_flow = demand
-
-    # Create the recorder with a threshold of 1
-    rec = AnnualCountIndexThresholdRecorder(model, [param] * params, 'TestRec', 1)
-
-    model.run()
-
-    # We expect no failures in the first ensemble, the reservoir starts failing halfway through
-    # the 3rd year
-    assert_allclose([[0, 0],
-                     [0, 0],
-                     [0, 183],
-                     [0, 365],
-                     [0, 365]], rec.data, atol=1e-7)
-
-class TestAnnualTotalFlowRecorder:
-
-    def test_annual_total_flow_recorder(self, simple_linear_model):
-        """
-        Test AnnualTotalFlowRecorder
-        """
-
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        otpt.max_flow = 30.0
-        model.nodes['Input'].max_flow = 10.0
-        otpt.cost = -2
-        rec = AnnualTotalFlowRecorder(model, 'Total Flow', [otpt])
-
-        model.run()
-
-        assert_allclose(3650.0, rec.data, atol=1e-7)
-
-    def test_annual_total_flow_recorder_factored(self, simple_linear_model):
-        """
-        Test AnnualTotalFlowRecorder with factors applied
-        """
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        otpt.max_flow = 30.0
-        inpt = model.nodes['Input']
-        inpt.max_flow = 10.0
-        otpt.cost = -2
-
-        factors = [2.0, 1.0]
-        rec_fact = AnnualTotalFlowRecorder(model, 'Total Flow', [otpt, inpt], factors=factors)
-
-        model.run()
-
-        assert_allclose(3650.0*3, rec_fact.data, atol=1e-7)
-
-
-def test_total_flow_node_recorder(simple_linear_model):
-    """
-    Test TotalDeficitNodeRecorder
-    """
-    model = simple_linear_model
-    otpt = model.nodes['Output']
-    otpt.max_flow = 30.0
-    model.nodes['Input'].max_flow = 10.0
-    otpt.cost = -2.0
-    rec = TotalFlowNodeRecorder(model, otpt)
-
-    model.step()
-    assert_allclose(10.0, rec.aggregated_value(), atol=1e-7)
-
-    model.step()
-    assert_allclose(20.0, rec.aggregated_value(), atol=1e-7)
-
-
-def test_mean_flow_node_recorder(simple_linear_model):
-    """
-    Test MeanFlowNodeRecorder
-    """
-    model = simple_linear_model
-    nt = len(model.timestepper)
-
-    otpt = model.nodes['Output']
-    otpt.max_flow = 30.0
-    model.nodes['Input'].max_flow = 10.0
-    otpt.cost = -2.0
-    rec = MeanFlowNodeRecorder(model, otpt)
-
-    model.run()
-    assert_allclose(10.0, rec.aggregated_value(), atol=1e-7)
-
-
-def custom_test_func(array, axis=None):
-    return np.sum(array**2, axis=axis)
-
-
-class TestAggregatedRecorder:
-    """Tests for AggregatedRecorder"""
-    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "sum": np.sum}
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum"])
-    def test_aggregated_recorder(self, simple_linear_model, agg_func):
-        model = simple_linear_model
-        otpt = model.nodes['Output']
-        otpt.max_flow = 30.0
-        model.nodes['Input'].max_flow = 10.0
-        otpt.cost = -2.0
-        rec1 = TotalFlowNodeRecorder(model, otpt)
-        rec2 = TotalDeficitNodeRecorder(model, otpt)
-
-        func = TestAggregatedRecorder.funcs[agg_func]
-
-        rec = AggregatedRecorder(model, [rec1, rec2], agg_func=agg_func)
-
-        assert(rec in rec1.parents)
-        assert(rec in rec2.parents)
-
-        model.step()
-        assert_allclose(func([10.0, 20.0]), rec.aggregated_value(), atol=1e-7)
-
-        model.step()
-        assert_allclose(func([20.0, 40.0]), rec.aggregated_value(), atol=1e-7)
-
-    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
-    def test_agg_func_get_set(self, simple_linear_model, agg_func):
-        """Test getter and setter for AggregatedRecorder.agg_func"""
-        if agg_func == "custom":
-            agg_func = custom_test_func
-        model = simple_linear_model
-        rec = AggregatedRecorder(model, [], agg_func=agg_func)
-        assert rec.agg_func == agg_func
-        rec.agg_func = "product"
-        assert rec.agg_func == "product"
-
-
-def test_reset_timestepper_recorder():
-    model = Model(
-        start=pandas.to_datetime('2016-01-01'),
-        end=pandas.to_datetime('2016-01-01')
-    )
-
-    inpt = Input(model, "input", max_flow=10)
-    otpt = Output(model, "output", max_flow=50, cost=-10)
-    inpt.connect(otpt)
-
-    rec = NumpyArrayNodeRecorder(model, otpt)
-
-    model.run()
-
-    model.timestepper.end = pandas.to_datetime("2016-01-02")
-
-    model.run()
-
-def test_mean_flow_recorder():
-    model = Model()
-    model.timestepper.start = pandas.to_datetime("2016-01-01")
-    model.timestepper.end = pandas.to_datetime("2016-01-04")
-
-    inpt = Input(model, "input")
-    otpt = Output(model, "output")
-    inpt.connect(otpt)
-
-    rec_flow = NumpyArrayNodeRecorder(model, inpt)
-    rec_mean = RollingMeanFlowNodeRecorder(model, node=inpt, timesteps=3)
-
-    scenario = Scenario(model, "dummy", size=2)
-
-    inpt.max_flow = inpt.min_flow = FunctionParameter(model, inpt, lambda model, t, si: 2 + t.index)
-    model.run()
-
-    expected = [
-        2.0,
-        (2.0 + 3.0) / 2,
-        (2.0 + 3.0 + 4.0) / 3,
-        (3.0 + 4.0 + 5.0) / 3,  # zeroth day forgotten
-    ]
-
-    for value, expected_value in zip(rec_mean.data[:, 0], expected):
-        assert_allclose(value, expected_value)
-
-def test_mean_flow_recorder_days():
-    model = Model()
-    model.timestepper.delta = 7
-
-    inpt = Input(model, "input")
-    otpt = Output(model, "output")
-    inpt.connect(otpt)
-
-    rec_mean = RollingMeanFlowNodeRecorder(model, node=inpt, days=31)
-
-    model.run()
-    assert(rec_mean.timesteps == 4)
-
-def test_mean_flow_recorder_json():
-    model = load_model("mean_flow_recorder.json")
-
-    # TODO: it's not possible to define a FunctionParameter in JSON yet
-    supply1 = model.nodes["supply1"]
-    supply1.max_flow = supply1.min_flow = FunctionParameter(model, supply1, lambda model, t, si: 2 + t.index)
-
-    assert(len(model.recorders) == 3)
-
-    rec_flow = model.recorders["Supply"]
-    rec_mean = model.recorders["Mean Flow"]
-    rec_check = model.recorders["Supply 2"]
-
-    model.run()
-
-    assert_allclose(rec_flow.data[:,0], [2.0, 3.0, 4.0, 5.0])
-    assert_allclose(rec_mean.data[:,0], [2.0, 2.5, 3.0, 4.0])
-    assert_allclose(rec_check.data[:,0], [50.0, 50.0, 60.0, 60.0])
-
-def test_annual_count_index_parameter_recorder(simple_storage_model):
-    """ Test AnnualCountIndexParameterRecord
-
-    The test sets uses a simple reservoir model with different inputs that
-    trigger a control curve failure after different numbers of years.
-    """
-    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
-    from pywr.parameters.control_curves import ControlCurveIndexParameter
-    model = simple_storage_model
-    scenario = Scenario(model, 'A', size=2)
-    # Simulate 5 years
-    model.timestepper.start = '2015-01-01'
-    model.timestepper.end = '2019-12-31'
-    # Control curve parameter
-    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
-
-    # Storage model has a capacity of 20, but starts at 10 Ml
-    # Demand is roughly 2 Ml/d per year
-    #  First ensemble balances the demand
-    #  Second ensemble should fail during 3rd year
-    demand = 2.0 / 365
-    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
-    model.nodes['Output'].max_flow = demand
-
-    # Create the recorder with a threshold of 1
-    rec = AnnualCountIndexParameterRecorder(model, param, 1)
-
-    model.run()
-    # We expect no failures in the first ensemble, but 3 out of 5 in the second
-    assert_allclose(rec.values(), [0, 3])
-
-
-# The following fixtures are used for testing the recorders in
-#  pywr.recorders.calibration which require an observed data set
-#  to compare with the model prediction.
-
-@pytest.fixture
-def timeseries2_model():
-    return load_model('timeseries2.json')
-
-
-@pytest.fixture
-def timeseries2_observed():
-    path = os.path.join(os.path.dirname(__file__), 'models')
-    df = pandas.read_csv(os.path.join(path, 'timeseries2.csv'),
-                         parse_dates=True, dayfirst=True, index_col=0)
-    df = df.asfreq(pandas.infer_freq(df.index))
-    # perturb a bit
-    df += np.random.normal(size=df.shape)
-    return df
-
-class TestCalibrationRecorders:
-    data = [
-        (RootMeanSquaredErrorNodeRecorder, lambda sim, obs: np.sqrt(np.mean((sim-obs)**2, axis=0))),
-        (MeanAbsoluteErrorNodeRecorder, lambda sim, obs: np.mean(np.abs(sim-obs), axis=0)),
-        (MeanSquareErrorNodeRecorder, lambda sim, obs: np.mean((sim-obs)**2, axis=0)),
-        (PercentBiasNodeRecorder, lambda sim, obs: np.sum(obs-sim, axis=0)*100/np.sum(obs, axis=0)),
-        (RMSEStandardDeviationRatioNodeRecorder, lambda sim, obs: np.sqrt(np.mean((obs-sim)**2, axis=0))/np.std(obs, axis=0)),
-        (NashSutcliffeEfficiencyNodeRecorder, lambda sim, obs: 1.0 - np.sum((obs-sim)**2, axis=0)/np.sum((obs-obs.mean())**2, axis=0)),
-    ]
-    ids = ["rmse", "mae", "mse", "pbias", "rmse", "ns"]
-
-    @pytest.mark.parametrize("cls,func", data, ids=ids)
-    def test_calibration_recorder(self, timeseries2_model, timeseries2_observed, cls, func):
-        model = timeseries2_model
-        observed = timeseries2_observed
-        node = model.nodes["river1"]
-        recorder = cls(model, node, observed)
-
-        model.run()
-
-        simulated = model.nodes["catchment1"].max_flow.dataframe
-        metric = func(simulated, observed)
-        values = recorder.values()
-        assert(values.shape[0] == len(model.scenarios.combinations))
-        assert(values.ndim == 1)
-        assert_allclose(metric, values)
-
-
-@pytest.fixture
-def cyclical_storage_model(simple_storage_model):
-    """ Extends simple_storage_model to have a cyclical boundary condition """
-    from pywr.parameters import AnnualHarmonicSeriesParameter, ConstantScenarioParameter
-    m = simple_storage_model
-    s = Scenario(m, name='Scenario A', size=3)
-
-    m.timestepper.end = '2017-12-31'
-    m.timestepper.delta = 5
-
-    inpt = m.nodes['Input']
-    inpt.max_flow = AnnualHarmonicSeriesParameter(m, 5, [0.1, 0.0, 0.25], [0.0, 0.0, 0.0])
-
-    otpt = m.nodes['Output']
-    otpt.max_flow = ConstantScenarioParameter(m, s, [5, 6, 2])
-
-    return m
-
-
-@pytest.fixture
-def cyclical_linear_model(simple_linear_model):
-    """ Extends simple_storage_model to have a cyclical boundary condition """
-    from pywr.parameters import AnnualHarmonicSeriesParameter, ConstantScenarioParameter
-    m = simple_linear_model
-    s = Scenario(m, name='Scenario A', size=3)
-
-    m.timestepper.end = '2017-12-31'
-    m.timestepper.delta = 5
-
-    inpt = m.nodes['Input']
-    inpt.max_flow = AnnualHarmonicSeriesParameter(m, 5, [1.0, 0.0, 0.5], [0.0, 0.0, 0.0])
-
-    otpt = m.nodes['Output']
-    otpt.max_flow = ConstantScenarioParameter(m, s, [5, 6, 2])
-    otpt.cost = -10.0
-
-    return m
-
-
-class TestEventRecorder:
-    """ Tests for EventRecorder """
-    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "median": np.median, "sum": np.sum}
-
-    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
-    def test_event_capture_with_storage(self, cyclical_storage_model, recorder_agg_func):
-        """ Test Storage events using a StorageThresholdRecorder """
-        m = cyclical_storage_model
-
-        strg = m.nodes['Storage']
-        arry = NumpyArrayStorageRecorder(m, strg)
-
-        # Create the trigger using a threhsold parameter
-        trigger = StorageThresholdRecorder(m, strg, 4.0, predicate='<=')
-        evt_rec = EventRecorder(m, trigger)
-        evt_dur = EventDurationRecorder(m, evt_rec, recorder_agg_func=recorder_agg_func, agg_func='max')
-
-        m.run()
-
-        # Ensure there is at least one event
-        assert evt_rec.events
-
-        # Build a timeseries of when the events say an event is active
-        triggered = np.zeros_like(arry.data, dtype=np.int)
-        for evt in evt_rec.events:
-            triggered[evt.start.index:evt.end.index, evt.scenario_index.global_id] = 1
-
-            # Check the duration
-            td = evt.end.datetime - evt.start.datetime
-            assert evt.duration == td.days
-
-        #   Test that the volumes in the Storage node during the event periods match
-        assert_equal(triggered, arry.data <= 4)
-
-        df = evt_rec.to_dataframe()
-
-        assert len(df) == len(evt_rec.events)
-
-        func = TestEventRecorder.funcs[recorder_agg_func]
-
-        # Now check the EventDurationRecorder does the aggregation correctly
-        expected_durations = []
-        for si in m.scenarios.combinations:
-            event_durations = []
-            for evt in evt_rec.events:
-                if evt.scenario_index.global_id == si.global_id:
-                    event_durations.append(evt.duration)
-
-            # If there are no events then the metric is zero
-            if len(event_durations) > 0:
-                expected_durations.append(func(event_durations))
-            else:
-                expected_durations.append(0.0)
-
-        assert_allclose(evt_dur.values(), expected_durations)
-        assert_allclose(evt_dur.aggregated_value(), np.max(expected_durations))
-
-    def test_event_capture_with_node(self, cyclical_linear_model):
-        """ Test Node flow events using a NodeThresholdRecorder """
-        m = cyclical_linear_model
-
-        otpt = m.nodes['Output']
-        arry = NumpyArrayNodeRecorder(m, otpt)
-
-        # Create the trigger using a threhsold parameter
-        trigger = NodeThresholdRecorder(m, otpt, 4.0, predicate='>')
-        evt_rec = EventRecorder(m, trigger)
-
-        m.run()
-
-        # Ensure there is at least one event
-        assert evt_rec.events
-
-        # Build a timeseries of when the events say an event is active
-        triggered = np.zeros_like(arry.data, dtype=np.int)
-        for evt in evt_rec.events:
-            triggered[evt.start.index:evt.end.index, evt.scenario_index.global_id] = 1
-
-            # Check the duration
-            td = evt.end.datetime - evt.start.datetime
-            assert evt.duration == td.days
-
-        # Test that the volumes in the Storage node during the event periods match
-        assert_equal(triggered, arry.data > 4)
-
-    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
-    def test_no_event_capture_with_storage(self, cyclical_storage_model, recorder_agg_func):
-        """ Test Storage events using a StorageThresholdRecorder """
-        m = cyclical_storage_model
-
-        strg = m.nodes['Storage']
-        arry = NumpyArrayStorageRecorder(m, strg)
-
-        # Create the trigger using a threhsold parameter
-        trigger = StorageThresholdRecorder(m, strg, -1.0, predicate='<')
-        evt_rec = EventRecorder(m, trigger)
-        evt_dur = EventDurationRecorder(m, evt_rec, recorder_agg_func=recorder_agg_func, agg_func='max')
-
-        m.run()
-
-        # Ensure there are no events in this test
-        assert len(evt_rec.events) == 0
-        df = evt_rec.to_dataframe()
-        assert len(df) == 0
-
-        assert_allclose(evt_dur.values(), np.zeros(len(m.scenarios.combinations)))
-        assert_allclose(evt_dur.aggregated_value(), 0)
-
-    @pytest.mark.parametrize("minimum_length", [1, 2, 3, 4])
-    def test_hysteresis(self, simple_linear_model, minimum_length):
-        """ Test the minimum_event_length keyword of EventRecorder """
-        m = simple_linear_model
-
-        flow = np.zeros(len(m.timestepper))
-
-        flow[:10] = [0, 0, 10, 0, 10, 10, 10, 0, 0, 0]
-        # With min event length of 1. There are two events with lengths (1, 3)
-        #                 |---|   |---------|
-        # With min event length up to 4. There is one event with length 3
-        #                         |---------|
-        # Min event length >= 4 gives no events
-
-        inpt = m.nodes['Input']
-        inpt.max_flow = ArrayIndexedParameter(m, flow)
-
-        # Force through whatever flow Input can provide
-        otpt = m.nodes['Output']
-        otpt.max_flow = 100
-        otpt.cost = -100
-
-        # Create the trigger using a threhsold parameter
-        trigger = NodeThresholdRecorder(m, otpt, 4.0, predicate='>')
-        evt_rec = EventRecorder(m, trigger, minimum_event_length=minimum_length)
-
-        m.run()
-
-        if minimum_length == 1:
-            assert len(evt_rec.events) == 2
-            assert_equal([1, 3], [e.duration for e in evt_rec.events])
-        elif minimum_length < 4:
-            assert len(evt_rec.events) == 1
-            assert_equal([3, ], [e.duration for e in evt_rec.events])
-        else:
-            assert len(evt_rec.events) == 0
-
-    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
-    def test_statistic_recorder(self, cyclical_storage_model, recorder_agg_func):
-        """ Test EventStatisticRecorder """
-        m = cyclical_storage_model
-
-        strg = m.nodes['Storage']
-        inpt = m.nodes['Input']
-        arry = NumpyArrayNodeRecorder(m, inpt)
-
-        # Create the trigger using a threhsold parameter
-        trigger = StorageThresholdRecorder(m, strg, 4.0, predicate='<=')
-        evt_rec = EventRecorder(m, trigger, tracked_parameter=inpt.max_flow)
-        evt_stat = EventStatisticRecorder(m, evt_rec, agg_func='max', event_agg_func='min', recorder_agg_func=recorder_agg_func)
-
-        m.run()
-
-        # Ensure there is at least one event
-        assert evt_rec.events
-
-        evt_values = {si.global_id:[] for si in m.scenarios.combinations}
-        for evt in evt_rec.events:
-            evt_values[evt.scenario_index.global_id].append(np.min(arry.data[evt.start.index:evt.end.index, evt.scenario_index.global_id]))
-
-        func = TestEventRecorder.funcs[recorder_agg_func]
-
-        agg_evt_values = []
-        for k, v in sorted(evt_values.items()):
-            if len(v) > 0:
-                agg_evt_values.append(func(v))
-            else:
-                agg_evt_values.append(np.nan)
-
-        # Test that the
-        assert_allclose(evt_stat.values(), agg_evt_values)
-        assert_allclose(evt_stat.aggregated_value(), np.max(agg_evt_values))
-
-
-def test_progress_recorder(simple_linear_model):
-    model = simple_linear_model
-    rec = ProgressRecorder(model)
-    model.run()
-
-
-class TestHydroPowerRecorder:
-
-    @pytest.mark.parametrize('efficiency', [1.0, 0.85])
-    def test_constant_level(self, simple_storage_model, efficiency):
-        """ Test HydropowerRecorder """
-        m = simple_storage_model
-
-        strg = m.nodes['Storage']
-        otpt = m.nodes['Output']
-
-        elevation = ConstantParameter(m, 100)
-        rec = HydropowerRecorder(m, otpt, elevation, efficiency=efficiency)
-        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation, efficiency=efficiency)
-
-        m.setup()
-        m.step()
-
-        # First step
-        # Head: 100m
-        # Flow: 8 m3/day
-        # Power: 1000 * 9.81 * 8 * 100
-        # Energy: power * 1 day = power
-        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
-        # Second step has the same answer in this model
-        m.step()
-        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
-        np.testing.assert_allclose(rec_total.values()[0], 2 * 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
-
-    def test_varying_level(self, simple_storage_model):
-        """ Test HydropowerRecorder with varying level on Storage node """
-        from pywr.parameters import InterpolatedVolumeParameter
-        m = simple_storage_model
-
-        strg = m.nodes['Storage']
-        otpt = m.nodes['Output']
-
-        elevation = InterpolatedVolumeParameter(m, strg, [0, 10, 20], [0, 100, 200])
-        rec = HydropowerRecorder(m, otpt, elevation)
-        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation)
-
-        m.setup()
-        m.step()
-
-        # First step
-        # Head: 100m
-        # Flow: 8 m3/day
-        # Power: 1000 * 9.81 * 8 * 100
-        # Energy: power * 1 day = power
-        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 100 * 1e-6)
-        # Second step is at a lower head
-        # Head: 70m
-        m.step()
-        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 70 * 1e-6)
-        np.testing.assert_allclose(rec_total.values()[0], 1000 * 9.81 * 8 * 170 * 1e-6)
-
-    def test_varying_level_with_turbine_level(self, simple_storage_model):
-        """ Test HydropowerRecorder with varying level on Storage and defined level on the recorder """
-        from pywr.parameters import InterpolatedVolumeParameter
-        m = simple_storage_model
-
-        strg = m.nodes['Storage']
-        otpt = m.nodes['Output']
-
-        elevation = InterpolatedVolumeParameter(m, strg, [0, 10, 20], [0, 100, 200])
-        rec = HydropowerRecorder(m, otpt, elevation, turbine_elevation=80)
-        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation, turbine_elevation=80)
-
-        m.setup()
-        m.step()
-
-        # First step
-        # Head: 100 - 80 = 20m
-        # Flow: 8 m3/day
-        # Power: 1000 * 9.81 * 8 * 100
-        # Energy: power * 1 day = power
-        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 20 * 1e-6)
-        # Second step is at a lower head
-        # Head: 70 - 80: -10m (i.e. not sufficient)
-        m.step()
-        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 0 * 1e-6)
-        np.testing.assert_allclose(rec_total.values()[0], 1000 * 9.81 * 8 * 20 * 1e-6)
-
-    def test_load_from_json(self, ):
-        """ Test example hydropower model loads and runs. """
-        model = load_model("hydropower_example.json")
-
-        r = model.recorders['turbine1_energy']
-
-        # Check the recorder has loaded correctly
-        assert r.water_elevation_parameter == model.parameters['reservoir1_level']
-        assert r.node == model.nodes['turbine1']
-
-        assert_allclose(r.turbine_elevation, 35.0)
-        assert_allclose(r.efficiency, 0.85)
-        assert_allclose(r.flow_unit_conversion, 1e3)
-
-        # Finally, check model runs with the loaded recorder.
-        model.run()
-
+# -*- coding: utf-8 -*-
+"""
+Test the Recorder object API
+
+"""
+import pywr.core
+from pywr.core import Model, Input, Output, Scenario, AggregatedNode
+import numpy as np
+import pandas
+import pytest
+import tables
+import json
+from numpy.testing import assert_allclose, assert_equal
+from fixtures import simple_linear_model, simple_storage_model
+from pywr.recorders import (Recorder, NumpyArrayNodeRecorder, NumpyArrayStorageRecorder, NumpyArrayAreaRecorder, NumpyArrayLevelRecorder,
+                            AggregatedRecorder, CSVRecorder, TablesRecorder, TotalDeficitNodeRecorder,
+                            TotalFlowNodeRecorder, RollingMeanFlowNodeRecorder, MeanFlowNodeRecorder, NumpyArrayParameterRecorder,
+                            NumpyArrayIndexParameterRecorder, RollingWindowParameterRecorder, AnnualCountIndexParameterRecorder,
+                            RootMeanSquaredErrorNodeRecorder, MeanAbsoluteErrorNodeRecorder, MeanSquareErrorNodeRecorder,
+                            PercentBiasNodeRecorder, RMSEStandardDeviationRatioNodeRecorder, NashSutcliffeEfficiencyNodeRecorder,
+                            EventRecorder, Event, StorageThresholdRecorder, NodeThresholdRecorder, EventDurationRecorder, EventStatisticRecorder,
+                            FlowDurationCurveRecorder, FlowDurationCurveDeviationRecorder, StorageDurationCurveRecorder,
+                            HydropowerRecorder, TotalHydroEnergyRecorder,
+                            TotalParameterRecorder, MeanParameterRecorder,
+                            NumpyArrayNodeDeficitRecorder, NumpyArrayNodeSuppliedRatioRecorder, NumpyArrayNodeCurtailmentRatioRecorder,
+                            SeasonalFlowDurationCurveRecorder, load_recorder, ParameterNameWarning, NumpyArrayDailyProfileParameterRecorder,
+                            AnnualTotalFlowRecorder, AnnualCountIndexThresholdRecorder, TimestepCountIndexParameterRecorder)
+
+from pywr.recorders.progress import ProgressRecorder
+
+from pywr.parameters import (DailyProfileParameter, FunctionParameter, ArrayIndexedParameter, ConstantParameter,
+                             InterpolatedVolumeParameter)
+from helpers import load_model
+import os
+import sys
+
+
+class TestRecorder:
+    """Tests for Recorder base class."""
+
+    def test_default_no_constraint(self, simple_linear_model):
+        """Test the constraint properties in the default instance (i.e. not a constraint)."""
+        r = Recorder(simple_linear_model)
+        assert r.constraint_lower_bounds is None
+        assert r.constraint_upper_bounds is None
+        assert not r.is_constraint
+        assert not r.is_lower_bounded_constraint
+        assert not r.is_double_bounded_constraint
+        assert not r.is_upper_bounded_constraint
+        assert not r.is_equality_constraint
+
+    def test_equality_constraint(self, simple_linear_model):
+        """Test equality constraint identification. """
+        r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=10.0)
+        assert r.is_constraint
+        assert not r.is_lower_bounded_constraint
+        assert not r.is_double_bounded_constraint
+        assert not r.is_upper_bounded_constraint
+        assert r.is_equality_constraint
+
+    def test_lower_bounded_constraint(self, simple_linear_model):
+        """Test lower bounded constraint identification. """
+        r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=None)
+        assert r.is_constraint
+        assert r.is_lower_bounded_constraint
+        assert not r.is_double_bounded_constraint
+        assert not r.is_upper_bounded_constraint
+        assert not r.is_equality_constraint
+
+    def test_upper_bounded_constraint(self, simple_linear_model):
+        """Test upper bounded constraint identification. """
+        r = Recorder(simple_linear_model, constraint_lower_bounds=None, constraint_upper_bounds=10.0)
+        assert r.is_constraint
+        assert not r.is_lower_bounded_constraint
+        assert not r.is_double_bounded_constraint
+        assert r.is_upper_bounded_constraint
+        assert not r.is_equality_constraint
+
+    def test_double_bounded_constraint(self, simple_linear_model):
+        """Test upper bounds constraint identification. """
+        r = Recorder(simple_linear_model, constraint_lower_bounds=2.0, constraint_upper_bounds=10.0)
+        assert r.is_constraint
+        assert not r.is_lower_bounded_constraint
+        assert r.is_double_bounded_constraint
+        assert not r.is_upper_bounded_constraint
+        assert not r.is_equality_constraint
+
+    def test_invalid_bounds_constraint(self, simple_linear_model):
+        """Test lower bounds greater than upper bounds."""
+        with pytest.raises(ValueError):
+            r = Recorder(simple_linear_model, constraint_lower_bounds=10.0, constraint_upper_bounds=2.0)
+
+    @pytest.mark.parametrize('lb, ub', (
+            (None, None),
+            (5.0, 10.0),  # Feasible double bounds
+            (15.0, 20.0),  # Infeasible double bounds
+            (0.0, 2.0),  # Infeasible double bounds
+            (0.0, None),  # Feasible lower bounds
+            (15.0, None),  # Infeasible lower bounds
+            (None, 15.0),  # Feasible upper bounds
+            (None, 5.0),  # Infeasible upper bounds
+    ))
+    def test_is_constraint_violated(self, simple_linear_model, lb, ub):
+        """Test the calculation of a violated constraint and model feasibility."""
+        m = simple_linear_model
+
+        class TestRecorder(Recorder):
+            def aggregated_value(self):
+                return 10.0
+
+        r = TestRecorder(m, constraint_lower_bounds=lb, constraint_upper_bounds=ub)
+
+        if lb is None and ub is None:
+            assert not r.is_constraint
+            with pytest.raises(ValueError):
+                r.is_constraint_violated()
+        elif lb is None and ub is not None:
+            # Upper bounded only
+            if ub >= 10.0:
+                assert not r.is_constraint_violated()
+                assert m.is_feasible()
+            else:
+                assert r.is_constraint_violated()
+                assert not m.is_feasible()
+        elif lb is not None and ub is None:
+            # Lower bounded only
+            if lb <= 10.0:
+                assert not r.is_constraint_violated()
+                assert m.is_feasible()
+            else:
+                assert r.is_constraint_violated()
+                assert not m.is_feasible()
+        else:
+            # Double bounds
+            if lb <= 10.0 <= ub:
+                assert not r.is_constraint_violated()
+                assert m.is_feasible()
+            else:
+                assert r.is_constraint_violated()
+                assert not m.is_feasible()
+
+
+def test_numpy_recorder(simple_linear_model):
+    """
+    Test the NumpyArrayNodeRecorder
+    """
+    model = simple_linear_model
+    otpt = model.nodes['Output']
+
+    model.nodes['Input'].max_flow = 10.0
+    otpt.cost = -2.0
+    rec = NumpyArrayNodeRecorder(model, otpt)
+
+    # test retrieval of recorder
+    assert model.recorders['numpyarraynoderecorder.Output'] == rec
+    # test changing name of recorder
+    rec.name = 'timeseries.Output'
+    assert model.recorders['timeseries.Output'] == rec
+    with pytest.raises(KeyError):
+        model.recorders['numpyarraynoderecorder.Output']
+
+    model.run()
+
+    assert rec.data.shape == (365, 1)
+    assert np.all((rec.data - 10.0) < 1e-12)
+
+    df = rec.to_dataframe()
+    assert df.shape == (365, 1)
+    assert np.all((df.values - 10.0) < 1e-12)
+
+
+def test_numpy_recorder_from_json(simple_linear_model):
+    """ Test loading NumpyArrayNodeRecorder from JSON style data """
+
+    model = simple_linear_model
+
+    data = {
+        "type": "numpyarraynode",
+        "node": "Output"
+    }
+
+    rec = load_recorder(model, data)
+    assert isinstance(rec, NumpyArrayNodeRecorder)
+
+def test_numpy_recorder_factored(simple_linear_model):
+    """Test the optional factor applies correctly """
+
+    model = simple_linear_model
+    otpt = model.nodes['Output']
+    otpt.max_flow = 30.0
+    model.nodes['Input'].max_flow = 10.0
+    otpt.cost = -2
+
+    factor = 2.0
+    rec_fact = NumpyArrayNodeRecorder(model, otpt, factor=factor)
+
+    model.run()
+
+    assert rec_fact.data.shape == (365, 1)
+    assert_allclose(20, rec_fact.data, atol=1e-7)
+
+class TestFlowDurationCurveRecorders:
+    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "sum": np.sum}
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum"])
+    def test_fdc_recorder(self, agg_func):
+        """
+        Test the FlowDurationCurveRecorder
+        """
+        model = load_model("timeseries2.json")
+        input = model.nodes['catchment1']
+
+        percentiles = np.linspace(20., 100., 5)
+        rec = FlowDurationCurveRecorder(model, input, percentiles, temporal_agg_func=agg_func, agg_func="min")
+
+        # test retrieval of recorder
+        assert model.recorders['flowdurationcurverecorder.catchment1'] == rec
+        # test changing name of recorder
+        rec.name = 'timeseries.Input'
+        assert model.recorders['timeseries.Input'] == rec
+        with pytest.raises(KeyError):
+            model.recorders['flowdurationcurverecorder.catchment1']
+
+        model.run()
+
+        func = TestAggregatedRecorder.funcs[agg_func]
+
+        assert_allclose(rec.fdc[:, 0], [20.42,  21.78,  23.22,  26.47,  29.31])
+        assert_allclose(func(rec.fdc, axis=0), rec.values())
+        assert_allclose(np.min(func(rec.fdc, axis=0)), rec.aggregated_value())
+
+        assert rec.fdc.shape == (len(percentiles), len(model.scenarios.combinations))
+        df = rec.to_dataframe()
+        assert df.shape == (len(percentiles), len(model.scenarios.combinations))
+
+    def test_seasonal_fdc_recorder(self):
+        """
+        Test the FlowDurationCurveRecorder
+        """
+        model = load_model("timeseries4.json")
+
+        df = pandas.read_csv(os.path.join(os.path.dirname(__file__), 'models', 'timeseries3.csv'),
+                             parse_dates=True, dayfirst=True, index_col=0)
+
+        percentiles = np.linspace(20., 100., 5)
+
+        summer_flows = df.loc[pandas.Timestamp("2014-06-01"):pandas.Timestamp("2014-08-31"), :]
+        summer_fdc = np.percentile(summer_flows, percentiles, axis=0)
+
+        model.run()
+
+        rec = model.recorders["seasonal_fdc"]
+        assert_allclose(rec.fdc, summer_fdc)
+
+    @pytest.mark.parametrize("agg_func, aggregate", [
+        ("min", False), ("max", False), ("mean", False), ("mean", True), ("sum", False)])
+    def test_fdc_dev_recorder(self, agg_func, aggregate):
+        """
+        Test the FlowDurationCurveDeviationRecorder
+        """
+        model = load_model("timeseries2.json")
+        input = model.nodes['catchment1']
+        term = model.nodes['term1']
+        scenarioA = model.scenarios['scenario A']
+
+        natural_flow = pandas.read_csv(os.path.join(os.path.dirname(__file__), 'models', 'timeseries2.csv'),
+                                       parse_dates=True, dayfirst=True, index_col=0)
+        percentiles = np.linspace(20., 100., 5)
+
+        natural_fdc = np.percentile(natural_flow, percentiles, axis=0)
+
+        # Lower target is 20% below natural
+        lower_input_fdc = natural_fdc * 0.8
+        # Upper is 10% above
+        upper_input_fdc = natural_fdc * 1.1
+
+        if aggregate:
+            # Setup only a single target for all scenarios.
+            lower_input_fdc = lower_input_fdc.mean(axis=1)
+            upper_input_fdc = upper_input_fdc.mean(axis=1)
+            scenarioA = None
+
+        rec = FlowDurationCurveDeviationRecorder(model, term, percentiles, lower_input_fdc, upper_input_fdc,
+                                                 temporal_agg_func=agg_func, agg_func="mean", scenario=scenarioA)
+
+        # test retrieval of recorder
+        assert model.recorders['flowdurationcurvedeviationrecorder.term1'] == rec
+        # test changing name of recorder
+        rec.name = 'timeseries.Input'
+        assert model.recorders['timeseries.Input'] == rec
+        with pytest.raises(KeyError):
+            model.recorders['flowdurationcurvedeviationrecorder.term1']
+
+        model.run()
+
+        actual_fdc = np.maximum(natural_fdc - 23, 0.0)
+
+        if aggregate:
+            lower_input_fdc = lower_input_fdc[:, np.newaxis]
+            upper_input_fdc = upper_input_fdc[:, np.newaxis]
+
+        # Compute deviation
+        lower_deviation = (lower_input_fdc - actual_fdc) / lower_input_fdc
+        upper_deviation = (actual_fdc - upper_input_fdc) / upper_input_fdc
+        deviation = np.maximum(np.maximum(lower_deviation, upper_deviation), np.zeros_like(lower_deviation))
+
+        func = TestAggregatedRecorder.funcs[agg_func]
+
+        assert_allclose(rec.fdc_deviations[:, 0], deviation[:, 0])
+        assert_allclose(func(rec.fdc_deviations, axis=0), rec.values())
+        assert_allclose(np.mean(func(rec.fdc_deviations, axis=0)), rec.aggregated_value())
+
+        assert rec.fdc_deviations.shape == (len(percentiles), len(model.scenarios.combinations))
+        df = rec.to_dataframe()
+        assert df.shape == (len(percentiles), len(model.scenarios.combinations))
+
+    def test_fdc_dev_from_json(self):
+
+        model = load_model("timeseries2_with_fdc.json")
+        model.run()
+
+        rec = model.recorders['fdc_dev1']
+        df = rec.to_dataframe()
+        assert df.shape == (5, len(model.scenarios.combinations))
+
+        rec = model.recorders['fdc_dev2']
+        df = rec.to_dataframe()
+        assert df.shape == (5, len(model.scenarios.combinations))
+
+
+def test_sdc_recorder():
+    """
+    Test the StorageDurationCurveRecorder
+    """
+    model = load_model("timeseries3.json")
+    inpt = model.nodes['catchment1']
+    strg = model.nodes['reservoir1']
+
+    percentiles = np.linspace(20., 100., 5)
+    flow_rec = NumpyArrayNodeRecorder(model, inpt)
+    rec = StorageDurationCurveRecorder(model, strg, percentiles, temporal_agg_func="max", agg_func="min")
+
+    # test retrieval of recorder
+    assert model.recorders['storagedurationcurverecorder.reservoir1'] == rec
+
+    model.run()
+
+    # Manually calculate expected storage and percentiles
+    strg_volume = strg.initial_volume + np.cumsum(flow_rec.data - 23.0, axis=0)
+    strg_pciles = np.percentile(strg_volume, percentiles, axis=0)
+
+    assert_allclose(rec.sdc, strg_pciles)
+    assert_allclose(np.max(rec.sdc, axis=0), rec.values())
+    assert_allclose(np.min(np.max(rec.sdc, axis=0)), rec.aggregated_value())
+
+    assert rec.sdc.shape == (len(percentiles), len(model.scenarios.combinations))
+    df = rec.to_dataframe()
+    assert df.shape == (len(percentiles), len(model.scenarios.combinations))
+
+
+@pytest.mark.parametrize('proportional', [True, False])
+def test_numpy_storage_recorder(simple_storage_model, proportional):
+    """
+    Test the NumpyArrayStorageRecorder
+    """
+    model = simple_storage_model
+
+    res = model.nodes['Storage']
+
+    rec = NumpyArrayStorageRecorder(model, res, proportional=proportional)
+
+    model.run()
+
+    expected = np.array([[7, 4, 1, 0, 0]]).T
+    if proportional:
+        expected = expected / 20
+
+    assert(rec.data.shape == (5, 1))
+    assert_allclose(rec.data, expected, atol=1e-7)
+
+    df = rec.to_dataframe()
+    assert df.shape == (5, 1)
+    assert_allclose(df.values, expected, atol=1e-7)
+
+
+def test_numpy_array_level_recorder(simple_storage_model):
+    model = simple_storage_model
+
+    storage = model.nodes["Storage"]
+    level_param = InterpolatedVolumeParameter(model, storage, [0, 20], [0, 100])
+    storage.level = level_param
+    level_rec = NumpyArrayLevelRecorder(model, storage, temporal_agg_func='min')
+
+    model.run()
+
+    expected = np.array([[50, 35, 20, 5, 0]]).T
+    assert_allclose(level_rec.data, expected, atol=1e-7)
+
+    df = level_rec.to_dataframe()
+    assert df.shape == (5, 1)
+    assert_allclose(df.values, expected, atol=1e-7)
+
+    assert_allclose(level_rec.aggregated_value(), np.min(expected))
+
+
+def test_numpy_array_area_recorder(simple_storage_model):
+
+    model = simple_storage_model
+
+    storage = model.nodes["Storage"]
+    area_param = InterpolatedVolumeParameter(model, storage, [0, 20], [0, 100])
+    storage.area = area_param
+    area_rec = NumpyArrayAreaRecorder(model, storage, temporal_agg_func='min')
+
+    model.run()
+
+    expected = np.array([[50, 35, 20, 5, 0]]).T
+    assert_allclose(area_rec.data, expected, atol=1e-7)
+
+    df = area_rec.to_dataframe()
+    assert df.shape == (5, 1)
+    assert_allclose(df.values, expected, atol=1e-7)
+
+    assert_allclose(area_rec.aggregated_value(), np.min(expected))
+
+
+def test_numpy_parameter_recorder(simple_linear_model):
+    """
+    Test the NumpyArrayParameterRecorder
+    """
+    from pywr.parameters import DailyProfileParameter
+
+    model = simple_linear_model
+    # using leap year simplifies tests
+    model.timestepper.start = pandas.to_datetime("2016-01-01")
+    model.timestepper.end = pandas.to_datetime("2016-12-31")
+    otpt = model.nodes['Output']
+
+    p = DailyProfileParameter(model, np.arange(366, dtype=np.float64), )
+    p.name = 'daily profile'
+    model.nodes['Input'].max_flow = p
+    otpt.cost = -2.0
+    rec = NumpyArrayParameterRecorder(model, model.nodes['Input'].max_flow)
+
+    # test retrieval of recorder
+    assert model.recorders['numpyarrayparameterrecorder.daily profile'] == rec
+
+    model.run()
+
+    assert rec.data.shape == (366, 1)
+    assert_allclose(rec.data, np.arange(366, dtype=np.float64)[:, np.newaxis])
+
+    df = rec.to_dataframe()
+    assert df.shape == (366, 1)
+    assert_allclose(df.values, np.arange(366, dtype=np.float64)[:, np.newaxis])
+
+
+def test_numpy_daily_profile_parameter_recorder(simple_linear_model):
+    """
+    Test the NumpyArrayDailyProfileParameterRecorder
+    """
+    from pywr.parameters import DailyProfileParameter
+
+    model = simple_linear_model
+    # using leap year simplifies tests
+    model.timestepper.start = pandas.to_datetime("2016-01-01")
+    model.timestepper.end = pandas.to_datetime("2017-12-31")
+    otpt = model.nodes['Output']
+
+    p = DailyProfileParameter(model, np.arange(366, dtype=np.float64), )
+    p.name = 'daily profile'
+    model.nodes['Input'].max_flow = p
+    otpt.cost = -2.0
+    rec = NumpyArrayDailyProfileParameterRecorder(model, model.nodes['Input'].max_flow)
+
+    # test retrieval of recorder
+    assert model.recorders['numpyarraydailyprofileparameterrecorder.daily profile'] == rec
+
+    model.run()
+
+    assert rec.data.shape == (366, 1)
+    assert_allclose(rec.data, np.arange(366, dtype=np.float64)[:, np.newaxis])
+
+    df = rec.to_dataframe()
+    assert df.shape == (366, 1)
+    assert_allclose(df.values, np.arange(366, dtype=np.float64)[:, np.newaxis])
+
+
+def test_numpy_index_parameter_recorder(simple_storage_model):
+    """
+    Test the NumpyArrayIndexParameterRecorder
+
+    Note the parameter is recorded at the start of the timestep, while the
+    storage is recorded at the end of the timestep.
+    """
+    from pywr.parameters.control_curves import ControlCurveIndexParameter
+
+    model = simple_storage_model
+
+    res = model.nodes['Storage']
+
+    p = ControlCurveIndexParameter(model, res, [5.0/20.0, 2.5/20.0])
+
+    res_rec = NumpyArrayStorageRecorder(model, res)
+    lvl_rec = NumpyArrayIndexParameterRecorder(model, p)
+
+    model.run()
+
+    assert(res_rec.data.shape == (5, 1))
+    assert_allclose(res_rec.data, np.array([[7, 4, 1, 0, 0]]).T, atol=1e-7)
+    assert (lvl_rec.data.shape == (5, 1))
+    assert_allclose(lvl_rec.data, np.array([[0, 0, 1, 2, 2]]).T, atol=1e-7)
+
+
+    df = lvl_rec.to_dataframe()
+    assert df.shape == (5, 1)
+    assert_allclose(df.values, np.array([[0, 0, 1, 2, 2]]).T, atol=1e-7)
+
+
+def test_parameter_recorder_json():
+    model = load_model("parameter_recorder.json")
+    rec_demand = model.recorders["demand_max_recorder"]
+    rec_supply = model.recorders["supply_max_recorder"]
+    model.run()
+    assert_allclose(rec_demand.data, 10)
+    assert_allclose(rec_supply.data, 15)
+
+
+def test_nested_recorder_json():
+    model = load_model("agg_recorder_nesting.json")
+    rec_demand = model.recorders["demand_max_recorder"]
+    rec_supply = model.recorders["supply_max_recorder"]
+    rec_total = model.recorders["max_recorder"]
+    model.run()
+    assert_allclose(rec_demand.aggregated_value(), 10)
+    assert_allclose(rec_supply.aggregated_value(), 15)
+    assert_allclose(rec_total.aggregated_value(), 25)
+
+
+@pytest.fixture()
+def daily_profile_model(simple_linear_model):
+    model = simple_linear_model
+    # using leap year simplifies test
+    model.timestepper.start = pandas.to_datetime("2016-01-01")
+    model.timestepper.end = pandas.to_datetime("2016-12-31")
+
+    node = model.nodes["Input"]
+    values = np.arange(0, 366, dtype=np.float64)
+    node.max_flow = DailyProfileParameter(model, values, name='profile')
+    return model
+
+
+def test_parameter_mean_recorder(daily_profile_model):
+    model = daily_profile_model
+    node = model.nodes["Input"]
+    scenario = Scenario(model, "dummy", size=3)
+
+    timesteps = 3
+    rec_mean = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
+                                              temporal_agg_func="mean", name="rec_mean")
+    rec_sum = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
+                                             temporal_agg_func="sum", name="rec_sum")
+    rec_min = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
+                                             temporal_agg_func="min", name="rec_min")
+    rec_max = RollingWindowParameterRecorder(model, node.max_flow, timesteps,
+                                             temporal_agg_func="max", name="rec_max")
+
+    model.run()
+
+    assert_allclose(rec_mean.data[[0, 1, 2, 3, 364], 0], [0, 0.5, 1, 2, 363])
+    assert_allclose(rec_max.data[[0, 1, 2, 3, 364], 0], [0, 1, 2, 3, 364])
+    assert_allclose(rec_min.data[[0, 1, 2, 3, 364], 0], [0, 0, 0, 1, 362])
+    assert_allclose(rec_sum.data[[0, 1, 2, 3, 364], 0], [0, 1, 3, 6, 1089])
+
+def test_parameter_mean_recorder_json(simple_linear_model):
+    model = simple_linear_model
+    node = model.nodes["Input"]
+    values = np.arange(0, 366, dtype=np.float64)
+    parameter = DailyProfileParameter(model, values, name="input_max_flow")
+
+    node.max_flow = parameter
+
+    data = {
+        "type": "rollingwindowparameter",
+        "parameter": "input_max_flow",
+        "window": 3,
+        "temporal_agg_func": "mean",
+    }
+
+    rec = load_recorder(model, data)
+
+
+class TestTotalParameterRecorder:
+
+    @pytest.mark.parametrize('factor, integrate',
+                             [[1.0, False], [1.0, True], [2.0, False], [0.5, True]])
+    def test_values(self, daily_profile_model, factor, integrate):
+        model = daily_profile_model
+        model.timestepper.delta = 2
+        param = model.parameters['profile']
+        rec = TotalParameterRecorder(model, param, name="total", factor=factor, integrate=integrate)
+        model.run()
+
+        expected = np.arange(0, 366, dtype=np.float64)[::2].sum()*factor
+        if integrate:
+            expected *= 2
+        assert_allclose(rec.values(), expected)
+
+    @pytest.mark.parametrize('integrate', [None, True, False])
+    def test_from_json(self, daily_profile_model, integrate):
+
+        model = daily_profile_model
+
+        data = {
+            "type": "totalparameter",
+            "parameter": "profile",
+            "agg_func": "mean",
+            "factor": 2.0,
+        }
+
+        if integrate is not None:
+            data['integrate'] = integrate
+
+        rec = load_recorder(model, data)
+        assert rec.factor == 2.0
+
+        if integrate is not None:
+            assert rec.integrate == integrate
+        else:
+            assert not rec.integrate
+
+
+class TestMeanParameterRecorder:
+    @pytest.mark.parametrize('factor', [1.0, 2.0, 0.5])
+    def test_values(self, daily_profile_model, factor):
+        model = daily_profile_model
+        model.timestepper.delta = 2
+        param = model.parameters['profile']
+        rec = MeanParameterRecorder(model, param, name="mean", factor=factor)
+        model.run()
+
+        expected = np.arange(0, 366, dtype=np.float64)[::2].mean()*factor
+        assert_allclose(rec.values(), expected)
+
+    def test_from_json(self, daily_profile_model):
+
+        model = daily_profile_model
+
+        data = {
+            "type": "meanparameter",
+            "parameter": "profile",
+            "agg_func": "mean",
+            "factor": 2.0,
+        }
+
+        rec = load_recorder(model, data)
+        assert rec.factor == 2.0
+
+
+def test_concatenated_dataframes(simple_storage_model):
+    """
+    Test that Model.to_dataframe returns something sensible.
+
+    """
+    model = simple_storage_model
+
+    scA = Scenario(model, 'A', size=2)
+    scB = Scenario(model, 'B', size=3)
+
+    res = model.nodes['Storage']
+    rec1 = NumpyArrayStorageRecorder(model, res)
+    otpt = model.nodes['Output']
+    rec2 = NumpyArrayNodeRecorder(model, otpt)
+    # The following can't return a DataFrame; is included to check
+    # it doesn't cause any issues
+    rec3 = TotalDeficitNodeRecorder(model, otpt)
+
+    model.run()
+
+    df = model.to_dataframe()
+    assert df.shape == (5, 2*2*3)
+    assert df.columns.names == ['Recorder', 'A', 'B']
+
+
+@pytest.mark.parametrize("complib", [None, "gzip", "bz2"])
+def test_csv_recorder(simple_storage_model, tmpdir, complib):
+    """Test the CSV Recorder
+    """
+    model = simple_storage_model
+    otpt = model.nodes['Output']
+    otpt.cost = -2.0
+
+    # Rename output to a unicode character to check encoding to files
+    otpt.name = u"\u03A9"
+    expected_header = ['Datetime', 'Input', 'Storage', u"\u03A9"]
+
+    csvfile = tmpdir.join('output.csv')
+    # By default the CSVRecorder saves all nodes in alphabetical order
+    # and scenario index 0.
+    rec = CSVRecorder(model, str(csvfile), complib=complib, complevel=5)
+
+    model.run()
+
+    import csv
+    kwargs = {"encoding": "utf-8"}
+    mode = "rt"
+
+    if complib == "gzip":
+        import gzip
+        fh = gzip.open(str(csvfile), mode, **kwargs)
+    elif complib in ("bz2", "bzip2"):
+        import bz2
+        fh = bz2.open(str(csvfile), mode, **kwargs)
+    else:
+        fh = open(str(csvfile), mode, **kwargs)
+
+    expected = [
+        expected_header,
+        ['2016-01-01T00:00:00', 5.0, 7.0, 8.0],
+        ['2016-01-02T00:00:00', 5.0, 4.0, 8.0],
+        ['2016-01-03T00:00:00', 5.0, 1.0, 8.0],
+        ['2016-01-04T00:00:00', 5.0, 0.0, 6.0],
+        ['2016-01-05T00:00:00', 5.0, 0.0, 5.0],
+    ]
+
+    data = fh.read(1024)
+    dialect = csv.Sniffer().sniff(data)
+    fh.seek(0)
+    reader = csv.reader(fh, dialect)
+
+    for irow, row in enumerate(reader):
+        expected_row = expected[irow]
+        if irow == 0:
+            assert expected_row == row
+        else:
+            assert expected_row[0] == row[0]  # Check datetime
+            # Check values
+            np.testing.assert_allclose([float(v) for v in row[1:]], expected_row[1:])
+    fh.close()
+
+
+def test_loading_csv_recorder_from_json(tmpdir):
+    """
+    Test the CSV Recorder which is loaded from json
+    """
+
+    filename = 'csv_recorder.json'
+
+    # This is a bit horrible, but need to edit the JSON dynamically
+    # so that the output.h5 is written in the temporary directory
+    path = os.path.join(os.path.dirname(__file__), 'models')
+    with open(os.path.join(path, filename), 'r') as f:
+        data = f.read()
+    data = json.loads(data)
+
+    # Make an absolute, but temporary, path for the recorder
+    url = data['recorders']['model_out']['url']
+    data['recorders']['model_out']['url'] = str(tmpdir.join(url))
+
+    model = Model.load(data, path=path)
+
+    csvfile = tmpdir.join('output.csv')
+    model.run()
+
+    periods = model.timestepper.datetime_index
+
+    import csv
+    with open(str(csvfile), 'r') as fh:
+        dialect = csv.Sniffer().sniff(fh.read(1024))
+        fh.seek(0)
+        reader = csv.reader(fh, dialect)
+        for irow, row in enumerate(reader):
+            if irow == 0:
+                expected = ['Datetime', 'inpt', 'otpt']
+                actual = row
+            else:
+                dt = periods[irow-1].to_timestamp()
+                expected = [dt.isoformat()]
+                actual = [row[0]]
+                assert np.all((np.array([float(v) for v in row[1:]]) - 10.0) < 1e-12)
+            assert expected == actual
+
+class TestTablesRecorder:
+
+    def test_create_directory(self, simple_linear_model, tmpdir):
+        """ Test TablesRecorder to create a new directory """
+
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
+
+        inpt.max_flow = 10.0
+        otpt.cost = -2.0
+        # Make a path with a new directory
+        folder = tmpdir.join('outputs')
+        h5file = folder.join('output.h5')
+        assert(not folder.exists())
+        rec = TablesRecorder(model, str(h5file), create_directories=True)
+        model.run()
+        assert(folder.exists())
+        assert(h5file.exists())
+
+    def test_nodes(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
+
+        inpt.max_flow = 10.0
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f)
+
+            model.run()
+
+            for node_name in model.nodes.keys():
+                ca = h5f.get_node('/', node_name)
+                assert ca.shape == (365, 1)
+                if node_name == 'Sum':
+                    np.testing.assert_allclose(ca, 20.0)
+                else:
+                    np.testing.assert_allclose(ca, 10.0)
+
+            from datetime import date, timedelta
+            d = date(2015, 1, 1)
+            time = h5f.get_node('/time')
+            for i in range(len(model.timestepper)):
+                row = time[i]
+                assert row['year'] == d.year
+                assert row['month'] == d.month
+                assert row['day'] == d.day
+
+                d += timedelta(1)
+
+            scenarios = h5f.get_node('/scenarios')
+            for i, s in enumerate(model.scenarios.scenarios):
+                row = scenarios[i]
+                assert row['name'] == s.name.encode('utf-8')
+                assert row['size'] == s.size
+
+            model.reset()
+            model.run()
+
+            time = h5f.get_node('/time')
+            assert len(time) == len(model.timestepper)
+
+    def test_multiple_scenarios(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        from pywr.parameters import ConstantScenarioParameter
+        model = simple_linear_model
+        scA = Scenario(model, name='A', size=4)
+        scB = Scenario(model, name='B', size=2)
+
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
+        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f)
+
+            model.run()
+
+            for node_name in model.nodes.keys():
+                ca = h5f.get_node('/', node_name)
+                assert ca.shape == (365, 4, 2)
+                np.testing.assert_allclose(ca[0, ...], [[10, 10], [20, 20], [20, 30], [20, 40]])
+
+            scenarios = h5f.get_node('/scenarios')
+            for i, s in enumerate(model.scenarios.scenarios):
+                row = scenarios[i]
+                assert row['name'] == s.name.encode('utf-8')
+                assert row['size'] == s.size
+
+    def test_user_scenarios(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder with user defined scenario subset
+
+        """
+        from pywr.parameters import ConstantScenarioParameter
+        model = simple_linear_model
+        scA = Scenario(model, name='A', size=4)
+        scB = Scenario(model, name='B', size=2)
+
+        # Use first and last combinations
+        model.scenarios.user_combinations = [[0, 0], [3, 1]]
+
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
+        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f)
+
+            model.run()
+
+            for node_name in model.nodes.keys():
+                ca = h5f.get_node('/', node_name)
+                assert ca.shape == (365, 2)
+                np.testing.assert_allclose(ca[0, ...], [10, 40])
+
+            # check combinations table exists
+            combinations = h5f.get_node('/scenario_combinations')
+            for i, comb in enumerate(model.scenarios.user_combinations):
+                row = combinations[i]
+                assert row['A'] == comb[0]
+                assert row['B'] == comb[1]
+
+    def test_parameters(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        from pywr.parameters import ConstantParameter
+
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        p = ConstantParameter(model, 10.0, name='max_flow')
+        inpt.max_flow = p
+
+        # ensure TablesRecorder can handle parameters with a / in the name
+        p_slash = ConstantParameter(model, 0.0, name='name with a / in it')
+        inpt.min_flow = p_slash
+
+        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
+
+        inpt.max_flow = 10.0
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            with pytest.warns(ParameterNameWarning):
+                rec = TablesRecorder(model, h5f, parameters=[p, p_slash])
+
+            # check parameters have been added to the component tree
+            # this is particularly important for parameters which update their
+            # values in `after`, e.g. DeficitParameter (see #465)
+            assert(not model.find_orphaned_parameters())
+            assert(p in rec.children)
+            assert(p_slash in rec.children)
+
+            with pytest.warns(tables.NaturalNameWarning):
+                model.run()
+
+            for node_name in model.nodes.keys():
+                ca = h5f.get_node('/', node_name)
+                assert ca.shape == (365, 1)
+                if node_name == 'Sum':
+                    np.testing.assert_allclose(ca, 20.0)
+                elif "name with a" in node_name:
+                    assert(node_name == "name with a _ in it")
+                    np.testing.assert_allclose(ca, 0.0)
+                else:
+                    np.testing.assert_allclose(ca, 10.0)
+
+    def test_nodes_with_str(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        from pywr.parameters import ConstantParameter
+
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
+        p = ConstantParameter(model, 10.0, name='max_flow')
+        inpt.max_flow = p
+
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            nodes = ['Output', 'Input', 'Sum']
+            where = "/agroup"
+            rec = TablesRecorder(model, h5f, nodes=nodes,
+                                 parameters=[p, ], where=where)
+
+            model.run()
+
+            for node_name in ['Output', 'Input', 'Sum', 'max_flow']:
+                ca = h5f.get_node("/agroup/" + node_name)
+                assert ca.shape == (365, 1)
+                if node_name == 'Sum':
+                    np.testing.assert_allclose(ca, 20.0)
+                else:
+                    np.testing.assert_allclose(ca, 10.0)
+
+    def test_demand_saving_with_indexed_array(self, tmpdir):
+        """Test recording various items from demand saving example
+
+        """
+        model = load_model("demand_saving2.json")
+
+        model.timestepper.end = "2016-01-31"
+
+        model.check()
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+
+            nodes = [
+                ('/outputs/demand', 'Demand'),
+                ('/storage/reservoir', 'Reservoir'),
+            ]
+
+            parameters = [
+                ('/parameters/demand_saving_level', 'demand_saving_level'),
+            ]
+
+            rec = TablesRecorder(model, h5f, nodes=nodes, parameters=parameters)
+
+            model.run()
+
+            max_volume = model.nodes["Reservoir"].max_volume
+            rec_demand = h5f.get_node('/outputs/demand', 'Demand').read()
+            rec_storage = h5f.get_node('/storage/reservoir', 'Reservoir').read()
+
+            # model starts with no demand saving
+            demand_baseline = 50.0
+            demand_factor = 0.9  # jan-apr
+            demand_saving = 1.0
+            assert_allclose(rec_demand[0, 0], demand_baseline * demand_factor * demand_saving)
+
+            # first control curve breached
+            demand_saving = 0.95
+            assert (rec_storage[4, 0] < (0.8 * max_volume))
+            assert_allclose(rec_demand[5, 0], demand_baseline * demand_factor * demand_saving)
+
+            # second control curve breached
+            demand_saving = 0.5
+            assert (rec_storage[11, 0] < (0.5 * max_volume))
+            assert_allclose(rec_demand[12, 0], demand_baseline * demand_factor * demand_saving)
+
+    def test_demand_saving_with_indexed_array(self, tmpdir):
+        """Test recording various items from demand saving example.
+
+        This time the TablesRecorder is defined in JSON.
+        """
+        filename = "demand_saving_with_tables_recorder.json"
+        # This is a bit horrible, but need to edit the JSON dynamically
+        # so that the output.h5 is written in the temporary directory
+        path = os.path.join(os.path.dirname(__file__), 'models')
+        with open(os.path.join(path, filename), 'r') as f:
+            data = f.read()
+        data = json.loads(data)
+
+        # Make an absolute, but temporary, path for the recorder
+        url = data['recorders']['database']['url']
+        data['recorders']['database']['url'] = str(tmpdir.join(url))
+
+        model = Model.load(data, path=path)
+
+        model.timestepper.end = "2016-01-31"
+        model.check()
+
+        # run model
+        model.run()
+
+        # run model again (to test reset behaviour)
+        model.run()
+        max_volume = model.nodes["Reservoir"].max_volume
+
+        h5file = tmpdir.join('output.h5')
+        with tables.open_file(str(h5file), 'r') as h5f:
+            assert model.metadata['title'] == h5f.title
+            # Check metadata on root node
+            assert h5f.root._v_attrs.author == 'pytest'
+            assert h5f.root._v_attrs.run_number == 0
+
+            rec_demand = h5f.get_node('/outputs/demand').read()
+            rec_storage = h5f.get_node('/storage/reservoir').read()
+
+            # model starts with no demand saving
+            demand_baseline = 50.0
+            demand_factor = 0.9  # jan-apr
+            demand_saving = 1.0
+            assert_allclose(rec_demand[0, 0], demand_baseline * demand_factor * demand_saving)
+
+            # first control curve breached
+            demand_saving = 0.95
+            assert (rec_storage[4, 0] < (0.8 * max_volume))
+            assert_allclose(rec_demand[5, 0], demand_baseline * demand_factor * demand_saving)
+
+            # second control curve breached
+            demand_saving = 0.5
+            assert (rec_storage[11, 0] < (0.5 * max_volume))
+            assert_allclose(rec_demand[12, 0], demand_baseline * demand_factor * demand_saving)
+
+    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+    def test_routes(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+        agg_node = AggregatedNode(model, 'Sum', [otpt, inpt])
+
+        inpt.max_flow = 10.0
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f, routes_flows='flows')
+
+            model.run()
+
+            flows = h5f.get_node('/flows')
+            assert flows.shape == (365, 1, 1)
+            np.testing.assert_allclose(flows.read(), np.ones((365, 1, 1))*10)
+
+            routes = h5f.get_node('/routes')
+            assert routes.shape[0] == 1
+            row = routes[0]
+            row['start'] = "Input"
+            row['end'] = "Output"
+
+            from datetime import date, timedelta
+            d = date(2015, 1, 1)
+            time = h5f.get_node('/time')
+            for i in range(len(model.timestepper)):
+                row = time[i]
+                assert row['year'] == d.year
+                assert row['month'] == d.month
+                assert row['day'] == d.day
+
+                d += timedelta(1)
+
+            scenarios = h5f.get_node('/scenarios')
+            for s in model.scenarios.scenarios:
+                row = scenarios[i]
+                assert row['name'] == s.name
+                assert row['size'] == s.size
+
+            model.reset()
+            model.run()
+
+            time = h5f.get_node('/time')
+            assert len(time) == len(model.timestepper)
+
+    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+    def test_routes_multiple_scenarios(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder
+
+        """
+        from pywr.parameters import ConstantScenarioParameter
+        model = simple_linear_model
+        scA = Scenario(model, name='A', size=4)
+        scB = Scenario(model, name='B', size=2)
+
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
+        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f, routes_flows='flows')
+
+            model.run()
+
+            flows = h5f.get_node('/flows')
+            assert flows.shape == (365, 1, 4, 2)
+            np.testing.assert_allclose(flows[0, 0], [[10, 10], [20, 20], [20, 30], [20, 40]])
+
+    @pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+    def test_routes_user_scenarios(self, simple_linear_model, tmpdir):
+        """
+        Test the TablesRecorder with user defined scenario subset
+
+        """
+        from pywr.parameters import ConstantScenarioParameter
+        model = simple_linear_model
+        scA = Scenario(model, name='A', size=4)
+        scB = Scenario(model, name='B', size=2)
+
+        # Use first and last combinations
+        model.scenarios.user_combinations = [[0, 0], [3, 1]]
+
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
+        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        import tables
+        with tables.open_file(str(h5file), 'w') as h5f:
+            rec = TablesRecorder(model, h5f, routes_flows='flows')
+
+            model.run()
+
+            flows = h5f.get_node('/flows')
+            assert flows.shape == (365, 1, 2)
+            np.testing.assert_allclose(flows[0, 0], [10, 40])
+
+            # check combinations table exists
+            combinations = h5f.get_node('/scenario_combinations')
+            for i, comb in enumerate(model.scenarios.user_combinations):
+                row = combinations[i]
+                assert row['A'] == comb[0]
+                assert row['B'] == comb[1]
+
+        # This part of the test requires IPython (see `pywr.notebook`)
+        pytest.importorskip("IPython")  # triggers a skip of the test if IPython not found.
+        from pywr.notebook.sankey import routes_to_sankey_links
+
+        links = routes_to_sankey_links(str(h5file), 'flows')
+        # Value is mean of 10 and 40
+
+        link = links[0]
+        assert link['source'] == 'Input'
+        assert link['target'] == 'Output'
+        np.testing.assert_allclose(link['value'], 25.0)
+
+        links = routes_to_sankey_links(str(h5file), 'flows', scenario_slice=0)
+        link = links[0]
+        assert link['source'] == 'Input'
+        assert link['target'] == 'Output'
+        np.testing.assert_allclose(link['value'], 10.0)
+
+        links = routes_to_sankey_links(str(h5file), 'flows', scenario_slice=1, time_slice=0)
+        link = links[0]
+        assert link['source'] == 'Input'
+        assert link['target'] == 'Output'
+        np.testing.assert_allclose(link['value'], 40.0)
+
+    def test_generate_dataframes(self, simple_linear_model, tmpdir):
+        """Test TablesRecorder.generate_dataframes """
+        from pywr.parameters import ConstantScenarioParameter
+        model = simple_linear_model
+        scA = Scenario(model, name='A', size=4)
+        scB = Scenario(model, name='B', size=2)
+
+        otpt = model.nodes['Output']
+        inpt = model.nodes['Input']
+
+        inpt.max_flow = ConstantScenarioParameter(model, scA, [10, 20, 30, 40])
+        otpt.max_flow = ConstantScenarioParameter(model, scB, [20, 40])
+        otpt.cost = -2.0
+
+        h5file = tmpdir.join('output.h5')
+        TablesRecorder(model, h5file)
+        model.run()
+
+        dfs = {}
+        for node, df in TablesRecorder.generate_dataframes(h5file):
+            dfs[node] = df
+
+        for node_name in model.nodes.keys():
+            df = dfs[node_name]
+            assert df.shape == (365, 8)
+            np.testing.assert_allclose(df.iloc[0, :], [10, 10, 20, 20, 20, 30, 20, 40])
+
+
+class TestDeficitRecorders:
+
+    def test_total_deficit_node_recorder(self, simple_linear_model):
+        """
+        Test TotalDeficitNodeRecorder
+        """
+        model = simple_linear_model
+        model.timestepper.delta = 5
+        otpt = model.nodes['Output']
+        otpt.max_flow = 30.0
+        model.nodes['Input'].max_flow = 10.0
+        otpt.cost = -2.0
+        rec = TotalDeficitNodeRecorder(model, otpt)
+
+        model.step()
+        assert_allclose(20.0*5, rec.aggregated_value(), atol=1e-7)
+
+        model.step()
+        assert_allclose(40.0*5, rec.aggregated_value(), atol=1e-7)
+
+    def test_array_deficit_recoder(self, simple_linear_model):
+        """Test `NumpyArrayNodeDeficitRecorder` """
+        model = simple_linear_model
+        model.timestepper.delta = 1
+        otpt = model.nodes['Output']
+
+        inflow = np.arange(365) * 0.1
+        demand = np.ones_like(inflow) * 30.0
+
+        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
+        otpt.max_flow = ArrayIndexedParameter(model, demand)
+        otpt.cost = -2.0
+
+        expected_supply = np.minimum(inflow, demand)
+        expected_deficit = demand - expected_supply
+
+        rec = NumpyArrayNodeDeficitRecorder(model, otpt)
+
+        model.run()
+
+        assert rec.data.shape == (365, 1)
+        np.testing.assert_allclose(expected_deficit[:, np.newaxis], rec.data)
+
+        df = rec.to_dataframe()
+        assert df.shape == (365, 1)
+        np.testing.assert_allclose(expected_deficit[:, np.newaxis], df.values)
+
+    def test_array_supplied_ratio_recoder(self, simple_linear_model):
+        """Test `NumpyArrayNodeSuppliedRatioRecorder` """
+        model = simple_linear_model
+        model.timestepper.delta = 1
+        otpt = model.nodes['Output']
+
+        inflow = np.arange(365) * 0.1
+        demand = np.ones_like(inflow) * 30.0
+
+        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
+        otpt.max_flow = ArrayIndexedParameter(model, demand)
+        otpt.cost = -2.0
+
+        expected_supply = np.minimum(inflow, demand)
+        expected_ratio = expected_supply / demand
+
+        rec = NumpyArrayNodeSuppliedRatioRecorder(model, otpt)
+
+        model.run()
+
+        assert rec.data.shape == (365, 1)
+        np.testing.assert_allclose(expected_ratio[:, np.newaxis], rec.data)
+
+        df = rec.to_dataframe()
+        assert df.shape == (365, 1)
+        np.testing.assert_allclose(expected_ratio[:, np.newaxis], df.values)
+
+    def test_array_curtailment_ratio_recoder(self, simple_linear_model):
+        """Test `NumpyArrayNodeCurtailmentRatioRecorder` """
+        model = simple_linear_model
+        model.timestepper.delta = 1
+        otpt = model.nodes['Output']
+
+        inflow = np.arange(365) * 0.1
+        demand = np.ones_like(inflow) * 30.0
+
+        model.nodes['Input'].max_flow = ArrayIndexedParameter(model, inflow)
+        otpt.max_flow = ArrayIndexedParameter(model, demand)
+        otpt.cost = -2.0
+
+        expected_supply = np.minimum(inflow, demand)
+        expected_curtailment_ratio = 1 - expected_supply / demand
+
+        rec = NumpyArrayNodeCurtailmentRatioRecorder(model, otpt)
+
+        model.run()
+
+        assert rec.data.shape == (365, 1)
+        np.testing.assert_allclose(expected_curtailment_ratio[:, np.newaxis], rec.data)
+
+        df = rec.to_dataframe()
+        assert df.shape == (365, 1)
+        np.testing.assert_allclose(expected_curtailment_ratio[:, np.newaxis], df.values)
+
+
+def test_timestep_count_index_parameter_recorder(simple_storage_model):
+    """
+    The test uses a simple reservoir model with different inputs that
+    trigger a control curve failure after a different number of years.
+    """
+    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
+    from pywr.parameters.control_curves import ControlCurveIndexParameter
+    model = simple_storage_model
+    scenario = Scenario(model, 'A', size=2)
+    # Simulate 5 years
+    model.timestepper.start = '2015-01-01'
+    model.timestepper.end = '2019-12-31'
+    # Control curve parameter
+    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
+
+    # Storage model has a capacity of 20, but starts at 10 Ml
+    # Demand is roughly 2 Ml/d per year
+    #  First ensemble balances the demand
+    #  Second ensemble should fail during 3rd year
+    demand = 2.0 / 365
+    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
+    model.nodes['Output'].max_flow = demand
+
+    # Create the recorder with a threshold of 1
+    rec = TimestepCountIndexParameterRecorder(model, param, 1)
+
+    model.run()
+
+    assert_allclose([0, 183 + 365 + 365], rec.values(), atol=1e-7)
+
+
+@pytest.mark.parametrize("params", [1, 2])
+def test_annual_count_index_threshold_recorder(simple_storage_model, params):
+    """
+    The test sets uses a simple reservoir model with different inputs that
+    trigger a control curve failure after different numbers of years.
+    """
+    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
+    from pywr.parameters.control_curves import ControlCurveIndexParameter
+    model = simple_storage_model
+    scenario = Scenario(model, 'A', size=2)
+    # Simulate 5 years
+    model.timestepper.start = '2015-01-01'
+    model.timestepper.end = '2019-12-31'
+    # Control curve parameter
+    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
+
+    # Storage model has a capacity of 20, but starts at 10 Ml
+    # Demand is roughly 2 Ml/d per year
+    #  First ensemble balances the demand
+    #  Second ensemble should fail during 3rd year
+    demand = 2.0 / 365
+    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
+    model.nodes['Output'].max_flow = demand
+
+    # Create the recorder with a threshold of 1
+    rec = AnnualCountIndexThresholdRecorder(model, [param] * params, 'TestRec', 1)
+
+    model.run()
+
+    # We expect no failures in the first ensemble, the reservoir starts failing halfway through
+    # the 3rd year
+    assert_allclose([[0, 0],
+                     [0, 0],
+                     [0, 183],
+                     [0, 365],
+                     [0, 365]], rec.data, atol=1e-7)
+
+class TestAnnualTotalFlowRecorder:
+
+    def test_annual_total_flow_recorder(self, simple_linear_model):
+        """
+        Test AnnualTotalFlowRecorder
+        """
+
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        otpt.max_flow = 30.0
+        model.nodes['Input'].max_flow = 10.0
+        otpt.cost = -2
+        rec = AnnualTotalFlowRecorder(model, 'Total Flow', [otpt])
+
+        model.run()
+
+        assert_allclose(3650.0, rec.data, atol=1e-7)
+
+    def test_annual_total_flow_recorder_factored(self, simple_linear_model):
+        """
+        Test AnnualTotalFlowRecorder with factors applied
+        """
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        otpt.max_flow = 30.0
+        inpt = model.nodes['Input']
+        inpt.max_flow = 10.0
+        otpt.cost = -2
+
+        factors = [2.0, 1.0]
+        rec_fact = AnnualTotalFlowRecorder(model, 'Total Flow', [otpt, inpt], factors=factors)
+
+        model.run()
+
+        assert_allclose(3650.0*3, rec_fact.data, atol=1e-7)
+
+
+def test_total_flow_node_recorder(simple_linear_model):
+    """
+    Test TotalDeficitNodeRecorder
+    """
+    model = simple_linear_model
+    otpt = model.nodes['Output']
+    otpt.max_flow = 30.0
+    model.nodes['Input'].max_flow = 10.0
+    otpt.cost = -2.0
+    rec = TotalFlowNodeRecorder(model, otpt)
+
+    model.step()
+    assert_allclose(10.0, rec.aggregated_value(), atol=1e-7)
+
+    model.step()
+    assert_allclose(20.0, rec.aggregated_value(), atol=1e-7)
+
+
+def test_mean_flow_node_recorder(simple_linear_model):
+    """
+    Test MeanFlowNodeRecorder
+    """
+    model = simple_linear_model
+    nt = len(model.timestepper)
+
+    otpt = model.nodes['Output']
+    otpt.max_flow = 30.0
+    model.nodes['Input'].max_flow = 10.0
+    otpt.cost = -2.0
+    rec = MeanFlowNodeRecorder(model, otpt)
+
+    model.run()
+    assert_allclose(10.0, rec.aggregated_value(), atol=1e-7)
+
+
+def custom_test_func(array, axis=None):
+    return np.sum(array**2, axis=axis)
+
+
+class TestAggregatedRecorder:
+    """Tests for AggregatedRecorder"""
+    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "sum": np.sum}
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum"])
+    def test_aggregated_recorder(self, simple_linear_model, agg_func):
+        model = simple_linear_model
+        otpt = model.nodes['Output']
+        otpt.max_flow = 30.0
+        model.nodes['Input'].max_flow = 10.0
+        otpt.cost = -2.0
+        rec1 = TotalFlowNodeRecorder(model, otpt)
+        rec2 = TotalDeficitNodeRecorder(model, otpt)
+
+        func = TestAggregatedRecorder.funcs[agg_func]
+
+        rec = AggregatedRecorder(model, [rec1, rec2], agg_func=agg_func)
+
+        assert(rec in rec1.parents)
+        assert(rec in rec2.parents)
+
+        model.step()
+        assert_allclose(func([10.0, 20.0]), rec.aggregated_value(), atol=1e-7)
+
+        model.step()
+        assert_allclose(func([20.0, 40.0]), rec.aggregated_value(), atol=1e-7)
+
+    @pytest.mark.parametrize("agg_func", ["min", "max", "mean", "sum", "custom"])
+    def test_agg_func_get_set(self, simple_linear_model, agg_func):
+        """Test getter and setter for AggregatedRecorder.agg_func"""
+        if agg_func == "custom":
+            agg_func = custom_test_func
+        model = simple_linear_model
+        rec = AggregatedRecorder(model, [], agg_func=agg_func)
+        assert rec.agg_func == agg_func
+        rec.agg_func = "product"
+        assert rec.agg_func == "product"
+
+
+def test_reset_timestepper_recorder():
+    model = Model(
+        start=pandas.to_datetime('2016-01-01'),
+        end=pandas.to_datetime('2016-01-01')
+    )
+
+    inpt = Input(model, "input", max_flow=10)
+    otpt = Output(model, "output", max_flow=50, cost=-10)
+    inpt.connect(otpt)
+
+    rec = NumpyArrayNodeRecorder(model, otpt)
+
+    model.run()
+
+    model.timestepper.end = pandas.to_datetime("2016-01-02")
+
+    model.run()
+
+def test_mean_flow_recorder():
+    model = Model()
+    model.timestepper.start = pandas.to_datetime("2016-01-01")
+    model.timestepper.end = pandas.to_datetime("2016-01-04")
+
+    inpt = Input(model, "input")
+    otpt = Output(model, "output")
+    inpt.connect(otpt)
+
+    rec_flow = NumpyArrayNodeRecorder(model, inpt)
+    rec_mean = RollingMeanFlowNodeRecorder(model, node=inpt, timesteps=3)
+
+    scenario = Scenario(model, "dummy", size=2)
+
+    inpt.max_flow = inpt.min_flow = FunctionParameter(model, inpt, lambda model, t, si: 2 + t.index)
+    model.run()
+
+    expected = [
+        2.0,
+        (2.0 + 3.0) / 2,
+        (2.0 + 3.0 + 4.0) / 3,
+        (3.0 + 4.0 + 5.0) / 3,  # zeroth day forgotten
+    ]
+
+    for value, expected_value in zip(rec_mean.data[:, 0], expected):
+        assert_allclose(value, expected_value)
+
+def test_mean_flow_recorder_days():
+    model = Model()
+    model.timestepper.delta = 7
+
+    inpt = Input(model, "input")
+    otpt = Output(model, "output")
+    inpt.connect(otpt)
+
+    rec_mean = RollingMeanFlowNodeRecorder(model, node=inpt, days=31)
+
+    model.run()
+    assert(rec_mean.timesteps == 4)
+
+def test_mean_flow_recorder_json():
+    model = load_model("mean_flow_recorder.json")
+
+    # TODO: it's not possible to define a FunctionParameter in JSON yet
+    supply1 = model.nodes["supply1"]
+    supply1.max_flow = supply1.min_flow = FunctionParameter(model, supply1, lambda model, t, si: 2 + t.index)
+
+    assert(len(model.recorders) == 3)
+
+    rec_flow = model.recorders["Supply"]
+    rec_mean = model.recorders["Mean Flow"]
+    rec_check = model.recorders["Supply 2"]
+
+    model.run()
+
+    assert_allclose(rec_flow.data[:,0], [2.0, 3.0, 4.0, 5.0])
+    assert_allclose(rec_mean.data[:,0], [2.0, 2.5, 3.0, 4.0])
+    assert_allclose(rec_check.data[:,0], [50.0, 50.0, 60.0, 60.0])
+
+def test_annual_count_index_parameter_recorder(simple_storage_model):
+    """ Test AnnualCountIndexParameterRecord
+
+    The test sets uses a simple reservoir model with different inputs that
+    trigger a control curve failure after different numbers of years.
+    """
+    from pywr.parameters import ConstantScenarioParameter, ConstantParameter
+    from pywr.parameters.control_curves import ControlCurveIndexParameter
+    model = simple_storage_model
+    scenario = Scenario(model, 'A', size=2)
+    # Simulate 5 years
+    model.timestepper.start = '2015-01-01'
+    model.timestepper.end = '2019-12-31'
+    # Control curve parameter
+    param = ControlCurveIndexParameter(model, model.nodes['Storage'], ConstantParameter(model, 0.25))
+
+    # Storage model has a capacity of 20, but starts at 10 Ml
+    # Demand is roughly 2 Ml/d per year
+    #  First ensemble balances the demand
+    #  Second ensemble should fail during 3rd year
+    demand = 2.0 / 365
+    model.nodes['Input'].max_flow = ConstantScenarioParameter(model, scenario, [demand, 0])
+    model.nodes['Output'].max_flow = demand
+
+    # Create the recorder with a threshold of 1
+    rec = AnnualCountIndexParameterRecorder(model, param, 1)
+
+    model.run()
+    # We expect no failures in the first ensemble, but 3 out of 5 in the second
+    assert_allclose(rec.values(), [0, 3])
+
+
+# The following fixtures are used for testing the recorders in
+#  pywr.recorders.calibration which require an observed data set
+#  to compare with the model prediction.
+
+@pytest.fixture
+def timeseries2_model():
+    return load_model('timeseries2.json')
+
+
+@pytest.fixture
+def timeseries2_observed():
+    path = os.path.join(os.path.dirname(__file__), 'models')
+    df = pandas.read_csv(os.path.join(path, 'timeseries2.csv'),
+                         parse_dates=True, dayfirst=True, index_col=0)
+    df = df.asfreq(pandas.infer_freq(df.index))
+    # perturb a bit
+    df += np.random.normal(size=df.shape)
+    return df
+
+class TestCalibrationRecorders:
+    data = [
+        (RootMeanSquaredErrorNodeRecorder, lambda sim, obs: np.sqrt(np.mean((sim-obs)**2, axis=0))),
+        (MeanAbsoluteErrorNodeRecorder, lambda sim, obs: np.mean(np.abs(sim-obs), axis=0)),
+        (MeanSquareErrorNodeRecorder, lambda sim, obs: np.mean((sim-obs)**2, axis=0)),
+        (PercentBiasNodeRecorder, lambda sim, obs: np.sum(obs-sim, axis=0)*100/np.sum(obs, axis=0)),
+        (RMSEStandardDeviationRatioNodeRecorder, lambda sim, obs: np.sqrt(np.mean((obs-sim)**2, axis=0))/np.std(obs, axis=0)),
+        (NashSutcliffeEfficiencyNodeRecorder, lambda sim, obs: 1.0 - np.sum((obs-sim)**2, axis=0)/np.sum((obs-obs.mean())**2, axis=0)),
+    ]
+    ids = ["rmse", "mae", "mse", "pbias", "rmse", "ns"]
+
+    @pytest.mark.parametrize("cls,func", data, ids=ids)
+    def test_calibration_recorder(self, timeseries2_model, timeseries2_observed, cls, func):
+        model = timeseries2_model
+        observed = timeseries2_observed
+        node = model.nodes["river1"]
+        recorder = cls(model, node, observed)
+
+        model.run()
+
+        simulated = model.nodes["catchment1"].max_flow.dataframe
+        metric = func(simulated, observed)
+        values = recorder.values()
+        assert(values.shape[0] == len(model.scenarios.combinations))
+        assert(values.ndim == 1)
+        assert_allclose(metric, values)
+
+
+@pytest.fixture
+def cyclical_storage_model(simple_storage_model):
+    """ Extends simple_storage_model to have a cyclical boundary condition """
+    from pywr.parameters import AnnualHarmonicSeriesParameter, ConstantScenarioParameter
+    m = simple_storage_model
+    s = Scenario(m, name='Scenario A', size=3)
+
+    m.timestepper.end = '2017-12-31'
+    m.timestepper.delta = 5
+
+    inpt = m.nodes['Input']
+    inpt.max_flow = AnnualHarmonicSeriesParameter(m, 5, [0.1, 0.0, 0.25], [0.0, 0.0, 0.0])
+
+    otpt = m.nodes['Output']
+    otpt.max_flow = ConstantScenarioParameter(m, s, [5, 6, 2])
+
+    return m
+
+
+@pytest.fixture
+def cyclical_linear_model(simple_linear_model):
+    """ Extends simple_storage_model to have a cyclical boundary condition """
+    from pywr.parameters import AnnualHarmonicSeriesParameter, ConstantScenarioParameter
+    m = simple_linear_model
+    s = Scenario(m, name='Scenario A', size=3)
+
+    m.timestepper.end = '2017-12-31'
+    m.timestepper.delta = 5
+
+    inpt = m.nodes['Input']
+    inpt.max_flow = AnnualHarmonicSeriesParameter(m, 5, [1.0, 0.0, 0.5], [0.0, 0.0, 0.0])
+
+    otpt = m.nodes['Output']
+    otpt.max_flow = ConstantScenarioParameter(m, s, [5, 6, 2])
+    otpt.cost = -10.0
+
+    return m
+
+
+class TestEventRecorder:
+    """ Tests for EventRecorder """
+    funcs = {"min": np.min, "max": np.max, "mean": np.mean, "median": np.median, "sum": np.sum}
+
+    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
+    def test_event_capture_with_storage(self, cyclical_storage_model, recorder_agg_func):
+        """ Test Storage events using a StorageThresholdRecorder """
+        m = cyclical_storage_model
+
+        strg = m.nodes['Storage']
+        arry = NumpyArrayStorageRecorder(m, strg)
+
+        # Create the trigger using a threhsold parameter
+        trigger = StorageThresholdRecorder(m, strg, 4.0, predicate='<=')
+        evt_rec = EventRecorder(m, trigger)
+        evt_dur = EventDurationRecorder(m, evt_rec, recorder_agg_func=recorder_agg_func, agg_func='max')
+
+        m.run()
+
+        # Ensure there is at least one event
+        assert evt_rec.events
+
+        # Build a timeseries of when the events say an event is active
+        triggered = np.zeros_like(arry.data, dtype=np.int)
+        for evt in evt_rec.events:
+            triggered[evt.start.index:evt.end.index, evt.scenario_index.global_id] = 1
+
+            # Check the duration
+            td = evt.end.datetime - evt.start.datetime
+            assert evt.duration == td.days
+
+        #   Test that the volumes in the Storage node during the event periods match
+        assert_equal(triggered, arry.data <= 4)
+
+        df = evt_rec.to_dataframe()
+
+        assert len(df) == len(evt_rec.events)
+
+        func = TestEventRecorder.funcs[recorder_agg_func]
+
+        # Now check the EventDurationRecorder does the aggregation correctly
+        expected_durations = []
+        for si in m.scenarios.combinations:
+            event_durations = []
+            for evt in evt_rec.events:
+                if evt.scenario_index.global_id == si.global_id:
+                    event_durations.append(evt.duration)
+
+            # If there are no events then the metric is zero
+            if len(event_durations) > 0:
+                expected_durations.append(func(event_durations))
+            else:
+                expected_durations.append(0.0)
+
+        assert_allclose(evt_dur.values(), expected_durations)
+        assert_allclose(evt_dur.aggregated_value(), np.max(expected_durations))
+
+    def test_event_capture_with_node(self, cyclical_linear_model):
+        """ Test Node flow events using a NodeThresholdRecorder """
+        m = cyclical_linear_model
+
+        otpt = m.nodes['Output']
+        arry = NumpyArrayNodeRecorder(m, otpt)
+
+        # Create the trigger using a threhsold parameter
+        trigger = NodeThresholdRecorder(m, otpt, 4.0, predicate='>')
+        evt_rec = EventRecorder(m, trigger)
+
+        m.run()
+
+        # Ensure there is at least one event
+        assert evt_rec.events
+
+        # Build a timeseries of when the events say an event is active
+        triggered = np.zeros_like(arry.data, dtype=np.int)
+        for evt in evt_rec.events:
+            triggered[evt.start.index:evt.end.index, evt.scenario_index.global_id] = 1
+
+            # Check the duration
+            td = evt.end.datetime - evt.start.datetime
+            assert evt.duration == td.days
+
+        # Test that the volumes in the Storage node during the event periods match
+        assert_equal(triggered, arry.data > 4)
+
+    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
+    def test_no_event_capture_with_storage(self, cyclical_storage_model, recorder_agg_func):
+        """ Test Storage events using a StorageThresholdRecorder """
+        m = cyclical_storage_model
+
+        strg = m.nodes['Storage']
+        arry = NumpyArrayStorageRecorder(m, strg)
+
+        # Create the trigger using a threhsold parameter
+        trigger = StorageThresholdRecorder(m, strg, -1.0, predicate='<')
+        evt_rec = EventRecorder(m, trigger)
+        evt_dur = EventDurationRecorder(m, evt_rec, recorder_agg_func=recorder_agg_func, agg_func='max')
+
+        m.run()
+
+        # Ensure there are no events in this test
+        assert len(evt_rec.events) == 0
+        df = evt_rec.to_dataframe()
+        assert len(df) == 0
+
+        assert_allclose(evt_dur.values(), np.zeros(len(m.scenarios.combinations)))
+        assert_allclose(evt_dur.aggregated_value(), 0)
+
+    @pytest.mark.parametrize("minimum_length", [1, 2, 3, 4])
+    def test_hysteresis(self, simple_linear_model, minimum_length):
+        """ Test the minimum_event_length keyword of EventRecorder """
+        m = simple_linear_model
+
+        flow = np.zeros(len(m.timestepper))
+
+        flow[:10] = [0, 0, 10, 0, 10, 10, 10, 0, 0, 0]
+        # With min event length of 1. There are two events with lengths (1, 3)
+        #                 |---|   |---------|
+        # With min event length up to 4. There is one event with length 3
+        #                         |---------|
+        # Min event length >= 4 gives no events
+
+        inpt = m.nodes['Input']
+        inpt.max_flow = ArrayIndexedParameter(m, flow)
+
+        # Force through whatever flow Input can provide
+        otpt = m.nodes['Output']
+        otpt.max_flow = 100
+        otpt.cost = -100
+
+        # Create the trigger using a threhsold parameter
+        trigger = NodeThresholdRecorder(m, otpt, 4.0, predicate='>')
+        evt_rec = EventRecorder(m, trigger, minimum_event_length=minimum_length)
+
+        m.run()
+
+        if minimum_length == 1:
+            assert len(evt_rec.events) == 2
+            assert_equal([1, 3], [e.duration for e in evt_rec.events])
+        elif minimum_length < 4:
+            assert len(evt_rec.events) == 1
+            assert_equal([3, ], [e.duration for e in evt_rec.events])
+        else:
+            assert len(evt_rec.events) == 0
+
+    @pytest.mark.parametrize("recorder_agg_func", ["min", "max", "mean", "median", "sum"])
+    def test_statistic_recorder(self, cyclical_storage_model, recorder_agg_func):
+        """ Test EventStatisticRecorder """
+        m = cyclical_storage_model
+
+        strg = m.nodes['Storage']
+        inpt = m.nodes['Input']
+        arry = NumpyArrayNodeRecorder(m, inpt)
+
+        # Create the trigger using a threhsold parameter
+        trigger = StorageThresholdRecorder(m, strg, 4.0, predicate='<=')
+        evt_rec = EventRecorder(m, trigger, tracked_parameter=inpt.max_flow)
+        evt_stat = EventStatisticRecorder(m, evt_rec, agg_func='max', event_agg_func='min', recorder_agg_func=recorder_agg_func)
+
+        m.run()
+
+        # Ensure there is at least one event
+        assert evt_rec.events
+
+        evt_values = {si.global_id:[] for si in m.scenarios.combinations}
+        for evt in evt_rec.events:
+            evt_values[evt.scenario_index.global_id].append(np.min(arry.data[evt.start.index:evt.end.index, evt.scenario_index.global_id]))
+
+        func = TestEventRecorder.funcs[recorder_agg_func]
+
+        agg_evt_values = []
+        for k, v in sorted(evt_values.items()):
+            if len(v) > 0:
+                agg_evt_values.append(func(v))
+            else:
+                agg_evt_values.append(np.nan)
+
+        # Test that the
+        assert_allclose(evt_stat.values(), agg_evt_values)
+        assert_allclose(evt_stat.aggregated_value(), np.max(agg_evt_values))
+
+
+def test_progress_recorder(simple_linear_model):
+    model = simple_linear_model
+    rec = ProgressRecorder(model)
+    model.run()
+
+
+class TestHydroPowerRecorder:
+
+    @pytest.mark.parametrize('efficiency', [1.0, 0.85])
+    def test_constant_level(self, simple_storage_model, efficiency):
+        """ Test HydropowerRecorder """
+        m = simple_storage_model
+
+        strg = m.nodes['Storage']
+        otpt = m.nodes['Output']
+
+        elevation = ConstantParameter(m, 100)
+        rec = HydropowerRecorder(m, otpt, elevation, efficiency=efficiency)
+        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation, efficiency=efficiency)
+
+        m.setup()
+        m.step()
+
+        # First step
+        # Head: 100m
+        # Flow: 8 m3/day
+        # Power: 1000 * 9.81 * 8 * 100
+        # Energy: power * 1 day = power
+        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
+        # Second step has the same answer in this model
+        m.step()
+        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
+        np.testing.assert_allclose(rec_total.values()[0], 2 * 1000 * 9.81 * 8 * 100 * 1e-6 * efficiency)
+
+    def test_varying_level(self, simple_storage_model):
+        """ Test HydropowerRecorder with varying level on Storage node """
+        from pywr.parameters import InterpolatedVolumeParameter
+        m = simple_storage_model
+
+        strg = m.nodes['Storage']
+        otpt = m.nodes['Output']
+
+        elevation = InterpolatedVolumeParameter(m, strg, [0, 10, 20], [0, 100, 200])
+        rec = HydropowerRecorder(m, otpt, elevation)
+        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation)
+
+        m.setup()
+        m.step()
+
+        # First step
+        # Head: 100m
+        # Flow: 8 m3/day
+        # Power: 1000 * 9.81 * 8 * 100
+        # Energy: power * 1 day = power
+        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 100 * 1e-6)
+        # Second step is at a lower head
+        # Head: 70m
+        m.step()
+        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 70 * 1e-6)
+        np.testing.assert_allclose(rec_total.values()[0], 1000 * 9.81 * 8 * 170 * 1e-6)
+
+    def test_varying_level_with_turbine_level(self, simple_storage_model):
+        """ Test HydropowerRecorder with varying level on Storage and defined level on the recorder """
+        from pywr.parameters import InterpolatedVolumeParameter
+        m = simple_storage_model
+
+        strg = m.nodes['Storage']
+        otpt = m.nodes['Output']
+
+        elevation = InterpolatedVolumeParameter(m, strg, [0, 10, 20], [0, 100, 200])
+        rec = HydropowerRecorder(m, otpt, elevation, turbine_elevation=80)
+        rec_total = TotalHydroEnergyRecorder(m, otpt, elevation, turbine_elevation=80)
+
+        m.setup()
+        m.step()
+
+        # First step
+        # Head: 100 - 80 = 20m
+        # Flow: 8 m3/day
+        # Power: 1000 * 9.81 * 8 * 100
+        # Energy: power * 1 day = power
+        np.testing.assert_allclose(rec.data[0, 0], 1000 * 9.81 * 8 * 20 * 1e-6)
+        # Second step is at a lower head
+        # Head: 70 - 80: -10m (i.e. not sufficient)
+        m.step()
+        np.testing.assert_allclose(rec.data[1, 0], 1000 * 9.81 * 8 * 0 * 1e-6)
+        np.testing.assert_allclose(rec_total.values()[0], 1000 * 9.81 * 8 * 20 * 1e-6)
+
+    def test_load_from_json(self, ):
+        """ Test example hydropower model loads and runs. """
+        model = load_model("hydropower_example.json")
+
+        r = model.recorders['turbine1_energy']
+
+        # Check the recorder has loaded correctly
+        assert r.water_elevation_parameter == model.parameters['reservoir1_level']
+        assert r.node == model.nodes['turbine1']
+
+        assert_allclose(r.turbine_elevation, 35.0)
+        assert_allclose(r.efficiency, 0.85)
+        assert_allclose(r.flow_unit_conversion, 1e3)
+
+        # Finally, check model runs with the loaded recorder.
+        model.run()
+
```

### Comparing `pywr-1.8.0/tests/test_river.py` & `pywr-1.9.0/tests/test_river.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,209 +1,209 @@
-"""
-A collection of tests for pywr.domains.river
-
-Specific additional functionality of the 'special' classes in the river domain
-are tested here.
-"""
-import pywr.core
-from pywr.core import Model, Input, Output, Catchment
-from pywr.parameters import MonthlyProfileParameter
-from pywr.domains import river
-from numpy.testing import assert_allclose
-import pytest
-
-from helpers import assert_model, load_model
-
-@pytest.fixture(params=[(10.0, 10.0, 10.0), (5.0, 5.0, 1.0)])
-def simple_gauge_model(request):
-    """
-    Make a simple model with a single Input and Output and RiverGauge
-
-    Input -> PiecewiseLink -> Output
-
-    """
-    in_flow, out_flow, benefit = request.param
-    min_flow_req = 5.0
-
-    model = pywr.core.Model()
-    inpt = river.Catchment(model, name="Catchment", flow=in_flow)
-    lnk = river.RiverGauge(model, name="Gauge", mrf=min_flow_req, mrf_cost=-1.0)
-    inpt.connect(lnk)
-    otpt = pywr.core.Output(model, name="Demand", max_flow=out_flow, cost=-benefit)
-    lnk.connect(otpt)
-
-    default = inpt.domain
-
-    expected_sent = in_flow if benefit > 1.0 else out_flow
-
-    expected_node_results = {
-        "Catchment": expected_sent,
-        "Gauge": expected_sent,
-        "Gauge Sublink 0": min(min_flow_req, expected_sent),
-        "Gauge Sublink 1": expected_sent - min(min_flow_req, expected_sent),
-        "Demand": expected_sent,
-    }
-    return model, expected_node_results
-
-
-@pytest.fixture
-def simple_river_split_gauge_model():
-    """
-    Make a simple model with a single Input and Output and RiverGauge
-
-    ::
-
-        Input -> RiverSplit -> Output 1
-                     \\ --->--> Output 2
-
-    """
-    in_flow = 100.0
-    min_flow_req = 40.0
-    out_flow = 50.0
-    model = pywr.core.Model()
-
-    inpt = river.Catchment(model, name="Catchment", flow=in_flow)
-    lnk = river.RiverSplitWithGauge(model, name="Gauge", mrf=min_flow_req, mrf_cost=-100,
-                                    slot_names=("river", "abstraction"), factors=[3, 1])
-    inpt.connect(lnk)
-    estuary = pywr.core.Output(model, name="Estuary")
-    lnk.connect(estuary, from_slot="river")
-    otpt = pywr.core.Output(model, name="Demand", max_flow=out_flow, cost=-10)
-    lnk.connect(otpt, from_slot="abstraction")
-
-    net_flow_after_mrf = in_flow - min_flow_req
-    expected_node_results = {
-        "Catchment": in_flow,
-        "Gauge": in_flow,
-        "Gauge Sublink 0": min_flow_req,
-        "Gauge Sublink 1": net_flow_after_mrf * 0.75,
-        "Gauge Sublink 2": net_flow_after_mrf * 0.25,
-        "Demand": min(out_flow, net_flow_after_mrf * 0.25),
-    }
-    return model, expected_node_results
-
-
-def test_river_gauge():
-    """
-    Test loading a model with a RiverGauge from JSON, modifying it, then running it
-    """
-    model = load_model("river_mrf1.json")
-
-    node = model.nodes["mrf"]
-    demand = model.nodes["demand"]
-
-    # test getting properties
-    assert(isinstance(node.mrf, MonthlyProfileParameter))
-    assert_allclose(node.mrf_cost, -1000)
-    assert_allclose(node.cost, 0.0)
-
-    # test setting properties
-    node.mrf = 40
-    node.mrf_cost = -999
-    assert_allclose(node.mrf, 40)
-    assert_allclose(node.mrf_cost, -999)
-
-    # run the model and see if it works
-    model.run()
-    assert_allclose(node.flow, 40)
-    assert_allclose(demand.flow, 60)
-
-
-def test_piecewise_model(simple_gauge_model):
-    assert_model(*simple_gauge_model)
-
-
-def test_river_split_gauge(simple_river_split_gauge_model):
-    assert_model(*simple_river_split_gauge_model)
-
-def test_river_split_gauge_json():
-    """As test_river_split_gauge, but model is defined in JSON"""
-
-    model = load_model("river_split_with_gauge1.json")
-    model.check()
-    model.run()
-
-    catchment_flow = 100.0
-    mrf = 40.0
-    expected_demand_flow = (catchment_flow - mrf) * 0.25
-
-    catchment_node = model.nodes["Catchment"]
-    demand_node = model.nodes["Demand"]
-    assert_allclose(catchment_node.flow, catchment_flow)
-    assert_allclose(demand_node.flow, expected_demand_flow)
-
-
-def test_control_curve():
-    """
-    Use a simple model of a Reservoir to test that a control curve
-    behaves as expected.
-
-    The control curve should alter the cost of the Reservoir when it
-    is above or below a particular threshold.
-
-    (flow = 8.0)          (max_flow = 10.0)
-    Catchment -> River -> DemandCentre
-                     |        ^
-    (max_flow = 2.0) v        | (max_flow = 2.0)
-                    Reservoir
-
-    """
-    in_flow = 8
-
-    model = pywr.core.Model()
-    catchment = river.Catchment(model, name="Catchment", flow=in_flow)
-    lnk = river.River(model, name="River")
-    catchment.connect(lnk)
-    demand = pywr.core.Output(model, name="Demand", cost=-10.0, max_flow=10)
-    lnk.connect(demand)
-    from pywr.parameters import ConstantParameter
-    control_curve = ConstantParameter(model, 0.8)
-    reservoir = river.Reservoir(model, name="Reservoir", max_volume=10, cost=-20, above_curve_cost=0.0,
-                                control_curve=control_curve, initial_volume=10)
-    reservoir.inputs[0].max_flow = 2.0
-    reservoir.outputs[0].max_flow = 2.0
-    lnk.connect(reservoir)
-    reservoir.connect(demand)
-
-    model.setup()
-
-    model.step()
-    # Reservoir is currently above control curve. 2 should be taken from the
-    # reservoir
-    assert(reservoir.volume == 8)
-    assert(demand.flow == 10)
-    # Reservoir is still at (therefore above) control curve. So 2 is still taken
-    model.step()
-    assert(reservoir.volume == 6)
-    assert(demand.flow == 10)
-    # Reservoir now below curve. Better to retain volume and divert some of the
-    # inflow
-    model.step()
-    assert(reservoir.volume == 8)
-    assert(demand.flow == 6)
-    # Set the above_curve_cost function to keep filling
-    from pywr.parameters.control_curves import ControlCurveParameter
-    # We know what we're doing with the control_curve Parameter so unset its parent before overriding
-    # the cost parameter.
-    # We need to call setup() again because we're adding a parameter.
-    reservoir.cost = ControlCurveParameter(model, reservoir, control_curve, [-20.0, -20.0])
-    reservoir.initial_volume = 8
-    model.setup()
-    model.step()
-    assert(reservoir.volume == 10)
-    assert(demand.flow == 6)
-
-def test_catchment_many_successors():
-    """Test if node with fixed flow can have multiple successors. See #225"""
-    model = Model()
-    catchment = Catchment(model, "catchment", flow=100)
-    out1 = Output(model, "out1", max_flow=10, cost=-100)
-    out2 = Output(model, "out2", max_flow=15, cost=-50)
-    out3 = Output(model, "out3")
-    catchment.connect(out1)
-    catchment.connect(out2)
-    catchment.connect(out3)
-    model.check()
-    model.run()
-    assert_allclose(out1.flow, 10)
-    assert_allclose(out2.flow, 15)
-    assert_allclose(out3.flow, 75)
+"""
+A collection of tests for pywr.domains.river
+
+Specific additional functionality of the 'special' classes in the river domain
+are tested here.
+"""
+import pywr.core
+from pywr.core import Model, Input, Output, Catchment
+from pywr.parameters import MonthlyProfileParameter
+from pywr.domains import river
+from numpy.testing import assert_allclose
+import pytest
+
+from helpers import assert_model, load_model
+
+@pytest.fixture(params=[(10.0, 10.0, 10.0), (5.0, 5.0, 1.0)])
+def simple_gauge_model(request):
+    """
+    Make a simple model with a single Input and Output and RiverGauge
+
+    Input -> PiecewiseLink -> Output
+
+    """
+    in_flow, out_flow, benefit = request.param
+    min_flow_req = 5.0
+
+    model = pywr.core.Model()
+    inpt = river.Catchment(model, name="Catchment", flow=in_flow)
+    lnk = river.RiverGauge(model, name="Gauge", mrf=min_flow_req, mrf_cost=-1.0)
+    inpt.connect(lnk)
+    otpt = pywr.core.Output(model, name="Demand", max_flow=out_flow, cost=-benefit)
+    lnk.connect(otpt)
+
+    default = inpt.domain
+
+    expected_sent = in_flow if benefit > 1.0 else out_flow
+
+    expected_node_results = {
+        "Catchment": expected_sent,
+        "Gauge": expected_sent,
+        "Gauge Sublink 0": min(min_flow_req, expected_sent),
+        "Gauge Sublink 1": expected_sent - min(min_flow_req, expected_sent),
+        "Demand": expected_sent,
+    }
+    return model, expected_node_results
+
+
+@pytest.fixture
+def simple_river_split_gauge_model():
+    """
+    Make a simple model with a single Input and Output and RiverGauge
+
+    ::
+
+        Input -> RiverSplit -> Output 1
+                     \\ --->--> Output 2
+
+    """
+    in_flow = 100.0
+    min_flow_req = 40.0
+    out_flow = 50.0
+    model = pywr.core.Model()
+
+    inpt = river.Catchment(model, name="Catchment", flow=in_flow)
+    lnk = river.RiverSplitWithGauge(model, name="Gauge", mrf=min_flow_req, mrf_cost=-100,
+                                    slot_names=("river", "abstraction"), factors=[3, 1])
+    inpt.connect(lnk)
+    estuary = pywr.core.Output(model, name="Estuary")
+    lnk.connect(estuary, from_slot="river")
+    otpt = pywr.core.Output(model, name="Demand", max_flow=out_flow, cost=-10)
+    lnk.connect(otpt, from_slot="abstraction")
+
+    net_flow_after_mrf = in_flow - min_flow_req
+    expected_node_results = {
+        "Catchment": in_flow,
+        "Gauge": in_flow,
+        "Gauge Sublink 0": min_flow_req,
+        "Gauge Sublink 1": net_flow_after_mrf * 0.75,
+        "Gauge Sublink 2": net_flow_after_mrf * 0.25,
+        "Demand": min(out_flow, net_flow_after_mrf * 0.25),
+    }
+    return model, expected_node_results
+
+
+def test_river_gauge():
+    """
+    Test loading a model with a RiverGauge from JSON, modifying it, then running it
+    """
+    model = load_model("river_mrf1.json")
+
+    node = model.nodes["mrf"]
+    demand = model.nodes["demand"]
+
+    # test getting properties
+    assert(isinstance(node.mrf, MonthlyProfileParameter))
+    assert_allclose(node.mrf_cost, -1000)
+    assert_allclose(node.cost, 0.0)
+
+    # test setting properties
+    node.mrf = 40
+    node.mrf_cost = -999
+    assert_allclose(node.mrf, 40)
+    assert_allclose(node.mrf_cost, -999)
+
+    # run the model and see if it works
+    model.run()
+    assert_allclose(node.flow, 40)
+    assert_allclose(demand.flow, 60)
+
+
+def test_piecewise_model(simple_gauge_model):
+    assert_model(*simple_gauge_model)
+
+
+def test_river_split_gauge(simple_river_split_gauge_model):
+    assert_model(*simple_river_split_gauge_model)
+
+def test_river_split_gauge_json():
+    """As test_river_split_gauge, but model is defined in JSON"""
+
+    model = load_model("river_split_with_gauge1.json")
+    model.check()
+    model.run()
+
+    catchment_flow = 100.0
+    mrf = 40.0
+    expected_demand_flow = (catchment_flow - mrf) * 0.25
+
+    catchment_node = model.nodes["Catchment"]
+    demand_node = model.nodes["Demand"]
+    assert_allclose(catchment_node.flow, catchment_flow)
+    assert_allclose(demand_node.flow, expected_demand_flow)
+
+
+def test_control_curve():
+    """
+    Use a simple model of a Reservoir to test that a control curve
+    behaves as expected.
+
+    The control curve should alter the cost of the Reservoir when it
+    is above or below a particular threshold.
+
+    (flow = 8.0)          (max_flow = 10.0)
+    Catchment -> River -> DemandCentre
+                     |        ^
+    (max_flow = 2.0) v        | (max_flow = 2.0)
+                    Reservoir
+
+    """
+    in_flow = 8
+
+    model = pywr.core.Model()
+    catchment = river.Catchment(model, name="Catchment", flow=in_flow)
+    lnk = river.River(model, name="River")
+    catchment.connect(lnk)
+    demand = pywr.core.Output(model, name="Demand", cost=-10.0, max_flow=10)
+    lnk.connect(demand)
+    from pywr.parameters import ConstantParameter
+    control_curve = ConstantParameter(model, 0.8)
+    reservoir = river.Reservoir(model, name="Reservoir", max_volume=10, cost=-20, above_curve_cost=0.0,
+                                control_curve=control_curve, initial_volume=10)
+    reservoir.inputs[0].max_flow = 2.0
+    reservoir.outputs[0].max_flow = 2.0
+    lnk.connect(reservoir)
+    reservoir.connect(demand)
+
+    model.setup()
+
+    model.step()
+    # Reservoir is currently above control curve. 2 should be taken from the
+    # reservoir
+    assert(reservoir.volume == 8)
+    assert(demand.flow == 10)
+    # Reservoir is still at (therefore above) control curve. So 2 is still taken
+    model.step()
+    assert(reservoir.volume == 6)
+    assert(demand.flow == 10)
+    # Reservoir now below curve. Better to retain volume and divert some of the
+    # inflow
+    model.step()
+    assert(reservoir.volume == 8)
+    assert(demand.flow == 6)
+    # Set the above_curve_cost function to keep filling
+    from pywr.parameters.control_curves import ControlCurveParameter
+    # We know what we're doing with the control_curve Parameter so unset its parent before overriding
+    # the cost parameter.
+    # We need to call setup() again because we're adding a parameter.
+    reservoir.cost = ControlCurveParameter(model, reservoir, control_curve, [-20.0, -20.0])
+    reservoir.initial_volume = 8
+    model.setup()
+    model.step()
+    assert(reservoir.volume == 10)
+    assert(demand.flow == 6)
+
+def test_catchment_many_successors():
+    """Test if node with fixed flow can have multiple successors. See #225"""
+    model = Model()
+    catchment = Catchment(model, "catchment", flow=100)
+    out1 = Output(model, "out1", max_flow=10, cost=-100)
+    out2 = Output(model, "out2", max_flow=15, cost=-50)
+    out3 = Output(model, "out3")
+    catchment.connect(out1)
+    catchment.connect(out2)
+    catchment.connect(out3)
+    model.check()
+    model.run()
+    assert_allclose(out1.flow, 10)
+    assert_allclose(out2.flow, 15)
+    assert_allclose(out3.flow, 75)
```

### Comparing `pywr-1.8.0/tests/test_run.py` & `pywr-1.9.0/tests/test_run.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,595 +1,764 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-
-import os
-import datetime
-import pytest
-import pandas
-from numpy.testing import assert_allclose
-
-import pywr.core
-from pywr.model import Model, ModelStructureError, ModelResult
-from pywr.nodes import Storage, Input, Output, Link
-import pywr.solvers
-import pywr.parameters.licenses
-import pywr.domains.river
-from pywr.recorders import assert_rec
-from helpers import load_model
-
-import pywr.parameters
-
-
-def test_run_simple1():
-    '''Test the most basic model possible'''
-    # parse the JSON into a model
-    model = load_model('simple1.json')
-
-    # run the model
-    t0 = model.timestepper.start.to_pydatetime()
-    model.step()
-
-    # check results
-    demand1 = model.nodes['demand1']
-    assert_allclose(demand1.flow, 10.0, atol=1e-7)
-    # initially the timestepper returns the first time-step, so timestepper.current
-    # does not change after the first 'step'.
-    assert(model.timestepper.current.datetime - t0 == datetime.timedelta(0))
-    # check the timestamp incremented
-    model.step()
-    assert(model.timestepper.current.datetime - t0 == datetime.timedelta(1))
-
-def test_model_results():
-    '''Test model results object'''
-    import pywr
-
-    model = load_model('simple1.json')
-    res = model.run()
-    assert (isinstance(res, ModelResult))
-    assert (res.timesteps == 365)
-    assert (res.version == pywr.__version__)
-    assert res.solver_stats['number_of_cols']
-    assert res.solver_stats['number_of_rows']
-    assert res.solver_name == model.solver.name
-    print(res)
-    print(res._repr_html_())
-
-
-@pytest.mark.parametrize("json_file", ['reservoir1.json', 'reservoir1_pc.json'])
-def test_run_reservoir1(json_file):
-    '''Test a reservoir with no refill
-
-    Without an additional supply the reservoir should empty and cause a failure.
-    '''
-    model = load_model(json_file)
-    demand1 = model.nodes['demand1']
-    supply1 = model.nodes['supply1']
-    for demand, stored in [(10.0, 25.0), (10.0, 15.0), (10.0, 5.0), (5.0, 0.0), (0.0, 0.0)]:
-        result = model.step()
-        assert_allclose(demand1.flow, demand, atol=1e-7)
-        assert_allclose(supply1.volume, stored, atol=1e-7)
-
-
-def test_run_reservoir2():
-    '''Test a reservoir fed by a river abstraction
-
-    The river abstraction should refill the reservoir, but not quickly enough
-    to keep up with the demand.
-    '''
-    model = load_model('reservoir2.json')
-
-    demand1 = model.nodes['demand1']
-    supply1 = model.nodes['supply1']
-    catchment = model.nodes['catchment1']
-    assert(catchment.min_flow == 5)
-    for demand, stored in [(15.0, 25.0), (15.0, 15.0), (15.0, 5.0), (10.0, 0.0), (5.0, 0.0)]:
-        result = model.step()
-        assert_allclose(demand1.flow[0], demand, atol=1e-7)
-        assert_allclose(supply1.volume[0], stored, atol=1e-7)
-
-def test_empty_storage_min_flow():
-
-    model = Model()
-    storage = Storage(model, "storage", initial_volume=100, max_volume=100, num_inputs=1, num_outputs=0)
-    otpt = Output(model, "output", min_flow=75)
-    storage.connect(otpt)
-    model.check()
-    model.step()
-    with pytest.raises(RuntimeError):
-        model.step()
-
-def test_run_river1():
-    '''Test a river abstraction with a simple catchment'''
-    model = load_model('river1.json')
-
-    result = model.step()
-    demand1 = model.nodes['demand1']
-    assert_allclose(demand1.flow, 5.0, atol=1e-7)
-
-
-def test_run_river2():
-    '''Test a river abstraction with two catchments, a confluence and a split'''
-    model = load_model('river2.json')
-
-    model.step()
-
-    demand1 = model.nodes['demand1']
-    assert_allclose(demand1.flow, 7.25, atol=1e-7)
-    demand2 = model.nodes['demand2']
-    assert_allclose(demand2.flow, 2.0, atol=1e-7)
-
-
-# Contains an out of range date for pandas.to_datetime
-@pytest.mark.parametrize("json_file", ['timeseries1.json', 'timeseries1_xlsx.json'])
-def test_run_timeseries1(json_file):
-    model = load_model(json_file)
-
-    # check first day initalised
-    assert(model.timestepper.start == datetime.datetime(2015, 1, 1))
-
-    # check results
-    demand1 = model.nodes['demand1']
-    catchment1 = model.nodes['catchment1']
-    for expected in (23.92, 22.14, 22.57, 24.97, 27.59):
-        result = model.step()
-        assert_allclose(catchment1.flow, expected, atol=1e-7)
-        assert_allclose(demand1.flow, min(expected, 23.0), atol=1e-7)
-
-
-# Contains an out of range date for pandas.to_datetime
-@pytest.mark.parametrize("json_file", ['timeseries1_weekly.json', 'timeseries1_weekly_hdf.json'])
-def test_run_timeseries1_weekly(json_file):
-    model = load_model(json_file)
-
-    # check first day initalised
-    assert(model.timestepper.start == datetime.datetime(2015, 1, 1))
-
-    # check results
-    demand1 = model.nodes['demand1']
-    catchment1 = model.nodes['catchment1']
-    for expected in (23.92, 25.67, 28.24, 25.28, 21.84):
-        result = model.step()
-        assert_allclose(catchment1.flow, expected, atol=1e-7)
-        assert_allclose(demand1.flow, min(expected, 23.0), atol=1e-7)
-
-
-def test_run_cost1():
-    model = load_model('cost1.json')
-
-    supply1 = model.nodes['supply1']
-    supply2 = model.nodes['supply2']
-    demand1 = model.nodes['demand1']
-
-    assert_allclose(supply1.get_cost(None), 1)
-    assert_allclose(supply2.get_cost(None), 2)  # more expensive
-
-    result = model.step()
-    # check entire demand was supplied by supply1
-    assert_allclose(supply1.flow, 10.0, atol=1e-7)
-    assert_allclose(supply2.flow, 0.0, atol=1e-7)
-    assert_allclose(demand1.flow, 10.0, atol=1e-7)
-
-    # increase demand to more than supply1 can provide on it's own
-    # and check that supply2 is used to pick up the slack
-    demand1.max_flow = 20.0
-    result = model.step()
-    assert_allclose(supply1.flow, 15.0, atol=1e-7)
-    assert_allclose(supply2.flow, 5.0, atol=1e-7)
-    assert_allclose(demand1.flow, 20.0, atol=1e-7)
-
-    # supply as much as possible, even if it isn't enough
-    demand1.max_flow = 40.0
-    result = model.step()
-    assert_allclose(supply1.flow, 15.0, atol=1e-7)
-    assert_allclose(supply2.flow, 15.0, atol=1e-7)
-    assert_allclose(demand1.flow, 30.0, atol=1e-7)
-
-
-def test_run_bottleneck():
-    '''Test max flow constraint on intermediate nodes is upheld'''
-    model = load_model('bottleneck.json')
-    result = model.step()
-    d1 = model.nodes['demand1']
-    d2 = model.nodes['demand2']
-    assert_allclose(d1.flow+d2.flow, 15.0, atol=1e-7)
-
-@pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-def test_run_discharge_upstream():
-    '''Test river with inline discharge (upstream)
-
-    In this instance the discharge is upstream of the abstraction, and so can
-    be abstracted in the same way as the water from the catchment
-    '''
-    model = load_model('river_discharge1.json')
-    model.step()
-    demand = model.nodes['demand1']
-    term = model.nodes['term1']
-    assert_allclose(demand.flow, 8.0, atol=1e-7)
-    assert_allclose(term.flow, 0.0, atol=1e-7)
-
-@pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
-def test_run_discharge_downstream():
-    '''Test river with inline discharge (downstream)
-
-    In this instance the discharge is downstream of the abstraction, so the
-    water shouldn't be available.
-    '''
-    model = load_model('river_discharge2.json')
-    model.step()
-    demand = model.nodes['demand1']
-    term = model.nodes['term1']
-    assert_allclose(demand.flow, 5.0, atol=1e-7)
-    assert_allclose(term.flow, 3.0, atol=1e-7)
-
-def test_new_storage():
-    """Test new-style storage node with multiple inputs"""
-    model = pywr.core.Model(
-        start=pandas.to_datetime('1888-01-01'),
-        end=pandas.to_datetime('1888-01-01'),
-        timestep=datetime.timedelta(1)
-    )
-
-    supply1 = pywr.core.Input(model, 'supply1')
-
-    splitter = pywr.core.Storage(model, 'splitter', num_outputs=1, num_inputs=2, max_volume=10, initial_volume=5)
-
-    demand1 = pywr.core.Output(model, 'demand1')
-    demand2 = pywr.core.Output(model, 'demand2')
-
-    supply1.connect(splitter)
-
-    splitter.connect(demand1, from_slot=0)
-    splitter.connect(demand2, from_slot=1)
-
-    supply1.max_flow = 45.0
-    demand1.max_flow = 20
-    demand2.max_flow = 40
-
-    demand1.cost = -150
-    demand2.cost = -100
-
-    model.run()
-
-    assert_allclose(supply1.flow, [45], atol=1e-7)
-    assert_allclose(splitter.volume, [0], atol=1e-7)  # New volume is zero
-    assert_allclose(demand1.flow, [20], atol=1e-7)
-    assert_allclose(demand2.flow, [30], atol=1e-7)
-
-
-def test_virtual_storage():
-    """ Test the VirtualStorage node """
-
-    model = pywr.core.Model()
-
-    inpt = Input(model, "Input", max_flow=20)
-    lnk = Link(model, "Link")
-    inpt.connect(lnk)
-    otpt = Output(model, "Output", max_flow=10, cost=-10.0)
-    lnk.connect(otpt)
-
-    vs = pywr.core.VirtualStorage(model, "Licence", [lnk], initial_volume=10.0, max_volume=10.0)
-
-    model.setup()
-
-    assert_allclose(vs.volume, [10], atol=1e-7)
-
-    model.step()
-
-    assert_allclose(otpt.flow, [10], atol=1e-7)
-    assert_allclose(vs.volume, [0], atol=1e-7)
-
-    model.step()
-
-    assert_allclose(otpt.flow, [0], atol=1e-7)
-    assert_allclose(vs.volume, [0], atol=1e-7)
-
-
-def test_virtual_storage_duplicate_route():
-    """ Test the VirtualStorage node """
-
-    model = pywr.core.Model()
-
-    inpt = Input(model, "Input", max_flow=20)
-    lnk = Link(model, "Link")
-    inpt.connect(lnk)
-    otpt = Output(model, "Output", max_flow=10, cost=-10.0)
-    lnk.connect(otpt)
-
-    vs = pywr.core.VirtualStorage(model, "Licence", [lnk, otpt], factors=[0.5, 1.0], initial_volume=10.0, max_volume=10.0)
-
-    model.setup()
-
-    assert_allclose(vs.volume, [10], atol=1e-7)
-
-    model.step()
-
-    assert_allclose(otpt.flow, [10/1.5], atol=1e-7)
-    assert_allclose(vs.volume, [0], atol=1e-7)
-
-    model.step()
-
-    assert_allclose(otpt.flow, [0], atol=1e-7)
-    assert_allclose(vs.volume, [0], atol=1e-7)
-
-
-def test_annual_virtual_storage():
-    model = load_model('virtual_storage1.json')
-    model.run()
-    node = model.nodes["supply1"]
-    rec = node.recorders[0]
-    assert_allclose(rec.data[0], 10) # licence is not a constraint
-    assert_allclose(rec.data[19], 10)
-    assert_allclose(rec.data[20], 5) # licence is constraint
-    assert_allclose(rec.data[21], 0) # licence is exhausted
-    assert_allclose(rec.data[365], 10) # licence is refreshed
-
-
-@pytest.mark.parametrize('reset_to_initial_volume', [None, False, True])
-def test_annual_virtual_storage_reset_to_max_volume(reset_to_initial_volume):
-    """Test that AnnualVirtualStorage resets to maximum volume. """
-    model = load_model('virtual_storage1.json')
-    licence1 = model.nodes['licence1']
-    if reset_to_initial_volume is not None:
-        licence1.reset_to_initial_volume = reset_to_initial_volume
-    licence1.initial_volume = 100
-    licence1.reset_month = 4
-
-    model.setup()
-    # After reset the current volume is always the initial volume
-    assert_allclose(licence1.volume, [100.0])
-
-
-    model.timestepper.start = '2015-04-01'
-    model.reset()
-    # After stepping over the reset day the volume should have been reset 
-    # before the solve to either initial volume or maximum volume.
-    model.step()
-    if reset_to_initial_volume:
-        expected_volume = 90.0
-    else:
-        expected_volume = 195.0
-    assert_allclose(licence1.volume, [expected_volume])
-
-
-def test_annual_virtual_storage_with_dynamic_cost():
-    model = load_model('virtual_storage2.json')
-    model.run()
-    node = model.nodes["supply1"]
-    rec = node.recorders[0]
-
-    assert_allclose(rec.data[0], 10)  # licence is not a constraint
-    assert_allclose(rec.data[1], 5)  # now used slightly too much; switch to the other source
-    assert_allclose(rec.data[2], 10)  # continue back and forth.
-    assert_allclose(rec.data[3], 5)
-
-
-def test_storage_spill_compensation():
-    """Test storage spill and compensation flows
-
-    The upstream catchment has min_flow == max_flow, so it "pushes" water into
-    the reservoir. The reservoir is already at it's maximum volume, so the
-    water must go *somewhere*. The compensation flow has the most negative cost,
-    so that is satisfied first. Once that is full, the demand is supplied.
-    Finally, any surplus is forced into the spill despite the cost.
-
-    Catchment -> Reservoir -> Demand
-                         |--> Spill        --|
-                         |--> Compensation --|
-                                             |--> Terminator
-    """
-    model = pywr.core.Model()
-
-    catchment = pywr.core.Input(model, name="Input", min_flow=10.0, max_flow=10.0, cost=1)
-    reservoir = pywr.core.Storage(model, name="Storage", max_volume=100, initial_volume=100.0)
-    spill = pywr.core.Link(model, name="Spill", cost=1.0)
-    compensation = pywr.core.Link(model, name="Compensation", max_flow=3.0, cost=-999)
-    terminator = pywr.core.Output(model, name="Terminator", cost=-1.0)
-    demand = pywr.core.Output(model, name="Demand", max_flow=5.0, cost=-500)
-
-    catchment.connect(reservoir)
-    reservoir.connect(spill)
-    reservoir.connect(compensation)
-    reservoir.connect(demand)
-    spill.connect(terminator)
-    compensation.connect(terminator)
-
-    model.check()
-    model.run()
-    assert_allclose(catchment.flow[0], 10.0, atol=1e-7)
-    assert_allclose(demand.flow[0], 5.0, atol=1e-7)
-    assert_allclose(compensation.flow[0], 3.0, atol=1e-7)
-    assert_allclose(spill.flow[0], 2.0, atol=1e-7)
-    assert_allclose(terminator.flow[0], (compensation.flow[0] + spill.flow[0]), atol=1e-7)
-
-
-def test_reservoir_circle():
-    """
-    Issue #140. A model with a circular route, from a reservoir Input back
-    around to it's own Output.
-
-                 Demand
-                    ^
-                    |
-                Reservoir <- Pumping
-                    |           ^
-                    v           |
-              Compensation      |
-                    |           |
-                    v           |
-    Catchment -> River 1 -> River 2 ----> MRFA -> Waste
-                                    |              ^
-                                    |---> MRFB ----|
-    """
-    model = Model()
-
-    catchment = Input(model, "catchment", max_flow=500, min_flow=500)
-
-    reservoir = Storage(model, "reservoir", max_volume=10000, initial_volume=5000)
-
-    demand = Output(model, "demand", max_flow=50, cost=-100)
-    pumping_station = Link(model, "pumping station", max_flow=100, cost=-10)
-    river1 = Link(model, "river1")
-    river2 = Link(model, "river2")
-    compensation = Link(model, "compensation", cost=600)
-    mrfA = Link(model, "mrfA", cost=-500, max_flow=50)
-    mrfB = Link(model, "mrfB")
-    waste = Output(model, "waste")
-
-    catchment.connect(river1)
-    river1.connect(river2)
-    river2.connect(mrfA)
-    river2.connect(mrfB)
-    mrfA.connect(waste)
-    mrfB.connect(waste)
-    river2.connect(pumping_station)
-    pumping_station.connect(reservoir)
-    reservoir.connect(compensation)
-    compensation.connect(river1)
-    reservoir.connect(demand)
-
-    model.check()
-    model.setup()
-
-    # not limited by mrf, pump capacity is constraint
-    model.step()
-    assert_allclose(catchment.flow, 500)
-    assert_allclose(waste.flow, 400)
-    assert_allclose(compensation.flow, 0)
-    assert_allclose(pumping_station.flow, 100)
-    assert_allclose(demand.flow, 50)
-
-    # limited by mrf
-    catchment.min_flow = catchment.max_flow = 100
-    model.step()
-    assert_allclose(waste.flow, 50)
-    assert_allclose(compensation.flow, 0)
-    assert_allclose(pumping_station.flow, 50)
-    assert_allclose(demand.flow, 50)
-
-    # reservoir can support mrf, but doesn't need to
-    compensation.cost = 200
-    model.step()
-    assert_allclose(waste.flow, 50)
-    assert_allclose(compensation.flow, 0)
-    assert_allclose(pumping_station.flow, 50)
-    assert_allclose(demand.flow, 50)
-
-    # reservoir supporting mrf
-    catchment.min_flow = catchment.max_flow = 0
-    model.step()
-    assert_allclose(waste.flow, 50)
-    assert_allclose(compensation.flow, 50)
-    assert_allclose(pumping_station.flow, 0)
-    assert_allclose(demand.flow, 50)
-
-
-def test_breaklink_node():
-    model = load_model('breaklink.json')
-    supply = model.nodes["A"]
-    transfer = model.nodes["B"]
-    demand = model.nodes["C"]
-    model.check()
-    model.run()
-    assert_allclose(supply.flow, 20)
-    assert_allclose(transfer.flow, 20)
-    assert_allclose(demand.flow, 20)
-    assert_allclose(transfer.storage.volume, 0)
-
-
-@pytest.mark.xfail(reason="Circular dependency in the JSON definition. "
-                          "See GitHub issue #380: https://github.com/pywr/pywr/issues/380")
-def test_reservoir_surface_area():
-    from pywr.parameters import InterpolatedVolumeParameter
-    model = load_model('reservoir_evaporation.json')
-    model.timestepper.start = "1920-01-01"
-    model.timestepper.end = "1920-01-02"
-    res = model.run()
-    assert (hasattr(Storage, area))
-    assert isinstance(model.nodes["reservoir1"].area, InterpolatedVolumeParameter)
-    assert_allclose(model.nodes["evaporation"].flow, 2.46875)
-
-
-def test_reservoir_surface_area_without_area_property():
-    """ Temporary test while the above test is not working. """
-    model = load_model('reservoir_evaporation_without_area_property.json')
-    model.timestepper.start = "1920-01-01"
-    model.timestepper.end = "1920-01-02"
-    res = model.run()
-    assert_allclose(model.nodes["evaporation"].flow, 2.46875)
-
-
-def test_run_empty():
-    # empty model should raise an exception if run
-    model = Model()
-    with pytest.raises(ModelStructureError):
-        model.run()
-
-
-def test_run():
-    model = load_model('simple1.json')
-
-    # run model from start to finish
-    result = model.run()
-    assert(result.timestep.index == 364)
-
-    # reset model and run again
-    result = model.run()
-    assert(result.timestep.index == 364)
-
-    # run remaining timesteps
-    model.reset(start=pandas.to_datetime('2015-12-01'))
-    result = model.run()
-    assert(result.timestep.index == 364)
-
-
-def test_reset_prev_flow():
-    """Test resetting the prev_flow attribte of a node."""
-    model = load_model('simple1.json')
-    demand1 = model.nodes['demand1']
-    model.run()
-    assert_allclose(demand1.flow, 10.0, atol=1e-7)
-    assert_allclose(demand1.prev_flow, 10.0, atol=1e-7)
-    # Reset and check flow attributes are zeroed
-    model.reset()
-    assert_allclose(demand1.flow, 0.0, atol=1e-7)
-    assert_allclose(demand1.prev_flow, 0.0, atol=1e-7)
-    # Run again
-    model.run()
-    assert_allclose(demand1.flow, 10.0, atol=1e-7)
-    assert_allclose(demand1.prev_flow, 10.0, atol=1e-7)
-
-
-def test_run_monthly():
-    model = load_model('simple1_monthly.json')
-
-    result = model.run()
-    assert result.timestep.index == 11
-
-    result = model.run()
-    assert result.timestep.index == 11
-
-
-def test_select_solver():
-    """Test specifying the solver in JSON"""
-    solver_names = [solver.name for solver in pywr.solvers.solver_registry]
-    for solver_name in solver_names:
-        data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s"}}''' % solver_name
-        model = load_model(data=data)
-        assert(model.solver.name.lower() == solver_name)
-
-def test_solver_unrecognised():
-    '''Test specifying an unrecognised solver JSON'''
-    solver_name = 'foobar'
-    data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s"}}''' % solver_name
-    with pytest.raises(KeyError):
-        model = load_model(data=data)
-
-@pytest.mark.skipif(Model().solver.name != "glpk", reason="only valid for glpk")
-@pytest.mark.parametrize("use_presolve", ["true", "false"])
-def test_select_glpk_presolve(use_presolve):
-    """Test specifying the solver in JSON"""
-    solver_names = ["glpk"]
-    for solver_name in solver_names:
-        data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s", "use_presolve": %s}}''' % (solver_name, use_presolve)
-        model = load_model(data=data)
-        assert(model.solver.name.lower() == solver_name)
-        assert(model.solver._cy_solver.use_presolve == (use_presolve == "true"))
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import os
+import datetime
+import pytest
+import pandas
+from numpy.testing import assert_allclose
+
+import pywr.core
+from pywr.model import Model, ModelStructureError, ModelResult
+from pywr.nodes import Storage, Input, Output, Link
+import pywr.solvers
+import pywr.parameters.licenses
+import pywr.domains.river
+from pywr.recorders import assert_rec
+from helpers import load_model
+
+import pywr.parameters
+
+
+def test_run_simple1():
+    '''Test the most basic model possible'''
+    # parse the JSON into a model
+    model = load_model('simple1.json')
+
+    # run the model
+    t0 = model.timestepper.start.to_pydatetime()
+    model.step()
+
+    # check results
+    demand1 = model.nodes['demand1']
+    assert_allclose(demand1.flow, 10.0, atol=1e-7)
+    # initially the timestepper returns the first time-step, so timestepper.current
+    # does not change after the first 'step'.
+    assert(model.timestepper.current.datetime - t0 == datetime.timedelta(0))
+    # check the timestamp incremented
+    model.step()
+    assert(model.timestepper.current.datetime - t0 == datetime.timedelta(1))
+
+def test_model_results():
+    '''Test model results object'''
+    import pywr
+
+    model = load_model('simple1.json')
+    res = model.run()
+    assert (isinstance(res, ModelResult))
+    assert (res.timesteps == 365)
+    assert (res.version == pywr.__version__)
+    assert res.solver_stats['number_of_cols']
+    assert res.solver_stats['number_of_rows']
+    assert res.solver_name == model.solver.name
+    print(res)
+    print(res._repr_html_())
+
+
+@pytest.mark.parametrize("json_file", ['reservoir1.json', 'reservoir1_pc.json'])
+def test_run_reservoir1(json_file):
+    '''Test a reservoir with no refill
+
+    Without an additional supply the reservoir should empty and cause a failure.
+    '''
+    model = load_model(json_file)
+    demand1 = model.nodes['demand1']
+    supply1 = model.nodes['supply1']
+    for demand, stored in [(10.0, 25.0), (10.0, 15.0), (10.0, 5.0), (5.0, 0.0), (0.0, 0.0)]:
+        result = model.step()
+        assert_allclose(demand1.flow, demand, atol=1e-7)
+        assert_allclose(supply1.volume, stored, atol=1e-7)
+
+
+def test_run_reservoir2():
+    '''Test a reservoir fed by a river abstraction
+
+    The river abstraction should refill the reservoir, but not quickly enough
+    to keep up with the demand.
+    '''
+    model = load_model('reservoir2.json')
+
+    demand1 = model.nodes['demand1']
+    supply1 = model.nodes['supply1']
+    catchment = model.nodes['catchment1']
+    assert(catchment.min_flow == 5)
+    for demand, stored in [(15.0, 25.0), (15.0, 15.0), (15.0, 5.0), (10.0, 0.0), (5.0, 0.0)]:
+        result = model.step()
+        assert_allclose(demand1.flow[0], demand, atol=1e-7)
+        assert_allclose(supply1.volume[0], stored, atol=1e-7)
+
+def test_empty_storage_min_flow():
+
+    model = Model()
+    storage = Storage(model, "storage", initial_volume=100, max_volume=100, num_inputs=1, num_outputs=0)
+    otpt = Output(model, "output", min_flow=75)
+    storage.connect(otpt)
+    model.check()
+    model.step()
+    with pytest.raises(RuntimeError):
+        model.step()
+
+def test_run_river1():
+    '''Test a river abstraction with a simple catchment'''
+    model = load_model('river1.json')
+
+    result = model.step()
+    demand1 = model.nodes['demand1']
+    assert_allclose(demand1.flow, 5.0, atol=1e-7)
+
+
+def test_run_river2():
+    '''Test a river abstraction with two catchments, a confluence and a split'''
+    model = load_model('river2.json')
+
+    model.step()
+
+    demand1 = model.nodes['demand1']
+    assert_allclose(demand1.flow, 7.25, atol=1e-7)
+    demand2 = model.nodes['demand2']
+    assert_allclose(demand2.flow, 2.0, atol=1e-7)
+
+
+# Contains an out of range date for pandas.to_datetime
+@pytest.mark.parametrize("json_file", ['timeseries1.json', 'timeseries1_xlsx.json'])
+def test_run_timeseries1(json_file):
+    model = load_model(json_file)
+
+    # check first day initalised
+    assert(model.timestepper.start == datetime.datetime(2015, 1, 1))
+
+    # check results
+    demand1 = model.nodes['demand1']
+    catchment1 = model.nodes['catchment1']
+    for expected in (23.92, 22.14, 22.57, 24.97, 27.59):
+        result = model.step()
+        assert_allclose(catchment1.flow, expected, atol=1e-7)
+        assert_allclose(demand1.flow, min(expected, 23.0), atol=1e-7)
+
+
+# Contains an out of range date for pandas.to_datetime
+@pytest.mark.parametrize("json_file", ['timeseries1_weekly.json', 'timeseries1_weekly_hdf.json'])
+def test_run_timeseries1_weekly(json_file):
+    model = load_model(json_file)
+
+    # check first day initalised
+    assert(model.timestepper.start == datetime.datetime(2015, 1, 1))
+
+    # check results
+    demand1 = model.nodes['demand1']
+    catchment1 = model.nodes['catchment1']
+    for expected in (23.92, 25.67, 28.24, 25.28, 21.84):
+        result = model.step()
+        assert_allclose(catchment1.flow, expected, atol=1e-7)
+        assert_allclose(demand1.flow, min(expected, 23.0), atol=1e-7)
+
+
+def test_run_cost1():
+    model = load_model('cost1.json')
+
+    supply1 = model.nodes['supply1']
+    supply2 = model.nodes['supply2']
+    demand1 = model.nodes['demand1']
+
+    assert_allclose(supply1.get_cost(None), 1)
+    assert_allclose(supply2.get_cost(None), 2)  # more expensive
+
+    result = model.step()
+    # check entire demand was supplied by supply1
+    assert_allclose(supply1.flow, 10.0, atol=1e-7)
+    assert_allclose(supply2.flow, 0.0, atol=1e-7)
+    assert_allclose(demand1.flow, 10.0, atol=1e-7)
+
+    # increase demand to more than supply1 can provide on it's own
+    # and check that supply2 is used to pick up the slack
+    demand1.max_flow = 20.0
+    result = model.step()
+    assert_allclose(supply1.flow, 15.0, atol=1e-7)
+    assert_allclose(supply2.flow, 5.0, atol=1e-7)
+    assert_allclose(demand1.flow, 20.0, atol=1e-7)
+
+    # supply as much as possible, even if it isn't enough
+    demand1.max_flow = 40.0
+    result = model.step()
+    assert_allclose(supply1.flow, 15.0, atol=1e-7)
+    assert_allclose(supply2.flow, 15.0, atol=1e-7)
+    assert_allclose(demand1.flow, 30.0, atol=1e-7)
+
+
+def test_run_bottleneck():
+    '''Test max flow constraint on intermediate nodes is upheld'''
+    model = load_model('bottleneck.json')
+    result = model.step()
+    d1 = model.nodes['demand1']
+    d2 = model.nodes['demand2']
+    assert_allclose(d1.flow+d2.flow, 15.0, atol=1e-7)
+
+@pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+def test_run_discharge_upstream():
+    '''Test river with inline discharge (upstream)
+
+    In this instance the discharge is upstream of the abstraction, and so can
+    be abstracted in the same way as the water from the catchment
+    '''
+    model = load_model('river_discharge1.json')
+    model.step()
+    demand = model.nodes['demand1']
+    term = model.nodes['term1']
+    assert_allclose(demand.flow, 8.0, atol=1e-7)
+    assert_allclose(term.flow, 0.0, atol=1e-7)
+
+@pytest.mark.skipif(Model().solver.name == "glpk-edge", reason="Not valid for GLPK Edge based solver.")
+def test_run_discharge_downstream():
+    '''Test river with inline discharge (downstream)
+
+    In this instance the discharge is downstream of the abstraction, so the
+    water shouldn't be available.
+    '''
+    model = load_model('river_discharge2.json')
+    model.step()
+    demand = model.nodes['demand1']
+    term = model.nodes['term1']
+    assert_allclose(demand.flow, 5.0, atol=1e-7)
+    assert_allclose(term.flow, 3.0, atol=1e-7)
+
+def test_new_storage():
+    """Test new-style storage node with multiple inputs"""
+    model = pywr.core.Model(
+        start=pandas.to_datetime('1888-01-01'),
+        end=pandas.to_datetime('1888-01-01'),
+        timestep=datetime.timedelta(1)
+    )
+
+    supply1 = pywr.core.Input(model, 'supply1')
+
+    splitter = pywr.core.Storage(model, 'splitter', num_outputs=1, num_inputs=2, max_volume=10, initial_volume=5)
+
+    demand1 = pywr.core.Output(model, 'demand1')
+    demand2 = pywr.core.Output(model, 'demand2')
+
+    supply1.connect(splitter)
+
+    splitter.connect(demand1, from_slot=0)
+    splitter.connect(demand2, from_slot=1)
+
+    supply1.max_flow = 45.0
+    demand1.max_flow = 20
+    demand2.max_flow = 40
+
+    demand1.cost = -150
+    demand2.cost = -100
+
+    model.run()
+
+    assert_allclose(supply1.flow, [45], atol=1e-7)
+    assert_allclose(splitter.volume, [0], atol=1e-7)  # New volume is zero
+    assert_allclose(demand1.flow, [20], atol=1e-7)
+    assert_allclose(demand2.flow, [30], atol=1e-7)
+
+
+def test_virtual_storage():
+    """ Test the VirtualStorage node """
+
+    model = pywr.core.Model()
+
+    inpt = Input(model, "Input", max_flow=20)
+    lnk = Link(model, "Link")
+    inpt.connect(lnk)
+    otpt = Output(model, "Output", max_flow=10, cost=-10.0)
+    lnk.connect(otpt)
+
+    vs = pywr.core.VirtualStorage(model, "Licence", [lnk], initial_volume=10.0, max_volume=10.0)
+
+    model.setup()
+
+    assert_allclose(vs.volume, [10], atol=1e-7)
+
+    model.step()
+
+    assert_allclose(otpt.flow, [10], atol=1e-7)
+    assert_allclose(vs.volume, [0], atol=1e-7)
+
+    model.step()
+
+    assert_allclose(otpt.flow, [0], atol=1e-7)
+    assert_allclose(vs.volume, [0], atol=1e-7)
+
+
+def test_virtual_storage_duplicate_route():
+    """ Test the VirtualStorage node """
+
+    model = pywr.core.Model()
+
+    inpt = Input(model, "Input", max_flow=20)
+    lnk = Link(model, "Link")
+    inpt.connect(lnk)
+    otpt = Output(model, "Output", max_flow=10, cost=-10.0)
+    lnk.connect(otpt)
+
+    vs = pywr.core.VirtualStorage(model, "Licence", [lnk, otpt], factors=[0.5, 1.0], initial_volume=10.0, max_volume=10.0)
+
+    model.setup()
+
+    assert_allclose(vs.volume, [10], atol=1e-7)
+
+    model.step()
+
+    assert_allclose(otpt.flow, [10/1.5], atol=1e-7)
+    assert_allclose(vs.volume, [0], atol=1e-7)
+
+    model.step()
+
+    assert_allclose(otpt.flow, [0], atol=1e-7)
+    assert_allclose(vs.volume, [0], atol=1e-7)
+
+
+class TestRollingVirtualStorage:
+    def test_run(self):
+        """Test RollingVirtualStorage node behaviour."""
+
+        model = pywr.core.Model()
+
+        inpt = Input(model, "Input", max_flow=20)
+        lnk = Link(model, "Link")
+        inpt.connect(lnk)
+        otpt = Output(model, "Output", max_flow=15, cost=-10.0)
+        lnk.connect(otpt)
+
+        vs = pywr.core.RollingVirtualStorage(model, "Licence", [lnk], days=3, initial_volume=30.0, max_volume=30.0)
+
+        model.setup()
+
+        assert_allclose(vs.volume, [30], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [15], atol=1e-7)
+        assert_allclose(vs.volume, [15], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [15], atol=1e-7)
+        assert_allclose(vs.volume, [0], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [0], atol=1e-7)
+        # End of third day the flow from the first day is return to the licence
+        assert_allclose(vs.volume, [15], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [15], atol=1e-7)
+        assert_allclose(vs.volume, [15], atol=1e-7)
+
+    def test_run_weekly(self):
+        """Test RollingVirtualStorage node behaviour with a weekly timestep."""
+
+        model = pywr.core.Model(timestep='7D')
+
+        inpt = Input(model, "Input", max_flow=20)
+        lnk = Link(model, "Link")
+        inpt.connect(lnk)
+        otpt = Output(model, "Output", max_flow=10, cost=-10.0)
+        lnk.connect(otpt)
+
+        vs = pywr.core.RollingVirtualStorage(model, "Licence", [lnk], timesteps=3, initial_volume=100.0,
+                                             max_volume=100.0)
+
+        model.setup()
+
+        assert_allclose(vs.volume, [100.0], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [10.0], atol=1e-7)
+        assert_allclose(vs.volume, [30.0], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [30.0 / 7], atol=1e-7)
+        assert_allclose(vs.volume, [0], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [0], atol=1e-7)
+        # End of third day the flow from the first day is return to the licence
+        assert_allclose(vs.volume, [70.0], atol=1e-7)
+
+        model.step()
+
+        assert_allclose(otpt.flow, [10.0], atol=1e-7)
+        assert_allclose(vs.volume, [30.0], atol=1e-7)
+
+
+def test_annual_virtual_storage():
+    model = load_model('virtual_storage1.json')
+    model.run()
+    node = model.nodes["supply1"]
+    rec = node.recorders[0]
+    assert_allclose(rec.data[0], 10) # licence is not a constraint
+    assert_allclose(rec.data[19], 10)
+    assert_allclose(rec.data[20], 5) # licence is constraint
+    assert_allclose(rec.data[21], 0) # licence is exhausted
+    assert_allclose(rec.data[365], 10) # licence is refreshed
+
+
+@pytest.mark.parametrize('reset_to_initial_volume', [None, False, True])
+def test_annual_virtual_storage_reset_to_max_volume(reset_to_initial_volume):
+    """Test that AnnualVirtualStorage resets to maximum volume. """
+    model = load_model('virtual_storage1.json')
+    licence1 = model.nodes['licence1']
+    if reset_to_initial_volume is not None:
+        licence1.reset_to_initial_volume = reset_to_initial_volume
+    licence1.initial_volume = 100
+    licence1.reset_month = 4
+
+    model.setup()
+    # After reset the current volume is always the initial volume
+    assert_allclose(licence1.volume, [100.0])
+
+
+    model.timestepper.start = '2015-04-01'
+    model.reset()
+    # After stepping over the reset day the volume should have been reset
+    # before the solve to either initial volume or maximum volume.
+    model.step()
+    if reset_to_initial_volume:
+        expected_volume = 90.0
+    else:
+        expected_volume = 195.0
+    assert_allclose(licence1.volume, [expected_volume])
+
+
+def test_annual_virtual_storage_with_dynamic_cost():
+    model = load_model('virtual_storage2.json')
+    model.run()
+    node = model.nodes["supply1"]
+    rec = node.recorders[0]
+
+    assert_allclose(rec.data[0], 10)  # licence is not a constraint
+    assert_allclose(rec.data[1], 5)  # now used slightly too much; switch to the other source
+    assert_allclose(rec.data[2], 10)  # continue back and forth.
+    assert_allclose(rec.data[3], 5)
+
+
+class TestSeasonalVirtualStorage:
+    def test_run(self):
+
+        model = load_model('seasonal_virtual_storage.json')
+        model.run()
+        supply_df = model.recorders["supply1"].to_dataframe()
+        licence_df = model.recorders["licence1"].to_dataframe()
+
+        # License is not constraining flow as volumne remains
+        assert_allclose(supply_df.loc["2015-01-01", :], 10)
+
+        # License is depleted and constrains flow
+        assert_allclose(supply_df.loc["2015-01-11", :], 0)
+        assert_allclose(licence_df.loc["2015-01-11", :], 0)
+
+        # License is depleted but does not constrain flow as it is not active
+        assert_allclose(supply_df.loc["2015-02-01", :], 10)
+        assert_allclose(licence_df.loc["2015-02-01", :], 0)
+
+        # check same values for second year
+        assert_allclose(supply_df.loc["2016-01-01", :], 10)
+        assert_allclose(supply_df.loc["2016-01-11", :], 0)
+        assert_allclose(licence_df.loc["2016-01-11", :], 0)
+        assert_allclose(supply_df.loc["2016-02-01", :], 10)
+        assert_allclose(licence_df.loc["2016-02-01", :], 0)
+
+    def test_year_overlap(self):
+        """test the the node works: end date < reset date < model start date. This
+        means that the node's active period extends from one year to the next
+        """
+
+        model = load_model('seasonal_virtual_storage.json')
+
+        vs = model.nodes["licence1"]
+        vs.reset_day = 1
+        vs.reset_month = 12
+        vs.end_day = 31
+        vs.end_month = 3
+        model.timestepper.start = "2015-12-15"
+        model.timestepper.end = "2016-12-02"
+
+        model.run()
+        supply_df = model.recorders["supply1"].to_dataframe()
+        licence_df = model.recorders["licence1"].to_dataframe()
+
+        # Start date is after reset data so there should be flow and volume should be reduced
+        assert_allclose(supply_df.loc["2015-12-15", :], 10)
+        assert_allclose(licence_df.loc["2015-12-15", :], 90)
+
+        # Licence is depleted but remains active so flow is 0
+        assert_allclose(supply_df.loc["2015-12-31", :], 0)
+        assert_allclose(licence_df.loc["2015-12-31", :], 0)
+
+        # License is turned off so flow is not constrained
+        assert_allclose(supply_df.loc["2016-03-31", :], 10)
+
+        # License is reset and made active
+        assert_allclose(supply_df.loc["2016-12-01", :], 10)
+        assert_allclose(licence_df.loc["2016-12-01", :], 90)
+
+    def test_start_deactivated(self):
+        """test the the node is not active when:  model start date < reset date < end date
+        """
+
+        model = load_model('seasonal_virtual_storage.json')
+
+        vs = model.nodes["licence1"]
+        vs.reset_day = 1
+        vs.reset_month = 2
+        vs.end_day = 1
+        vs.end_month = 3
+        model.timestepper.start = "2015-01-01"
+        model.timestepper.end = "2015-04-01"
+
+        model.run()
+        supply_df = model.recorders["supply1"].to_dataframe()
+        licence_df = model.recorders["licence1"].to_dataframe()
+
+        # Start date before reset date and end date so there should be flow but no reduction in node storage
+        assert_allclose(supply_df.loc["2015-01-15", :], 10)
+        assert_allclose(licence_df.loc["2015-01-15", :], 100)
+
+        # Licence active but depleted so flow is 0
+        assert_allclose(supply_df.loc["2015-2-15", :], 0)
+        assert_allclose(licence_df.loc["2015-2-15", :], 0)
+
+        # License is turned off so flow is not constrained
+        assert_allclose(supply_df.loc["2015-03-01", :], 10)
+
+
+def test_storage_spill_compensation():
+    """Test storage spill and compensation flows
+
+    The upstream catchment has min_flow == max_flow, so it "pushes" water into
+    the reservoir. The reservoir is already at it's maximum volume, so the
+    water must go *somewhere*. The compensation flow has the most negative cost,
+    so that is satisfied first. Once that is full, the demand is supplied.
+    Finally, any surplus is forced into the spill despite the cost.
+
+    Catchment -> Reservoir -> Demand
+                         |--> Spill        --|
+                         |--> Compensation --|
+                                             |--> Terminator
+    """
+    model = pywr.core.Model()
+
+    catchment = pywr.core.Input(model, name="Input", min_flow=10.0, max_flow=10.0, cost=1)
+    reservoir = pywr.core.Storage(model, name="Storage", max_volume=100, initial_volume=100.0)
+    spill = pywr.core.Link(model, name="Spill", cost=1.0)
+    compensation = pywr.core.Link(model, name="Compensation", max_flow=3.0, cost=-999)
+    terminator = pywr.core.Output(model, name="Terminator", cost=-1.0)
+    demand = pywr.core.Output(model, name="Demand", max_flow=5.0, cost=-500)
+
+    catchment.connect(reservoir)
+    reservoir.connect(spill)
+    reservoir.connect(compensation)
+    reservoir.connect(demand)
+    spill.connect(terminator)
+    compensation.connect(terminator)
+
+    model.check()
+    model.run()
+    assert_allclose(catchment.flow[0], 10.0, atol=1e-7)
+    assert_allclose(demand.flow[0], 5.0, atol=1e-7)
+    assert_allclose(compensation.flow[0], 3.0, atol=1e-7)
+    assert_allclose(spill.flow[0], 2.0, atol=1e-7)
+    assert_allclose(terminator.flow[0], (compensation.flow[0] + spill.flow[0]), atol=1e-7)
+
+
+def test_reservoir_circle():
+    """
+    Issue #140. A model with a circular route, from a reservoir Input back
+    around to it's own Output.
+
+                 Demand
+                    ^
+                    |
+                Reservoir <- Pumping
+                    |           ^
+                    v           |
+              Compensation      |
+                    |           |
+                    v           |
+    Catchment -> River 1 -> River 2 ----> MRFA -> Waste
+                                    |              ^
+                                    |---> MRFB ----|
+    """
+    model = Model()
+
+    catchment = Input(model, "catchment", max_flow=500, min_flow=500)
+
+    reservoir = Storage(model, "reservoir", max_volume=10000, initial_volume=5000)
+
+    demand = Output(model, "demand", max_flow=50, cost=-100)
+    pumping_station = Link(model, "pumping station", max_flow=100, cost=-10)
+    river1 = Link(model, "river1")
+    river2 = Link(model, "river2")
+    compensation = Link(model, "compensation", cost=600)
+    mrfA = Link(model, "mrfA", cost=-500, max_flow=50)
+    mrfB = Link(model, "mrfB")
+    waste = Output(model, "waste")
+
+    catchment.connect(river1)
+    river1.connect(river2)
+    river2.connect(mrfA)
+    river2.connect(mrfB)
+    mrfA.connect(waste)
+    mrfB.connect(waste)
+    river2.connect(pumping_station)
+    pumping_station.connect(reservoir)
+    reservoir.connect(compensation)
+    compensation.connect(river1)
+    reservoir.connect(demand)
+
+    model.check()
+    model.setup()
+
+    # not limited by mrf, pump capacity is constraint
+    model.step()
+    assert_allclose(catchment.flow, 500)
+    assert_allclose(waste.flow, 400)
+    assert_allclose(compensation.flow, 0)
+    assert_allclose(pumping_station.flow, 100)
+    assert_allclose(demand.flow, 50)
+
+    # limited by mrf
+    catchment.min_flow = catchment.max_flow = 100
+    model.step()
+    assert_allclose(waste.flow, 50)
+    assert_allclose(compensation.flow, 0)
+    assert_allclose(pumping_station.flow, 50)
+    assert_allclose(demand.flow, 50)
+
+    # reservoir can support mrf, but doesn't need to
+    compensation.cost = 200
+    model.step()
+    assert_allclose(waste.flow, 50)
+    assert_allclose(compensation.flow, 0)
+    assert_allclose(pumping_station.flow, 50)
+    assert_allclose(demand.flow, 50)
+
+    # reservoir supporting mrf
+    catchment.min_flow = catchment.max_flow = 0
+    model.step()
+    assert_allclose(waste.flow, 50)
+    assert_allclose(compensation.flow, 50)
+    assert_allclose(pumping_station.flow, 0)
+    assert_allclose(demand.flow, 50)
+
+
+def test_breaklink_node():
+    model = load_model('breaklink.json')
+    supply = model.nodes["A"]
+    transfer = model.nodes["B"]
+    demand = model.nodes["C"]
+    model.check()
+    model.run()
+    assert_allclose(supply.flow, 20)
+    assert_allclose(transfer.flow, 20)
+    assert_allclose(demand.flow, 20)
+    assert_allclose(transfer.storage.volume, 0)
+
+
+@pytest.mark.xfail(reason="Circular dependency in the JSON definition. "
+                          "See GitHub issue #380: https://github.com/pywr/pywr/issues/380")
+def test_reservoir_surface_area():
+    from pywr.parameters import InterpolatedVolumeParameter
+    model = load_model('reservoir_evaporation.json')
+    model.timestepper.start = "1920-01-01"
+    model.timestepper.end = "1920-01-02"
+    res = model.run()
+    assert (hasattr(Storage, area))
+    assert isinstance(model.nodes["reservoir1"].area, InterpolatedVolumeParameter)
+    assert_allclose(model.nodes["evaporation"].flow, 2.46875)
+
+
+def test_reservoir_surface_area_without_area_property():
+    """ Temporary test while the above test is not working. """
+    model = load_model('reservoir_evaporation_without_area_property.json')
+    model.timestepper.start = "1920-01-01"
+    model.timestepper.end = "1920-01-02"
+    res = model.run()
+    assert_allclose(model.nodes["evaporation"].flow, 2.46875)
+
+
+def test_run_empty():
+    # empty model should raise an exception if run
+    model = Model()
+    with pytest.raises(ModelStructureError):
+        model.run()
+
+
+def test_run():
+    model = load_model('simple1.json')
+
+    # run model from start to finish
+    result = model.run()
+    assert(result.timestep.index == 364)
+
+    # reset model and run again
+    result = model.run()
+    assert(result.timestep.index == 364)
+
+    # run remaining timesteps
+    model.reset(start=pandas.to_datetime('2015-12-01'))
+    result = model.run()
+    assert(result.timestep.index == 364)
+
+
+def test_reset_prev_flow():
+    """Test resetting the prev_flow attribte of a node."""
+    model = load_model('simple1.json')
+    demand1 = model.nodes['demand1']
+    model.run()
+    assert_allclose(demand1.flow, 10.0, atol=1e-7)
+    assert_allclose(demand1.prev_flow, 10.0, atol=1e-7)
+    # Reset and check flow attributes are zeroed
+    model.reset()
+    assert_allclose(demand1.flow, 0.0, atol=1e-7)
+    assert_allclose(demand1.prev_flow, 0.0, atol=1e-7)
+    # Run again
+    model.run()
+    assert_allclose(demand1.flow, 10.0, atol=1e-7)
+    assert_allclose(demand1.prev_flow, 10.0, atol=1e-7)
+
+
+def test_run_monthly():
+    model = load_model('simple1_monthly.json')
+
+    result = model.run()
+    assert result.timestep.index == 11
+
+    result = model.run()
+    assert result.timestep.index == 11
+
+
+def test_select_solver():
+    """Test specifying the solver in JSON"""
+    solver_names = [solver.name for solver in pywr.solvers.solver_registry]
+    for solver_name in solver_names:
+        data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s"}}''' % solver_name
+        model = load_model(data=data)
+        assert(model.solver.name.lower() == solver_name)
+
+def test_solver_unrecognised():
+    '''Test specifying an unrecognised solver JSON'''
+    solver_name = 'foobar'
+    data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s"}}''' % solver_name
+    with pytest.raises(KeyError):
+        model = load_model(data=data)
+
+@pytest.mark.skipif(Model().solver.name != "glpk", reason="only valid for glpk")
+@pytest.mark.parametrize("use_presolve", ["true", "false"])
+def test_select_glpk_presolve(use_presolve):
+    """Test specifying the solver in JSON"""
+    solver_names = ["glpk"]
+    for solver_name in solver_names:
+        data = '''{"metadata": {"minimum_version": "0.1"}, "nodes": {}, "edges": {}, "timestepper": {"start": "1990-01-01","end": "1999-12-31","timestep": 1}, "solver": {"name": "%s", "use_presolve": %s}}''' % (solver_name, use_presolve)
+        model = load_model(data=data)
+        assert(model.solver.name.lower() == solver_name)
+        assert(model.solver._cy_solver.use_presolve == (use_presolve == "true"))
```

### Comparing `pywr-1.8.0/tests/test_scenarios.py` & `pywr-1.9.0/tests/test_scenarios.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,380 +1,380 @@
-# -*- coding: utf-8 -*-
-"""
-A series of tests of the Scenario objects and associated infrastructure
-
-
-"""
-from pywr.core import Model, Input, Output, Link, Storage, Scenario
-from pywr.parameters import ConstantScenarioParameter
-from pywr.recorders import NumpyArrayStorageRecorder, NumpyArrayNodeRecorder
-from pywr.hashes import HashMismatchError
-from helpers import assert_model, load_model
-from fixtures import simple_linear_model
-import numpy as np
-from numpy.testing import assert_equal, assert_allclose
-import pytest
-
-
-def test_scenario_collection():
-    """ Basic test of Scenario and ScenarioCollection API """
-
-    model = Model()
-
-    # There is 1 combination when there are no Scenarios
-    model.scenarios.setup()
-    assert(len(model.scenarios.combinations) == 1)
-    assert(len(model.scenarios) == 0)
-    scA = Scenario(model, 'Scenario A', size=3)
-    model.scenarios.setup()
-    assert(len(model.scenarios.combinations) == 3)
-    assert(len(model.scenarios) == 1)
-    scA = Scenario(model, 'Scenario B', size=2)
-    model.scenarios.setup()
-    assert(len(model.scenarios.combinations) == 6)
-    assert(len(model.scenarios) == 2)
-
-    assert_equal([comb.indices for comb in model.scenarios.combinations],
-                 [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]])
-
-    names = model.scenarios.combination_names
-    for n, (ia, ib) in zip(names, [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]):
-        assert n == 'Scenario A.{:03d}-Scenario B.{:03d}'.format(ia, ib)
-
-    index = model.scenarios.multiindex
-    assert_equal(index.tolist(),
-                 [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]])
-    assert_equal(index.names, ['Scenario A', 'Scenario B'])
-
-
-def test_scenario(simple_linear_model, ):
-    """Basic test of Scenario functionality"""
-    model = simple_linear_model  # Convenience renaming
-
-    scenario = Scenario(model, 'Inflow', size=2)
-    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario, [5.0, 10.0])
-
-    model.nodes["Output"].max_flow = 5.0
-    model.nodes["Output"].cost = -2.0
-
-    expected_node_results = {
-        "Input": [5.0, 5.0],
-        "Link": [5.0, 5.0],
-        "Output": [5.0, 5.0],
-    }
-
-    assert_model(model, expected_node_results)
-
-
-def test_two_scenarios(simple_linear_model, ):
-    """Basic test of Scenario functionality"""
-    model = simple_linear_model  # Convenience renaming
-
-    scenario_input = Scenario(model, 'Inflow', size=2)
-    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
-
-    scenario_outflow = Scenario(model, 'Outflow', size=2, ensemble_names=['High', 'Low'])
-    model.nodes["Output"].max_flow = ConstantScenarioParameter(model, scenario_outflow, [3.0, 8.0])
-    model.nodes["Output"].cost = -2.0
-
-    # Check ensemble names are provided in the multi-index
-    index = model.scenarios.multiindex
-    assert index.levels[0].name == 'Inflow'
-    assert index.levels[1].name == 'Outflow'
-    assert np.all(index.levels[1] == ['High', 'Low'])
-
-    # add numpy recorders to input and output nodes
-    NumpyArrayNodeRecorder(model, model.nodes["Input"], name="input")
-    NumpyArrayNodeRecorder(model, model.nodes["Output"], name="output")
-
-    expected_node_results = {
-        "Input": [3.0, 5.0, 3.0, 8.0],
-        "Link": [3.0, 5.0, 3.0, 8.0],
-        "Output": [3.0, 5.0, 3.0, 8.0],
-    }
-
-    assert_model(model, expected_node_results)
-
-    model.run()
-
-    # combine recorder outputs to a single dataframe
-    df = model.to_dataframe()
-    assert(df.shape == (365, 2 * 2 * 2))
-    assert_allclose(df["input", 0, 'High'].iloc[0], 3.0)
-    assert_allclose(df["input", 0, 'Low'].iloc[0], 5.0)
-    assert_allclose(df["input", 1, 'High'].iloc[0], 3.0)
-    assert_allclose(df["input", 1, 'Low'].iloc[0], 8.0)
-
-
-def test_scenario_two_parameter(simple_linear_model, ):
-    """Basic test of Scenario functionality"""
-    model = simple_linear_model  # Convenience renaming
-
-    scenario_input = Scenario(model, 'Inflow', size=2)
-    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
-
-    model.nodes["Output"].max_flow = ConstantScenarioParameter(model, scenario_input, [8.0, 3.0])
-    model.nodes["Output"].cost = -2.0
-
-    expected_node_results = {
-        "Input": [5.0, 3.0],
-        "Link": [5.0, 3.0],
-        "Output": [5.0, 3.0],
-    }
-
-    assert_model(model, expected_node_results)
-
-
-def test_scenario_storage():
-    """Test the behaviour of Storage nodes with multiple scenarios
-
-    The model defined has two inflow scenarios: 5 and 10. It is expected that
-    the volume in the storage node should increase at different rates in the
-    two scenarios.
-    """
-    model = Model()
-
-    i = Input(model, 'input', max_flow=999)
-    s = Storage(model, 'storage', num_inputs=1, num_outputs=1, max_volume=1000, initial_volume=500)
-    o = Output(model, 'output', max_flow=999)
-
-    scenario_input = Scenario(model, 'Inflow', size=2)
-    i.min_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
-
-    i.connect(s)
-    s.connect(o)
-
-    s_rec = NumpyArrayStorageRecorder(model, s)
-
-    model.run()
-
-    assert_allclose(i.flow, [5, 10])
-    assert_allclose(s_rec.data[0], [505, 510])
-    assert_allclose(s_rec.data[1], [510, 520])
-
-
-@pytest.mark.parametrize("json_file", ['simple_with_scenario.json', 'simple_with_scenario_wrapper.json'])
-def test_scenarios_from_json(json_file):
-    """
-    Test a simple model with two scenarios.
-
-    The model varies in the inflow by "scenario A" and the demand
-    by "scenario B". The test ensures the correct size of model is
-    created, and uses a `NumpyArrayNodeRecorder` to check the output
-    in multiple dimensions is correct. The latter is done using
-    the `MultiIndex` on the `DataFrame` from the recorder.
-    """
-
-    model = load_model(json_file)
-    assert len(model.scenarios) == 2
-
-    model.setup()
-    assert len(model.scenarios.combinations) == 20
-    model.run()
-
-    # Test the recorder data is correct
-    df = model.recorders['demand1'].to_dataframe()
-
-    assert df.shape[1] == 20
-    assert df.columns.names[0] == 'scenario A'
-    assert_equal(df.columns.levels[0], np.arange(10))
-    assert df.columns.names[1] == 'scenario B'
-    assert_equal(df.columns.levels[1], np.array(['First', 'Second']))
-    # Data for first demand (B) ensemble
-    d1 = df.xs('First', level='scenario B', axis=1).iloc[0, :].values
-    assert_allclose(d1, [10]*10)
-    # Data for second demand (B) ensemble
-    d2 = df.xs('Second', level='scenario B', axis=1).iloc[0, :]
-    assert_allclose(d2, [10, 11, 12, 13, 14]+[15]*5)
-
-
-def test_timeseries_with_scenarios():
-
-    model = load_model('timeseries2.json')
-
-    model.setup()
-
-    assert len(model.scenarios) == 1
-
-    model.step()
-    catchment1 = model.nodes['catchment1']
-
-    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
-    assert_allclose(catchment1.flow, step1)
-
-    model.step()
-    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
-    # Low tolerance because test values were truncated to 2 decimal places.
-    assert_allclose(catchment1.flow, step2)
-
-    model.finish()
-
-
-def test_timeseries_with_scenarios_hdf():
-    # this test uses TablesArrayParameter
-    model = load_model('timeseries2_hdf.json')
-
-    model.setup()
-
-    assert len(model.scenarios) == 1
-
-    catchment1 = model.nodes['catchment1']
-
-    model.step()
-    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
-    # Low tolerance because test values were truncated to 2 decimal places.
-    assert_allclose(catchment1.flow, step1, atol=1e-1)
-
-    model.step()
-    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
-    # Low tolerance because test values were truncated to 2 decimal places.
-    assert_allclose(catchment1.flow, step2, atol=1e-1)
-
-    model.finish()
-
-
-def test_timeseries_with_wrong_hash():
-    with pytest.raises(HashMismatchError):
-        load_model('timeseries2_hdf_wrong_hash.json')
-
-
-def test_tablesarrayparameter_scenario_slice():
-    model = load_model('timeseries2_hdf.json')
-    catchment1 = model.nodes['catchment1']
-    scenario = model.scenarios["scenario A"]
-    scenario.slice = slice(0, 10, 2)
-    model.setup()
-    model.reset()
-    model.step()
-    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
-    assert_allclose(catchment1.flow, step1[::2], atol=1e-1)
-    model.step()
-    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
-    assert_allclose(catchment1.flow, step2[::2], atol=1e-1)
-    model.finish()
-
-def test_tablesarrayparameter_scenario_user_combinations():
-    """Test TablesArrayParameter with user defined combination of scenarios"""
-    model = load_model('timeseries2_hdf.json')
-    catchment1 = model.nodes['catchment1']
-    scenario = model.scenarios["scenario A"]
-    scenario2 = Scenario(model, "scenario B", size=2)
-    # combinations are intentially out of order and with duplicates
-    model.scenarios.user_combinations = [(0, 0), (2, 0), (6, 1), (2, 1)]
-    model.setup()
-    model.reset()
-    assert(len(model.scenarios.combinations) == 4)
-    model.step()
-    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
-    assert_allclose(catchment1.flow, step1[[0, 2, 6, 2]], atol=1e-1)
-    model.step()
-    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
-    assert_allclose(catchment1.flow, step2[[0, 2, 6, 2]], atol=1e-1)
-    model.finish()
-
-def test_tables_array_index_error():
-    # check an exception is raised (before the model starts) if the length
-    # of the data passed to a TablesArrayParameter is not long enough
-    model = load_model('timeseries2_hdf.json')
-    model.timestepper.start = "1920-01-01"
-    model.timestepper.end = "1980-01-01"
-    with pytest.raises(IndexError):
-        model.run()
-    assert(model.timestepper.current is None)
-
-    # check the HDF5 file was closed, despite an exception being raised
-    catchment1 = model.nodes['catchment1']
-    print(type(catchment1))
-    param = catchment1.max_flow
-    assert(param.h5store is None)
-
-def test_dirty_scenario(simple_linear_model):
-    """Adding a scenario to a model makes it dirty"""
-    model = simple_linear_model
-    model.setup()
-    assert(not model.dirty)
-    scenario = Scenario(model, "test", size=42)
-    assert(model.dirty)
-
-def test_scenario_slices(simple_linear_model):
-    """Test slicing of scenarios"""
-    model = simple_linear_model
-
-    # create two scenarios
-    s1 = Scenario(model=model, name="A", size=20)
-    s2 = Scenario(model=model, name="B", size=3)
-
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 20 * 3)
-
-    s1.slice = slice(0, None, 2)
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 10 * 3)
-
-    # check multiindex respects scenario slices
-    index = model.scenarios.multiindex
-    assert(len(index.levels) == 2)
-    assert(len(index.levels[0]) == 10)
-    assert(len(index.levels[1]) == 3)
-    assert(len(index.codes) == 2)
-    assert(len(index.codes[0]) == 10 * 3)
-    assert(len(index.codes[1]) == 10 * 3)
-    assert(index.names == ["A", "B"])
-
-    s2.slice = slice(1, 3, 1)
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 10 * 2)
-
-    assert(combinations[0].global_id == 0)
-    assert(tuple(combinations[0].indices) == (0, 1))
-
-    assert(combinations[-1].global_id == 19)
-    assert(tuple(combinations[-1].indices) == (18, 2))
-
-    model.run()
-
-    node = model.nodes["Input"]
-    assert((len(combinations),) == node.flow.shape)
-
-    s1.slice = None
-    s2.slice = None
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 20 * 3)
-
-def test_scenario_user_combinations(simple_linear_model):
-    model = simple_linear_model
-
-    # create two scenarios
-    s1 = Scenario(model=model, name="A", size=20)
-    s2 = Scenario(model=model, name="B", size=3)
-
-    model.scenarios.user_combinations = [[0, 1], [1, 1]]
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 2)
-    # ScenarioCollection.shape is simply the number of combinations
-    assert model.scenarios.shape == (2, )
-
-    # Test wrong number of dimensions
-    with pytest.raises(ValueError):
-        model.scenarios.user_combinations = [0, 1, 1, 1]
-
-    # Test out of range index
-    with pytest.raises(ValueError):
-        model.scenarios.user_combinations = [[19, 3], [2, 2]]
-    with pytest.raises(ValueError):
-        model.scenarios.user_combinations = [[-1, 2], [2, 2]]
-
-def test_scenario_slices_json():
-    model = load_model('scenario_with_slices.json')
-    scenarios = model.scenarios
-    assert(len(scenarios) == 2)
-    assert(scenarios["scenario A"].slice == slice(0, None, 2))
-    assert(scenarios["scenario B"].slice == slice(0, 1, 1))
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 5)
-
-def test_scenario_slices_json():
-    model = load_model('scenario_with_user_combinations.json')
-    scenarios = model.scenarios
-    assert(len(scenarios) == 2)
-    combinations = model.scenarios.get_combinations()
-    assert(len(combinations) == 3)
+# -*- coding: utf-8 -*-
+"""
+A series of tests of the Scenario objects and associated infrastructure
+
+
+"""
+from pywr.core import Model, Input, Output, Link, Storage, Scenario
+from pywr.parameters import ConstantScenarioParameter
+from pywr.recorders import NumpyArrayStorageRecorder, NumpyArrayNodeRecorder
+from pywr.hashes import HashMismatchError
+from helpers import assert_model, load_model
+from fixtures import simple_linear_model
+import numpy as np
+from numpy.testing import assert_equal, assert_allclose
+import pytest
+
+
+def test_scenario_collection():
+    """ Basic test of Scenario and ScenarioCollection API """
+
+    model = Model()
+
+    # There is 1 combination when there are no Scenarios
+    model.scenarios.setup()
+    assert(len(model.scenarios.combinations) == 1)
+    assert(len(model.scenarios) == 0)
+    scA = Scenario(model, 'Scenario A', size=3)
+    model.scenarios.setup()
+    assert(len(model.scenarios.combinations) == 3)
+    assert(len(model.scenarios) == 1)
+    scA = Scenario(model, 'Scenario B', size=2)
+    model.scenarios.setup()
+    assert(len(model.scenarios.combinations) == 6)
+    assert(len(model.scenarios) == 2)
+
+    assert_equal([comb.indices for comb in model.scenarios.combinations],
+                 [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]])
+
+    names = model.scenarios.combination_names
+    for n, (ia, ib) in zip(names, [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]):
+        assert n == 'Scenario A.{:03d}-Scenario B.{:03d}'.format(ia, ib)
+
+    index = model.scenarios.multiindex
+    assert_equal(index.tolist(),
+                 [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]])
+    assert_equal(index.names, ['Scenario A', 'Scenario B'])
+
+
+def test_scenario(simple_linear_model, ):
+    """Basic test of Scenario functionality"""
+    model = simple_linear_model  # Convenience renaming
+
+    scenario = Scenario(model, 'Inflow', size=2)
+    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario, [5.0, 10.0])
+
+    model.nodes["Output"].max_flow = 5.0
+    model.nodes["Output"].cost = -2.0
+
+    expected_node_results = {
+        "Input": [5.0, 5.0],
+        "Link": [5.0, 5.0],
+        "Output": [5.0, 5.0],
+    }
+
+    assert_model(model, expected_node_results)
+
+
+def test_two_scenarios(simple_linear_model, ):
+    """Basic test of Scenario functionality"""
+    model = simple_linear_model  # Convenience renaming
+
+    scenario_input = Scenario(model, 'Inflow', size=2)
+    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
+
+    scenario_outflow = Scenario(model, 'Outflow', size=2, ensemble_names=['High', 'Low'])
+    model.nodes["Output"].max_flow = ConstantScenarioParameter(model, scenario_outflow, [3.0, 8.0])
+    model.nodes["Output"].cost = -2.0
+
+    # Check ensemble names are provided in the multi-index
+    index = model.scenarios.multiindex
+    assert index.levels[0].name == 'Inflow'
+    assert index.levels[1].name == 'Outflow'
+    assert np.all(index.levels[1] == ['High', 'Low'])
+
+    # add numpy recorders to input and output nodes
+    NumpyArrayNodeRecorder(model, model.nodes["Input"], name="input")
+    NumpyArrayNodeRecorder(model, model.nodes["Output"], name="output")
+
+    expected_node_results = {
+        "Input": [3.0, 5.0, 3.0, 8.0],
+        "Link": [3.0, 5.0, 3.0, 8.0],
+        "Output": [3.0, 5.0, 3.0, 8.0],
+    }
+
+    assert_model(model, expected_node_results)
+
+    model.run()
+
+    # combine recorder outputs to a single dataframe
+    df = model.to_dataframe()
+    assert(df.shape == (365, 2 * 2 * 2))
+    assert_allclose(df["input", 0, 'High'].iloc[0], 3.0)
+    assert_allclose(df["input", 0, 'Low'].iloc[0], 5.0)
+    assert_allclose(df["input", 1, 'High'].iloc[0], 3.0)
+    assert_allclose(df["input", 1, 'Low'].iloc[0], 8.0)
+
+
+def test_scenario_two_parameter(simple_linear_model, ):
+    """Basic test of Scenario functionality"""
+    model = simple_linear_model  # Convenience renaming
+
+    scenario_input = Scenario(model, 'Inflow', size=2)
+    model.nodes["Input"].max_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
+
+    model.nodes["Output"].max_flow = ConstantScenarioParameter(model, scenario_input, [8.0, 3.0])
+    model.nodes["Output"].cost = -2.0
+
+    expected_node_results = {
+        "Input": [5.0, 3.0],
+        "Link": [5.0, 3.0],
+        "Output": [5.0, 3.0],
+    }
+
+    assert_model(model, expected_node_results)
+
+
+def test_scenario_storage():
+    """Test the behaviour of Storage nodes with multiple scenarios
+
+    The model defined has two inflow scenarios: 5 and 10. It is expected that
+    the volume in the storage node should increase at different rates in the
+    two scenarios.
+    """
+    model = Model()
+
+    i = Input(model, 'input', max_flow=999)
+    s = Storage(model, 'storage', num_inputs=1, num_outputs=1, max_volume=1000, initial_volume=500)
+    o = Output(model, 'output', max_flow=999)
+
+    scenario_input = Scenario(model, 'Inflow', size=2)
+    i.min_flow = ConstantScenarioParameter(model, scenario_input, [5.0, 10.0])
+
+    i.connect(s)
+    s.connect(o)
+
+    s_rec = NumpyArrayStorageRecorder(model, s)
+
+    model.run()
+
+    assert_allclose(i.flow, [5, 10])
+    assert_allclose(s_rec.data[0], [505, 510])
+    assert_allclose(s_rec.data[1], [510, 520])
+
+
+@pytest.mark.parametrize("json_file", ['simple_with_scenario.json', 'simple_with_scenario_wrapper.json'])
+def test_scenarios_from_json(json_file):
+    """
+    Test a simple model with two scenarios.
+
+    The model varies in the inflow by "scenario A" and the demand
+    by "scenario B". The test ensures the correct size of model is
+    created, and uses a `NumpyArrayNodeRecorder` to check the output
+    in multiple dimensions is correct. The latter is done using
+    the `MultiIndex` on the `DataFrame` from the recorder.
+    """
+
+    model = load_model(json_file)
+    assert len(model.scenarios) == 2
+
+    model.setup()
+    assert len(model.scenarios.combinations) == 20
+    model.run()
+
+    # Test the recorder data is correct
+    df = model.recorders['demand1'].to_dataframe()
+
+    assert df.shape[1] == 20
+    assert df.columns.names[0] == 'scenario A'
+    assert_equal(df.columns.levels[0], np.arange(10))
+    assert df.columns.names[1] == 'scenario B'
+    assert_equal(df.columns.levels[1], np.array(['First', 'Second']))
+    # Data for first demand (B) ensemble
+    d1 = df.xs('First', level='scenario B', axis=1).iloc[0, :].values
+    assert_allclose(d1, [10]*10)
+    # Data for second demand (B) ensemble
+    d2 = df.xs('Second', level='scenario B', axis=1).iloc[0, :]
+    assert_allclose(d2, [10, 11, 12, 13, 14]+[15]*5)
+
+
+def test_timeseries_with_scenarios():
+
+    model = load_model('timeseries2.json')
+
+    model.setup()
+
+    assert len(model.scenarios) == 1
+
+    model.step()
+    catchment1 = model.nodes['catchment1']
+
+    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
+    assert_allclose(catchment1.flow, step1)
+
+    model.step()
+    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
+    # Low tolerance because test values were truncated to 2 decimal places.
+    assert_allclose(catchment1.flow, step2)
+
+    model.finish()
+
+
+def test_timeseries_with_scenarios_hdf():
+    # this test uses TablesArrayParameter
+    model = load_model('timeseries2_hdf.json')
+
+    model.setup()
+
+    assert len(model.scenarios) == 1
+
+    catchment1 = model.nodes['catchment1']
+
+    model.step()
+    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
+    # Low tolerance because test values were truncated to 2 decimal places.
+    assert_allclose(catchment1.flow, step1, atol=1e-1)
+
+    model.step()
+    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
+    # Low tolerance because test values were truncated to 2 decimal places.
+    assert_allclose(catchment1.flow, step2, atol=1e-1)
+
+    model.finish()
+
+
+def test_timeseries_with_wrong_hash():
+    with pytest.raises(HashMismatchError):
+        load_model('timeseries2_hdf_wrong_hash.json')
+
+
+def test_tablesarrayparameter_scenario_slice():
+    model = load_model('timeseries2_hdf.json')
+    catchment1 = model.nodes['catchment1']
+    scenario = model.scenarios["scenario A"]
+    scenario.slice = slice(0, 10, 2)
+    model.setup()
+    model.reset()
+    model.step()
+    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
+    assert_allclose(catchment1.flow, step1[::2], atol=1e-1)
+    model.step()
+    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
+    assert_allclose(catchment1.flow, step2[::2], atol=1e-1)
+    model.finish()
+
+def test_tablesarrayparameter_scenario_user_combinations():
+    """Test TablesArrayParameter with user defined combination of scenarios"""
+    model = load_model('timeseries2_hdf.json')
+    catchment1 = model.nodes['catchment1']
+    scenario = model.scenarios["scenario A"]
+    scenario2 = Scenario(model, "scenario B", size=2)
+    # combinations are intentially out of order and with duplicates
+    model.scenarios.user_combinations = [(0, 0), (2, 0), (6, 1), (2, 1)]
+    model.setup()
+    model.reset()
+    assert(len(model.scenarios.combinations) == 4)
+    model.step()
+    step1 = np.array([21.64, 21.72, 23.97, 23.35, 21.79, 21.52, 21.21, 22.58, 26.19, 25.71], dtype=np.float64)
+    assert_allclose(catchment1.flow, step1[[0, 2, 6, 2]], atol=1e-1)
+    model.step()
+    step2 = np.array([20.03, 20.10, 22.18, 21.62, 20.17, 19.92, 19.63, 20.90, 24.24, 23.80], dtype=np.float64)
+    assert_allclose(catchment1.flow, step2[[0, 2, 6, 2]], atol=1e-1)
+    model.finish()
+
+def test_tables_array_index_error():
+    # check an exception is raised (before the model starts) if the length
+    # of the data passed to a TablesArrayParameter is not long enough
+    model = load_model('timeseries2_hdf.json')
+    model.timestepper.start = "1920-01-01"
+    model.timestepper.end = "1980-01-01"
+    with pytest.raises(IndexError):
+        model.run()
+    assert(model.timestepper.current is None)
+
+    # check the HDF5 file was closed, despite an exception being raised
+    catchment1 = model.nodes['catchment1']
+    print(type(catchment1))
+    param = catchment1.max_flow
+    assert(param.h5store is None)
+
+def test_dirty_scenario(simple_linear_model):
+    """Adding a scenario to a model makes it dirty"""
+    model = simple_linear_model
+    model.setup()
+    assert(not model.dirty)
+    scenario = Scenario(model, "test", size=42)
+    assert(model.dirty)
+
+def test_scenario_slices(simple_linear_model):
+    """Test slicing of scenarios"""
+    model = simple_linear_model
+
+    # create two scenarios
+    s1 = Scenario(model=model, name="A", size=20)
+    s2 = Scenario(model=model, name="B", size=3)
+
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 20 * 3)
+
+    s1.slice = slice(0, None, 2)
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 10 * 3)
+
+    # check multiindex respects scenario slices
+    index = model.scenarios.multiindex
+    assert(len(index.levels) == 2)
+    assert(len(index.levels[0]) == 10)
+    assert(len(index.levels[1]) == 3)
+    assert(len(index.codes) == 2)
+    assert(len(index.codes[0]) == 10 * 3)
+    assert(len(index.codes[1]) == 10 * 3)
+    assert(index.names == ["A", "B"])
+
+    s2.slice = slice(1, 3, 1)
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 10 * 2)
+
+    assert(combinations[0].global_id == 0)
+    assert(tuple(combinations[0].indices) == (0, 1))
+
+    assert(combinations[-1].global_id == 19)
+    assert(tuple(combinations[-1].indices) == (18, 2))
+
+    model.run()
+
+    node = model.nodes["Input"]
+    assert((len(combinations),) == node.flow.shape)
+
+    s1.slice = None
+    s2.slice = None
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 20 * 3)
+
+def test_scenario_user_combinations(simple_linear_model):
+    model = simple_linear_model
+
+    # create two scenarios
+    s1 = Scenario(model=model, name="A", size=20)
+    s2 = Scenario(model=model, name="B", size=3)
+
+    model.scenarios.user_combinations = [[0, 1], [1, 1]]
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 2)
+    # ScenarioCollection.shape is simply the number of combinations
+    assert model.scenarios.shape == (2, )
+
+    # Test wrong number of dimensions
+    with pytest.raises(ValueError):
+        model.scenarios.user_combinations = [0, 1, 1, 1]
+
+    # Test out of range index
+    with pytest.raises(ValueError):
+        model.scenarios.user_combinations = [[19, 3], [2, 2]]
+    with pytest.raises(ValueError):
+        model.scenarios.user_combinations = [[-1, 2], [2, 2]]
+
+def test_scenario_slices_json():
+    model = load_model('scenario_with_slices.json')
+    scenarios = model.scenarios
+    assert(len(scenarios) == 2)
+    assert(scenarios["scenario A"].slice == slice(0, None, 2))
+    assert(scenarios["scenario B"].slice == slice(0, 1, 1))
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 5)
+
+def test_scenario_slices_json():
+    model = load_model('scenario_with_user_combinations.json')
+    scenarios = model.scenarios
+    assert(len(scenarios) == 2)
+    combinations = model.scenarios.get_combinations()
+    assert(len(combinations) == 3)
```

### Comparing `pywr-1.8.0/tests/timeseries1.xlsx` & `pywr-1.9.0/tests/timeseries1.xlsx`

 * *Files identical despite different names*

### Comparing `pywr-1.8.0/travis/build-wheels.sh` & `pywr-1.9.0/travis/build-wheels.sh`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-#!/bin/bash
-# Build script for use with manylinux1 docker image to construct Linux wheels
-
-set -e -x
-
-cd /io
-# Setup path to use PYBIN's binary folder
-export PATH=${PYBIN}:${PATH}
-
-export PYWR_BUILD_GLPK=true
-export PYWR_BUILD_LPSOLVE=true
-
-# Install the build dependencies of the project. If some dependencies contain
-# compiled extensions and are not provided as pre-built wheel packages,
-# pip will build them from source matching the target Python version and architecture
-pip install cython packaging numpy jupyter pytest pytest-cov wheel setuptools_scm 'coverage<5.0'
-# Install run-time packages; install is allowed to fail for these as they are optional.
-pip install platypus-opt pygmo || true
-
-if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
-    # For tracing we need to build in-place
-    python setup.py build_ext -i develop
-
-    PYWR_SOLVER=glpk pytest tests --cov=pywr --cov-report=term --cov-append
-    PYWR_SOLVER=glpk-edge pytest tests --cov=pywr --cov-report=term --cov-append
-    PYWR_SOLVER=lpsolve pytest tests --cov=pywr --cov-report=term  --cov-append
-
-    # https://github.com/pytest-dev/pytest-cov/issues/146#issuecomment-272971136
-    # coverage needs this to be named as such for the combine to work
-    mv .coverage .coverage.docker
-else
-    # If not tracing build the wheels
-    python setup.py build_ext bdist_wheel -d wheelhouse/
-
-    # Bundle external shared libraries into the wheels
-    for whl in wheelhouse/pywr*.whl; do
-        auditwheel repair "$whl" -w wheelhouse/
-    done
-
-    # Move the source package to prevent import conflicts when running the tests
-    mv pywr pywr.build
-
-    # List the built wheels
-    ls -l wheelhouse
-
-    # Only test the manylinux wheels
-    for whl in wheelhouse/pywr*manylinux*.whl; do
-        pip install --force-reinstall --ignore-installed "$whl"
-        PYWR_SOLVER=glpk pytest tests
-        PYWR_SOLVER=glpk-edge pytest tests
-        PYWR_SOLVER=lpsolve pytest tests
-    done
-
-    # Move src directory back
-    mv pywr.build pywr
-fi
-
-if [[ "${BUILD_DOC}" -eq "1" ]]; then
-  echo "Building documentation!"
-  pip install sphinx sphinx_rtd_theme numpydoc
-  cd docs
-  make html
-  mkdir -p /io/pywr-docs
-  cp -r build/html /io/pywr-docs/
-  cd -
-fi
+#!/bin/bash
+# Build script for use with manylinux1 docker image to construct Linux wheels
+
+set -e -x
+
+cd /io
+# Setup path to use PYBIN's binary folder
+export PATH=${PYBIN}:${PATH}
+
+export PYWR_BUILD_GLPK=true
+export PYWR_BUILD_LPSOLVE=true
+
+# Install the build dependencies of the project. If some dependencies contain
+# compiled extensions and are not provided as pre-built wheel packages,
+# pip will build them from source matching the target Python version and architecture
+pip install cython packaging numpy jupyter pytest pytest-cov wheel setuptools_scm 'coverage<5.0'
+# Install run-time packages; install is allowed to fail for these as they are optional.
+pip install platypus-opt pygmo || true
+
+if [[ "${PYWR_BUILD_TRACE}" == "true" ]]; then
+    # For tracing we need to build in-place
+    python setup.py build_ext -i develop
+
+    PYWR_SOLVER=glpk pytest tests --cov=pywr --cov-report=term --cov-append
+    PYWR_SOLVER=glpk-edge pytest tests --cov=pywr --cov-report=term --cov-append
+    PYWR_SOLVER=lpsolve pytest tests --cov=pywr --cov-report=term  --cov-append
+
+    # https://github.com/pytest-dev/pytest-cov/issues/146#issuecomment-272971136
+    # coverage needs this to be named as such for the combine to work
+    mv .coverage .coverage.docker
+else
+    # If not tracing build the wheels
+    python setup.py build_ext bdist_wheel -d wheelhouse/
+
+    # Bundle external shared libraries into the wheels
+    for whl in wheelhouse/pywr*.whl; do
+        auditwheel repair "$whl" -w wheelhouse/
+    done
+
+    # Move the source package to prevent import conflicts when running the tests
+    mv pywr pywr.build
+
+    # List the built wheels
+    ls -l wheelhouse
+
+    # Only test the manylinux wheels
+    for whl in wheelhouse/pywr*manylinux*.whl; do
+        pip install --force-reinstall --ignore-installed "$whl"
+        PYWR_SOLVER=glpk pytest tests
+        PYWR_SOLVER=glpk-edge pytest tests
+        PYWR_SOLVER=lpsolve pytest tests
+    done
+
+    # Move src directory back
+    mv pywr.build pywr
+fi
+
+if [[ "${BUILD_DOC}" -eq "1" ]]; then
+  echo "Building documentation!"
+  pip install sphinx sphinx_rtd_theme numpydoc
+  cd docs
+  make html
+  mkdir -p /io/pywr-docs
+  cp -r build/html /io/pywr-docs/
+  cd -
+fi
```

### Comparing `pywr-1.8.0/travis/manylinux1_x86_64-glpk/build_lpsolve.sh` & `pywr-1.9.0/travis/manylinux2010_x86_64-glpk/build_lpsolve.sh`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,26 @@
-#!/usr/bin/env bash
-
-LPS_VER="5.5.2.5"
-
-# Must download this manually due to old OpenSSL
-# wget https://sourceforge.net/projects/lpsolve/files/lpsolve/${LPS_VER}/lp_solve_${LPS_VER}_source.tar.gz
-
-# TODO add checksum
-
-SRC_DIR="/app/lp_solve_5.5"
-BUILD_DIR=${SRC_DIR}/lpsolve55
-# Unpack
-tar -xzvf lp_solve_${LPS_VER}_source.tar.gz
-
-patch -p0 < fix-lpsolve-compilation.patch
-
-cd ${BUILD_DIR}
-sh ccc
-
-cp ${BUILD_DIR}/bin/ux64/* /usr/lib64/
-mkdir /usr/include/lpsolve
-cp ${SRC_DIR}/*.h /usr/include/lpsolve/
+#!/usr/bin/env bash
+
+LPS_VER="5.5.2.5"
+LPS_MD5="3be57261fc41dd8e210f54017220d5f7"
+
+wget https://sourceforge.net/projects/lpsolve/files/lpsolve/${LPS_VER}/lp_solve_${LPS_VER}_source.tar.gz
+
+if [[ "$(md5sum < lp_solve_${LPS_VER}_source.tar.gz)" != "${LPS_MD5}  -" ]]
+then
+    echo ERROR: Failed to verify lpsolve source.
+    exit 1 # terminate and indicate error
+fi
+
+SRC_DIR="/app/lp_solve_5.5"
+BUILD_DIR=${SRC_DIR}/lpsolve55
+# Unpack
+tar -xzvf lp_solve_${LPS_VER}_source.tar.gz
+
+patch -p0 < fix-lpsolve-compilation.patch
+
+cd ${BUILD_DIR}
+sh ccc
+
+cp ${BUILD_DIR}/bin/ux64/* /usr/lib64/
+mkdir /usr/include/lpsolve
+cp ${SRC_DIR}/*.h /usr/include/lpsolve/
```

### Comparing `pywr-1.8.0/travis/manylinux1_x86_64-glpk/fix-lpsolve-compilation.patch` & `pywr-1.9.0/travis/manylinux1_x86_64-glpk/fix-lpsolve-compilation.patch`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-Index: lp_solve_5.5/lp_solve/ccc.osx
-===================================================================
---- lp_solve_5.5/lp_solve/ccc.osx	(revision 11215)
-+++ lp_solve_5.5/lp_solve/ccc.osx	(revision 11216)
-@@ -20,7 +20,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lp_solve/ccc
-===================================================================
---- lp_solve_5.5/lp_solve/ccc	(revision 11215)
-+++ lp_solve_5.5/lp_solve/ccc	(revision 11216)
-@@ -20,7 +20,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/cccLUSOL.osx
-===================================================================
---- lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11216)
-@@ -17,7 +17,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/ccc
-===================================================================
---- lp_solve_5.5/lpsolve55/ccc	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/ccc	(revision 11216)
-@@ -18,7 +18,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/ccc.osx
-===================================================================
---- lp_solve_5.5/lpsolve55/ccc.osx	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/ccc.osx	(revision 11216)
-@@ -18,7 +18,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-
+Index: lp_solve_5.5/lp_solve/ccc.osx
+===================================================================
+--- lp_solve_5.5/lp_solve/ccc.osx	(revision 11215)
++++ lp_solve_5.5/lp_solve/ccc.osx	(revision 11216)
+@@ -20,7 +20,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lp_solve/ccc
+===================================================================
+--- lp_solve_5.5/lp_solve/ccc	(revision 11215)
++++ lp_solve_5.5/lp_solve/ccc	(revision 11216)
+@@ -20,7 +20,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/cccLUSOL.osx
+===================================================================
+--- lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11215)
++++ lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11216)
+@@ -17,7 +17,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/ccc
+===================================================================
+--- lp_solve_5.5/lpsolve55/ccc	(revision 11215)
++++ lp_solve_5.5/lpsolve55/ccc	(revision 11216)
+@@ -18,7 +18,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/ccc.osx
+===================================================================
+--- lp_solve_5.5/lpsolve55/ccc.osx	(revision 11215)
++++ lp_solve_5.5/lpsolve55/ccc.osx	(revision 11216)
+@@ -18,7 +18,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+
```

### Comparing `pywr-1.8.0/travis/manylinux2010_x86_64-glpk/fix-lpsolve-compilation.patch` & `pywr-1.9.0/travis/manylinux2010_x86_64-glpk/fix-lpsolve-compilation.patch`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-Index: lp_solve_5.5/lp_solve/ccc.osx
-===================================================================
---- lp_solve_5.5/lp_solve/ccc.osx	(revision 11215)
-+++ lp_solve_5.5/lp_solve/ccc.osx	(revision 11216)
-@@ -20,7 +20,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lp_solve/ccc
-===================================================================
---- lp_solve_5.5/lp_solve/ccc	(revision 11215)
-+++ lp_solve_5.5/lp_solve/ccc	(revision 11216)
-@@ -20,7 +20,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/cccLUSOL.osx
-===================================================================
---- lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11216)
-@@ -17,7 +17,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/ccc
-===================================================================
---- lp_solve_5.5/lpsolve55/ccc	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/ccc	(revision 11216)
-@@ -18,7 +18,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-Index: lp_solve_5.5/lpsolve55/ccc.osx
-===================================================================
---- lp_solve_5.5/lpsolve55/ccc.osx	(revision 11215)
-+++ lp_solve_5.5/lpsolve55/ccc.osx	(revision 11216)
-@@ -18,7 +18,7 @@
- echo '#include <stdio.h>'>>/tmp/isnan.c
- echo '#include <stdlib.h>'>>/tmp/isnan.c
- echo '#include <math.h>'>>/tmp/isnan.c
--echo 'main(){isnan(0);}'>>/tmp/isnan.c
-+echo 'main(){isnan(0);}'>>/tmp/isnan.c
- $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
- if [ $? = 0 ]
- then NOISNAN=
-
+Index: lp_solve_5.5/lp_solve/ccc.osx
+===================================================================
+--- lp_solve_5.5/lp_solve/ccc.osx	(revision 11215)
++++ lp_solve_5.5/lp_solve/ccc.osx	(revision 11216)
+@@ -20,7 +20,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lp_solve/ccc
+===================================================================
+--- lp_solve_5.5/lp_solve/ccc	(revision 11215)
++++ lp_solve_5.5/lp_solve/ccc	(revision 11216)
+@@ -20,7 +20,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/cccLUSOL.osx
+===================================================================
+--- lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11215)
++++ lp_solve_5.5/lpsolve55/cccLUSOL.osx	(revision 11216)
+@@ -17,7 +17,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/ccc
+===================================================================
+--- lp_solve_5.5/lpsolve55/ccc	(revision 11215)
++++ lp_solve_5.5/lpsolve55/ccc	(revision 11216)
+@@ -18,7 +18,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0.0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+Index: lp_solve_5.5/lpsolve55/ccc.osx
+===================================================================
+--- lp_solve_5.5/lpsolve55/ccc.osx	(revision 11215)
++++ lp_solve_5.5/lpsolve55/ccc.osx	(revision 11216)
+@@ -18,7 +18,7 @@
+ echo '#include <stdio.h>'>>/tmp/isnan.c
+ echo '#include <stdlib.h>'>>/tmp/isnan.c
+ echo '#include <math.h>'>>/tmp/isnan.c
+-echo 'main(){isnan(0);}'>>/tmp/isnan.c
++echo 'main(){isnan(0);}'>>/tmp/isnan.c
+ $c /tmp/isnan.c -o /tmp/isnan $math >/dev/null 2>&1
+ if [ $? = 0 ]
+ then NOISNAN=
+
```

