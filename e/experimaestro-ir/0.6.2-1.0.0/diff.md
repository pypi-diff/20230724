# Comparing `tmp/experimaestro-ir-0.6.2.zip` & `tmp/experimaestro-ir-1.0.0.zip`

## zipinfo {}

```diff
@@ -1,243 +1,243 @@
-Zip file size: 200811 bytes, number of entries: 241
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/hf_models/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/snippets/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/examples/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/
--rw-r--r--  2.0 unx      406 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.readthedocs.yml
--rw-r--r--  2.0 unx     1310 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/pyproject.toml
--rw-r--r--  2.0 unx    35149 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/LICENSE
--rw-r--r--  2.0 unx     3229 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/PKG-INFO
--rw-r--r--  2.0 unx      147 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github_changelog_generator
--rw-r--r--  2.0 unx      232 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/tox.ini
--rw-r--r--  2.0 unx      569 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/setup.py
--rw-r--r--  2.0 unx      434 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.pre-commit-config.yaml
--rw-r--r--  2.0 unx      904 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/setup.cfg
--rw-r--r--  2.0 unx     2220 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/README.md
--rw-r--r--  2.0 unx      270 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/requirements.txt
--rw-r--r--  2.0 unx     1234 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.gitignore
--rw-r--r--  2.0 unx      460 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/CHANGELOG.md
--rw-r--r--  2.0 unx     1706 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/hf_models/splade.py
--rw-r--r--  2.0 unx      105 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/hf_models/README.md
--rw-r--r--  2.0 unx      803 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/hf_models/tas_balanced.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/
--rw-r--r--  2.0 unx      272 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/requires.txt
--rw-r--r--  2.0 unx        1 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/not-zip-safe
--rw-r--r--  2.0 unx     3229 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/PKG-INFO
--rw-r--r--  2.0 unx        1 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/dependency_links.txt
--rw-r--r--  2.0 unx      134 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/entry_points.txt
--rw-r--r--  2.0 unx        6 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/top_level.txt
--rw-r--r--  2.0 unx     5645 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/SOURCES.txt
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/metrics/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/index/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/text/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tokenization/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/rankers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/documents/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tasks/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/configuration/
--rw-r--r--  2.0 unx     1122 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/measures.py
--rw-r--r--  2.0 unx      160 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/_version.py
--rw-r--r--  2.0 unx     1323 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/context.py
--rw-r--r--  2.0 unx      193 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/compat.py
--rw-r--r--  2.0 unx       72 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/__init__.py
--rw-r--r--  2.0 unx      221 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/__main__.py
--rw-r--r--  2.0 unx     3043 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/distributed.py
--rw-r--r--  2.0 unx    10267 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/evaluation.py
--rw-r--r--  2.0 unx     2761 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/models.py
--rw-r--r--  2.0 unx      205 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/mkdocs_init.py
--rw-r--r--  2.0 unx      510 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/metrics/__init__.py
--rw-r--r--  2.0 unx     2238 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/index/anserini.py
--rw-r--r--  2.0 unx     6747 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/index/sparse.py
--rw-r--r--  2.0 unx       57 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/index/__init__.py
--rw-r--r--  2.0 unx     7882 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/index/faiss.py
--rw-r--r--  2.0 unx     5314 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/text/wordvec_vocab.py
--rw-r--r--  2.0 unx    16592 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/text/huggingface.py
--rw-r--r--  2.0 unx       50 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/text/__init__.py
--rw-r--r--  2.0 unx     7411 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/text/encoders.py
--rw-r--r--  2.0 unx     2139 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tokenization/align.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tokenization/__init__.py
--rw-r--r--  2.0 unx      218 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tokenization/tokenizers.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/interaction/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/modules/
--rw-r--r--  2.0 unx     1281 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/common.py
--rw-r--r--  2.0 unx     8445 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/dual.py
--rw-r--r--  2.0 unx     2750 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/huggingface.py
--rw-r--r--  2.0 unx     3266 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/colbert.py
--rw-r--r--  2.0 unx     5589 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/splade.py
--rw-r--r--  2.0 unx     3284 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/cross.py
--rw-r--r--  2.0 unx      125 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/jointclassifier.py
--rw-r--r--  2.0 unx     4354 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/__init__.py
--rw-r--r--  2.0 unx     5474 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/interaction/drmm.py
--rw-r--r--  2.0 unx     1926 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/interaction/__init__.py
--rw-r--r--  2.0 unx     1476 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/modules/rbf_kernels.py
--rw-r--r--  2.0 unx      249 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/modules/__init__.py
--rw-r--r--  2.0 unx     2510 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/neural/modules/interaction_matrix.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/helpers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/
--rw-r--r--  2.0 unx      631 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/__init__.py
--rw-r--r--  2.0 unx      449 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/results.py
--rw-r--r--  2.0 unx     9480 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/cli.py
--rw-r--r--  2.0 unx     5619 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/experiment.py
--rw-r--r--  2.0 unx     1349 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/configuration.py
--rw-r--r--  2.0 unx      159 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/__init__.py
--rw-r--r--  2.0 unx      719 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/small.yaml
--rw-r--r--  2.0 unx      735 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/monobert/normal.yaml
--rw-r--r--  2.0 unx     7444 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/experiment.py
--rw-r--r--  2.0 unx     2989 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/configuration.py
--rw-r--r--  2.0 unx     7899 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/pipelines.py
--rw-r--r--  2.0 unx     1211 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_doc.yaml
--rw-r--r--  2.0 unx      185 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/__init__.py
--rw-r--r--  2.0 unx     1315 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/small.yaml
--rw-r--r--  2.0 unx     1173 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal.yaml
--rw-r--r--  2.0 unx     1364 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_DistilMSE.yaml
--rw-r--r--  2.0 unx     2445 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/helpers/msmarco.py
--rw-r--r--  2.0 unx     1523 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/helpers/optim.py
--rw-r--r--  2.0 unx     1366 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/helpers/__init__.py
--rw-r--r--  2.0 unx     5140 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/experiment.py
--rw-r--r--  2.0 unx      484 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/configuration.py
--rw-r--r--  2.0 unx      139 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/__init__.py
--rw-r--r--  2.0 unx     1078 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/small.yaml
--rw-r--r--  2.0 unx     1244 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/papers/duobert/normal.yaml
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/index/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/neural/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/rankers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/letor/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/utils/
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/__init__.py
--rw-r--r--  2.0 unx     1846 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/index/test_faiss.py
--rw-r--r--  2.0 unx     4777 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/index/test_sparse.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/index/__init__.py
--rw-r--r--  2.0 unx     6200 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/neural/test_forward.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/neural/__init__.py
--rw-r--r--  2.0 unx     2694 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/rankers/test_full.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/rankers/__init__.py
--rw-r--r--  2.0 unx     1280 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/letor/test_samplers.py
--rw-r--r--  2.0 unx      281 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/letor/test_trainers.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/letor/__init__.py
--rw-r--r--  2.0 unx      927 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/utils/test_iter.py
--rw-r--r--  2.0 unx     2699 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/utils/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/test/utils/__init__.py
--rw-r--r--  2.0 unx      348 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/rankers/standard.py
--rw-r--r--  2.0 unx    17796 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/rankers/__init__.py
--rw-r--r--  2.0 unx     5220 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/rankers/full.py
--rw-r--r--  2.0 unx      634 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/rankers/mergers.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/irds/
--rw-r--r--  2.0 unx    12180 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/adapters.py
--rw-r--r--  2.0 unx     4885 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/robust.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/__init__.py
--rw-r--r--  2.0 unx     1435 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/irds/utils.py
--rw-r--r--  2.0 unx      707 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/irds/__init__.py
--rw-r--r--  2.0 unx     3785 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/irds/data.py
--rw-r--r--  2.0 unx     5756 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/datasets/irds/datasets.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/documents/__init__.py
--rw-r--r--  2.0 unx     4946 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/documents/samplers.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/distillation/
--rw-r--r--  2.0 unx     7537 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/batchers.py
--rw-r--r--  2.0 unx    10425 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/optim.py
--rw-r--r--  2.0 unx     1853 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/metrics.py
--rw-r--r--  2.0 unx     8528 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/context.py
--rw-r--r--  2.0 unx     2437 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/utils.py
--rw-r--r--  2.0 unx     2126 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/schedulers.py
--rw-r--r--  2.0 unx      628 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/__init__.py
--rw-r--r--  2.0 unx     9765 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/records.py
--rw-r--r--  2.0 unx     3456 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/devices.py
--rw-r--r--  2.0 unx    21917 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/samplers.py
--rw-r--r--  2.0 unx    16934 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/learner.py
--rw-r--r--  2.0 unx     4927 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pointwise.py
--rw-r--r--  2.0 unx     3360 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/batchwise.py
--rw-r--r--  2.0 unx     3783 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/__init__.py
--rw-r--r--  2.0 unx     1423 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/multiple.py
--rw-r--r--  2.0 unx    10308 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pairwise.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/distillation/__init__.py
--rw-r--r--  2.0 unx     5032 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/distillation/pairwise.py
--rw-r--r--  2.0 unx     3219 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/letor/distillation/samplers.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/
--rw-r--r--  2.0 unx      137 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/co/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/com/
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/
--rw-r--r--  2.0 unx      752 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/__init__.py
--rw-r--r--  2.0 unx     1196 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/com/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/sebastian-hofstaetter/
--rw-r--r--  2.0 unx     2064 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py
--rw-r--r--  2.0 unx    10178 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/anserini.py
--rw-r--r--  2.0 unx    12040 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/trec.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/__init__.py
--rw-r--r--  2.0 unx      749 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/apex.py
--rw-r--r--  2.0 unx      713 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/interfaces/plaintext.py
--rw-r--r--  2.0 unx      354 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/utils/functools.py
--rw-r--r--  2.0 unx     3844 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/utils/iter.py
--rw-r--r--  2.0 unx     5964 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/utils/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/utils/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/tasks/__init__.py
--rw-r--r--  2.0 unx     1146 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/src/xpmir/configuration/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/snippets/irds/
--rw-r--r--  2.0 unx      249 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/snippets/irds/irds-documents.py
--rw-r--r--  2.0 unx      217 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/snippets/irds/irds-query.py
--rw-r--r--  2.0 unx      843 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/snippets/irds/irds-qrels.py
--rw-r--r--  2.0 unx     1683 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/examples/bm25.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github/workflows/
--rw-r--r--  2.0 unx      408 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github/release.yaml
--rw-r--r--  2.0 unx     1438 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github/workflows/pytest.yml
--rw-r--r--  2.0 unx      857 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/.github/workflows/python-publish.yml
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/
--rw-r--r--  2.0 unx      637 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/Makefile
--rw-r--r--  2.0 unx      191 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/README.md
--rw-r--r--  2.0 unx       78 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/requirements.txt
--rw-r--r--  2.0 unx      804 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/make.bat
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/text/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/letor/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/
--rw-r--r--  2.0 unx     2217 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/retrieval.rst
--rw-r--r--  2.0 unx     1106 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/index.rst
--rw-r--r--  2.0 unx      663 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/hooks.rst
--rw-r--r--  2.0 unx      733 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/evaluation.rst
--rw-r--r--  2.0 unx     1702 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/neural.rst
--rw-r--r--  2.0 unx       49 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/misc.rst
--rw-r--r--  2.0 unx     2027 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/pretrained.rst
--rw-r--r--  2.0 unx     3644 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/conf.py
--rw-r--r--  2.0 unx      635 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/text/index.rst
--rw-r--r--  2.0 unx      340 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/text/huggingface.rst
-drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/helpers/
--rw-r--r--  2.0 unx     1765 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/index.rst
--rw-r--r--  2.0 unx      301 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/splade.rst
--rw-r--r--  2.0 unx      328 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/monobert.rst
--rw-r--r--  2.0 unx      220 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/helpers/index.rst
--rw-r--r--  2.0 unx      568 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/papers/helpers/msmarco.rst
--rw-r--r--  2.0 unx      796 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/letor/optimization.rst
--rw-r--r--  2.0 unx     1129 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/letor/index.rst
--rw-r--r--  2.0 unx     1355 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/letor/samplers.rst
--rw-r--r--  2.0 unx     1499 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/letor/trainers.rst
--rw-r--r--  2.0 unx      201 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/types.rst
--rw-r--r--  2.0 unx      231 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/index.rst
--rw-r--r--  2.0 unx      165 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/xpmir.rst
--rw-r--r--  2.0 unx      410 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/adapters.rst
--rw-r--r--  2.0 unx      338 b- defN 23-Apr-13 06:54 experimaestro-ir-0.6.2/docs/source/data/irds.rst
-241 files, 488878 bytes uncompressed, 157953 bytes compressed:  67.7%
+Zip file size: 197153 bytes, number of entries: 241
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/hf_models/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/snippets/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/examples/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/
+-rw-r--r--  2.0 unx      275 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/requirements.txt
+-rw-r--r--  2.0 unx      904 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/setup.cfg
+-rw-r--r--  2.0 unx     3784 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/PKG-INFO
+-rw-r--r--  2.0 unx      460 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/CHANGELOG.md
+-rw-r--r--  2.0 unx     1234 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.gitignore
+-rw-r--r--  2.0 unx      147 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github_changelog_generator
+-rw-r--r--  2.0 unx    35149 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/LICENSE
+-rw-r--r--  2.0 unx     2775 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/README.md
+-rw-r--r--  2.0 unx      569 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/setup.py
+-rw-r--r--  2.0 unx     1270 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/pyproject.toml
+-rw-r--r--  2.0 unx      232 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/tox.ini
+-rw-r--r--  2.0 unx      406 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.readthedocs.yml
+-rw-r--r--  2.0 unx      434 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.pre-commit-config.yaml
+-rw-r--r--  2.0 unx      105 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/hf_models/README.md
+-rw-r--r--  2.0 unx      803 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/hf_models/tas_balanced.py
+-rw-r--r--  2.0 unx     1706 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/hf_models/splade.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/snippets/irds/
+-rw-r--r--  2.0 unx      256 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/snippets/irds/irds-documents.py
+-rw-r--r--  2.0 unx      843 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/snippets/irds/irds-qrels.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/snippets/irds/irds-query.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github/workflows/
+-rw-r--r--  2.0 unx      408 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github/release.yaml
+-rw-r--r--  2.0 unx     1438 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github/workflows/pytest.yml
+-rw-r--r--  2.0 unx      857 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/.github/workflows/python-publish.yml
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/
+-rw-r--r--  2.0 unx       65 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/requirements.txt
+-rw-r--r--  2.0 unx      191 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/README.md
+-rw-r--r--  2.0 unx      804 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/make.bat
+-rw-r--r--  2.0 unx      637 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/Makefile
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/letor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/text/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/
+-rw-r--r--  2.0 unx      666 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/hooks.rst
+-rw-r--r--  2.0 unx     1582 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/index.rst
+-rw-r--r--  2.0 unx     1702 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/neural.rst
+-rw-r--r--  2.0 unx      733 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/evaluation.rst
+-rw-r--r--  2.0 unx     2027 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/pretrained.rst
+-rw-r--r--  2.0 unx     3909 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/conf.py
+-rw-r--r--  2.0 unx     1807 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/retrieval.rst
+-rw-r--r--  2.0 unx       49 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/misc.rst
+-rw-r--r--  2.0 unx     1515 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/letor/samplers.rst
+-rw-r--r--  2.0 unx      969 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/letor/optimization.rst
+-rw-r--r--  2.0 unx     1152 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/letor/index.rst
+-rw-r--r--  2.0 unx     1432 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/letor/trainers.rst
+-rw-r--r--  2.0 unx      191 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/types.rst
+-rw-r--r--  2.0 unx      165 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/xpmir.rst
+-rw-r--r--  2.0 unx      421 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/adapters.rst
+-rw-r--r--  2.0 unx      338 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/irds.rst
+-rw-r--r--  2.0 unx      231 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/data/index.rst
+-rw-r--r--  2.0 unx      635 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/text/index.rst
+-rw-r--r--  2.0 unx      449 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/text/huggingface.rst
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/helpers/
+-rw-r--r--  2.0 unx      328 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/monobert.rst
+-rw-r--r--  2.0 unx     1765 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/index.rst
+-rw-r--r--  2.0 unx      301 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/splade.rst
+-rw-r--r--  2.0 unx      568 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/helpers/msmarco.rst
+-rw-r--r--  2.0 unx      220 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/docs/source/papers/helpers/index.rst
+-rw-r--r--  2.0 unx     1683 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/examples/bm25.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/metrics/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/configuration/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/text/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tasks/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/documents/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/rankers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/index/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/datasets/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tokenization/
+-rw-r--r--  2.0 unx     3043 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/distributed.py
+-rw-r--r--  2.0 unx     1122 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/measures.py
+-rw-r--r--  2.0 unx      205 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/mkdocs_init.py
+-rw-r--r--  2.0 unx      160 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/_version.py
+-rw-r--r--  2.0 unx     2761 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/models.py
+-rw-r--r--  2.0 unx    10400 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/evaluation.py
+-rw-r--r--  2.0 unx      193 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/compat.py
+-rw-r--r--  2.0 unx     1323 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/context.py
+-rw-r--r--  2.0 unx       72 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/__init__.py
+-rw-r--r--  2.0 unx      221 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/__main__.py
+-rw-r--r--  2.0 unx      510 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/metrics/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/distillation/
+-rw-r--r--  2.0 unx    10732 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/records.py
+-rw-r--r--  2.0 unx     5442 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/learner.py
+-rw-r--r--  2.0 unx     2437 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/utils.py
+-rw-r--r--  2.0 unx      364 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/__init__.py
+-rw-r--r--  2.0 unx    21417 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/samplers.py
+-rw-r--r--  2.0 unx    10390 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pairwise.py
+-rw-r--r--  2.0 unx     3330 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/batchwise.py
+-rw-r--r--  2.0 unx     2715 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/__init__.py
+-rw-r--r--  2.0 unx     1426 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/multiple.py
+-rw-r--r--  2.0 unx     4848 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pointwise.py
+-rw-r--r--  2.0 unx     5035 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/distillation/pairwise.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/distillation/__init__.py
+-rw-r--r--  2.0 unx     3294 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/letor/distillation/samplers.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/interaction/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/modules/
+-rw-r--r--  2.0 unx     3322 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/colbert.py
+-rw-r--r--  2.0 unx     3366 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/cross.py
+-rw-r--r--  2.0 unx     7521 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/dual.py
+-rw-r--r--  2.0 unx      125 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/jointclassifier.py
+-rw-r--r--  2.0 unx     2753 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/huggingface.py
+-rw-r--r--  2.0 unx     4409 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/__init__.py
+-rw-r--r--  2.0 unx     5574 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/splade.py
+-rw-r--r--  2.0 unx     1929 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/interaction/__init__.py
+-rw-r--r--  2.0 unx     5484 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/interaction/drmm.py
+-rw-r--r--  2.0 unx     1476 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/modules/rbf_kernels.py
+-rw-r--r--  2.0 unx     2537 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/modules/interaction_matrix.py
+-rw-r--r--  2.0 unx      249 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/neural/modules/__init__.py
+-rw-r--r--  2.0 unx     6276 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/utils/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/utils/__init__.py
+-rw-r--r--  2.0 unx      354 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/utils/functools.py
+-rw-r--r--  2.0 unx     3844 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/utils/iter.py
+-rw-r--r--  2.0 unx     1146 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/configuration/__init__.py
+-rw-r--r--  2.0 unx    16696 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/text/huggingface.py
+-rw-r--r--  2.0 unx     7426 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/text/encoders.py
+-rw-r--r--  2.0 unx       50 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/text/__init__.py
+-rw-r--r--  2.0 unx     5351 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/text/wordvec_vocab.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tasks/__init__.py
+-rw-r--r--  2.0 unx    12040 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/trec.py
+-rw-r--r--  2.0 unx      749 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/apex.py
+-rw-r--r--  2.0 unx      713 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/plaintext.py
+-rw-r--r--  2.0 unx    10347 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/anserini.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/interfaces/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/documents/__init__.py
+-rw-r--r--  2.0 unx     4938 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/documents/samplers.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/
+-rw-r--r--  2.0 unx      137 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/co/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/com/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/
+-rw-r--r--  2.0 unx      752 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/__init__.py
+-rw-r--r--  2.0 unx     1196 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/com/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/sebastian-hofstaetter/
+-rw-r--r--  2.0 unx     2064 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py
+-rw-r--r--  2.0 unx      348 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/rankers/standard.py
+-rw-r--r--  2.0 unx      634 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/rankers/mergers.py
+-rw-r--r--  2.0 unx    15095 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/rankers/__init__.py
+-rw-r--r--  2.0 unx     5265 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/rankers/full.py
+-rw-r--r--  2.0 unx     3456 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/devices.py
+-rw-r--r--  2.0 unx     1389 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/trainers.py
+-rw-r--r--  2.0 unx    11117 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/optim.py
+-rw-r--r--  2.0 unx     1853 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/metrics.py
+-rw-r--r--  2.0 unx     2126 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/schedulers.py
+-rw-r--r--  2.0 unx     7537 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/batchers.py
+-rw-r--r--  2.0 unx    11608 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/learner.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/base.py
+-rw-r--r--  2.0 unx     8525 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/context.py
+-rw-r--r--  2.0 unx       55 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/learning/__init__.py
+-rw-r--r--  2.0 unx     7723 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/index/faiss.py
+-rw-r--r--  2.0 unx     2238 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/index/anserini.py
+-rw-r--r--  2.0 unx       57 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/index/__init__.py
+-rw-r--r--  2.0 unx     6689 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/index/sparse.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/helpers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/
+-rw-r--r--  2.0 unx      449 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/results.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/__init__.py
+-rw-r--r--  2.0 unx    10116 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/cli.py
+-rw-r--r--  2.0 unx     4744 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/helpers/msmarco.py
+-rw-r--r--  2.0 unx     2149 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/helpers/optim.py
+-rw-r--r--  2.0 unx     1376 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/helpers/__init__.py
+-rw-r--r--  2.0 unx      898 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/small.yaml
+-rw-r--r--  2.0 unx     6035 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/experiment.py
+-rw-r--r--  2.0 unx      819 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/normal.yaml
+-rw-r--r--  2.0 unx     1471 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/configuration.py
+-rw-r--r--  2.0 unx      159 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/monobert/__init__.py
+-rw-r--r--  2.0 unx     1078 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/small.yaml
+-rw-r--r--  2.0 unx     5179 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/experiment.py
+-rw-r--r--  2.0 unx     1244 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/normal.yaml
+-rw-r--r--  2.0 unx      484 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/configuration.py
+-rw-r--r--  2.0 unx      139 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/duobert/__init__.py
+-rw-r--r--  2.0 unx     1437 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/small.yaml
+-rw-r--r--  2.0 unx     1318 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal_DistilMSE.yaml
+-rw-r--r--  2.0 unx     8011 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/experiment.py
+-rw-r--r--  2.0 unx     1115 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal.yaml
+-rw-r--r--  2.0 unx     2762 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/configuration.py
+-rw-r--r--  2.0 unx      185 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/__init__.py
+-rw-r--r--  2.0 unx     1081 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal_doc.yaml
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/letor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/neural/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/rankers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/learning/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/index/
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/__init__.py
+-rw-r--r--  2.0 unx      281 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/letor/test_trainers.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/letor/__init__.py
+-rw-r--r--  2.0 unx     1274 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/letor/test_samplers.py
+-rw-r--r--  2.0 unx     6242 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/neural/test_forward.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/neural/__init__.py
+-rw-r--r--  2.0 unx      927 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/utils/test_iter.py
+-rw-r--r--  2.0 unx     2941 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/utils/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/utils/__init__.py
+-rw-r--r--  2.0 unx     3056 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/rankers/test_full.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/rankers/__init__.py
+-rw-r--r--  2.0 unx     1385 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/learning/test_learner.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/learning/__init__.py
+-rw-r--r--  2.0 unx     4927 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/index/test_sparse.py
+-rw-r--r--  2.0 unx     1948 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/index/test_faiss.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/test/index/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/datasets/__init__.py
+-rw-r--r--  2.0 unx    11991 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/datasets/adapters.py
+-rw-r--r--  2.0 unx     2139 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tokenization/align.py
+-rw-r--r--  2.0 unx      218 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tokenization/tokenizers.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/xpmir/tokenization/__init__.py
+-rw-r--r--  2.0 unx     3784 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/PKG-INFO
+-rw-r--r--  2.0 unx        1 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/not-zip-safe
+-rw-r--r--  2.0 unx        6 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/top_level.txt
+-rw-r--r--  2.0 unx     5628 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/SOURCES.txt
+-rw-r--r--  2.0 unx      278 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/requires.txt
+-rw-r--r--  2.0 unx        1 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/dependency_links.txt
+-rw-r--r--  2.0 unx       96 b- defN 23-Jul-24 11:53 experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/entry_points.txt
+241 files, 471576 bytes uncompressed, 154289 bytes compressed:  67.3%
```

## zipnote {}

```diff
@@ -1,724 +1,724 @@
-Filename: experimaestro-ir-0.6.2/
+Filename: experimaestro-ir-1.0.0/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/hf_models/
+Filename: experimaestro-ir-1.0.0/hf_models/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/
+Filename: experimaestro-ir-1.0.0/snippets/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/snippets/
+Filename: experimaestro-ir-1.0.0/.github/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/examples/
+Filename: experimaestro-ir-1.0.0/docs/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github/
+Filename: experimaestro-ir-1.0.0/examples/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/
+Filename: experimaestro-ir-1.0.0/src/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.readthedocs.yml
+Filename: experimaestro-ir-1.0.0/requirements.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/pyproject.toml
+Filename: experimaestro-ir-1.0.0/setup.cfg
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/LICENSE
+Filename: experimaestro-ir-1.0.0/PKG-INFO
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/PKG-INFO
+Filename: experimaestro-ir-1.0.0/CHANGELOG.md
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github_changelog_generator
+Filename: experimaestro-ir-1.0.0/.gitignore
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/tox.ini
+Filename: experimaestro-ir-1.0.0/.github_changelog_generator
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/setup.py
+Filename: experimaestro-ir-1.0.0/LICENSE
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.pre-commit-config.yaml
+Filename: experimaestro-ir-1.0.0/README.md
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/setup.cfg
+Filename: experimaestro-ir-1.0.0/setup.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/README.md
+Filename: experimaestro-ir-1.0.0/pyproject.toml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/requirements.txt
+Filename: experimaestro-ir-1.0.0/tox.ini
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.gitignore
+Filename: experimaestro-ir-1.0.0/.readthedocs.yml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/CHANGELOG.md
+Filename: experimaestro-ir-1.0.0/.pre-commit-config.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/hf_models/splade.py
+Filename: experimaestro-ir-1.0.0/hf_models/README.md
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/hf_models/README.md
+Filename: experimaestro-ir-1.0.0/hf_models/tas_balanced.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/hf_models/tas_balanced.py
+Filename: experimaestro-ir-1.0.0/hf_models/splade.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/
+Filename: experimaestro-ir-1.0.0/snippets/irds/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/
+Filename: experimaestro-ir-1.0.0/snippets/irds/irds-documents.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/requires.txt
+Filename: experimaestro-ir-1.0.0/snippets/irds/irds-qrels.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/not-zip-safe
+Filename: experimaestro-ir-1.0.0/snippets/irds/irds-query.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/PKG-INFO
+Filename: experimaestro-ir-1.0.0/.github/workflows/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/dependency_links.txt
+Filename: experimaestro-ir-1.0.0/.github/release.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/entry_points.txt
+Filename: experimaestro-ir-1.0.0/.github/workflows/pytest.yml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/top_level.txt
+Filename: experimaestro-ir-1.0.0/.github/workflows/python-publish.yml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/SOURCES.txt
+Filename: experimaestro-ir-1.0.0/docs/source/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/metrics/
+Filename: experimaestro-ir-1.0.0/docs/requirements.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/index/
+Filename: experimaestro-ir-1.0.0/docs/README.md
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/text/
+Filename: experimaestro-ir-1.0.0/docs/make.bat
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tokenization/
+Filename: experimaestro-ir-1.0.0/docs/Makefile
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/
+Filename: experimaestro-ir-1.0.0/docs/source/letor/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/
+Filename: experimaestro-ir-1.0.0/docs/source/data/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/
+Filename: experimaestro-ir-1.0.0/docs/source/text/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/rankers/
+Filename: experimaestro-ir-1.0.0/docs/source/papers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/
+Filename: experimaestro-ir-1.0.0/docs/source/hooks.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/documents/
+Filename: experimaestro-ir-1.0.0/docs/source/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/
+Filename: experimaestro-ir-1.0.0/docs/source/neural.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/
+Filename: experimaestro-ir-1.0.0/docs/source/evaluation.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/
+Filename: experimaestro-ir-1.0.0/docs/source/pretrained.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/utils/
+Filename: experimaestro-ir-1.0.0/docs/source/conf.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tasks/
+Filename: experimaestro-ir-1.0.0/docs/source/retrieval.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/configuration/
+Filename: experimaestro-ir-1.0.0/docs/source/misc.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/measures.py
+Filename: experimaestro-ir-1.0.0/docs/source/letor/samplers.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/_version.py
+Filename: experimaestro-ir-1.0.0/docs/source/letor/optimization.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/context.py
+Filename: experimaestro-ir-1.0.0/docs/source/letor/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/compat.py
+Filename: experimaestro-ir-1.0.0/docs/source/letor/trainers.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/__init__.py
+Filename: experimaestro-ir-1.0.0/docs/source/data/types.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/__main__.py
+Filename: experimaestro-ir-1.0.0/docs/source/data/xpmir.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/distributed.py
+Filename: experimaestro-ir-1.0.0/docs/source/data/adapters.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/evaluation.py
+Filename: experimaestro-ir-1.0.0/docs/source/data/irds.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/models.py
+Filename: experimaestro-ir-1.0.0/docs/source/data/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/mkdocs_init.py
+Filename: experimaestro-ir-1.0.0/docs/source/text/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/metrics/__init__.py
+Filename: experimaestro-ir-1.0.0/docs/source/text/huggingface.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/index/anserini.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/helpers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/index/sparse.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/monobert.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/index/__init__.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/index/faiss.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/splade.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/text/wordvec_vocab.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/helpers/msmarco.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/text/huggingface.py
+Filename: experimaestro-ir-1.0.0/docs/source/papers/helpers/index.rst
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/text/__init__.py
+Filename: experimaestro-ir-1.0.0/examples/bm25.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/text/encoders.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tokenization/align.py
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tokenization/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/metrics/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tokenization/tokenizers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/interaction/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/modules/
+Filename: experimaestro-ir-1.0.0/src/xpmir/utils/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/common.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/configuration/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/dual.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/text/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/huggingface.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/tasks/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/colbert.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/splade.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/documents/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/cross.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/jointclassifier.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/rankers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/interaction/drmm.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/index/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/interaction/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/modules/rbf_kernels.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/modules/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/datasets/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/neural/modules/interaction_matrix.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/tokenization/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/
+Filename: experimaestro-ir-1.0.0/src/xpmir/distributed.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/
+Filename: experimaestro-ir-1.0.0/src/xpmir/measures.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/helpers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/mkdocs_init.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/
+Filename: experimaestro-ir-1.0.0/src/xpmir/_version.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/models.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/results.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/evaluation.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/cli.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/compat.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/experiment.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/context.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/configuration.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/__main__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/small.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/metrics/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/monobert/normal.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/experiment.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/distillation/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/configuration.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/records.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/pipelines.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/learner.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_doc.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/utils.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/small.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/samplers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pairwise.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_DistilMSE.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/batchwise.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/helpers/msmarco.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/helpers/optim.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/multiple.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/helpers/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pointwise.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/experiment.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/distillation/pairwise.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/configuration.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/distillation/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/letor/distillation/samplers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/small.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/interaction/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/papers/duobert/normal.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/modules/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/index/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/colbert.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/neural/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/cross.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/rankers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/dual.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/letor/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/jointclassifier.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/utils/
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/huggingface.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/index/test_faiss.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/splade.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/index/test_sparse.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/interaction/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/index/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/interaction/drmm.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/neural/test_forward.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/modules/rbf_kernels.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/neural/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/modules/interaction_matrix.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/rankers/test_full.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/neural/modules/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/rankers/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/utils/utils.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/letor/test_samplers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/utils/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/letor/test_trainers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/utils/functools.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/letor/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/utils/iter.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/utils/test_iter.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/configuration/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/utils/utils.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/text/huggingface.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/test/utils/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/text/encoders.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/rankers/standard.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/text/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/rankers/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/text/wordvec_vocab.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/rankers/full.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/tasks/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/rankers/mergers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/trec.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/irds/
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/apex.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/adapters.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/plaintext.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/robust.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/anserini.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/interfaces/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/irds/utils.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/documents/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/irds/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/documents/samplers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/irds/data.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/datasets/irds/datasets.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/documents/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/co/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/documents/samplers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/com/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/distillation/
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/batchers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/optim.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/metrics.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/context.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/utils.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/schedulers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/records.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/devices.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/samplers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/learner.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pointwise.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/com/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/batchwise.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/sebastian-hofstaetter/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/multiple.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/rankers/standard.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pairwise.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/rankers/mergers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/distillation/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/rankers/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/distillation/pairwise.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/rankers/full.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/letor/distillation/samplers.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/devices.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/trainers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/optim.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/co/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/metrics.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/schedulers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/com/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/batchers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/learner.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/base.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/context.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/learning/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/index/faiss.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/
+Filename: experimaestro-ir-1.0.0/src/xpmir/index/anserini.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/index/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/
+Filename: experimaestro-ir-1.0.0/src/xpmir/index/sparse.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/helpers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/com/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/results.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/sebastian-hofstaetter/
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/cli.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/anserini.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/helpers/msmarco.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/trec.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/helpers/optim.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/helpers/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/apex.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/small.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/interfaces/plaintext.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/experiment.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/utils/functools.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/normal.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/utils/iter.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/configuration.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/utils/utils.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/monobert/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/utils/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/small.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/tasks/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/experiment.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/src/xpmir/configuration/__init__.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/normal.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/snippets/irds/
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/configuration.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/snippets/irds/irds-documents.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/duobert/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/snippets/irds/irds-query.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/small.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/snippets/irds/irds-qrels.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal_DistilMSE.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/examples/bm25.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/experiment.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github/workflows/
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github/release.yaml
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/configuration.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github/workflows/pytest.yml
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/.github/workflows/python-publish.yml
+Filename: experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal_doc.yaml
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/letor/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/Makefile
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/neural/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/README.md
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/utils/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/requirements.txt
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/rankers/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/make.bat
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/learning/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/text/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/index/
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/letor/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/letor/test_trainers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/letor/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/retrieval.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/letor/test_samplers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/index.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/neural/test_forward.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/hooks.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/neural/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/evaluation.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/utils/test_iter.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/neural.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/utils/utils.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/misc.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/utils/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/pretrained.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/rankers/test_full.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/conf.py
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/rankers/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/text/index.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/learning/test_learner.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/text/huggingface.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/learning/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/helpers/
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/index/test_sparse.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/index.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/index/test_faiss.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/splade.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/test/index/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/monobert.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/datasets/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/helpers/index.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/datasets/adapters.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/papers/helpers/msmarco.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/tokenization/align.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/letor/optimization.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/tokenization/tokenizers.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/letor/index.rst
+Filename: experimaestro-ir-1.0.0/src/xpmir/tokenization/__init__.py
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/letor/samplers.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/PKG-INFO
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/letor/trainers.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/not-zip-safe
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/types.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/top_level.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/index.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/SOURCES.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/xpmir.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/requires.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/adapters.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/dependency_links.txt
 Comment: 
 
-Filename: experimaestro-ir-0.6.2/docs/source/data/irds.rst
+Filename: experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/entry_points.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `experimaestro-ir-0.6.2/pyproject.toml` & `experimaestro-ir-1.0.0/pyproject.toml`

 * *Files 3% similar despite different names*

```diff
@@ -24,15 +24,14 @@
 repository = "https://github.com/bpiwowar/experimaestro-ir"
 
 [project.scripts]
 xpmir = "xpmir.__main__:main"
 
 [project.entry-points."datamaestro.repositories"]
 ir = "xpmir:Repository"
-irds = "xpmir.datasets.irds:Repository"
 
 [tool.setuptools_scm]
 write_to = "src/xpmir/_version.py"
 fallback_version = "0.0.0-dev"
 
 [build-system]
 requires = ["setuptools>=45", "setuptools_scm[toml]>=6.2", "wheel"]
```

## Comparing `experimaestro-ir-0.6.2/LICENSE` & `experimaestro-ir-1.0.0/LICENSE`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/PKG-INFO` & `experimaestro-ir-1.0.0/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: experimaestro-ir
-Version: 0.6.2
+Version: 1.0.0
 Summary: Experimaestro common module for IR experiments
 Author-email: Benjamin Piwowarski <benjamin@piwowarski.fr>
 License: GPL-3
 Project-URL: homepage, https://github.com/bpiwowar/experimaestro-ir
 Project-URL: documentation, https://experimaestro-ir.readthedocs.io/en/latest/
 Project-URL: repository, https://github.com/bpiwowar/experimaestro-ir
 Keywords: neural information retrieval,information retrieval,experiments
@@ -27,22 +27,31 @@
 
 # Information Retrieval for experimaestro
 
 Information Retrieval module for [experimaestro](https://experimaestro-python.readthedocs.io/)
 
 The full documentation can be read at [IR@experimaestro](https://experimaestro-ir.readthedocs.io/).
 
+##  Version 1.0 roadmap
+
+Version 1.0 will be released before SIGIR 2023. This version aims to clean-up some design issues that are summarized in the [roadmap](https://github.com/experimaestro/experimaestro-ir/issues/9), so expect some changes in the API before the release.
+
 ## Install
 
 Base experimaestro-IR can be installed with `pip install xpmir`.
 Functionalities can be added by installing optional dependencies:
 
 - `pip install xpmir[neural]` to install neural-IR packages (torch, etc.)
 - `pip install xpmir[anserini]` to install Anserini related packages
 
+For the development version, you can:
+
+- If you just want the development version: install with `pip install git+https://github.com/experimaestro/experimaestro-ir.git`
+- If you want to edit the code: clone and then do a `pip install -e .` within the directory
+
 ## What's inside?
 
 - Collection management (using datamaestro)
     - Interface for the [IR datasets library](https://ir-datasets.com/)
     - Splitting IR datasets
     - Shuffling training triplets
 - Representation
@@ -60,15 +69,16 @@
 - Neural IR
     - Cross-Encoder
     - Splade
     - DRMM
     - ColBERT
 - Paper reproduction:
     - *MonoBERT* (Passage Re-ranking with BERT. Rodrigo Nogueira and Kyunghyun Cho. 2019)
-    - (planned) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
-    - (planned) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (alpha) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
+    - (beta) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (planned) ANCE
 - Pre-trained models
     - [HuggingFace](https://huggingface.co) [integration](https://experimaestro-ir.readthedocs.io/en/latest/pretrained.html) (direct, through the Sentence Transformers library)
 
 ## Thanks
 
 Some parts of the code have been adapted from [OpenNIR](https://github.com/Georgetown-IR-Lab/OpenNIR)
```

## Comparing `experimaestro-ir-0.6.2/setup.py` & `experimaestro-ir-1.0.0/setup.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/setup.cfg` & `experimaestro-ir-1.0.0/setup.cfg`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/README.md` & `experimaestro-ir-1.0.0/README.md`

 * *Files 21% similar despite different names*

```diff
@@ -3,22 +3,31 @@
 
 # Information Retrieval for experimaestro
 
 Information Retrieval module for [experimaestro](https://experimaestro-python.readthedocs.io/)
 
 The full documentation can be read at [IR@experimaestro](https://experimaestro-ir.readthedocs.io/).
 
+##  Version 1.0 roadmap
+
+Version 1.0 will be released before SIGIR 2023. This version aims to clean-up some design issues that are summarized in the [roadmap](https://github.com/experimaestro/experimaestro-ir/issues/9), so expect some changes in the API before the release.
+
 ## Install
 
 Base experimaestro-IR can be installed with `pip install xpmir`.
 Functionalities can be added by installing optional dependencies:
 
 - `pip install xpmir[neural]` to install neural-IR packages (torch, etc.)
 - `pip install xpmir[anserini]` to install Anserini related packages
 
+For the development version, you can:
+
+- If you just want the development version: install with `pip install git+https://github.com/experimaestro/experimaestro-ir.git`
+- If you want to edit the code: clone and then do a `pip install -e .` within the directory
+
 ## What's inside?
 
 - Collection management (using datamaestro)
     - Interface for the [IR datasets library](https://ir-datasets.com/)
     - Splitting IR datasets
     - Shuffling training triplets
 - Representation
@@ -36,15 +45,16 @@
 - Neural IR
     - Cross-Encoder
     - Splade
     - DRMM
     - ColBERT
 - Paper reproduction:
     - *MonoBERT* (Passage Re-ranking with BERT. Rodrigo Nogueira and Kyunghyun Cho. 2019)
-    - (planned) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
-    - (planned) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (alpha) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
+    - (beta) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (planned) ANCE
 - Pre-trained models
     - [HuggingFace](https://huggingface.co) [integration](https://experimaestro-ir.readthedocs.io/en/latest/pretrained.html) (direct, through the Sentence Transformers library)
 
 ## Thanks
 
 Some parts of the code have been adapted from [OpenNIR](https://github.com/Georgetown-IR-Lab/OpenNIR)
```

## Comparing `experimaestro-ir-0.6.2/.gitignore` & `experimaestro-ir-1.0.0/.gitignore`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/hf_models/splade.py` & `experimaestro-ir-1.0.0/hf_models/splade.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/hf_models/tas_balanced.py` & `experimaestro-ir-1.0.0/hf_models/tas_balanced.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/PKG-INFO` & `experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: experimaestro-ir
-Version: 0.6.2
+Version: 1.0.0
 Summary: Experimaestro common module for IR experiments
 Author-email: Benjamin Piwowarski <benjamin@piwowarski.fr>
 License: GPL-3
 Project-URL: homepage, https://github.com/bpiwowar/experimaestro-ir
 Project-URL: documentation, https://experimaestro-ir.readthedocs.io/en/latest/
 Project-URL: repository, https://github.com/bpiwowar/experimaestro-ir
 Keywords: neural information retrieval,information retrieval,experiments
@@ -27,22 +27,31 @@
 
 # Information Retrieval for experimaestro
 
 Information Retrieval module for [experimaestro](https://experimaestro-python.readthedocs.io/)
 
 The full documentation can be read at [IR@experimaestro](https://experimaestro-ir.readthedocs.io/).
 
+##  Version 1.0 roadmap
+
+Version 1.0 will be released before SIGIR 2023. This version aims to clean-up some design issues that are summarized in the [roadmap](https://github.com/experimaestro/experimaestro-ir/issues/9), so expect some changes in the API before the release.
+
 ## Install
 
 Base experimaestro-IR can be installed with `pip install xpmir`.
 Functionalities can be added by installing optional dependencies:
 
 - `pip install xpmir[neural]` to install neural-IR packages (torch, etc.)
 - `pip install xpmir[anserini]` to install Anserini related packages
 
+For the development version, you can:
+
+- If you just want the development version: install with `pip install git+https://github.com/experimaestro/experimaestro-ir.git`
+- If you want to edit the code: clone and then do a `pip install -e .` within the directory
+
 ## What's inside?
 
 - Collection management (using datamaestro)
     - Interface for the [IR datasets library](https://ir-datasets.com/)
     - Splitting IR datasets
     - Shuffling training triplets
 - Representation
@@ -60,15 +69,16 @@
 - Neural IR
     - Cross-Encoder
     - Splade
     - DRMM
     - ColBERT
 - Paper reproduction:
     - *MonoBERT* (Passage Re-ranking with BERT. Rodrigo Nogueira and Kyunghyun Cho. 2019)
-    - (planned) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
-    - (planned) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (alpha) *DuoBERT* (Multi-Stage Document Ranking with BERT. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin. 2019)
+    - (beta) *Splade v2* (SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval, Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stphane Clinchant. SIGIR 2021)
+    - (planned) ANCE
 - Pre-trained models
     - [HuggingFace](https://huggingface.co) [integration](https://experimaestro-ir.readthedocs.io/en/latest/pretrained.html) (direct, through the Sentence Transformers library)
 
 ## Thanks
 
 Some parts of the code have been adapted from [OpenNIR](https://github.com/Georgetown-IR-Lab/OpenNIR)
```

## Comparing `experimaestro-ir-0.6.2/src/experimaestro_ir.egg-info/SOURCES.txt` & `experimaestro-ir-1.0.0/src/experimaestro_ir.egg-info/SOURCES.txt`

 * *Files 6% similar despite different names*

```diff
@@ -64,19 +64,14 @@
 src/xpmir/evaluation.py
 src/xpmir/measures.py
 src/xpmir/mkdocs_init.py
 src/xpmir/models.py
 src/xpmir/configuration/__init__.py
 src/xpmir/datasets/__init__.py
 src/xpmir/datasets/adapters.py
-src/xpmir/datasets/robust.py
-src/xpmir/datasets/irds/__init__.py
-src/xpmir/datasets/irds/data.py
-src/xpmir/datasets/irds/datasets.py
-src/xpmir/datasets/irds/utils.py
 src/xpmir/dm/__init__.py
 src/xpmir/dm/config/__init__.py
 src/xpmir/dm/config/ca/__init__.py
 src/xpmir/dm/config/ca/uwaterloo/__init__.py
 src/xpmir/dm/config/ca/uwaterloo/jimmylin/__init__.py
 src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py
 src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py
@@ -89,37 +84,40 @@
 src/xpmir/index/faiss.py
 src/xpmir/index/sparse.py
 src/xpmir/interfaces/__init__.py
 src/xpmir/interfaces/anserini.py
 src/xpmir/interfaces/apex.py
 src/xpmir/interfaces/plaintext.py
 src/xpmir/interfaces/trec.py
+src/xpmir/learning/__init__.py
+src/xpmir/learning/base.py
+src/xpmir/learning/batchers.py
+src/xpmir/learning/context.py
+src/xpmir/learning/devices.py
+src/xpmir/learning/learner.py
+src/xpmir/learning/metrics.py
+src/xpmir/learning/optim.py
+src/xpmir/learning/schedulers.py
+src/xpmir/learning/trainers.py
 src/xpmir/letor/__init__.py
-src/xpmir/letor/batchers.py
-src/xpmir/letor/context.py
-src/xpmir/letor/devices.py
 src/xpmir/letor/learner.py
-src/xpmir/letor/metrics.py
-src/xpmir/letor/optim.py
 src/xpmir/letor/records.py
 src/xpmir/letor/samplers.py
-src/xpmir/letor/schedulers.py
 src/xpmir/letor/utils.py
 src/xpmir/letor/distillation/__init__.py
 src/xpmir/letor/distillation/pairwise.py
 src/xpmir/letor/distillation/samplers.py
 src/xpmir/letor/trainers/__init__.py
 src/xpmir/letor/trainers/batchwise.py
 src/xpmir/letor/trainers/multiple.py
 src/xpmir/letor/trainers/pairwise.py
 src/xpmir/letor/trainers/pointwise.py
 src/xpmir/metrics/__init__.py
 src/xpmir/neural/__init__.py
 src/xpmir/neural/colbert.py
-src/xpmir/neural/common.py
 src/xpmir/neural/cross.py
 src/xpmir/neural/dual.py
 src/xpmir/neural/huggingface.py
 src/xpmir/neural/jointclassifier.py
 src/xpmir/neural/splade.py
 src/xpmir/neural/interaction/__init__.py
 src/xpmir/neural/interaction/drmm.py
@@ -144,25 +142,26 @@
 src/xpmir/papers/monobert/small.yaml
 src/xpmir/papers/splade/__init__.py
 src/xpmir/papers/splade/configuration.py
 src/xpmir/papers/splade/experiment.py
 src/xpmir/papers/splade/normal.yaml
 src/xpmir/papers/splade/normal_DistilMSE.yaml
 src/xpmir/papers/splade/normal_doc.yaml
-src/xpmir/papers/splade/pipelines.py
 src/xpmir/papers/splade/small.yaml
 src/xpmir/rankers/__init__.py
 src/xpmir/rankers/full.py
 src/xpmir/rankers/mergers.py
 src/xpmir/rankers/standard.py
 src/xpmir/tasks/__init__.py
 src/xpmir/test/__init__.py
 src/xpmir/test/index/__init__.py
 src/xpmir/test/index/test_faiss.py
 src/xpmir/test/index/test_sparse.py
+src/xpmir/test/learning/__init__.py
+src/xpmir/test/learning/test_learner.py
 src/xpmir/test/letor/__init__.py
 src/xpmir/test/letor/test_samplers.py
 src/xpmir/test/letor/test_trainers.py
 src/xpmir/test/neural/__init__.py
 src/xpmir/test/neural/test_forward.py
 src/xpmir/test/rankers/__init__.py
 src/xpmir/test/rankers/test_full.py
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/measures.py` & `experimaestro-ir-1.0.0/src/xpmir/measures.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/context.py` & `experimaestro-ir-1.0.0/src/xpmir/context.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/distributed.py` & `experimaestro-ir-1.0.0/src/xpmir/distributed.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/evaluation.py` & `experimaestro-ir-1.0.0/src/xpmir/evaluation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 import sys
 from itertools import chain
 import pandas as pd
 from pathlib import Path
 from typing import DefaultDict, Dict, List, Protocol, Union, Tuple
 import ir_measures
-from datamaestro_text.data.ir import Adhoc, AdhocAssessments, AdhocDocuments
+from datamaestro_text.data.ir import Adhoc, AdhocAssessments, Documents
 from experimaestro import Task, Param, pathgenerator, Annotated, tags, TagDict
 from datamaestro_text.data.ir import AdhocResults
 from datamaestro_text.data.ir.trec import TrecAdhocRun, TrecAdhocResults
 from xpmir.measures import Measure
 import xpmir.measures as m
 from xpmir.metrics import evaluator
 from xpmir.rankers import Retriever
 
 from experimaestro.launchers import Launcher
 
 
 def get_evaluator(metrics: List[ir_measures.Metric], assessments: AdhocAssessments):
     qrels = {
-        assessedTopic.qid: {r.docno: r.rel for r in assessedTopic.assessments}
+        assessedTopic.topic_id: {r.doc_id: r.rel for r in assessedTopic.assessments}
         for assessedTopic in assessments.iter()
     }
     return evaluator(metrics, qrels)
 
 
 class BaseEvaluation(Task):
     """Base class for evaluation tasks"""
@@ -32,50 +32,53 @@
 
     aggregated: Annotated[Path, pathgenerator("aggregated.txt")]
     """Path for aggregated results"""
 
     detailed: Annotated[Path, pathgenerator("detailed.dat")]
     """Path for detailed results"""
 
-    def config(self):
-        return TrecAdhocResults(
-            id="",
-            results=self.aggregated,
-            detailed=self.detailed,
-            metrics=self.measures,
+    def task_outputs(self, dep):
+        return dep(
+            TrecAdhocResults(
+                id="",
+                results=self.aggregated,
+                detailed=self.detailed,
+                metrics=self.measures,
+            )
         )
 
     def _execute(self, run, assessments):
         """Evaluate an IR ad-hoc run with trec-eval"""
 
         evaluator = get_evaluator([measure() for measure in self.measures], assessments)
 
         def print_line(fp, measure, scope, value):
             fp.write("{:25s}{:8s}{:.4f}\n".format(measure, scope, value))
 
         with self.detailed.open("w") as fp:
             for metric in evaluator.iter_calc(run):
                 print_line(fp, str(metric.measure), metric.query_id, metric.value)
 
-        # TODO: work-around bug in pytrec_eval
-        # https://github.com/terrierteam/ir_measures/issues/49
+        # # TODO: work-around bug in pytrec_eval
+        # # https://github.com/terrierteam/ir_measures/issues/49
+        # (the issue is closed, but no new release)
         evaluator = get_evaluator([m() for m in self.measures], assessments)
 
         with self.aggregated.open("w") as fp:
             for key, value in evaluator.calc_aggregate(run).items():
                 print_line(fp, str(key), "all", value)
 
 
 def get_run(retriever: Retriever, dataset: Adhoc):
     """Evaluate a retriever on a dataset"""
     results = retriever.retrieve_all(
-        {topic.qid: topic.text for topic in dataset.topics.iter()}
+        {topic.get_id(): topic.get_text() for topic in dataset.topics.iter()}
     )
     return {
-        qid: {sd.docid: sd.score for sd in scoredocs}
+        qid: {sd.document.get_id(): sd.score for sd in scoredocs}
         for qid, scoredocs in results.items()
     }
 
 
 def evaluate(retriever: Retriever, dataset: Adhoc, measures: List[str], details=False):
     evaluator = get_evaluator(
         [ir_measures.parse_measure(m) for m in measures], dataset.assessments
@@ -121,15 +124,15 @@
         run = get_run(self.retriever, self.dataset)
         self._execute(run, self.dataset.assessments)
 
 
 class RetrieverFactory(Protocol):
     """Generates a retriever for a given dataset"""
 
-    def __call__(self, dataset: AdhocDocuments) -> Retriever:
+    def __call__(self, dataset: Documents) -> Retriever:
         ...
 
 
 class Evaluations:
     """Holds experiment results for several models
     on one dataset"""
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/models.py` & `experimaestro-ir-1.0.0/src/xpmir/models.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/index/anserini.py` & `experimaestro-ir-1.0.0/src/xpmir/index/anserini.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/index/sparse.py` & `experimaestro-ir-1.0.0/src/xpmir/index/sparse.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,59 +1,56 @@
 """Index for sparse models"""
 
 import torch
 import numpy as np
+import sys
 from pathlib import Path
 from typing import Dict, List, Tuple
 from experimaestro import (
     Annotated,
     Config,
     Task,
     Param,
     Meta,
     pathgenerator,
     tqdm,
     Constant,
 )
-from datamaestro_text.data.ir import AdhocDocument, AdhocDocumentStore
-from xpmir.letor.batchers import Batcher
+from datamaestro_text.data.ir import Document, DocumentStore
+from xpmir.learning.batchers import Batcher
 from xpmir.utils.utils import batchiter, easylog
 from xpmir.letor import Device, DEFAULT_DEVICE
 from xpmir.text.encoders import TextEncoder
 from xpmir.rankers import Retriever, ScoredDocument
 import xpmir_rust
 
 logger = easylog()
 
 # --- Index and retriever
 
 
 class SparseRetrieverIndex(Config):
     index_path: Meta[Path]
-    documents: Param[AdhocDocumentStore]
+    documents: Param[DocumentStore]
 
     index: xpmir_rust.index.SparseBuilderIndex
     ordered = False
 
     def initialize(self, in_memory: bool):
         self.index = xpmir_rust.index.SparseBuilderIndex.load(
             str(self.index_path.absolute()), in_memory
         )
 
-    def retrieve(
-        self, query: Dict[int, float], top_k: int, content=False
-    ) -> List[ScoredDocument]:
+    def retrieve(self, query: Dict[int, float], top_k: int) -> List[ScoredDocument]:
         results = []
         for sd in self.index.search_maxscore(query, top_k):
-            doc_id = self.documents.docid_internal2external(sd.docid)
             results.append(
                 ScoredDocument(
-                    doc_id,
+                    self.documents.document_int(sd.docid),
                     sd.score,
-                    self.documents.document_text(doc_id) if content else None,
                 )
             )
 
         return results
 
 
 class SparseRetriever(Retriever):
@@ -85,52 +82,52 @@
             progress,
         ):
             for (key, _), vector in zip(
                 batch, self.encoder([text for _, text in batch]).cpu().detach().numpy()
             ):
                 (ix,) = vector.nonzero()
                 query = {ix: float(v) for ix, v in zip(ix, vector[ix])}
-                results[key] = self.index.retrieve(query, self.topk, content=False)
+                results[key] = self.index.retrieve(query, self.topk)
                 progress.update(1)
             return results
 
         batcher = self.batcher.initialize(self.batchsize)
         results = {}
         items = list(queries.items())
         with tqdm(
             desc="Retrieve documents", total=len(items), unit="queries"
         ) as progress:
             for batch in batchiter(self.batchsize, items):
                 results = batcher.reduce(batch, reducer, results, progress)
 
         return results
 
-    def retrieve(self, query: str, content=False, top_k=None) -> List[ScoredDocument]:
+    def retrieve(self, query: str, top_k=None) -> List[ScoredDocument]:
         """Search with document-at-a-time (DAAT) strategy
 
         :param top_k: Overrides the default top-K value
         """
 
         # Build up iterators
         vector = self.encoder([query])[0].cpu().detach().numpy()
         (ix,) = vector.nonzero()  # ix represents the position without 0 in the vector
         query = {
             ix: float(v) for ix, v in zip(ix, vector[ix])
         }  # generate a dict: {position:value}
-        return self.index.retrieve(query, top_k or self.topk, content=content)
+        return self.index.retrieve(query, top_k or self.topk)
 
 
 class SparseRetrieverIndexBuilder(Task):
     """Builds an index from a sparse representation
 
     Assumes that document and queries have the same dimension, and
     that the score is computed through an inner product
     """
 
-    documents: Param[AdhocDocumentStore]
+    documents: Param[DocumentStore]
     """Set of documents to index"""
 
     encoder: Param[TextEncoder]
     """The encoder"""
 
     batcher: Meta[Batcher] = Batcher()
     """Batcher used when computing representations"""
@@ -152,36 +149,44 @@
     in_memory: Meta[bool] = False
     """Whether the index should be fully loaded in memory (otherwise, uses
     virtual memory)"""
 
     version: Constant[int] = 3
     """Version 3 of the index"""
 
-    def taskoutputs(self):
+    max_docs: Param[int] = 0
+    """Maximum number of indexed documents"""
+
+    def task_outputs(self, dep):
         """Returns a sparse retriever index that can be used by a
         SparseRetriever to search efficiently for documents"""
 
-        return SparseRetrieverIndex(
-            index_path=self.index_path, documents=self.documents
+        return dep(
+            SparseRetrieverIndex(index_path=self.index_path, documents=self.documents)
         )
 
     def execute(self):
         # Encode all documents
         logger.info(
             f"Load the encoder and transfer to the target device {self.device.value}"
         )
 
         self.encoder.initialize()
         self.encoder.to(self.device.value).eval()
 
         batcher = self.batcher.initialize(self.batch_size)
 
         doc_iter = tqdm(
-            self.documents.iter_documents(),
-            total=self.documents.documentcount,
+            zip(
+                range(sys.maxint if self.max_docs == 0 else self.max_docs),
+                self.documents.iter_documents(),
+            ),
+            total=self.documents.documentcount
+            if self.max_docs == 0
+            else min(self.max_docs, self.documents.documentcount),
             desc="Building the index",
         )
 
         # Create the index builder
         from shutil import rmtree
         import xpmir_rust
 
@@ -197,19 +202,15 @@
         with torch.no_grad():
             for batch in batchiter(self.batch_size, doc_iter):
                 batcher.process(batch, self.encode_documents)
 
         # Build the index
         self.indexer.build(self.in_memory)
 
-    def encode_documents(self, batch: List[AdhocDocument]):
+    def encode_documents(self, batch: List[Tuple[int, Document]]):
         # Assumes for now dense vectors
-        assert all(
-            d.internal_docid is not None for d in batch
-        ), f"No internal document ID provided by document store {type(self.documents)}"
-
-        vectors = self.encoder([d.text for d in batch]).cpu().numpy()  # bs * vocab
-        for vector, d in zip(vectors, batch):
+        vectors = (
+            self.encoder([d.get_text() for _, d in batch]).cpu().numpy()
+        )  # bs * vocab
+        for vector, (docid, _) in zip(vectors, batch):
             (nonzero_ix,) = vector.nonzero()
-            self.indexer.add(
-                d.internal_docid, nonzero_ix.astype(np.uint64), vector[nonzero_ix]
-            )
+            self.indexer.add(docid, nonzero_ix.astype(np.uint64), vector[nonzero_ix])
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/index/faiss.py` & `experimaestro-ir-1.0.0/src/xpmir/index/faiss.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,17 +6,17 @@
 from pathlib import Path
 from typing import Callable, Iterator, List, Optional, Tuple
 from experimaestro import Config, initializer
 import torch
 import numpy as np
 from experimaestro import Annotated, Meta, Task, pathgenerator, Param, tqdm
 import logging
-from datamaestro_text.data.ir import AdhocDocumentStore
+from datamaestro_text.data.ir import DocumentStore
 from xpmir.rankers import Retriever, ScoredDocument
-from xpmir.letor.batchers import Batcher
+from xpmir.learning.batchers import Batcher
 from xpmir.text.encoders import TextEncoder
 from xpmir.letor import (
     Device,
     DEFAULT_DEVICE,
     DeviceInformation,
 )
 from xpmir.utils.utils import batchiter, easylog, foreach
@@ -37,15 +37,15 @@
 
     normalize: Param[bool]
     """Whether vectors should be normalized (L2)"""
 
     faiss_index: Annotated[Path, pathgenerator("faiss.dat")]
     """Path to the file containing the index"""
 
-    documents: Param[AdhocDocumentStore]
+    documents: Param[DocumentStore]
     """The set of documents"""
 
 
 class IndexBackedFaiss(FaissIndex, Task):
     """Constructs a FAISS index backed up by an index
 
     During executions, InitializationHooks are used (pre/post)
@@ -191,17 +191,14 @@
 
     def index_documents(self, batch: List[str], index):
         x = self.encoder(batch)
         if self.normalize:
             x /= x.norm(2, keepdim=True, dim=1)
         index.add(np.ascontiguousarray(x.cpu().numpy()))
 
-    def docid_internal2external(self, docid: int):
-        return self.documents.docid_internal2external(docid)
-
 
 class FaissRetriever(Retriever):
     """Retriever based on Faiss"""
 
     encoder: Param[TextEncoder]
     """The query encoder"""
 
@@ -225,13 +222,11 @@
             self.encoder.eval()  # pass the model to the evaluation model
             encoded_query = self.encoder([query])
             if self.index.normalize:
                 encoded_query /= encoded_query.norm(2)
 
             values, indices = self._index.search(encoded_query.cpu().numpy(), self.topk)
             return [
-                ScoredDocument(
-                    self.index.docid_internal2external(int(ix)), float(value)
-                )
+                ScoredDocument(self.index.documents.document_int(int(ix)), float(value))
                 for ix, value in zip(indices[0], values[0])
                 if ix >= 0
             ]
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/text/wordvec_vocab.py` & `experimaestro-ir-1.0.0/src/xpmir/text/wordvec_vocab.py`

 * *Files 1% similar despite different names*

```diff
@@ -135,15 +135,16 @@
     random weight.
     """
 
     hashspace: Param[int] = 1000
     init_stddev: Param[float] = 0.5
     log_miss: Param[bool] = False
 
-    def initialize(self, random):
+    def __initialize__(self, random):
+        super().__initialize__()
         hash_weights = random.normal(
             scale=self.init_stddev, size=(self.hashspace, self._weights.shape[1])
         )
         self._weights = np.concatenate([self._weights, hash_weights])
 
     def tok2id(self, tok):
         try:
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/text/huggingface.py` & `experimaestro-ir-1.0.0/src/xpmir/text/huggingface.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 from typing import List, Optional, Tuple, Union
 import logging
 import re
 import torch
 import torch.nn as nn
 from experimaestro import Param, Constant, deprecate
 from xpmir.distributed import DistributableModel
-from xpmir.letor.context import InitializationTrainingHook, TrainState
+from xpmir.learning.context import InitializationTrainingHook, TrainState
 from xpmir.text.encoders import (
     Encoder,
     TokensEncoder,
     DualTextEncoder,
     TextEncoder,
     TripletTextEncoder,
 )
@@ -50,25 +50,25 @@
     def tokenizer(self):
         return AutoTokenizer.from_pretrained(self.model_id, use_fast=True)
 
     @property
     def pad_tokenid(self) -> int:
         return self.tokenizer.pad_token_id
 
-    def initialize(self, noinit=False, automodel=AutoModel):
+    def __initialize__(self, noinit=False, automodel=AutoModel):
         """Initialize the HuggingFace transformer
 
         Args:
             noinit (bool, optional): True when the weights don't need to be
             loaded. Defaults to False.
 
             automodel (type, optional): The class
             used to initialize the model. Defaults to AutoModel.
         """
-        super().initialize()
+        super().__initialize__()
 
         config = AutoConfig.from_pretrained(self.model_id)
         if noinit:
             self.model = automodel.from_config(config)
         else:
             if self.dropout == 0:
                 self.model = automodel.from_pretrained(self.model_id)
@@ -188,15 +188,16 @@
 
 
 class SentenceTransformerTextEncoder(TextEncoder):
     """A Sentence Transformers text encoder"""
 
     model_id: Param[str] = "sentence-transformers/all-MiniLM-L6-v2"
 
-    def initialize(self):
+    def __initialize__(self):
+        super().__initialize__()
         from sentence_transformers import SentenceTransformer
 
         self.model = SentenceTransformer(self.model_id)
 
     def forward(self, texts: List[str]) -> torch.Tensor:
         return self.model.encode(texts)
 
@@ -209,15 +210,16 @@
     """Model ID from huggingface"""
 
     maxlen: Param[Optional[int]] = None
     """Max length for texts"""
 
     version: Constant[int] = 2
 
-    def initialize(self):
+    def __initialize__(self):
+        super().__initialize__()
         self._tokenizer = AutoTokenizer.from_pretrained(self.model_id, use_fast=True)
         self.CLS = self._tokenizer.cls_token_id
         self.SEP = self._tokenizer.sep_token_id
         self.PAD = self._tokenizer.pad_token_id
         self._dummy_params = nn.Parameter(torch.Tensor())
 
     @property
@@ -254,15 +256,14 @@
     def static(self):
         return False
 
 
 @deprecate
 class HuggingfaceTokenizer(OneHotHuggingFaceEncoder):
     """The old encoder for one hot"""
-
     pass
 
 
 class TransformerEncoder(BaseTransformer, TextEncoder, DistributableModel):
     """Encodes using the [CLS] token"""
 
     maxlen: Param[Optional[int]] = None
@@ -287,16 +288,16 @@
         self.model = update(self.model)
 
 
 class TransformerTextEncoderAdapter(TextEncoder, DistributableModel):
     encoder: Param[TransformerEncoder]
     maxlen: Param[Optional[int]] = None
 
-    def initialize(self):
-        self.encoder.initialize()
+    def __initialize__(self):
+        self.encoder.__initialize__()
 
     @property
     def dimension(self):
         return self.encoder.dimension
 
     def forward(self, texts: List[str], maxlen=None):
         return self.encoder.forward(texts, maxlen=self.maxlen)
@@ -352,16 +353,16 @@
 
     maxlen_query: Param[int] = 64
     """Maximum length for the query, the first document and the second one"""
 
     maxlen_doc: Param[int] = 224
     """Maximum length for the query, the first document and the second one"""
 
-    def initialize(self, noinit=False, automodel=AutoModel):
-        super().initialize(noinit, automodel)
+    def __initialize__(self, noinit=False, automodel=AutoModel):
+        super().__initialize__(noinit, automodel)
 
         # Add an extra token type
         data = self.model.embeddings.token_type_embeddings.weight.data
         if len(data) < 3:
             logger.info("Adding an extra token type in transformer")
             data = torch.cat((data, torch.zeros(1, data.shape[1])))
             self.model.embeddings.token_type_embeddings = nn.Embedding.from_pretrained(
@@ -485,16 +486,18 @@
     transformer: Param[BaseTransformer]
     """The model for which parameters should be frozen"""
 
     freeze_embeddings: Param[bool] = False
     """Whether embeddings should be frozen"""
 
     frozen: Param[int] = 0
-    """Number of frozen layers, counting from the first processing layers (can be negative, i.e. -1 meaning until the last
-    layer excluded, etc. / 0 means no layer)"""
+    """Number of frozen layers
+
+    Counting from the first processing layers (can be negative, i.e. -1 meaning
+    until the last layer excluded, etc. / 0 means no layer)"""
 
     def __init__(self):
         self._initialized = False
 
     def __validate__(self):
         if not self.freeze_embeddings and self.frozen == 0:
             raise AssertionError("The layer freezer would do nothing")
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/text/encoders.py` & `experimaestro-ir-1.0.0/src/xpmir/text/encoders.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,23 +2,23 @@
 import sys
 import re
 
 import torch
 import torch.nn as nn
 
 from experimaestro import Config, Param
-from xpmir.letor.optim import Module
+from xpmir.learning.optim import Module
 from xpmir.letor.records import TokenizedTexts
 from xpmir.utils.utils import EasyLogger
 
 
 class Encoder(Module, EasyLogger):
     """Base class for all word and text encoders"""
 
-    def initialize(self):
+    def __initialize__(self):
         # Easy and hacky way to get the device
         self._dummy_params = nn.Parameter(torch.Tensor())
 
     def static(self):
         return True
 
     @property
@@ -225,16 +225,16 @@
 
 
 class MeanTextEncoder(TextEncoder):
     """Returns the mean of the word embeddings"""
 
     encoder: Param[TokensEncoder]
 
-    def initialize(self):
-        self.encoder.initialize()
+    def __initialize__(self):
+        self.encoder.__initialize__()
 
     def static(self):
         return self.encoder.static()
 
     @property
     def dimension(self):
         return self.encoder.dim()
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/tokenization/align.py` & `experimaestro-ir-1.0.0/src/xpmir/tokenization/align.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/dual.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/dual.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from typing import List, Optional
 import torch
 from experimaestro import Param
 from xpmir.distributed import DistributableModel
-from xpmir.letor.batchers import Batcher
+from xpmir.learning.batchers import Batcher
 from xpmir.neural import DualRepresentationScorer
 from xpmir.rankers import Retriever
 from xpmir.utils.utils import easylog, foreach
 from xpmir.text.encoders import TextEncoder
-from xpmir.letor.context import Loss, TrainerContext, TrainingHook
-from xpmir.letor.metrics import ScalarMetric
+from xpmir.learning.context import Loss, TrainerContext, TrainingHook
+from xpmir.learning.metrics import ScalarMetric
 
 logger = easylog()
 
 
 class DualVectorListener(TrainingHook):
     """Listener called with the (vectorial) representation of queries and
     documents
@@ -93,45 +93,14 @@
         """
         from xpmir.text.huggingface import SentenceTransformerTextEncoder
 
         encoder = SentenceTransformerTextEncoder(hf_id)
         return cls(encoder, **kwargs)
 
 
-class DenseBaseEncoder(TextEncoder):
-    """A text encoder adapter for dense scorers (either query or document encoder)"""
-
-    scorer: Param[Dense]
-
-    def initialize(self):
-        self.scorer.initialize(None)
-
-
-class DenseDocumentEncoder(DenseBaseEncoder):
-    @property
-    def dimension(self):
-        """Returns the dimension of the representation"""
-        return self.scorer.encoder.dimension
-
-    def forward(self, texts: List[str]) -> torch.Tensor:
-        """Returns a matrix encoding the provided texts"""
-        return self.scorer.encode_documents(texts)
-
-
-class DenseQueryEncoder(DenseBaseEncoder):
-    @property
-    def dimension(self):
-        """Returns the dimension of the representation"""
-        return self.scorer._query_encoder.dimension
-
-    def forward(self, texts: List[str]) -> torch.Tensor:
-        """Returns a matrix encoding the provided texts"""
-        return self.scorer.encode_queries(texts)
-
-
 class CosineDense(Dense):
     """Dual model based on cosine similarity."""
 
     def encode_queries(self, texts):
         queries = (self.query_encoder or self.encoder)(texts)
         return queries / queries.norm(dim=1, keepdim=True)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/huggingface.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/huggingface.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import TrainerContext
 from xpmir.letor.records import BaseRecords
 from xpmir.neural import TorchLearnableScorer
 from experimaestro import Param
 from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig
 from xpmir.letor.records import TokenizedTexts
 from typing import List, Tuple
 from xpmir.distributed import DistributableModel
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/colbert.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/colbert.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # https://github.com/stanford-futuredata/ColBERT/blob/v0.2/colbert/modeling/colbert.py
 
 from typing import List
 from experimaestro import Config, Constant, Param, default, Annotated
 import torch
 from torch import nn
 import torch.nn.functional as F
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import TrainerContext
 from xpmir.letor.records import BaseRecords
 from xpmir.neural.interaction import InteractionScorer
 
 
 class Similarity(Config):
     def __call__(self, queries, documents) -> torch.Tensor:
         raise NotImplementedError()
@@ -88,11 +88,13 @@
         if maskoutput:
             mask = tokens.mask.unsqueeze(2).float().to(output.device)
             output = output * mask
 
         return F.normalize(output, p=2, dim=2)
 
     def _forward(self, inputs: BaseRecords, info: TrainerContext = None):
-        queries = self._encode([q.text for q in inputs.queries], False)
-        documents = self._encode([d.text for d in inputs.documents], True)
+        queries = self._encode([qr.topic.get_text() for qr in inputs.queries], False)
+        documents = self._encode(
+            [dr.document.get_text() for dr in inputs.documents], True
+        )
 
         return self.similarity(queries, documents)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/splade.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/splade.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 from typing import List, Optional
 from experimaestro import Config, Param
 import torch.nn as nn
 import torch
-from experimaestro import initializer
 from xpmir.distributed import DistributableModel
 from transformers import AutoModelForMaskedLM
 from xpmir.text.huggingface import TransformerTokensEncoder, OneHotHuggingFaceEncoder
 from xpmir.text.encoders import TextEncoder
 from xpmir.neural.dual import DotDense, ScheduledFlopsRegularizer
 from xpmir.utils.utils import easylog
 
@@ -68,17 +67,16 @@
 
     aggregation: Param[Aggregation]
     """How to aggregate the vectors"""
 
     maxlen: Param[Optional[int]] = None
     """Max length for texts"""
 
-    @initializer
-    def initialize(self):
-        self.encoder.initialize(automodel=AutoModelForMaskedLM)
+    def __initialize__(self, random=None):
+        self.encoder.initialize(noinit=random is None, automodel=AutoModelForMaskedLM)
         self.model = SpladeTextEncoderModel(self.encoder, self.aggregation)
 
     def forward(self, texts: List[str]) -> torch.Tensor:
         """Returns a batch x vocab tensor"""
         tokenized = self.encoder.batch_tokenize(texts, mask=True, maxlen=self.maxlen)
         out = self.model(tokenized)
         return out
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/cross.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/cross.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import torch
 from experimaestro import Param
 from xpmir.distributed import DistributableModel
-from xpmir.letor.batchers import Batcher
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.batchers import Batcher
+from xpmir.learning.context import TrainerContext
 from xpmir.letor.records import (
     BaseRecords,
     PairwiseRecords,
 )
 from xpmir.neural import TorchLearnableScorer
 from xpmir.text.encoders import DualTextEncoder, TripletTextEncoder
 from xpmir.rankers import (
@@ -38,15 +38,18 @@
     def _initialize(self, random):
         self.encoder.initialize()
         self.classifier = torch.nn.Linear(self.encoder.dimension, 1)
 
     def forward(self, inputs: BaseRecords, info: TrainerContext = None):
         # Encode queries and documents
         pairs = self.encoder(
-            [(q.text, d.text) for q, d in zip(inputs.queries, inputs.documents)]
+            [
+                (tr.topic.get_text(), dr.document.get_text())
+                for tr, dr in zip(inputs.topics, inputs.documents)
+            ]
         )  # shape (batch_size * dimension)
         return self.classifier(pairs).squeeze(1)
 
     def distribute_models(self, update):
         self.encoder.model = update(self.encoder.model)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 import itertools
 from typing import Iterable, List, Optional
 import torch
 import torch.nn as nn
-from xpmir.letor.batchers import Sliceable
+from xpmir.learning.batchers import Sliceable
 
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import TrainerContext
 from xpmir.letor.records import BaseRecords
-from xpmir.letor.optim import Module
-from xpmir.rankers import AbstractLearnableScorer, LearnableScorer
+from xpmir.learning.optim import Module
+from xpmir.rankers import LearnableScorer
 
 
 class TorchLearnableScorer(LearnableScorer, Module):
     """Base class for torch-learnable scorers"""
 
     def __init__(self):
         nn.Module.__init__(self)
@@ -30,16 +30,20 @@
 
     This is the base class for all scorers that depend on a map
     of cosine/inner products between query and document tokens.
     """
 
     def forward(self, inputs: BaseRecords, info: Optional[TrainerContext] = None):
         # Forward to model
-        enc_queries = self.encode_queries([q.text for q in inputs.unique_queries])
-        enc_documents = self.encode_documents([d.text for d in inputs.unique_documents])
+        enc_queries = self.encode_queries(
+            [q.topic.get_text() for q in inputs.unique_queries]
+        )
+        enc_documents = self.encode_documents(
+            [d.document.get_text() for d in inputs.unique_documents]
+        )
 
         # Get the pairs
         pairs = inputs.pairs()
         q_ix, d_ix = pairs
 
         # TODO: Use a product query x document if possible
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/interaction/drmm.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/interaction/drmm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import math
 from typing import Optional
-from experimaestro import param, Config, Choices, Param, default
+from experimaestro import Config, Param, default
 import torch
 from torch import nn
 from typing_extensions import Annotated
 from xpmir.index import Index
 from xpmir.neural.interaction import InteractionScorer
 import xpmir.neural.modules as modules
 
@@ -25,34 +25,38 @@
 
         # +1e-5 to nudge scores of 1 to above threshold
         bins = ((simmat + 1.00001) / 2.0 * (self.nbins - 1)).int()
         weights = (
             (dtoks != -1).reshape(BATCH, 1, DLEN).expand(BATCH, QLEN, DLEN)
             * (qtoks != -1).reshape(BATCH, QLEN, 1).expand(BATCH, QLEN, DLEN)
         ).float()
-        # apparently no way to batch this... https://discuss.pytorch.org/t/histogram-function-in-pytorch/5350
+        # apparently no way to batch this...
+        # https://discuss.pytorch.org/t/histogram-function-in-pytorch/5350
+
+        # WARNING: this line (and the similar line below) improve performance
+        # tenfold when on GPU
         bins, weights = (
             bins.cpu(),
             weights.cpu(),
-        )  # WARNING: this line (and the similar line below) improve performance tenfold when on GPU
+        )
         histogram = []
         for superbins, w in zip(bins, weights):
             result = []
             for b in superbins:
                 result.append(
                     torch.stack(
                         [torch.bincount(q, x, self.nbins) for q, x in zip(b, w)], dim=0
                     )
                 )
             result = torch.stack(result, dim=0)
             histogram.append(result)
         histogram = torch.stack(histogram, dim=0)
-        histogram = histogram.to(
-            simmat.device
-        )  # WARNING: this line (and the similar line above) improve performance tenfold when on GPU
+        # WARNING: this line (and the similar line above) improve performance
+        # tenfold when on GPU
+        histogram = histogram.to(simmat.device)
         return histogram
 
 
 class NormalizedHistogram(CountHistogram):
     def forward(self, simmat, dlens, dtoks, qtoks):
         result = super().forward(simmat, dlens, dtoks, qtoks)
         BATCH, QLEN, _ = simmat.shape
@@ -81,16 +85,16 @@
 
 
 class Drmm(InteractionScorer):
     """Deep Relevance Matching Model (DRMM)
 
     Implementation of the DRMM model from:
 
-      Jiafeng Guo, Yixing Fan, Qingyao Ai, and William Bruce Croft. 2016. A Deep Relevance
-      Matching Model for Ad-hoc Retrieval. In CIKM.
+      Jiafeng Guo, Yixing Fan, Qingyao Ai, and William Bruce Croft. 2016. A Deep
+      Relevance Matching Model for Ad-hoc Retrieval. In CIKM.
     """
 
     hist: Annotated[CountHistogram, default(LogCountHistogram())]
     """The histogram type"""
 
     hidden: Param[int] = 5
     """Hidden layer dimension for the feed forward matching network"""
@@ -105,15 +109,15 @@
         super().__validate__()
         assert (self.combine != "idf") or (
             self.index is not None
         ), "index must be provided if using IDF"
 
     def _initialize(self, random):
         if not self.vocab.static():
-            self.logger.warn(
+            self.logger.warning(
                 "In most cases, using vocab.train=True will not have an effect on DRMM "
                 "because the histogram is not differentiable. An exception might be if "
                 "the gradient is proped back by another means, e.g. BERT [CLS] token."
             )
         super()._initialize(random)
         self.simmat = modules.InteractionMatrix(self.vocab.pad_tokenid)
         channels = self.vocab.emb_views()
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/interaction/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/interaction/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import torch
 from experimaestro import Param
 from xpmir.neural import TorchLearnableScorer
 from xpmir.text import TokensEncoder
 from xpmir.letor.records import BaseRecords
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import TrainerContext
 
 
 class InteractionScorer(TorchLearnableScorer):
     """Interaction-based neural scorer
 
     This is the base class for all scorers that depend on a map
     of cosine/inner products between query and document token representations.
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/modules/rbf_kernels.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/modules/rbf_kernels.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/neural/modules/interaction_matrix.py` & `experimaestro-ir-1.0.0/src/xpmir/neural/modules/interaction_matrix.py`

 * *Files 9% similar despite different names*

```diff
@@ -62,13 +62,13 @@
         return torch.stack(simmats, dim=1)
 
     def encode_query_doc(
         self, encoder: TokensEncoder, inputs: BaseRecords, d_maxlen=None, q_maxlen=None
     ):
         """Returns a (batch x ... x #q x #d) tensor"""
         tokq, q, tokd, d = encoder.enc_query_doc(
-            [q.text for q in inputs.queries],
-            [d.text for d in inputs.documents],
+            [q.topic.get_text() for q in inputs.queries],
+            [d.document.get_text() for d in inputs.documents],
             d_maxlen=d_maxlen,
             q_maxlen=q_maxlen,
         )
         return self(q, d, tokq.ids, tokd.ids), tokq, tokd
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 import attr
 
 try:
     from typing import dataclass_transform
 except ImportError:
     from typing_extensions import dataclass_transform
 
+from functools import cached_property as attrs_cached_property  # noqa: F401
+
 
 @dataclass_transform(kw_only_default=True)
 def configuration(*args, **kwargs):
     """Method to define keyword only dataclasses
 
     Configurations are keyword-only
     """
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/cli.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/cli.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,33 +2,33 @@
 
 from functools import reduce
 import inspect
 import io
 import logging
 import json
 import sys
-from typing import Dict, List
+from typing import Dict, List, Tuple
 from pathlib import Path
 import pkgutil
 from typing import Optional
 import click
 from importlib import import_module
 import docstring_parser
 from termcolor import cprint
 import omegaconf
-from experimaestro import experiment, RunMode
+from experimaestro import experiment, RunMode, LauncherRegistry
 from omegaconf import OmegaConf, SCMode
 from xpmir.configuration import omegaconf_argument
 from xpmir.evaluation import EvaluationsCollection
 import xpmir.papers as papers
 from xpmir.models import XPMIRHFHub
 from xpmir.rankers import Scorer
 from xpmir.papers.results import PaperResults
 from xpmir.papers.helpers import PaperExperiment
-from xpmir.letor.optim import TensorboardService
+from xpmir.learning.optim import TensorboardService
 
 
 class ExperimentsCli(click.MultiCommand):
     def __init__(
         self, pkg_name: str, experiments: List[papers.Experiment], *args, **kwargs
     ):
         super().__init__(*args, **kwargs)
@@ -140,15 +140,15 @@
     """General command line decorator for an XPM-IR experiment
 
     This annotation adds a set of arguments for the
 
     HuggingFace upload: the documentation comes from the docstring
 
     :param tensorboard_service: If true, register a tensorboard service and transmits it
-        as ``tensorboard_service`` to function
+        as ``tensorboard_service`` to function.
     """
 
     omegaconf_schema = None
     if schema is not None:
         omegaconf_schema = OmegaConf.structured(schema())
 
     def _decorate(fn):
@@ -172,14 +172,21 @@
             click.option(
                 "--run-mode",
                 type=click.Choice(RunMode),
                 default=RunMode.NORMAL,
                 help="Sets the run mode",
             ),
             click.option(
+                "--xpm-config-dir",
+                type=Path,
+                default=None,
+                help="Path for the experimaestro config directory "
+                "(if not specified, use $HOME/.config/experimaestro)",
+            ),
+            click.option(
                 "--port",
                 type=int,
                 default=None,
                 help="Port for monitoring (can be defined in the settings.yaml file)",
             ),
             click.option(
                 "--upload-to-hub",
@@ -198,29 +205,34 @@
             click.argument("args", nargs=-1, type=click.UNPROCESSED),
         ]
 
         def cli(
             show,
             debug,
             configuration,
-            workdir,
-            host,
-            port,
+            workdir: Path,
+            host: Optional[str],
+            port: Optional[int],
+            xpm_config_dir: Optional[Path],
+            env: List[Tuple[str, str]],
             args,
-            env,
-            run_mode,
+            run_mode: RunMode,
             upload_to_hub: Optional[str],
             **kwargs,
         ):
             nonlocal omegaconf_schema
             assert schema is None or omegaconf_schema is not None
 
             logging.getLogger().setLevel(logging.DEBUG if debug else logging.INFO)
             conf_args = OmegaConf.from_dotlist(args)
 
+            if xpm_config_dir is not None:
+                assert xpm_config_dir.is_dir()
+                LauncherRegistry.set_config_dir(xpm_config_dir)
+
             configuration: PaperExperiment = OmegaConf.merge(configuration, conf_args)
             if omegaconf_schema is not None:
                 try:
                     configuration: PaperExperiment = OmegaConf.merge(
                         omegaconf_schema, configuration
                     )
                 except omegaconf.errors.ConfigKeyError as e:
@@ -245,15 +257,17 @@
                 kwargs["upload_to_hub"] = upload_to_hub
 
             kwargs = {**kwargs, "debug": debug, "run_mode": run_mode}
 
             kwargs = {key: value for key, value in kwargs.items() if key in parameters}
 
             # Run the experiment
-            logging.info("Starting experimaestro server (%s:%s)", host, port)
+            if run_mode == RunMode.NORMAL:
+                logging.info("Starting experimaestro server (%s:%s)", host, port)
+
             with experiment(
                 workdir, configuration.id, host=host, port=port, run_mode=run_mode
             ) as xp:
                 if tensorboard_service:
                     kwargs["tensorboard_service"] = xp.add_service(
                         TensorboardService(xp.resultspath / "runs")
                     )
@@ -268,15 +282,15 @@
                     if upload_to_hub is not None and "upload_to_hub" not in parameters:
                         upload_to_hub.send_scorer(
                             results.models,
                             evaluations=results.evaluations,
                             tb_logs=results.tb_logs,
                         )
 
-                    results.evaluations.output_results()
+                    print(results.evaluations.to_dataframe())  # noqa: T201
                 return results
 
         cli.__doc__ = fn.__doc__
         cmd = reduce(lambda fn, decorator: decorator(fn), decorators, cli)
         return cmd
 
     return _decorate
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/monobert/experiment.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/monobert/experiment.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 from functools import partial
 import logging
 
 from xpmir.distributed import DistributedHook
-from xpmir.letor.learner import Learner, ValidationListener
+from xpmir.learning.learner import Learner
+from xpmir.letor.learner import ValidationListener
 import xpmir.letor.trainers.pairwise as pairwise
 from xpmir.neural.cross import CrossScorer
 from experimaestro import experiment, setmeta
 from experimaestro.launcherfinder import find_launcher
-from xpmir.letor.batchers import PowerAdaptativeBatcher
-from xpmir.letor.optim import (
+from xpmir.learning.batchers import PowerAdaptativeBatcher
+from xpmir.learning.optim import (
     TensorboardService,
 )
 from xpmir.papers.cli import paper_command
 from xpmir.rankers.standard import BM25
 from xpmir.text.huggingface import DualTransformerEncoder
 from xpmir.papers.results import PaperResults
 from xpmir.utils.functools import cache
@@ -61,55 +62,64 @@
 def run(
     xp: experiment, cfg: Monobert, tensorboard_service: TensorboardService
 ) -> PaperResults:
     """monoBERT model"""
 
     launcher_learner = find_launcher(cfg.monobert.requirements)
     launcher_evaluate = find_launcher(cfg.retrieval.requirements)
+    launcher_preprocessing = find_launcher(cfg.preprocessing.requirements)
     device = cfg.device
     random = cfg.random
 
     documents = v1_passages()
-    ds_val = v1_validation_dataset(cfg.validation)
+    ds_val = v1_validation_dataset(cfg.validation, launcher=launcher_preprocessing)
 
-    tests = v1_tests()
+    tests = v1_tests(cfg.dev_test_size)
 
     # Setup indices and validation/test base retrievers
     retrievers, model_based_retrievers = get_retrievers(cfg)
-
-    test_retrievers = partial(retrievers, k=cfg.retrieval.k)  #: Test retrievers
+    val_retrievers = partial(
+        retrievers, store=documents, k=cfg.monobert.validation_top_k
+    )
+    test_retrievers = partial(
+        retrievers, store=documents, k=cfg.retrieval.k
+    )  #: Test retrievers
 
     # Search and evaluate with a random re-ranker
     random_scorer = RandomScorer(random=random).tag("scorer", "random")
     tests.evaluate_retriever(
         partial(
             model_based_retrievers,
             retrievers=test_retrievers,
             scorer=random_scorer,
             device=None,
-        )
+        ),
+        launcher=launcher_preprocessing,
     )
 
     # Search and evaluate with the base model
     tests.evaluate_retriever(test_retrievers, cfg.indexation.launcher)
 
     # Define the different launchers
-    val_retrievers = partial(retrievers, k=cfg.monobert.validation_top_k)
 
     # define the trainer for monobert
     monobert_trainer = pairwise.PairwiseTrainer(
         lossfn=pairwise.PointwiseCrossEntropyLoss(),
-        sampler=v1_docpairs_sampler(),
+        sampler=v1_docpairs_sampler(
+            sample_rate=cfg.monobert.sample_rate,
+            sample_max=cfg.monobert.sample_max,
+            launcher=launcher_preprocessing,
+        ),
         batcher=PowerAdaptativeBatcher(),
         batch_size=cfg.monobert.optimization.batch_size,
     )
 
     monobert_scorer: CrossScorer = CrossScorer(
         encoder=DualTransformerEncoder(
-            model_id="bert-base-uncased", trainable=True, maxlen=512, dropout=0.1
+            model_id=cfg.base, trainable=True, maxlen=512, dropout=0.1
         )
     ).tag("scorer", "monobert")
 
     # The validation listener evaluates the full retriever
     # (retriever + scorer) and keep the best performing model
     # on the validation set
     validation = ValidationListener(
@@ -129,15 +139,15 @@
     learner = Learner(
         # Misc settings
         device=device,
         random=random,
         # How to train the model
         trainer=monobert_trainer,
         # The model to train
-        scorer=monobert_scorer,
+        model=monobert_scorer,
         # Optimization settings
         steps_per_epoch=cfg.monobert.optimization.steps_per_epoch,
         optimizers=cfg.monobert.optimization.optimizer,
         max_epochs=cfg.monobert.optimization.max_epochs,
         # The listeners (here, for validation)
         listeners=[validation],
         # The hook used for evaluation
@@ -146,28 +156,28 @@
 
     # Submit job and link
     outputs = learner.submit(launcher=launcher_learner)
     tensorboard_service.add(learner, learner.logpath)
 
     # Evaluate the neural model on test collections
     for metric_name in validation.monitored():
-        model = outputs.listeners["bestval"][metric_name]  # type: CrossScorer
+        model = outputs.listeners[validation.id][metric_name]  # type: CrossScorer
         tests.evaluate_retriever(
             partial(
                 model_based_retrievers,
                 scorer=model,
                 retrievers=test_retrievers,
                 device=device,
             ),
             launcher_evaluate,
             model_id=f"monobert-{metric_name}",
         )
 
     return PaperResults(
-        models={"monobert-RR@10": outputs.listeners["bestval"]["RR@10"]},
+        models={"monobert-RR@10": outputs.listeners[validation.id]["RR@10"]},
         evaluations=tests,
         tb_logs={"monobert-RR@10": learner.logpath},
     )
 
 
 @paper_command(schema=Monobert, package=__package__, tensorboard_service=True)
 def cli(xp: experiment, cfg: Monobert, tensorboard_service: TensorboardService):
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/monobert/configuration.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/monobert/configuration.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 from attrs import Factory, field
-from experimaestro.launcherfinder import find_launcher
 from xpmir.papers import configuration
 from xpmir.papers.helpers import LauncherSpecification
 from xpmir.papers.helpers.optim import TransformerOptimization
 from xpmir.papers.helpers.msmarco import RerankerMSMarcoV1Configuration
 
 
 @configuration()
@@ -14,29 +13,39 @@
 @configuration()
 class Learner:
     validation_interval: int = field(default=32)
     validation_top_k: int = 1000
 
     optimization: TransformerOptimization = Factory(TransformerOptimization)
     requirements: str = "duration=4 days & cuda(mem=24G) * 2"
+    sample_rate: float = 1.0
+    """Sample rate for triplets"""
 
-    # FIXME: still not good!
-    # def __attrs_post_init__(self):
-    #     assert self.optimizer.max_epochs % self.validation_interval == 0, (
-    #         f"Number of epochs ({self.optimizer.max_epochs}) is not a multiple "
-    #         f"of validation interval ({self.validation_interval})"
-    #     )
+    sample_max: int = 0
+    """Maximum number of samples considered (before shuffling). 0 for no limit."""
 
 
 @configuration()
 class Retrieval:
     k: int = 1000
     batch_size: int = 512
     requirements: str = "duration=2 days & cuda(mem=24G)"
 
 
 @configuration()
+class Preprocessing:
+    requirements: str = "duration=12h & cpu(cores=4)"
+
+
+@configuration()
 class Monobert(RerankerMSMarcoV1Configuration):
     indexation: Indexation = Factory(Indexation)
     retrieval: Retrieval = Factory(Retrieval)
 
     monobert: Learner = Factory(Learner)
+    preprocessing: Preprocessing = Factory(Preprocessing)
+
+    dev_test_size: int = 0
+    """Development test size (0 to leave it like this)"""
+
+    base: str = "bert-base-uncased"
+    """Identifier for the base model"""
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/monobert/small.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/monobert/normal.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,30 +1,37 @@
-id: monobert-small
-title: "monoBERT trained on MS-Marco (debug)"
+id: monobert
+title: "monoBERT trained on MS-Marco"
 description: |
     Passage Re-ranking with BERT (Rodrigo Nogueira, Kyunghyun Cho). 2019.
     https://arxiv.org/abs/1901.04085
 
-    This model has been trained on MsMarco v1 but only a few iterations (debug)
+    This model has been trained on MsMarco v1
 
-validation:
-    size: 10
+gpu: true
+
+preprocessing:
+    requirements: duration=6h & cpu(mem=4G, cores=8)
 
 indexation:
-    requirements: duration=2 days & cpu(mem=4G)
+    requirements: duration=6h & cpu(mem=4G, cores=8)
 
-retrieval:
-    requirements: duration=2 days & cuda(mem=8G)
-    k: 20
+validation:
+    # Use 500 topics for validation
+    size: 500
 
 monobert:
     optimization:
-        scheduler: false
         steps_per_epoch: 32
-        max_epochs: 4
-        batch_size: 16
-        num_warmup_steps: 30
-        warmup_min_factor: 0.1
-
-    validation_interval: 1
-    validation_top_k: 20
-    requirements: duration=2 days & cuda(mem=8G)
+        batch_size: 64
+        max_epochs: 3200
+        num_warmup_steps: 10000
+        warmup_min_factor: 0
+        weight_decay: 0.01
+        lr: 3.0e-6
+        eps: 1.0e-6
+
+    validation_interval: 32
+    requirements: duration=4 days & cuda(mem=24G) * 2
+
+retrieval:
+    requirements: duration=12h & cuda(mem=24G)
+    k: 1000
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/monobert/normal.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/duobert/normal.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,52 @@
-id: monobert
-title: "monoBERT trained on MS-Marco"
+id: duobert
+title: "DuoBERT trained on MS-Marco"
 description: |
-    Passage Re-ranking with BERT (Rodrigo Nogueira, Kyunghyun Cho). 2019.
-    https://arxiv.org/abs/1901.04085
+    DuoBERT model
+
+        R. Nogueira, W. Yang, K. Cho, et J. Lin, Multi-Stage Document Ranking with BERT, arXiv:1910.14424 [cs], oct. 2019. http://arxiv.org/abs/1910.14424
 
-    This model has been trained on MsMarco v1
 
 gpu: true
 indexation:
     requirements: duration=6 days & cpu(mem=4G, cores=8)
 
 validation:
-    # Use 500 topics for validation
     size: 500
 
+retrieval:
+    requirements: duration=2 days & cuda(mem=24G)
+    k: 100
+    base_k: 50
+
 monobert:
+    requirements: duration=4 days & cuda(mem=24G) * 2
+
     optimization:
         steps_per_epoch: 32
         batch_size: 64
         max_epochs: 3200
         num_warmup_steps: 10000
-        warmup_min_facor: 0
-        weight_decay: 0.01
+        warmup_min_factor: 0
         lr: 3.0e-6
+        weight_decay: .01
 
     validation_interval: 32
+
+duobert:
     requirements: duration=4 days & cuda(mem=24G) * 2
 
-retrieval:
-    requirements: duration=2 days & cuda(mem=24G)
-    k: 1000
+    optimization:
+        # Train on 100k iterations
+        max_epochs: 1_000
+        steps_per_epoch: 100
+
+        # Learning rate warmup over the first 10,000 steps, and linear decay of the learning rate
+        num_warmup_steps: 10_000
+        batch_size: 64
+        warmup_min_factor: 0
+        lr: 3.0e-6
+
+    # Validate 20 times over the 3200 epochs
+    validation_interval: 50
+    base_validation_top_k: 1000
+    validation_top_k: 50
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/splade/experiment.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/splade/experiment.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,214 +1,240 @@
 # Implementation of the experiments in the paper SPLADE v2: Sparse Lexical and
 # Expansion Model for Information Retrieval, (Thibault Formal, Carlos Lassance,
 # Benjamin Piwowarski, Stphane Clinchant), 2021
 # https://arxiv.org/abs/2109.10086
 
+from functools import partial
 import logging
-from functools import lru_cache
 
+from experimaestro import experiment, setmeta, copyconfig
 from experimaestro.launcherfinder import find_launcher
 
-from experimaestro import experiment, tag, setmeta
+from xpmir.learning.optim import (
+    TensorboardService,
+)
 from xpmir.distributed import DistributedHook
-from xpmir.letor.learner import Learner, ValidationListener
-from xpmir.letor.schedulers import LinearWithWarmup
+from xpmir.learning.learner import Learner
+from xpmir.letor.learner import ValidationListener
 from xpmir.index.sparse import (
     SparseRetriever,
     SparseRetrieverIndexBuilder,
 )
 from xpmir.letor.distillation.pairwise import (
     DistillationPairwiseTrainer,
     MSEDifferenceLoss,
 )
+from xpmir.letor.samplers import PairwiseInBatchNegativesSampler
 from xpmir.papers.cli import paper_command
 from xpmir.letor.trainers.batchwise import BatchwiseTrainer, SoftmaxCrossEntropy
-from xpmir.letor.batchers import PowerAdaptativeBatcher
-from xpmir.neural.dual import DenseDocumentEncoder, DenseQueryEncoder
-from xpmir.letor.optim import (
-    ParameterOptimizer,
-    AdamW,
-    get_optimizers,
-)
+from xpmir.learning.batchers import PowerAdaptativeBatcher
 from xpmir.rankers.standard import BM25
 from xpmir.neural.splade import spladeV2_max, spladeV2_doc
 from xpmir.papers.results import PaperResults
-from .pipelines import SPLADEMSMarcoV1Experiment
-from .configuration import SPLADE, Learner as LearnerConfig
+from xpmir.papers.helpers.msmarco import (
+    v1_tests,
+    v1_validation_dataset,
+    v1_passages,
+    v1_docpairs_sampler,
+    hofstaetter_ensemble_hard_negatives,
+)
+from xpmir.datasets.adapters import RetrieverBasedCollection
+from xpmir.rankers.full import FullRetriever
+from .configuration import SPLADE
+import xpmir.interfaces.anserini as anserini
+
 
 logging.basicConfig(level=logging.INFO)
 
 # Run by:
 # $ xpmir papers splade spladeV2 --configuration config_name experiment/
 
 
-class SPLADEExperiment(SPLADEMSMarcoV1Experiment):
-    """SPLADEv2 models"""
-
-    cfg: SPLADE
-
-    basemodel = BM25()
-
-    def __init__(self, xp: experiment, cfg: SPLADE):
-        super().__init__(xp, cfg)
-        self.gpu_launcher_learner = find_launcher(cfg.learner.requirements)
-        self.gpu_launcher_evaluate = find_launcher(cfg.evaluation.requirements)
-
-    @lru_cache
-    def get_optimizers(self, cfg: LearnerConfig):
-        scheduler = (
-            LinearWithWarmup(num_warmup_steps=cfg.num_warmup_steps)
-            if cfg.scheduler
-            else None
-        )
-
-        return get_optimizers(
-            [
-                ParameterOptimizer(
-                    scheduler=scheduler,
-                    optimizer=AdamW(lr=cfg.lr),
-                )
-            ]
-        )
-
-    def run(self) -> PaperResults:
-        """SPLADE model"""
-
-        cfg = self.cfg
-        # -----Learning to rank component preparation part-----
-
-        # Define the model and the flop loss for regularization
-        # Model of class: DotDense()
-        # The parameters are the regularization coeff for the query and document
-        if cfg.learner.model == "splade_max":
-            spladev2, flops = spladeV2_max(
-                cfg.learner.lambda_q,
-                cfg.learner.lambda_d,
-                cfg.learner.lamdba_warmup_steps,
-            )
-        elif cfg.learner.model == "splade_doc":
-            spladev2, flops = spladeV2_doc(
-                cfg.learner.lambda_q,
-                cfg.learner.lambda_d,
-                cfg.learner.lamdba_warmup_steps,
+def run(
+    xp: experiment, cfg: SPLADE, tensorboard_service: TensorboardService
+) -> PaperResults:
+    """SPLADE model"""
+
+    gpu_launcher_learner = find_launcher(cfg.splade.requirements)
+    gpu_launcher_retrieval = find_launcher(cfg.retrieval.requirements)
+    cpu_launcher_index = find_launcher(cfg.indexation.requirements)
+    gpu_launcher_index = find_launcher(cfg.indexation.training_requirements)
+
+    device = cfg.device
+    random = cfg.random
+
+    documents = v1_passages()
+    ds_val_all = v1_validation_dataset(cfg.validation)
+
+    tests = v1_tests(cfg.dev_test_size)
+
+    # -----The baseline------
+    base_model = BM25()
+    index_builder = anserini.index_builder(launcher=cfg.indexation.launcher)
+
+    retrievers = partial(
+        anserini.retriever,
+        index_builder,
+        model=base_model,
+    )  #: Anserini based retrievers
+
+    tests.evaluate_retriever(retrievers, cpu_launcher_index)
+
+    # Building the validation set of the splade
+    # We cannot use the full document dataset to build the validation set.
+
+    # This one could be generic for both sparse and dense methods
+
+    ds_val = RetrieverBasedCollection(
+        dataset=ds_val_all,
+        retrievers=[retrievers(ds_val_all.documents, k=cfg.retrieval.retTopK)],
+    ).submit(launcher=gpu_launcher_index)
+
+    # Base retrievers for validation
+    # It retrieve all the document of the collection with score 0
+    base_retriever_full = FullRetriever(documents=ds_val.documents)
+
+    # -----Learning to rank component preparation part-----
+    # Define the model and the flop loss for regularization
+    # Model of class: DotDense()
+    # The parameters are the regularization coeff for the query and document
+    if cfg.splade.model == "splade_max":
+        spladev2, flops = spladeV2_max(
+            cfg.splade.lambda_q,
+            cfg.splade.lambda_d,
+            cfg.splade.lamdba_warmup_steps,
+        )
+    elif cfg.splade.model == "splade_doc":
+        spladev2, flops = spladeV2_doc(
+            cfg.splade.lambda_q,
+            cfg.splade.lambda_d,
+            cfg.splade.lamdba_warmup_steps,
+        )
+    else:
+        raise NotImplementedError
+
+    # Sampler
+    if cfg.splade.dataset == "":
+        splade_sampler = PairwiseInBatchNegativesSampler(
+            sampler=v1_docpairs_sampler(
+                sample_rate=cfg.splade.sample_rate, sample_max=cfg.splade.sample_max
             )
-        else:
-            raise NotImplementedError
+        )
 
-        # define the trainer based on different dataset
-        if cfg.learner.dataset == "":
-            batchwise_trainer_flops = BatchwiseTrainer(
-                batch_size=cfg.learner.splade_batch_size,
-                sampler=self.splade_sampler,
-                lossfn=SoftmaxCrossEntropy(),
-                hooks=[flops],
+        batchwise_trainer_flops = BatchwiseTrainer(
+            batch_size=cfg.splade.optimization.batch_size,
+            sampler=splade_sampler,
+            lossfn=SoftmaxCrossEntropy(),
+            hooks=[flops],
+        )
+    elif cfg.splade.dataset == "hofstaetter_kd_hard_negatives":
+        batchwise_trainer_flops = DistillationPairwiseTrainer(
+            batch_size=cfg.splade.optimization.batch_size,
+            sampler=hofstaetter_ensemble_hard_negatives(),
+            lossfn=MSEDifferenceLoss(),
+            hooks=[flops],
+        )
+
+    # hooks for the learner
+    if cfg.splade.model == "splade_doc":
+        hooks = [
+            setmeta(
+                DistributedHook(models=[spladev2.encoder]),
+                True,
             )
-        elif cfg.learner.dataset == "bert_hard_negative":
-            batchwise_trainer_flops = DistillationPairwiseTrainer(
-                batch_size=cfg.learner.splade_batch_size,
-                sampler=self.splade_sampler,
-                lossfn=MSEDifferenceLoss(),
-                hooks=[flops],
+        ]
+    else:
+        hooks = [
+            setmeta(
+                DistributedHook(models=[spladev2.encoder, spladev2.query_encoder]),
+                True,
             )
+        ]
 
-        # hooks for the learner
-        if cfg.learner.model == "splade_doc":
-            hooks = [
-                setmeta(
-                    DistributedHook(models=[spladev2.encoder]),
-                    True,
-                )
-            ]
-        else:
-            hooks = [
-                setmeta(
-                    DistributedHook(models=[spladev2.encoder, spladev2.query_encoder]),
-                    True,
-                )
-            ]
-
-        # establish the validation listener
-        validation = ValidationListener(
-            id="bestval",
-            dataset=self.ds_val,
-            # a retriever which use the splade model to score all the
-            # documents and then do the retrieve
-            retriever=spladev2.getRetriever(
-                self.base_retriever_full,
-                cfg.full_retriever.batch_size_full_retriever,
-                PowerAdaptativeBatcher(),
-                device=self.device,
-            ),
-            early_stop=cfg.learner.early_stop,
-            validation_interval=cfg.learner.validation_interval,
-            metrics={"RR@10": True, "AP": False, "nDCG@10": False},
-            store_last_checkpoint=True if cfg.learner.model == "splade_doc" else False,
-        )
-
-        # the learner: Put the components together
-        learner = Learner(
-            # Misc settings
-            random=self.random,
-            device=self.device,
-            # How to train the model
-            trainer=batchwise_trainer_flops,
-            # the model to be trained
-            scorer=spladev2.tag("model", "splade-v2"),
-            # Optimization settings
-            optimizers=self.get_optimizers(cfg.learner),
-            steps_per_epoch=cfg.learner.steps_per_epoch,
-            use_fp16=True,
-            max_epochs=tag(cfg.learner.max_epochs),
-            # the listener for the validation
-            listeners=[validation],
-            # the hooks
-            hooks=hooks,
-        )
-
-        # submit the learner and build the symbolique link
-        outputs = learner.submit(launcher=self.gpu_launcher_learner)
-        self.tb.add(learner, learner.logpath)
-
-        # get the trained model
-        trained_model = (
-            outputs.listeners["bestval"]["last_checkpoint"]
-            if cfg.learner.model == "splade_doc"
-            else outputs.listeners["bestval"]["RR@10"]
-        )
-
-        # build a retriever for the documents
-        sparse_index = SparseRetrieverIndexBuilder(
-            batch_size=512,
-            batcher=PowerAdaptativeBatcher(),
-            encoder=DenseDocumentEncoder(scorer=trained_model),
-            device=self.device,
-            documents=self.documents,
-            ordered_index=False,
-        ).submit(launcher=self.gpu_launcher_index)
-
-        # Build the sparse retriever based on the index
-        splade_retriever = SparseRetriever(
-            index=sparse_index,
-            topk=cfg.base_retriever.topK,
-            batchsize=1,
-            encoder=DenseQueryEncoder(scorer=trained_model),
-        )
-
-        # evaluate the best model
-        self.tests.evaluate_retriever(
-            splade_retriever,
-            self.gpu_launcher_evaluate,
-            model_id=f"{cfg.learner.model}-{cfg.learner.dataset}-RR@10",
-        )
-
-        return PaperResults(
-            models={f"{cfg.learner.model}-{cfg.learner.dataset}-RR@10": trained_model},
-            evaluations=self.tests,
-            tb_logs={
-                f"{cfg.learner.model}-{cfg.learner.dataset}-RR@10": learner.logpath
-            },
-        )
-
-
-@paper_command(schema=SPLADE, package=__package__)
-def cli(xp: experiment, cfg: SPLADE):
-    return SPLADEExperiment(xp, cfg).run()
+    # establish the validation listener
+    validation = ValidationListener(
+        id="bestval",
+        dataset=ds_val,
+        # a retriever which use the splade model to score all the
+        # documents and then do the retrieve
+        retriever=spladev2.getRetriever(
+            base_retriever_full,
+            cfg.retrieval.batch_size_full_retriever,
+            PowerAdaptativeBatcher(),
+            device=device,
+        ),
+        early_stop=cfg.splade.early_stop,
+        validation_interval=cfg.splade.validation_interval,
+        metrics={"RR@10": True, "AP": False, "nDCG@10": False},
+    )
+
+    # the learner: Put the components together
+    learner = Learner(
+        # Misc settings
+        random=random,
+        device=device,
+        # How to train the model
+        trainer=batchwise_trainer_flops,
+        # the model to be trained
+        model=spladev2,
+        # Optimization settings
+        optimizers=cfg.splade.optimization.optimizer_splade,
+        steps_per_epoch=cfg.splade.optimization.steps_per_epoch,
+        use_fp16=True,
+        max_epochs=cfg.splade.optimization.max_epochs,
+        # the listener for the validation
+        listeners=[validation],
+        # the hooks
+        hooks=hooks,
+    )
+
+    # submit the learner and build the symbolique link
+    outputs = learner.submit(launcher=gpu_launcher_learner)
+    tensorboard_service.add(learner, learner.logpath)
+
+    # get the trained model
+    trained_model = (
+        outputs.learned_model
+        if cfg.splade.model == "splade_doc"
+        else outputs.listeners["bestval"]["RR@10"]
+    ).tag("model", "splade-v2")
+
+    # build a retriever for the documents
+    encoder = copyconfig(trained_model.encoder).add_pretasks_from(trained_model)
+    query_encoder = copyconfig(trained_model._query_encoder).add_pretasks_from(
+        trained_model
+    )
+    sparse_index = SparseRetrieverIndexBuilder(
+        batch_size=512,
+        batcher=PowerAdaptativeBatcher(),
+        encoder=encoder,
+        device=device,
+        documents=documents,
+        ordered_index=False,
+        max_docs=cfg.indexation.max_docs,
+    ).submit(launcher=gpu_launcher_index)
+
+    # Build the sparse retriever based on the index
+    splade_retriever = SparseRetriever(
+        index=sparse_index,
+        topk=cfg.retrieval.topK,
+        batchsize=1,
+        encoder=query_encoder,
+    )
+
+    # evaluate the best model
+    tests.evaluate_retriever(
+        splade_retriever,
+        gpu_launcher_retrieval,
+        model_id=f"{cfg.splade.model}-{cfg.splade.dataset}-RR@10",
+    )
+
+    return PaperResults(
+        models={f"{cfg.splade.model}-{cfg.splade.dataset}-RR@10": trained_model},
+        evaluations=tests,
+        tb_logs={f"{cfg.splade.model}-{cfg.splade.dataset}-RR@10": learner.logpath},
+    )
+
+
+@paper_command(schema=SPLADE, package=__package__, tensorboard_service=True)
+def cli(xp: experiment, cfg: SPLADE, tensorboard_service: TensorboardService):
+    return run(xp, cfg, tensorboard_service)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_doc.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -1,39 +1,37 @@
-# The configuration specific for SPLADE_doc
-id: splade_doc
-title: 'SPLADE_doc: SPLADEv2 model with document encoder only'
+id: splade_max
+title: 'SPLADE_max: SPLADEv2 with max aggregation'
 description: |
     SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval
     (Thibault Formal, Carlos Lassance, Benjamin Piwowarski, Stphane Clinchant).
     2021. https://arxiv.org/abs/2109.10086
 
 gpu: true
 indexation:
     requirements: duration=2 days & cpu(mem=2G)
     training_requirements: duration=4 days & cuda(mem=24G)
-learner:
-    model: splade_doc
-    validation_size: 500
+    indexspec: OPQ4_16,IVF65536_HNSW32,PQ4
+    faiss_max_traindocs: 800_000
+
+splade:
+    optimization:
+        steps_per_epoch: 128
+        # In the paper it is 128 but it is too large for our gpu.
+        batch_size: 96
+        # 150k steps for training
+        max_epochs: 1200
+        lr: 2.0e-5
+        num_warmup_steps: 6000
+
+    model: splade_max
     # validation for each (steps_per_epoch * validation interval) steps
-    steps_per_epoch: 128
     validation_interval: 8
-    # maybe it is too large for a gpu of 24G
-    splade_batch_size: 48
-    # About 50k steps for training
-    max_epochs: 400
-    num_warmup_steps: 6000
-    early_stop: 0
-    lr: 2.0e-5
-    lambda_q: 0
+    lambda_q: 3.0e-4
     lambda_d: 1.0e-4
-    lamdba_warmup_steps: 10000
-    requirements: duration=6 days & cuda(mem=24G)
-evaluation:
-    requirements: duration=6 days & cuda(mem=24G)
-base_retriever:
+    lamdba_warmup_steps: 50000
+    requirements: duration=4 days & cuda(mem=40G)
+
+retrieval:
+    requirements: duration=2 days & cuda(mem=24G)
     topK: 1000
-tas_balance_retriever:
     retTopK: 50
-    indexspec: OPQ4_16,IVF65536_HNSW32,PQ4
-    faiss_max_traindocs: 800_000
-full_retriever:
     batch_size_full_retriever: 200
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/splade/normal_DistilMSE.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/splade/normal_DistilMSE.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -8,35 +8,34 @@
     More Effective (Thibault Formal, Carlos Lassance, Benjamin Piwowarski,
     Stphane Clinchant). 2022. https://arxiv.org/abs/2205.04733
 
 gpu: true
 indexation:
     requirements: duration=2 days & cpu(mem=2G)
     training_requirements: duration=4 days & cuda(mem=24G)
-learner:
+    indexspec: OPQ4_16,IVF65536_HNSW32,PQ4
+    faiss_max_traindocs: 800_000
+
+splade:
+    optimization:
+        steps_per_epoch: 128
+        # maybe it is too large for a gpu of 24G
+        batch_size: 96
+        # 150k steps for training
+        max_epochs: 1200
+        num_warmup_steps: 6000
+        lr: 2.0e-5
+
     model: splade_max
-    dataset: bert_hard_negative
-    validation_size: 500
+    dataset: hofstaetter_kd_hard_negatives
     # validation for each (steps_per_epoch * validation interval) steps
-    steps_per_epoch: 128
-    validation_interval: 8
-    # maybe it is too large for a gpu of 24G
-    splade_batch_size: 96
-    # 150k steps for training
-    max_epochs: 1200
-    num_warmup_steps: 6000
-    early_stop: 0
-    lr: 2.0e-5
+    validation_interval: 40
     lambda_q: 0.5
     lambda_d: 0.4
     lamdba_warmup_steps: 50000
-    requirements: duration=6 days & cuda(mem=24G)
-evaluation:
-    requirements: duration=6 days & cuda(mem=24G)
-base_retriever:
+    requirements: duration=2 days & cuda(mem=24G)
+
+retrieval:
+    requirements: duration=2 days & cuda(mem=24G)
     topK: 1000
-tas_balance_retriever:
     retTopK: 50
-    indexspec: OPQ4_16,IVF65536_HNSW32,PQ4
-    faiss_max_traindocs: 800_000
-full_retriever:
     batch_size_full_retriever: 200
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/helpers/optim.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/helpers/optim.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from functools import cached_property
-from xpmir.letor.optim import (
+from xpmir.learning.optim import (
     AdamW,
     ParameterOptimizer,
     RegexParameterFilter,
     get_optimizers,
 )
-from xpmir.letor.schedulers import LinearWithWarmup
+from xpmir.learning.schedulers import LinearWithWarmup
 from xpmir.papers import configuration
 
 
 @configuration
 class TransformerOptimization:
     """Configuration for a transformer optimization"""
 
@@ -20,14 +20,15 @@
     batch_size: int = 64
     max_epochs: int = 3200
     steps_per_epoch: int = 32
     """Number of steps (batches) per epoch"""
 
     lr: float = 3.0e-6
     weight_decay: float = 1e-2
+    eps: float = 1e-8
 
     @cached_property
     def optimizer(self):
         scheduler = (
             LinearWithWarmup(
                 num_warmup_steps=self.num_warmup_steps,
                 min_factor=self.warmup_min_factor,
@@ -36,20 +37,42 @@
             else None
         )
 
         return get_optimizers(
             [
                 ParameterOptimizer(
                     scheduler=scheduler,
-                    optimizer=AdamW(lr=self.lr, weight_decay=0, eps=self.lr),
+                    optimizer=AdamW(lr=self.lr, weight_decay=0, eps=self.eps),
                     filter=RegexParameterFilter(
                         includes=[r"\.bias$", r"\.LayerNorm\."]
                     ),
                 ),
                 ParameterOptimizer(
                     scheduler=scheduler,
                     optimizer=AdamW(
-                        lr=self.lr, weight_decay=self.weight_decay, eps=self.lr
+                        lr=self.lr, weight_decay=self.weight_decay, eps=self.eps
+                    ),
+                ),
+            ]
+        )
+
+    @cached_property
+    def optimizer_splade(self):
+        scheduler = (
+            LinearWithWarmup(
+                num_warmup_steps=self.num_warmup_steps,
+                min_factor=self.warmup_min_factor,
+            )
+            if self.scheduler
+            else None
+        )
+
+        return get_optimizers(
+            [
+                ParameterOptimizer(
+                    scheduler=scheduler,
+                    optimizer=AdamW(
+                        lr=self.lr, weight_decay=self.weight_decay, eps=self.eps
                     ),
                 ),
             ]
         )
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/helpers/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/helpers/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from experimaestro.launcherfinder import find_launcher
 from omegaconf import MISSING
 from xpmir.letor import Random
 
-from xpmir.letor.devices import Device, CudaDevice
+from xpmir.learning.devices import Device, CudaDevice
 from xpmir.papers import configuration, attrs_cached_property
 
 
 @configuration()
 class PaperExperiment:
     id: str = MISSING
     """The experiment ID
@@ -55,8 +55,8 @@
     12Go of memory)
     """
 
     requirements: str
 
     @attrs_cached_property
     def launcher(self):
-        find_launcher(self.requirements)
+        return find_launcher(self.requirements)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/duobert/experiment.py` & `experimaestro-ir-1.0.0/src/xpmir/papers/duobert/experiment.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,21 +5,22 @@
 # An imitation of examples/msmarco-reranking.py
 
 
 from functools import partial
 import logging
 from experimaestro.launcherfinder import find_launcher
 from xpmir.distributed import DistributedHook
-from xpmir.letor.learner import Learner, ValidationListener
-from xpmir.letor.optim import TensorboardService
+from xpmir.learning.learner import Learner
+from xpmir.letor.learner import ValidationListener
+from xpmir.learning.optim import TensorboardService
 
 import xpmir.letor.trainers.pairwise as pairwise
 from xpmir.neural.cross import DuoCrossScorer
 from experimaestro import experiment, setmeta
-from xpmir.letor.batchers import PowerAdaptativeBatcher
+from xpmir.learning.batchers import PowerAdaptativeBatcher
 from xpmir.papers.cli import paper_command
 from xpmir.papers.helpers.msmarco import (
     v1_docpairs_sampler,
     v1_tests,
     v1_validation_dataset,
 )
 from xpmir.text.huggingface import DualDuoBertTransformerEncoder
@@ -93,15 +94,15 @@
     learner = Learner(
         # Misc settings
         device=device,
         random=random,
         # How to train the model
         trainer=duobert_trainer,
         # The model to train
-        scorer=duobert_scorer,
+        model=duobert_scorer,
         # Optimization settings
         steps_per_epoch=cfg.duobert.optimization.steps_per_epoch,
         optimizers=cfg.duobert.optimization.optimizer,
         max_epochs=cfg.duobert.optimization.max_epochs,
         # The listeners (here, for validation)
         listeners=[validation],
         # The hook used for evaluation
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/duobert/small.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/duobert/small.yaml`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/papers/duobert/normal.yaml` & `experimaestro-ir-1.0.0/src/xpmir/papers/monobert/small.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,52 +1,39 @@
-id: duobert
-title: "DuoBERT trained on MS-Marco"
+id: monobert-small
+title: "monoBERT trained on MS-Marco (debug)"
 description: |
-    DuoBERT model
-
-        R. Nogueira, W. Yang, K. Cho, et J. Lin, Multi-Stage Document Ranking with BERT, arXiv:1910.14424 [cs], oct. 2019. http://arxiv.org/abs/1910.14424
+    Passage Re-ranking with BERT (Rodrigo Nogueira, Kyunghyun Cho). 2019.
+    https://arxiv.org/abs/1901.04085
 
+    This model has been trained on MsMarco v1 but only a few iterations (debug)
 
 gpu: true
-indexation:
-    requirements: duration=6 days & cpu(mem=4G, cores=8)
+base: "microsoft/MiniLM-L12-H384-uncased"
+dev_test_size: 50
 
 validation:
-    size: 500
+    size: 10
+
+indexation:
+    requirements: duration=2 days & cpu(mem=4G)
 
 retrieval:
-    requirements: duration=2 days & cuda(mem=24G)
-    k: 100
-    base_k: 50
+    requirements: duration=2 days & cuda(mem=8G)
+    k: 20
 
 monobert:
-    requirements: duration=4 days & cuda(mem=24G) * 2
-
     optimization:
+        scheduler: false
         steps_per_epoch: 32
-        batch_size: 64
-        max_epochs: 3200
-        num_warmup_steps: 10000
-        warmup_min_factor: 0
-        lr: 3.0e-6
-        weight_decay: .01
-
-    validation_interval: 32
-
-duobert:
-    requirements: duration=4 days & cuda(mem=24G) * 2
-
-    optimization:
-        # Train on 100k iterations
-        max_epochs: 1_000
-        steps_per_epoch: 100
-
-        # Learning rate warmup over the first 10,000 steps, and linear decay of the learning rate
-        num_warmup_steps: 10_000
-        batch_size: 64
-        warmup_min_factor: 0
-        lr: 3.0e-6
-
-    # Validate 20 times over the 3200 epochs
-    validation_interval: 50
-    base_validation_top_k: 1000
-    validation_top_k: 50
+        max_epochs: 4
+        batch_size: 16
+        num_warmup_steps: 30
+        warmup_min_factor: 0.1
+        eps: 1.0e-6
+
+    # Only use 1% of the 100_000 triplets
+    sample_rate: .01
+    sample_max: 100_000
+
+    validation_interval: 1
+    validation_top_k: 20
+    requirements: duration=2 days & cuda(mem=8G)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/index/test_sparse.py` & `experimaestro-ir-1.0.0/src/xpmir/test/index/test_sparse.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,50 +1,52 @@
+from experimaestro import ObjectStore
 from experimaestro.xpmutils import DirectoryContext
 from pathlib import Path
 import pytest
 import torch
 import numpy as np
 from xpmir.index.sparse import SparseRetriever, SparseRetrieverIndexBuilder
-from xpmir.test.utils.utils import SampleAdhocDocumentStore, SparseRandomTextEncoder
+from xpmir.test.utils.utils import SampleDocumentStore, SparseRandomTextEncoder
 
 
 @pytest.fixture
 def context(tmp_path: Path):
     from experimaestro.taskglobals import Env
 
     Env.taskpath = tmp_path / "task"
     Env.taskpath.mkdir()
     return DirectoryContext(tmp_path)
 
 
 class SparseIndex:
     def __init__(self, context, ordered_index: bool = False):
         # Build the index
-        documents = SampleAdhocDocumentStore(num_docs=500)
+        objects = ObjectStore()
+        documents = SampleDocumentStore(num_docs=500)
         self.encoder = SparseRandomTextEncoder(dim=1000, sparsity=0.8)
         builder = SparseRetrieverIndexBuilder(
             encoder=self.encoder,
             documents=documents,
             max_postings=10,
             batch_size=5,
             ordered_index=ordered_index,
         )
 
         # Build the index
-        builder_instance = builder.instance(context=context)
+        builder_instance = builder.instance(context=context, objects=objects)
         builder_instance.execute()
 
         self.document_store = builder_instance.documents
         self.x_docs = builder_instance.encoder(
             [d.text for d in self.document_store.documents.values()]
         )
 
         # Check index
-        self.index = builder.taskoutputs()
-        self.index_instance = self.index.instance()
+        self.index = builder.task_outputs(lambda x: x)
+        self.index_instance = self.index.instance(objects=objects)
         self.topk = 10
 
 
 @pytest.fixture(params=[False])
 def sparse_index(context, request):
     return SparseIndex(context, ordered_index=request.param)
 
@@ -90,28 +92,28 @@
 def test_sparse_retrieve(sparse_index: SparseIndex, retriever):
     # Computes the score directly
     x_docs = sparse_index.x_docs.type(torch.float32)
 
     # Choose a few documents
     chosen_ix = np.random.choice(np.arange(len(sparse_index.x_docs)), 10)
     for ix in chosen_ix:
-        document = sparse_index.document_store.document(ix)
+        document = sparse_index.document_store.document_int(ix)
 
         # Use the retriever
-        scoredDocuments = retriever.retrieve(document.text)
+        scoredDocuments = retriever.retrieve(document.get_text())
         # scoredDocuments.sort(reverse=True)
         # scoredDocuments = scoredDocuments[:retriever.topk]
 
         # Use the pre-computed scores
         scores = x_docs[ix] @ x_docs.T
         sorted = scores.sort(descending=True, stable=True)
         indices = sorted.indices[: retriever.topk]
         expected = list(indices.numpy())
 
-        observed = [int(sd.docid) for sd in scoredDocuments]
+        observed = [int(sd.document.get_id()) for sd in scoredDocuments]
         expected_scores = sorted.values[: retriever.topk].numpy()
         observed_scores = np.array([float(sd.score) for sd in scoredDocuments])
 
         np.testing.assert_allclose(
             expected_scores,
             observed_scores,
             1e-5,
@@ -131,16 +133,16 @@
         "q5": "Query 55",
     }
     all_results = retriever.retrieve_all(queries)
 
     for key, query in queries.items():
         query_results = retriever.retrieve(query)
 
-        observed = [d.docid for d in all_results[key]]
-        expected = [d.docid for d in query_results]
+        observed = [d.document.get_id() for d in all_results[key]]
+        expected = [d.document.get_id() for d in query_results]
         assert observed == expected
 
         observed_scores = [d.score for d in all_results[key]]
         expected_scores = [d.score for d in query_results]
         np.testing.assert_allclose(
             expected_scores, observed_scores, 1e-5, err_msg=f"{expected} vs {observed}"
         )
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/neural/test_forward.py` & `experimaestro-ir-1.0.0/src/xpmir/test/neural/test_forward.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,23 +4,24 @@
 import pytest
 import torch
 from collections import defaultdict
 from experimaestro import Constant
 from xpmir.index import Index
 from xpmir.letor import Random
 from xpmir.neural.dual import CosineDense, DotDense
+from datamaestro_text.data.ir.base import GenericDocument
 from xpmir.letor.records import (
-    Document,
+    DocumentRecord,
     PairwiseRecord,
     PairwiseRecords,
     PointwiseRecord,
     PointwiseRecords,
     ProductRecords,
-    Query,
     TokenizedTexts,
+    TopicRecord,
 )
 from xpmir.text.encoders import TokensEncoder, DualTextEncoder, MeanTextEncoder
 
 
 class RandomTokensEncoder(TokensEncoder):
     DIMENSION = 7
     MAX_WORDS = 100
@@ -80,15 +81,16 @@
 
 
 @registermodel
 def drmm():
     """Drmm factory"""
     from xpmir.neural.interaction.drmm import Drmm
 
-    return Drmm(vocab=RandomTokensEncoder(), index=CustomIndex()).instance()
+    drmm = Drmm(vocab=RandomTokensEncoder(), index=CustomIndex())
+    return drmm.instance()
 
 
 @registermodel
 def colbert():
     """Colbert model factory"""
     from xpmir.neural.colbert import Colbert
 
@@ -139,56 +141,48 @@
     return CrossScorer(encoder=DummyDualTextEncoder()).instance()
 
 
 # ---
 # --- Input factory
 # ---
 
-QUERIES = [Query(None, "purple cat"), Query(None, "yellow house")]
+QUERIES = [TopicRecord.from_text("purple cat"), TopicRecord.from_text("yellow house")]
 DOCUMENTS = [
-    Document("1", "the cat sat on the mat", 1),
-    Document("2", "the purple car", 1),
-    Document("3", "my little dog", 1),
-    Document("4", "the truck was on track", 1),
+    DocumentRecord(GenericDocument("1", "the cat sat on the mat")),
+    DocumentRecord(GenericDocument("2", "the purple car")),
+    DocumentRecord(GenericDocument("3", "my little dog")),
+    DocumentRecord(GenericDocument("4", "the truck was on track")),
 ]
 
 
 def pointwise():
     # Pointwise inputs
     inputs = PointwiseRecords()
 
     # Implicit order (Q0, D0) (Q1, D0) (Q0, D1) (Q1, D1)
-    inputs.add(
-        PointwiseRecord(QUERIES[0], DOCUMENTS[0].docid, DOCUMENTS[1].text, 0.0, 0.0)
-    )
-    inputs.add(
-        PointwiseRecord(QUERIES[0], DOCUMENTS[0].docid, DOCUMENTS[0].text, 0.0, 0.0)
-    )
-    inputs.add(
-        PointwiseRecord(QUERIES[1], DOCUMENTS[2].docid, DOCUMENTS[2].text, 0.0, 0.0)
-    )
-    inputs.add(
-        PointwiseRecord(QUERIES[1], DOCUMENTS[1].docid, DOCUMENTS[2].text, 0.0, 0.0)
-    )
+    inputs.add(PointwiseRecord(QUERIES[0], DOCUMENTS[0], 0.0))
+    inputs.add(PointwiseRecord(QUERIES[0], DOCUMENTS[0], 0.0))
+    inputs.add(PointwiseRecord(QUERIES[1], DOCUMENTS[2], 0.0))
+    inputs.add(PointwiseRecord(QUERIES[1], DOCUMENTS[1], 0.0))
     return inputs
 
 
 def pairwise():
     # Implicit order (Q0, D0) (Q1, D0) (Q0, D1) (Q1, D1)
     inputs = PairwiseRecords()
     inputs.add(PairwiseRecord(QUERIES[0], DOCUMENTS[0], DOCUMENTS[1]))
     inputs.add(PairwiseRecord(QUERIES[1], DOCUMENTS[2], DOCUMENTS[3]))
     return inputs
 
 
 def product():
     # Implicit order (Q0, D0) (Q0, D1) (Q1, D0)
     inputs = ProductRecords()
-    inputs.addQueries(QUERIES[0], QUERIES[1])
-    inputs.addDocuments(DOCUMENTS[0], DOCUMENTS[1])
+    inputs.add_topics(QUERIES[0], QUERIES[1])
+    inputs.add_documents(DOCUMENTS[0], DOCUMENTS[1])
 
     return inputs
 
 
 inputfactories = [pointwise, pairwise, product]
 
 
@@ -224,16 +218,16 @@
     maps = []
     with torch.no_grad():
         for f in inputfactoriescouple:
             input = f()
             outputs.append(model(input, None))
             maps.append(
                 {
-                    (q.text, d.text): ix
-                    for ix, (q, d) in enumerate(zip(input.queries, input.documents))
+                    (qr.topic.get_text(), dr.document.get_text()): ix
+                    for ix, (qr, dr) in enumerate(zip(input.queries, input.documents))
                 }
             )
 
     inter = set(maps[0].keys() & maps[1].keys())
     assert len(inter) > 0, "No common query/document pair"
     for key in inter:
         s1 = outputs[0][maps[0][key]].item()
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/rankers/test_full.py` & `experimaestro-ir-1.0.0/src/xpmir/test/rankers/test_full.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,90 +1,98 @@
 from collections import defaultdict
 import itertools
 from typing import List, Optional
 import random
 import torch
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import TrainerContext
 from xpmir.rankers import ScoredDocument
 from xpmir.rankers.full import FullRetrieverRescorer
 from xpmir.neural.dual import DualRepresentationScorer
-from xpmir.test.utils.utils import SampleAdhocDocumentStore
+from xpmir.letor.records import TopicRecord, DocumentRecord
+from xpmir.test.utils.utils import SampleDocumentStore
 
 
 class ListWrapper(list):
     device = None
 
     def to(self, device):
         return self
 
     def __getitem__(self, item):
         return ListWrapper(list.__getitem__(self, item))
 
 
 class CachedRandomScorer(DualRepresentationScorer):
     def _initialize(self, _random):
-        self.cache = defaultdict(lambda: random.uniform(0, 1))
+        self._cache = defaultdict(lambda: random.uniform(0, 1))
+
+    def cache(self, query: TopicRecord, document: DocumentRecord):
+        return self._cache[(query.topic.get_text(), document.document.get_text())]
 
     def encode(self, texts: List[str]):
         return ListWrapper(texts)
 
     def score_pairs(
         self, queries, documents, info: Optional[TrainerContext]
     ) -> torch.Tensor:
-        scores = [self.cache[(q, d)] for q, d in zip(queries, documents)]
+        scores = [self.cache(q, d) for q, d in zip(queries, documents)]
         return torch.DoubleTensor(scores)
 
     def score_product(
         self, queries, documents, info: Optional[TrainerContext]
     ) -> torch.Tensor:
         scores = []
         for q in queries:
-            scores.append([self.cache[(q, d)] for d in documents])
+            scores.append([self.cache(q, d) for d in documents])
 
         return torch.DoubleTensor(scores)
 
     def merge_queries(self, queries):
         return ListWrapper(itertools.chain(*queries))
 
 
 class _FullRetrieverRescorer(FullRetrieverRescorer):
     def retrieve(self, query: str):
         scored_documents = [
-            ScoredDocument(d.docid, self.scorer.cache[(query, d.text)])
+            # Randomly get a score (and cache it)
+            ScoredDocument(
+                d, self.scorer.cache(TopicRecord.from_text(query), DocumentRecord(d))
+            )
             for d in self.documents
         ]
         scored_documents.sort(reverse=True)
         return scored_documents
 
 
 def test_fullretrieverescorer():
     NUM_DOCS = 7
     NUM_QUERIES = 9
 
-    documents = SampleAdhocDocumentStore(num_docs=NUM_DOCS)
+    documents = SampleDocumentStore(num_docs=NUM_DOCS)
     scorer = CachedRandomScorer()
     retriever = _FullRetrieverRescorer(documents=documents, scorer=scorer, batchsize=20)
 
     _retriever = retriever.instance()
     _retriever.initialize()
 
     # Retrieve normally
     scoredDocuments = {}
     queries = {i: f"Query {i}" for i in range(NUM_QUERIES)}
 
+    # Retrieve query per query
     for qid, query in queries.items():
         scoredDocuments[qid] = _retriever.retrieve(query)
 
     # Retrieve with batching
     all_results = _retriever.retrieve_all(queries)
 
     for qid, results in all_results.items():
         expected = scoredDocuments[qid]
         results.sort(reverse=True)
         expected.sort(reverse=True)
 
-        assert [d.docid for d in expected] == [
-            d.docid for d in results
+        assert [d.document.get_id() for d in expected] == [
+            d.document.get_id() for d in results
         ], "Document IDs do not match"
         assert [d.score for d in expected] == [
             d.score for d in results
         ], "Scores do not match"
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/letor/test_samplers.py` & `experimaestro-ir-1.0.0/src/xpmir/test/letor/test_samplers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,40 @@
 from typing import Iterator, Tuple
 from xpmir.letor.samplers import TrainingTriplets, TripletBasedSampler
 
 # ---- Serialization
 
 
-class TestTrainingTriplets(TrainingTriplets):
+class MyTrainingTriplets(TrainingTriplets):
     def iter(self) -> Iterator[Tuple[str, str, str]]:
         count = 0
 
         while True:
             yield f"q{count}", f"doc+{count}", f"doc-{count}"
 
 
 def test_serializing_tripletbasedsampler():
     """Serialized samplers should start back from the saved state"""
     # Collect samples and state after 10 samples
     sampler = TripletBasedSampler(
-        source=TestTrainingTriplets(id="test-triplets", ids=False)
+        source=MyTrainingTriplets(id="test-triplets", ids=False)
     ).instance()
     iter = sampler.pairwise_iter()
 
     for _, _ in zip(range(10), iter):
         pass
     data = iter.state_dict()
 
     samples = []
     for _, record in zip(range(10), sampler.pairwise_iter()):
         samples.append(record)
 
     # Test
     sampler = TripletBasedSampler(
-        source=TestTrainingTriplets(id="test-triplets", ids=False)
+        source=MyTrainingTriplets(id="test-triplets", ids=False)
     ).instance()
     iter = sampler.pairwise_iter()
     iter.load_state_dict(data)
     for _, record, expected in zip(range(10), iter, samples):
         assert expected.query.text == record.query.text
         assert expected.positive.text == record.positive.text
         assert expected.negative.text == record.negative.text
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/utils/test_iter.py` & `experimaestro-ir-1.0.0/src/xpmir/test/utils/test_iter.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/test/utils/utils.py` & `experimaestro-ir-1.0.0/src/xpmir/test/utils/utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,38 +1,50 @@
 from collections import OrderedDict, defaultdict
 from typing import ClassVar, Dict, Iterator, List, Tuple
 import torch
+from attrs import define
 import numpy as np
-from datamaestro_text.data.ir import AdhocDocument, AdhocDocumentStore
+from datamaestro_text.data.ir import Document, DocumentStore
+from datamaestro_text.data.ir.base import GenericDocument
+
 from experimaestro import Param
 from xpmir.text.encoders import TextEncoder
 
 
-class SampleAdhocDocumentStore(AdhocDocumentStore):
+@define(frozen=True)
+class GenericDocumentWithID(GenericDocument):
+    internal_docid: int
+
+
+class SampleDocumentStore(DocumentStore):
     id: Param[str] = ""
     num_docs: Param[int] = 200
 
     def __post_init__(self):
+        # Generate all the documents
         self.documents = OrderedDict(
-            (str(ix), AdhocDocument(str(ix), f"Document {ix}", internal_docid=ix))
+            (
+                str(ix),
+                GenericDocumentWithID(str(ix), f"Document {ix}", internal_docid=ix),
+            )
             for ix in range(self.num_docs)
         )
 
     @property
     def documentcount(self):
         return len(self.documents)
 
-    def document(self, internal_docid: int) -> AdhocDocument:
+    def document_int(self, internal_docid: int) -> Document:
         return self.documents[str(internal_docid)]
 
-    def document_text(self, docid: str) -> str:
+    def document_ext(self, docid: str) -> Document:
         """Returns the text of the document given its id"""
-        return self.documents[docid].text
+        return self.documents[docid]
 
-    def iter_documents(self) -> Iterator[AdhocDocument]:
+    def iter_documents(self) -> Iterator[Document]:
         return iter(self.documents.values())
 
     def docid_internal2external(self, docid: int):
         """Converts an internal collection ID (integer) to an external ID"""
         return str(docid)
 
     def __iter__(self):
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/rankers/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/rankers/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,67 +1,68 @@
 # This package contains all rankers
 
 from experimaestro import tqdm
-
 from enum import Enum
-from pathlib import Path
 from typing import (
     Dict,
     Generic,
     Iterable,
     List,
     Optional,
     Protocol,
     Tuple,
-    Callable,
     TypeVar,
     Union,
     final,
     TYPE_CHECKING,
 )
 import torch
 import torch.nn as nn
-
+import attrs
 import numpy as np
-from experimaestro import Param, Config, Meta, DataPath
+from experimaestro import Param, Config, Meta
 from datamaestro_text.data.ir import (
-    AdhocDocuments,
-    AdhocDocumentStore,
+    Document,
+    Documents,
+    DocumentStore,
 )
+from datamaestro_text.data.ir.base import TextDocument, TextTopic
+from xpmir.utils.utils import Initializable
 from xpmir.letor import Device, Random
-from xpmir.letor.batchers import Batcher
-from xpmir.letor.context import TrainerContext
-from xpmir.letor.optim import Module
+from xpmir.learning.batchers import Batcher
+from xpmir.learning.context import TrainerContext
+from xpmir.learning.optim import Module
 from xpmir.letor.records import (
-    Document,
+    ScoredDocumentRecord,
+    TopicRecord,
     BaseRecords,
     PairwiseRecord,
     PairwiseRecords,
     ProductRecords,
-    Query,
 )
 from xpmir.utils.utils import EasyLogger, easylog
 
 if TYPE_CHECKING:
     from xpmir.evaluation import RetrieverFactory
 
 logger = easylog()
 
 
+@attrs.define()
 class ScoredDocument:
-    """A data structure which contains the id, the content(optional) and a
-    calculated score for a document"""
+    """A data structure that associated a score with a document"""
+
+    document: Document
+    """The document"""
 
-    def __init__(self, docid: Optional[str], score: float, content: str = None):
-        self.docid = docid
-        self.score = score
-        self.content = content
+    score: float
+    """The associated score"""
 
     def __repr__(self):
-        return f"document({self.docid}, {self.score}, {self.content})"
+        return f"document({self.document}, {self.score})"
 
     def __lt__(self, other):
         return self.score < other.score
 
 
 class ScorerOutputType(Enum):
     REAL = 0
@@ -76,24 +77,24 @@
 
 class LearnableModel(Config):
     """All learnable models"""
 
     pass
 
 
-class Scorer(Config, EasyLogger):
+class Scorer(Config, Initializable, EasyLogger):
     """Query-document scorer
 
     A model able to give a score to a list of documents given a query
     """
 
     outputType: ScorerOutputType = ScorerOutputType.REAL
     """Determines the type of output scalar (log probability, probability, logit) """
 
-    def initialize(self, random: Optional[np.random.RandomState]):
+    def __initialize__(self, random: Optional[np.random.RandomState]):
         """Initialize the scorer
 
         Arguments:
 
             random:
                 Random state for random number generation; when random is None,
                 this means that the state will be loaded from
@@ -138,15 +139,15 @@
             batcher=batcher,
             device=device,
             top_k=top_k if top_k else None,
         )
 
 
 def scorer_retriever(
-    documents: AdhocDocuments,
+    documents: Documents,
     *,
     retrievers: "RetrieverFactory",
     scorer: Scorer,
     **kwargs,
 ):
     """Helper function that returns a two stage retriever. This is useful
     when used with partial (when the scorer is not known).
@@ -164,29 +165,26 @@
 class RandomScorer(Scorer):
     """A random scorer"""
 
     random: Param[Random]
     """The random number generator"""
 
     def rsv(
-        self, query: str, documents: Iterable[ScoredDocument], keepcontent=False
+        self, query: str, scored_documents: Iterable[ScoredDocument]
     ) -> List[ScoredDocument]:
-        scoredDocuments = []
+        result = []
         random = self.random.state
-        for doc in documents:
-            scoredDocuments.append(ScoredDocument(doc.docid, random.random()))
-        return scoredDocuments
+        for scored_document in scored_documents:
+            result.append(ScoredDocument(scored_document.document, random.random()))
+        return result
 
 
 class AbstractLearnableScorer(Scorer, Module):
     """Base class for all learnable scorer"""
 
-    checkpoint: DataPath[Optional[Path]]
-    """A checkpoint path from which the model should be loaded (or None otherwise)"""
-
     __call__ = nn.Module.__call__
     to = nn.Module.to
 
     def __init__(self):
         nn.Module.__init__(self)
         super().__init__()
         self._initialized = False
@@ -208,26 +206,22 @@
 
         Initialization can either be determined by a checkpoint (if set) or
         otherwise (random or pre-trained checkpoint depending on the models)
         """
         if self._initialized:
             return
 
-        if self.checkpoint is None:
-            # Sets the current random seed
-            if random is not None:
-                seed = random.randint((2**32) - 1)
-                torch.manual_seed(seed)
-                torch.cuda.manual_seed_all(seed)
+        # Sets the current random seed
+        if random is not None:
+            seed = random.randint((2**32) - 1)
+            torch.manual_seed(seed)
+            torch.cuda.manual_seed_all(seed)
             self._initialize(random)
         else:
-            logger.info("Loading model from path %s", self.checkpoint)
-            path = Path(self.checkpoint)
             self._initialize(None)
-            self.load_state_dict(torch.load(path / "model.pth"))
 
         self._initialized = True
 
         return self
 
 
 class LearnableScorer(AbstractLearnableScorer):
@@ -243,41 +237,39 @@
         pairwise, or structured)
         """
         raise NotImplementedError(f"forward in {self.__class__}")
 
     def rsv(
         self,
         query: str,
-        documents: Union[List[ScoredDocument], ScoredDocument, str, List[str]],
-        content=False,
+        scored_documents: Union[List[ScoredDocument], ScoredDocument, str, List[str]],
     ) -> List[ScoredDocument]:
-        if isinstance(documents, str):
-            documents = [ScoredDocument(None, None, documents)]
-        elif isinstance(documents[0], str):
-            documents = [ScoredDocument(None, None, text) for text in documents]
+        if isinstance(scored_documents, str):
+            scored_documents = [ScoredDocument(TextDocument(scored_documents), None)]
+        elif isinstance(scored_documents[0], str):
+            scored_documents = [ScoredDocument(TextDocument(scored_documents[0]), None)]
 
         # Prepare the inputs and call the model
         inputs = ProductRecords()
-        for doc in documents:
-            assert doc.content is not None
+        inputs.add_topics(TopicRecord(TextTopic(query)))
 
-        inputs.addQueries(Query(None, query))
-        inputs.addDocuments(*[Document(d.docid, d.content, d.score) for d in documents])
+        inputs.add_documents(
+            *[ScoredDocumentRecord(sd.document, sd.score) for sd in scored_documents]
+        )
 
         with torch.no_grad():
             scores = self(inputs, None).cpu().numpy()
 
         # Returns the scored documents
         scoredDocuments = []
-        for i in range(len(documents)):
+        for i in range(len(scored_documents)):
             scoredDocuments.append(
                 ScoredDocument(
-                    documents[i].docid,
+                    scored_documents[i].document,
                     float(scores[i]),
-                    documents[i].content if content else None,
                 )
             )
 
         return scoredDocuments
 
 
 class DuoLearnableScorer(AbstractLearnableScorer):
@@ -287,15 +279,15 @@
         """Returns scores for pairs of documents (given a query)"""
         raise NotImplementedError(f"abstract __call__ in {self.__class__}")
 
 
 class Retriever(Config):
     """A retriever is a model to return top-scored documents given a query"""
 
-    store: Param[Optional[AdhocDocumentStore]] = None
+    store: Param[Optional[DocumentStore]] = None
     """Give the document store associated with this retriever"""
 
     def initialize(self):
         pass
 
     def collection(self):
         """Returns the document collection object"""
@@ -313,26 +305,26 @@
                 is the text
         """
         results = {}
         for key, text in tqdm(list(queries.items())):
             results[key] = self.retrieve(text)
         return results
 
-    def retrieve(self, query: str, content=False) -> List[ScoredDocument]:
+    def retrieve(self, query: str) -> List[ScoredDocument]:
         """Retrieves documents, returning a list sorted by decreasing score
 
         if `content` is true, includes the document full text
         """
         raise NotImplementedError()
 
-    def _store(self) -> Optional[AdhocDocumentStore]:
+    def _store(self) -> Optional[DocumentStore]:
         """Returns the associated document store (if any) that can be
         used to get the full text of the documents"""
 
-    def get_store(self) -> Optional[AdhocDocumentStore]:
+    def get_store(self) -> Optional[DocumentStore]:
         return self.store or self._store()
 
 
 class AbstractTwoStageRetriever(Retriever):
     """Abstract class for all two stage retrievers (i.e. scorers and duo-scorers)"""
 
     retriever: Param[Retriever]
@@ -368,28 +360,27 @@
     given a scorer"""
 
     def _retrieve(
         self,
         batch: List[ScoredDocument],
         query: str,
         scoredDocuments: List[ScoredDocument],
-        content: bool,
     ):
-        scoredDocuments.extend(self.scorer.rsv(query, batch, content))
+        scoredDocuments.extend(self.scorer.rsv(query, batch))
 
-    def retrieve(self, query: str, content=False):
+    def retrieve(self, query: str):
         # Calls the retriever
-        scoredDocuments = self.retriever.retrieve(query, content=True)
+        scoredDocuments = self.retriever.retrieve(query)
 
         # Scorer in evaluation mode
         self.scorer.eval()
 
         _scoredDocuments = []
         scoredDocuments = self._batcher.process(
-            scoredDocuments, self._retrieve, query, _scoredDocuments, content
+            scoredDocuments, self._retrieve, query, _scoredDocuments
         )
 
         _scoredDocuments.sort(reverse=True)
         return _scoredDocuments[: (self.top_k or len(_scoredDocuments))]
 
 
 class DuoTwoStageRetriever(AbstractTwoStageRetriever):
@@ -412,15 +403,15 @@
         scoredDocuments.extend(self.rsv(query, batch))
 
     def retrieve(self, query: str):
         """call the _retrieve function by using the batcher and do an
         aggregation of all the scores
         """
         # get the documents from the retriever
-        scoredDocuments_previous = self.retriever.retrieve(query, content=True)
+        scoredDocuments_previous = self.retriever.retrieve(query)
 
         # transform them into the pairs (i, j)
         # for i != j ranging from 1 to nb of documents
         pairs = []
         for i in range(len(scoredDocuments_previous)):
             for j in range(len(scoredDocuments_previous)):
                 if i != j:
@@ -443,144 +434,78 @@
         )  # scores for each document.
 
         # construct the ScoredDocument object from the score we just get.
         scoredDocuments = []
         for i in range(len(scoredDocuments_previous)):
             scoredDocuments.append(
                 ScoredDocument(
-                    scoredDocuments_previous[i].docid, float(_scores_per_document[i])
+                    scoredDocuments_previous[i], float(_scores_per_document[i])
                 )
             )
         scoredDocuments.sort(reverse=True)
         return scoredDocuments[: (self.top_k or len(scoredDocuments))]
 
     def rsv(
         self, query: str, documents: List[Tuple[ScoredDocument, ScoredDocument]]
     ) -> List[float]:
         """Given the query and documents in tuple
         return the score for each triplets
         """
-        qry = Query(None, query)
+        qry = TopicRecord(TextTopic(query))
         inputs = PairwiseRecords()
         for doc1, doc2 in documents:
-            doc1 = Document(doc1.docid, doc1.content, doc1.score)
-            doc2 = Document(doc2.docid, doc2.content, doc2.score)
             inputs.add(PairwiseRecord(qry, doc1, doc2))
 
         with torch.no_grad():
             scores = self.scorer(inputs, None).cpu().float()  # shape (batchsizes)
             return scores.tolist()
 
 
 ARGS = TypeVar("ARGS")
 KWARGS = TypeVar("KWARGS")
 T = TypeVar("T")
 
 
 class DocumentsFunction(Protocol, Generic[KWARGS, ARGS, T]):
-    def __call__(self, documents: AdhocDocuments, *args: ARGS, **kwargs: KWARGS) -> T:
+    def __call__(self, documents: Documents, *args: ARGS, **kwargs: KWARGS) -> T:
         ...
 
 
 def document_cache(fn: DocumentsFunction[KWARGS, ARGS, T]):
-    """Cache"""
+    """Decorator
+
+    Allows to cache the result of a function that should depend
+    on the document dataset ID
+    """
     retrievers = {}
 
     def _fn(*args: ARGS, **kwargs: KWARGS):
-        def cached(documents: AdhocDocuments) -> T:
+        def cached(documents: Documents) -> T:
             dataset_id = documents.__identifier__().all
 
             if dataset_id not in retrievers:
                 retrievers[dataset_id] = fn(documents, *args, **kwargs)
 
             return retrievers[dataset_id]
 
         return cached
 
     return _fn
 
 
-class ParametricRetrieverFactory(Protocol, Generic[KWARGS]):
-    def __call__(**kwargs: KWARGS) -> Retriever:
-        ...
-
-
-class CollectionBasedRetrievers(Generic[KWARGS]):
-    """Handles various retrievers depending on the collection"""
-
-    def __init__(
-        self,
-        generator: Callable[
-            [AdhocDocuments], Union[ParametricRetrieverFactory, Retriever]
-        ],
-    ):
-        """Given a document collection, returns a factory"""
-        self.generator = generator
-
-    def factory(self, **kwargs) -> "RetrieverFactory":
-        """Returns a retriever factory
-
-        Caches the retrievers based on the document collection"""
-        retrievers = {}
-
-        def _factory(documents: AdhocDocuments):
-            dataset_id = documents.__identifier__().all
-
-            if dataset_id not in retrievers:
-                retrievers[dataset_id] = self.generator(documents)
-
-            if callable(retrievers[dataset_id]):
-                return retrievers[dataset_id](**kwargs)
-
-            assert not kwargs, f"{kwargs}"
-            return retrievers[dataset_id]
-
-        return _factory
-
-
-def collection_based_retrievers(
-    generator: Callable[[AdhocDocuments], Union[ParametricRetrieverFactory, Retriever]]
-) -> CollectionBasedRetrievers:
-    """Decorator for collection-dependent retriever factories
-
-    Example of use
-
-    .. highlight:: python
-    .. code-block:: python
-
-        @collection_based_retrievers
-        def retrievers(documents: AdhocDocuments):
-            index = IndexCollection(documents=documents).submit(launcher=launcher_index)
-            return lambda *, k: RetrieverHydrator(store=documents,
-                retriever=AnseriniRetriever(index=index, k=k, model=basemodel)
-            )
-
-        # Then this can be used to get
-        test_retrievers = retrievers.factory(k=100)
-
-    :param generator: A function that, given a document collection, should return
-        either a retriever or a callable that returns a retriever
-    :return: a collection based retriever
-    """
-    return CollectionBasedRetrievers(generator)
-
-
 class RetrieverHydrator(Retriever):
     """Hydrate retrieved results with document text"""
 
     retriever: Param[Retriever]
     """The retriever to hydrate"""
 
-    store: Param[AdhocDocumentStore]
+    store: Param[DocumentStore]
     """The store for document texts"""
 
     def initialize(self):
         return self.retriever.initialize()
 
-    def retrieve(self, query: str, content=False) -> List[ScoredDocument]:
-        results = self.retriever.retrieve(query, content)
-
-        if content:
-            for result in results:
-                result.content = self.store.document_text(result.docid)
-
-        return results
+    def retrieve(self, query: str) -> List[ScoredDocument]:
+        return [
+            ScoredDocument(self.store.document_ext(sd.document.get_id()), sd.score)
+            for sd in self.retriever.retrieve(query)
+        ]
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/rankers/full.py` & `experimaestro-ir-1.0.0/src/xpmir/rankers/full.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,40 +1,35 @@
 from typing import List, Optional, Tuple, Dict, Any
-from experimaestro import Param, Meta
+from experimaestro import Param, Meta, tqdm
 import torch
 from . import Retriever, ScoredDocument
-from datamaestro_text.data.ir import AdhocDocument, AdhocDocuments
+from datamaestro_text.data.ir import Document, Documents
 from xpmir.neural.dual import DualRepresentationScorer
-from xpmir.letor.batchers import Batcher
+from xpmir.learning.batchers import Batcher
 from xpmir.letor import Device
 
 
 class FullRetriever(Retriever):
     """Retrieves all the documents of the collection
 
     This can be used to build a small validation set on a subset of the
     collection - in that case, the scorer can be used through a
     TwoStageRetriever
     """
 
-    documents: Param[AdhocDocuments]
+    documents: Param[Documents]
 
-    def retrieve(self, query: str, content=False) -> List[ScoredDocument]:
-        if content:
-            return [
-                ScoredDocument(doc.docid, 0.0, doc.text)
-                for doc in self.documents.iter()
-            ]
-        return [ScoredDocument(docid, 0.0, None) for docid in self.documents.iter_ids()]
+    def retrieve(self, query: str) -> List[ScoredDocument]:
+        return [ScoredDocument(doc, 0.0) for doc in self.documents]
 
 
 class FullRetrieverRescorer(Retriever):
     """Scores all the documents from a collection (for a dual representation scorer)"""
 
-    documents: Param[AdhocDocuments]
+    documents: Param[Documents]
     """The set of documents to consider"""
 
     scorer: Param[DualRepresentationScorer]
     """The scorer (a dual representation scorer)"""
 
     batchsize: Param[int] = 0
     batcher: Meta[Batcher] = Batcher()
@@ -53,65 +48,67 @@
         self,
         batch: List[ScoredDocument],
         query: str,
         scoredDocuments: List[ScoredDocument],
     ):
         scoredDocuments.extend(self.scorer.rsv(query, batch))
 
-    def encode_queries(
-        self,
-        queries: List[Tuple[str, str]],
-        encoded: List[Any],
-    ):
+    def encode_queries(self, queries: List[Tuple[str, str]], encoded: List[Any], pbar):
         """Encode queries and append the tensor of encoded queries to the encoded
 
         Args:
             queries (List[Tuple[str, str]]): The input queries (id/text)
             encoded (List[Tuple[List[str], torch.Tensor]]): Full list of topics ??
             it should be the List[torch.Tensor]
         """
+
         encoded.append(self.scorer.encode_queries([text for _, text in queries]))
+        pbar.update(len(queries))
         return encoded
 
     def score(
         self,
-        documents: List[AdhocDocument],
+        documents: List[Document],
         queries: List,
         scored_documents: List[List[ScoredDocument]],
+        pbar,
     ):
         """Score documents for a set of queries
 
         Every time the score process a batch of document together with whole set
         of queries
 
         scored_documents is filled with document batches, i.e. it contains [
         [s(q_0, d_0), ..., s(q_n, d0)], ..., [s(q_0, d_m), ..., s(q_n, d_m)] ]
         --> list of m*n
 
-        Args:
-            documents (List[AdhocDocument]): _description_ queries (List): Lis
-            of queries scored_documents (List[List[ScoredDocument]]): list of
-            scores for each document and for each query (in this order)
+        :param documents: the documents
+
+        :param queries: List of queries
+
+        :param scored_documents: (output) current lists of scored documents (one
+            per query)
         """
         # Encode documents
-        docids = [d.docid for d in documents]
-        encoded = self.scorer.encode_documents(d.text for d in documents)
+        encoded = self.scorer.encode_documents(d.get_text() for d in documents)
 
         # Process query by query (TODO: improve the process)
-        new_scores = [[] for _ in range(len(docids))]
+        new_scores = [[] for _ in documents]
         for ix in range(len(queries)):
+            # Get a range of query records
             query = queries[ix : (ix + 1)]
 
             # Returns a query x document matrix
             scores = self.scorer.score_product(query.to(encoded.device), encoded, None)
 
             # Adds up to the lists
             scores = scores.flatten().detach()
-            for docix, score in enumerate(scores):
-                new_scores[docix].append(ScoredDocument(docids[docix], float(score)))
+            for ix, (document, score) in enumerate(zip(documents, scores)):
+                new_scores[ix].append(ScoredDocument(document, float(score)))
+                pbar.update(1)
 
         # Add each result to the full document list
         scored_documents.extend(new_scores)
 
     def retrieve(self, query: str):
         # Only use retrieve_all
         return self.retrieve_all({"_": query})["_"]
@@ -124,22 +121,26 @@
         self.scorer.eval()
         all_queries = list(queries.items())
 
         with torch.no_grad():
             # Encode all queries
             # each time the batcher will just encode a batchsize of queries
             # and then concat them together
-            enc_queries = self.query_batcher.reduce(
-                all_queries, self.encode_queries, []
-            )
+            with tqdm(total=len(all_queries), desc="Encoding queries") as pbar:
+                enc_queries = self.query_batcher.reduce(
+                    all_queries, self.encode_queries, [], pbar
+                )
             enc_queries = self.scorer.merge_queries(
                 enc_queries
             )  # shape (len(queries), dimension)
 
             # Encode documents and score them
             scored_documents: List[List[ScoredDocument]] = []
-            self.document_batcher.process(
-                self.documents, self.score, enc_queries, scored_documents
-            )
+            with tqdm(
+                total=len(all_queries) * len(self.documents), desc="Scoring documents"
+            ) as pbar:
+                self.document_batcher.process(
+                    self.documents, self.score, enc_queries, scored_documents, pbar
+                )
 
         qids = [qid for qid, _ in all_queries]
         return {qid: [sd[ix] for sd in scored_documents] for ix, qid in enumerate(qids)}
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/rankers/mergers.py` & `experimaestro-ir-1.0.0/src/xpmir/rankers/mergers.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/datasets/adapters.py` & `experimaestro-ir-1.0.0/src/xpmir/datasets/adapters.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,35 +10,35 @@
     Annotated,
     Meta,
 )
 from experimaestro.compat import cached_property
 from datamaestro_text.data.ir import (
     Adhoc,
     AdhocAssessments,
-    AdhocDocument,
-    AdhocDocumentStore,
-    AdhocDocuments,
-    AdhocTopics,
+    Document,
+    DocumentStore,
+    Documents,
+    Topics,
 )
 
 from datamaestro_text.data.ir.trec import TrecAdhocAssessments
-from datamaestro_text.data.ir.csv import AdhocTopics as CSVAdhocTopics
+from datamaestro_text.data.ir.csv import Topics as CSVTopics
 from xpmir.rankers import Retriever
 from xpmir.utils.utils import easylog
 
 logger = easylog()
 
 
-class AdhocTopicFold(AdhocTopics):
+class TopicFold(Topics):
     """ID-based topic selection"""
 
     ids: Param[List[str]]
     """A set of the ids for the topics where we select from"""
 
-    topics: Param[AdhocTopics]
+    topics: Param[Topics]
     """The collection of the topics"""
 
     def iter(self):
         ids = set(self.ids)
         for topic in self.topics.iter():
             if topic.qid in ids:
                 yield topic
@@ -55,29 +55,29 @@
     def trecpath(self, path):
         ids = set(self.ids)
         if not path.is_file():
             with path.open("wt") as fp:
                 for qrels in self.iter():
                     if qrels.qid in ids:
                         for qrel in qrels.assessments:
-                            fp.write(f"""{qrels.qid} 0 {qrel.docno} {qrel.rel}\n""")
+                            fp.write(f"""{qrels.qid} 0 {qrel.docid} {qrel.rel}\n""")
 
         return path
 
     def iter(self):
         ids = set(self.ids)
         for qrels in self.qrels.iter():
             if qrels.qid in ids:
                 yield qrels
 
 
 def fold(ids: Iterable[str], dataset: Adhoc):
     """Returns a fold of a dataset, given topic ids"""
     ids = sorted(list(ids))
-    topics = AdhocTopicFold(topics=dataset.topics, ids=ids)
+    topics = TopicFold(topics=dataset.topics, ids=ids)
     qrels = AdhocAssessmentFold(assessments=dataset.assessments, ids=ids)
     return Adhoc(topics=topics, assessments=qrels, documents=dataset.documents)
 
 
 class ConcatFold(Task):
     """
     Concatenation of several datasets to get a full dataset.
@@ -88,23 +88,23 @@
 
     assessments: Annotated[Path, pathgenerator("assessments.tsv")]
     """Generated assessments file"""
 
     topics: Annotated[Path, pathgenerator("topics.tsv")]
     """Generated topics file"""
 
-    def config(self) -> Adhoc:
+    def task_outputs(self, dep) -> Adhoc:
         dataset_document_id = set(dataset.document.id for dataset in self.datasets)
         assert (
             len(dataset_document_id) == 1
         ), "At the moment only one set of documents supported."
         return Adhoc(
             id="",  # No need to have a more specific id since it is generated
-            topics=CSVAdhocTopics(id="", path=self.topics),
-            assessments=TrecAdhocAssessments(id="", path=self.assessments),
+            topics=dep(CSVTopics(id="", path=self.topics)),
+            assessments=dep(TrecAdhocAssessments(id="", path=self.assessments)),
             documents=self.datasets[0].documents,
         )
 
     def execute(self):
         topics = []
         # concat the topics
         for dataset in self.datasets:
@@ -120,15 +120,15 @@
                 fp.write(f"""{topic.qid}\t{topic.text.replace(slash_t, ' ')}\n""")
 
         with self.assessments.open("wt") as fp:
             for dataset in self.datasets:
                 for qrels in dataset.assessments.iter():
                     if qrels.qid in ids:
                         for qrel in qrels.assessments:
-                            fp.write(f"""{qrels.qid} 0 {qrel.docno} {qrel.rel}\n""")
+                            fp.write(f"""{qrels.qid} 0 {qrel.docid} {qrel.rel}\n""")
 
 
 class RandomFold(Task):
     """Extracts a random subset of topics from a dataset"""
 
     seed: Param[int]
     """Random seed used to compute the fold"""
@@ -138,15 +138,15 @@
 
     dataset: Param[Adhoc]
     """The Adhoc dataset from which a fold is extracted"""
 
     fold: Param[int]
     """Which fold should be taken"""
 
-    exclude: Param[Optional[AdhocTopics]]
+    exclude: Param[Optional[Topics]]
     """Exclude some topics from the random fold"""
 
     assessments: Annotated[Path, pathgenerator("assessments.tsv")]
     """Generated assessments file"""
 
     topics: Annotated[Path, pathgenerator("topics.tsv")]
     """Generated topics file"""
@@ -155,15 +155,15 @@
         assert self.fold < len(self.sizes)
 
     @staticmethod
     def folds(
         seed: int,
         sizes: List[float],
         dataset: Param[Adhoc],
-        exclude: Param[Optional[AdhocTopics]] = None,
+        exclude: Param[Optional[Topics]] = None,
         submit=True,
     ):
         """Creates folds
 
         Parameters:
 
         - submit: if true (default), submits the fold tasks to experimaestro
@@ -176,31 +176,37 @@
             )
             if submit:
                 fold = fold.submit()
             folds.append(fold)
 
         return folds
 
-    def config(self) -> Adhoc:
-        return Adhoc(
-            id="",  # No need to have a more specific id since it is generated
-            topics=CSVAdhocTopics(id="", path=self.topics),
-            assessments=TrecAdhocAssessments(id="", path=self.assessments),
-            documents=self.dataset.documents,
+    def task_outputs(self, dep) -> Adhoc:
+        return dep(
+            Adhoc(
+                id="",  # No need to have a more specific id since it is generated
+                topics=dep(CSVTopics(id="", path=self.topics)),
+                assessments=dep(TrecAdhocAssessments(id="", path=self.assessments)),
+                documents=self.dataset.documents,
+            )
         )
 
     def execute(self):
         import numpy as np
 
         # Get topics
         badids = (
-            set(topic.qid for topic in self.exclude.iter()) if self.exclude else set()
+            set(topic.get_id() for topic in self.exclude.iter())
+            if self.exclude
+            else set()
         )
         topics = [
-            topic for topic in self.dataset.topics.iter() if topic.qid not in badids
+            topic
+            for topic in self.dataset.topics.iter()
+            if topic.get_id() not in badids
         ]
         random = np.random.RandomState(self.seed)
         random.shuffle(topics)
 
         # Get the fold
         sizes = np.array([0.0] + self.sizes)
         s = sizes.sum()
@@ -214,28 +220,28 @@
         topics = topics[indices[self.fold] : indices[self.fold + 1]]
 
         # Write topics and assessments
         ids = set()
         self.topics.parent.mkdir(parents=True, exist_ok=True)
         with self.topics.open("wt") as fp:
             for topic in topics:
-                ids.add(topic.qid)
-                fp.write(f"""{topic.qid}\t{topic.text}\n""")
+                ids.add(topic.get_id())
+                fp.write(f"""{topic.get_id()}\t{topic.get_text()}\n""")
 
         with self.assessments.open("wt") as fp:
             for qrels in self.dataset.assessments.iter():
-                if qrels.qid in ids:
+                if qrels.topic_id in ids:
                     for qrel in qrels.assessments:
-                        fp.write(f"""{qrels.qid} 0 {qrel.docno} {qrel.rel}\n""")
+                        fp.write(f"""{qrels.topic_id} 0 {qrel.doc_id} {qrel.rel}\n""")
 
 
-class AdhocDocumentSubset(AdhocDocuments):
+class DocumentSubset(Documents):
     """ID-based topic selection"""
 
-    base: Param[AdhocDocumentStore]
+    base: Param[DocumentStore]
     """The full document store"""
 
     docids_path: Meta[Path]
     """Path to the file containing the document IDs"""
 
     def __len__(self):
         return len(self.docids)
@@ -243,59 +249,48 @@
     @property
     def documentcount(self):
         return len(self.docids)
 
     def __getitem__(self, slice: Union[int, slice]):
         docids = self.docids[slice]
         if isinstance(docids, List):
-            return AdhocDocumentSubsetSlice(
-                self, docids, range(len(self.docids))[slice]
-            )
-        return AdhocDocument(docids, self.base.document_text(docids), slice)
+            return DocumentSubsetSlice(self, self.docids[slice])
+        return self.base.document_ext(docids)
 
     @cached_property
     def docids(self) -> List[str]:
         # Read document IDs
         with self.docids_path.open("rt") as fp:
             return [line.strip() for line in fp]
 
     def iter_ids(self):
         yield from self.docids
 
-    def iter(self) -> Iterator[AdhocDocument]:
+    def iter(self) -> Iterator[Document]:
         for docid in self.iter_ids():
             content = self.base.document_text(docid)
-            yield AdhocDocument(docid, content)
+            yield Document(docid, content)
 
 
-class AdhocDocumentSubsetSlice:
-    def __init__(
-        self, subset: AdhocDocumentSubset, docids: List[str], internal_ids: List[int]
-    ):
+class DocumentSubsetSlice:
+    """A slice of a `DocumentSubset`"""
+
+    def __init__(self, subset: DocumentSubset, doc_ids: List[int]):
         self.subset = subset
-        self.docids = docids
-        self.internal_ids = internal_ids
+        self.doc_ids = doc_ids
 
     def __iter__(self):
-        for internal_docid, docid in zip(self.internal_ids, self.docids):
-            yield AdhocDocument(
-                docid,
-                self.subset.base.document_text(docid),
-                internal_docid=internal_docid,
-            )
+        for docid in self.doc_ids:
+            yield self.subset.base.document_ext(docid)
 
     def __len__(self):
-        return len(self.docids)
+        return len(self.doc_ids)
 
     def __getitem__(self, ix):
-        docid = self.docids[ix]
-        internal_docid = self.internal_ids[ix]
-        return AdhocDocument(
-            docid, self.subset.base.document_text(docid), internal_docid=internal_docid
-        )
+        return self.subset.base.document_ext(self.doc_ids[ix])
 
 
 class RetrieverBasedCollection(Task):
     """Buils a subset of documents based on the output of a set of retrievers
     and on relevance assessment.
     First get all the document based on the assessment then add the retrieved ones.
     """
@@ -317,64 +312,70 @@
 
     docids_path: Annotated[Path, pathgenerator("docids.txt")]
     """The file containing the document identifiers of the collection"""
 
     def __validate__(self):
         assert len(self.retrievers) > 0, "At least one retriever should be given"
 
-    def config(self) -> Adhoc:
+    def task_outputs(self, dep) -> Adhoc:
         return Adhoc(
             id="",  # No need to have a more specific id since it is generated
             topics=self.dataset.topics,
             assessments=self.dataset.assessments,
-            documents=AdhocDocumentSubset(
-                id="", base=self.dataset.documents, docids_path=self.docids_path
+            documents=dep(
+                DocumentSubset(
+                    id="", base=self.dataset.documents, docids_path=self.docids_path
+                )
             ),
         )
 
     def execute(self):
         for retriever in self.retrievers:
             retriever.initialize()
 
         # Selected document IDs
         docids: Set[str] = set()
 
-        topics = {t.qid: t for t in self.dataset.assessments.iter()}
+        topics = {t.topic_id: t for t in self.dataset.assessments.iter()}
 
         # Retrieve all documents
         for topic in tqdm(
             self.dataset.topics.iter(), total=self.dataset.topics.count()
         ):
-            qrels = topics.get(topic.qid)
+            qrels = topics.get(topic.get_id())
             if qrels is None:
                 logger.warning(
-                    "Skipping topic %s [%s], (no assessment)", topic.qid, topic.text
+                    "Skipping topic %s [%s], (no assessment)",
+                    topic.get_id(),
+                    topic.get_text(),
                 )
                 continue
 
             # Add (not) relevant documents
             if self.keepRelevant:
                 docids.update(
-                    a.docno
+                    a.doc_id
                     for a in qrels.assessments
                     if a.rel > self.relevance_threshold
                 )
 
             if self.keepNotRelevant:
                 docids.update(
-                    a.docno
+                    a.doc_id
                     for a in qrels.assessments
                     if a.rel <= self.relevance_threshold
                 )
 
             # Retrieve and add
             # already defined the numbers to retrieve inside the retriever, so
             # don't need to worry about the threshold here
             for retriever in self.retrievers:
-                docids.update(sd.docid for sd in retriever.retrieve(topic.text))
+                docids.update(
+                    sd.document.get_id() for sd in retriever.retrieve(topic.text)
+                )
 
         # Write the document IDs
         with self.docids_path.open("wt") as fp:
             fp.writelines(f"{docid}\n" for docid in docids)
 
 
 class TextStore(Config):
@@ -383,16 +384,16 @@
     def __getitem__(self, key: str) -> str:
         raise NotImplementedError()
 
 
 class MemoryTopicStore(TextStore):
     """View a set of topics as a (in memory) text store"""
 
-    topics: Param[AdhocTopics]
+    topics: Param[Topics]
     """The collection of the topics to build the store"""
 
     @cached_property
     def store(self):
-        return {topic.qid: topic.text for topic in self.topics.iter()}
+        return {topic.get_id(): topic.text for topic in self.topics.iter()}
 
     def __getitem__(self, key: str) -> str:
         return self.store[key]
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/documents/samplers.py` & `experimaestro-ir-1.0.0/src/xpmir/documents/samplers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from typing import Optional, Tuple, Iterator
 from experimaestro import Config, Param
 import torch
 import numpy as np
-from datamaestro_text.data.ir import AdhocDocumentStore
+from datamaestro_text.data.ir import DocumentStore
 from xpmir.letor import Random
 from xpmir.letor.records import Document, ProductRecords, Query
 from xpmir.letor.samplers import BatchwiseSampler
 from xpmir.utils.iter import RandomSerializableIterator, SerializableIterator
 
 
 class DocumentSampler(Config):
     """How to sample from a document store"""
 
-    documents: Param[AdhocDocumentStore]
+    documents: Param[DocumentStore]
 
     def __call__(self) -> Tuple[int, Iterator[str]]:
         raise NotImplementedError()
 
 
 class HeadDocumentSampler(DocumentSampler):
     """A basic sampler that iterates over the first documents"""
@@ -124,13 +124,13 @@
                     if text[end2 - 1] != " ":
                         end2 = text.rfind(" ", 0, end2)
 
                     # Rejet wrong samples
                     if end2 <= start2 or end1 <= start1:
                         continue
 
-                    batch.addQueries(Query(None, text[start1:end1]))
-                    batch.addDocuments(Document(None, text[start2:end2], 0))
-                batch.setRelevances(relevances)
+                    batch.add_topics(Query(None, text[start1:end1]))
+                    batch.add_documents(Document(None, text[start2:end2], 0))
+                batch.set_relevances(relevances)
                 yield batch
 
         return RandomSerializableIterator(self.random, iterator)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/batchers.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/batchers.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/optim.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/optim.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,30 @@
 import threading
 from typing import Any, Callable, List, Optional, TYPE_CHECKING, Union
 from pathlib import Path
 import torch
+import logging
 import re
 
-from experimaestro import Config, Param, tagspath, TaskOutput, Task
+from experimaestro import (
+    Config,
+    Param,
+    tagspath,
+    Task,
+    PathSerializationLWTask,
+)
 from experimaestro.scheduler import Job, Listener
 from experimaestro.utils import cleanupdir
 from experimaestro.scheduler.services import WebService
-from xpmir.utils.utils import easylog
-from xpmir.letor.metrics import ScalarMetric
+from xpmir.utils.utils import easylog, Initializable
+from xpmir.learning.metrics import ScalarMetric
 from .schedulers import Scheduler
 
 if TYPE_CHECKING:
-    from xpmir.letor.context import TrainerContext
+    from xpmir.learning.context import TrainerContext
 
 logger = easylog()
 
 
 class Optimizer(Config):
     def __call__(self, parameters) -> torch.optim.Optimizer:
         raise NotImplementedError()
@@ -57,21 +64,34 @@
         from torch.optim import AdamW
 
         return AdamW(
             parameters, lr=self.lr, weight_decay=self.weight_decay, eps=self.eps
         )
 
 
-class Module(Config, torch.nn.Module):
+class Module(Config, Initializable, torch.nn.Module):
     """A module contains parameters"""
 
+    def __init__(self):
+        Initializable.__init__(self)
+        torch.nn.Module.__init__(self)
+
     def __call__(self, *args, **kwargs):
         return torch.nn.Module.__call__(self, *args, **kwargs)
 
 
+class ModuleLoader(PathSerializationLWTask):
+    def execute(self):
+        """Loads the model from disk using the given serialization path"""
+        logging.info("Loading model from disk: %s", self.path)
+        self.value.initialize(None)
+        data = torch.load(self.path)
+        self.value.load_state_dict(data)
+
+
 class ParameterFilter(Config):
     """One abstract class which doesn't do the filtrage"""
 
     def __call__(self, name, params) -> bool:
         """Returns true if the parameters should be optimized with the
         associated optimizer"""
         return True
@@ -291,24 +311,28 @@
         self.path = path
         cleanupdir(self.path)
         self.path.mkdir(exist_ok=True, parents=True)
         logger.info("You can monitor learning with:")
         logger.info("tensorboard --logdir=%s", self.path)
         self.url = None
 
-    def add(self, task: Union[TaskOutput, Task], path: Path):
+    def add(self, task: Task, path: Path):
         # Wait until config has started
-        if isinstance(task, TaskOutput):
-            task = task.__xpm__.task
-
         if job := task.__xpm__.job:
             if job.scheduler is not None:
-                job.scheduler.addlistener(
-                    TensorboardServiceListener(self.path / tagspath(task), path)
-                )
+                tag_path = tagspath(task)
+                if tag_path:
+                    job.scheduler.addlistener(
+                        TensorboardServiceListener(self.path / tag_path, path)
+                    )
+                else:
+                    logger.error(
+                        "The task is not associated with tags: "
+                        "cannot link to tensorboard data"
+                    )
             else:
                 logger.debug("No scheduler: not adding the tensorboard data")
         else:
             logger.error("Task was not started: cannot link to tensorboard job path")
 
     def description(self):
         return "Tensorboard service"
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/metrics.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/metrics.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/context.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/context.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,23 +9,22 @@
     NamedTuple,
     Optional,
     TYPE_CHECKING,
 )
 from shutil import rmtree
 from xpmir.context import Context, InitializationHook
 from xpmir.utils.utils import easylog
-from xpmir.letor import DeviceInformation
-from xpmir.letor.metrics import Metric, Metrics
+from xpmir.learning.devices import DeviceInformation
+from xpmir.learning.metrics import Metric, Metrics
 from experimaestro.utils import cleanupdir
 from contextlib import contextmanager
 
 if TYPE_CHECKING:
-    from xpmir.rankers import AbstractLearnableScorer
-    from xpmir.letor.optim import ScheduledOptimizer
-    from xpmir.letor.trainers import Trainer
+    from xpmir.learning.optim import ScheduledOptimizer, Module
+    from xpmir.learning.trainers import Trainer
 
 logger = easylog()
 
 
 class Loss(NamedTuple):
     """A loss"""
 
@@ -33,23 +32,25 @@
     value: torch.Tensor
     weight: float
 
 
 class TrainState:
     """Represents a training state for serialization"""
 
+    MODEL_PATH = "model.pth"
+
     epoch: int
     """The epoch"""
 
     steps: int
     """The number of steps (each epoch is composed of sptes)"""
 
     def __init__(
         self,
-        model: "AbstractLearnableScorer",
+        model: "Module",
         trainer: "Trainer",
         optimizer: "ScheduledOptimizer",
         epoch=0,
         steps=0,
     ):
         # Initialize the state
         self.model = model
@@ -86,35 +87,35 @@
     def save(self, path):
         """Save the state"""
         cleanupdir(path)
 
         with (path / "info.json").open("wt") as fp:
             json.dump(self.state_dict(), fp)
 
-        torch.save(self.model.state_dict(), path / "model.pth")
+        torch.save(self.model.state_dict(), path / TrainState.MODEL_PATH)
         torch.save(self.trainer.state_dict(), path / "trainer.pth")
         torch.save(self.optimizer.state_dict(), path / "optimizer.pth")
 
         self.path = path
 
     def load(self, path, onlyinfo=False):
         """Loads the state from disk"""
         if not onlyinfo:
-            self.model.load_state_dict(torch.load(path / "model.pth"))
+            self.model.load_state_dict(torch.load(path / TrainState.MODEL_PATH))
             self.trainer.load_state_dict(torch.load(path / "trainer.pth"))
             self.optimizer.load_state_dict(torch.load(path / "optimizer.pth"))
 
         with (path / "info.json").open("rt") as fp:
             self.load_state_dict(json.load(fp))
         self.path = path
         self.cached = True
 
     def copy_model(self, path: Path):
         assert self.path is not None
-        for name in ["model.pth", "info.json"]:
+        for name in [TrainState.MODEL_PATH, "info.json"]:
             os.link(self.path / name, path / name)
 
 
 class TrainingHook(Config):
     """Base class for all training hooks"""
 
     pass
@@ -140,15 +141,15 @@
         pass
 
 
 class TrainerContext(Context):
     """Contains all the information about the training context
     for a spefic
 
-    This object is used when training to provide scorers and losses'
+    This object is used when training to provide models and losses'
     with extra information - as well as the possibility to add
     regularization losses
     """
 
     metrics: Optional[Metrics]
     """Metrics to be reported"""
 
@@ -164,27 +165,27 @@
         self,
         device_information: DeviceInformation,
         logpath: Path,
         path: Path,
         max_epoch: int,
         steps_per_epoch: int,
         trainer,
-        ranker: "AbstractLearnableScorer",
+        model: "Module",
         optimizer: "ScheduledOptimizer",
     ):
         super().__init__(device_information)
         self.path = path
         self.logpath = logpath
         self.max_epoch = max_epoch
         self.steps_per_epoch = steps_per_epoch
         self._writer = None
         self._scope = []
         self._losses = None
 
-        self.state = TrainState(ranker, trainer, optimizer)
+        self.state = TrainState(model, trainer, optimizer)
 
     @property
     def writer(self):
         """Returns a tensorboard writer
 
         by default, purges the entries beside the current epoch
         """
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/utils.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/utils.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/schedulers.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/schedulers.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,28 +1,26 @@
-from experimaestro import Config, Param
-from experimaestro.compat import cached_property
 import numpy as np
-from xpmir.utils.utils import easylog
-
-# flake8: noqa: F401
-from .devices import (
-    DEFAULT_DEVICE,
-    Device,
-    DeviceInformation,
-    DistributedDeviceInformation,
-)
-
-logger = easylog()
+from typing import Optional
+from functools import cached_property
+from experimaestro import Config, Param
+from xpmir.utils.utils import EasyLogger
 
 
 class Random(Config):
     """Random configuration"""
 
     seed: Param[int] = 0
     """The seed to use so the random process is deterministic"""
 
     @cached_property
     def state(self) -> np.random.RandomState:
         return np.random.RandomState(self.seed)
 
     def __getstate__(self):
         return {"seed": self.seed}
+
+
+class Sampler(Config, EasyLogger):
+    """Abstract data sampler"""
+
+    def initialize(self, random: Optional[np.random.RandomState]):
+        self.random = random or np.random.RandomState(random.randint(0, 2**31))
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/records.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/records.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,59 +1,90 @@
-from dataclasses import dataclass
 import torch
 import itertools
+from attrs import define
+from datamaestro_text.data.ir.base import (
+    Topic,
+    Document as BaseDocument,
+    TextTopic,
+    TextDocument,
+)
 from typing import (
     Iterable,
     List,
     NamedTuple,
     Optional,
     Tuple,
     TypeVar,
     Union,
 )
 
 
-@dataclass
-class Query:
-    id: Optional[str]
-    text: Optional[str]
+@define()
+class TopicRecord:
+    """A query record when training models"""
+
+    topic: Topic
+
+    @staticmethod
+    def from_text(text: str):
+        return TopicRecord(TextTopic(text))
 
 
-class Document(NamedTuple):
-    """A document (the docid or the text can be None, but not both)"""
-
-    docid: Optional[str]
-    text: Optional[str]
-    score: Optional[float]
+@define()
+class DocumentRecord:
+    """A document record when training models"""
+
+    document: BaseDocument
+    """The document"""
+
+    @staticmethod
+    def from_text(text: str):
+        return DocumentRecord(TextDocument(text))
+
+
+@define()
+class ScoredDocumentRecord(DocumentRecord):
+    """A document record when training models"""
+
+    score: float
+    """A retrieval score associated with this record (e.g. of the first-stage
+    retriever)"""
+
+
+# Aliases for deprecated types
+Document = DocumentRecord
+Query = TopicRecord
 
 
 class PointwiseRecord:
     """A record from a pointwise sampler"""
 
     # The query
-    query: Query
+    topic: TopicRecord
 
     # The document
-    document: Document
+    document: DocumentRecord
 
     # The relevance
     relevance: Optional[float]
 
     def __init__(
         self,
-        query: Query,
-        docid: str,
-        content: str,
-        score: float,
-        relevance: Optional[float],
+        topic: TopicRecord,
+        document: DocumentRecord,
+        relevance: Optional[float] = None,
     ):
-        self.query = query
-        self.document = Document(docid, content, score)
+        self.topic = topic
+        self.document = document
         self.relevance = relevance
 
+    @property
+    def query(self):
+        return self.topic
+
 
 class TokenizedTexts(NamedTuple):
     tokens: List[List[str]]
     ids: torch.LongTensor
     lens: List[int]
     mask: torch.LongTensor
     token_type_ids: torch.LongTensor = None
@@ -66,26 +97,33 @@
     """Base records just exposes iterables on (query, document) pairs
 
     Records can be structured, i.e. the same queries and documents
     can be used more than once. To allow optimization (e.g. pre-computing
     document/query representation),
     """
 
-    queries: Iterable[Query]
-    documents: Iterable[Document]
+    topics: Iterable[TopicRecord]
+    documents: Iterable[DocumentRecord]
     is_product = False
 
     @property
-    def unique_queries(self) -> Iterable[Query]:
-        return self.queries
+    def unique_topics(self) -> Iterable[TopicRecord]:
+        return self.topics
+
+    unique_queries = unique_topics
 
     @property
-    def unique_documents(self) -> Iterable[Document]:
+    def unique_documents(self) -> Iterable[DocumentRecord]:
         return self.documents
 
+    @property
+    def queries(self):
+        """Deprecated: use topics"""
+        return self.topics
+
     def pairs(self) -> Tuple[Iterable[int], Iterable[int]]:
         """Returns two iterators (over queries and documents)
 
         Returns the list of query/document indices for which we should compute
         the score, or None if all (cartesian product). This method should be
         used with `unique` set to true to get the queries/documents
         """
@@ -104,24 +142,24 @@
         raise NotImplementedError(f"__len__() in {self.__class__}")
 
 
 class PointwiseRecords(BaseRecords[PointwiseRecord]):
     """Pointwise records are a set of triples (query, document, relevance)"""
 
     # The queries
-    queries: List[Query]
+    topics: List[TopicRecord]
 
     # Text of the documents
-    documents: List[Document]
+    documents: List[DocumentRecord]
 
     # The relevances
     relevances: List[float]
 
     def __init__(self):
-        self.queries = []
+        self.topics = []
         self.documents = []
         self.relevances = []
 
     def add(self, record: PointwiseRecord):
         self.queries.append(record.query)
         self.relevances.append(record.relevance or 0)
         self.documents.append(record.document)
@@ -131,141 +169,147 @@
 
     def pairs(self) -> Tuple[List[int], List[int]]:
         ix = list(range(len(self.queries)))
         return (ix, ix)
 
     @staticmethod
     def from_texts(
-        queries: List[str],
+        topics: List[str],
         documents: List[str],
         relevances: Optional[List[float]] = None,
     ):
         records = PointwiseRecords()
-        records.queries = list(map(lambda t: Query(None, t), queries))
-        records.documents = list(map(lambda t: Query(None, t), documents))
+        records.topics = list(map(lambda t: TopicRecord(TextTopic(t)), topics))
+        records.documents = list(
+            map(lambda t: DocumentRecord(TextDocument(t)), documents)
+        )
         records.relevances = relevances
         return records
 
 
 class PairwiseRecord:
     """A pairwise record is composed of a query, a positive and a negative document"""
 
-    query: Query
+    query: TopicRecord
     positive: Document
     negative: Document
 
-    def __init__(self, query: Query, positive: Document, negative: Document):
+    def __init__(self, query: TopicRecord, positive: Document, negative: Document):
         self.query = query
         self.positive = positive
         self.negative = negative
 
 
 class PairwiseRecordWithTarget(PairwiseRecord):
     """A pairwise record is composed of a query, a positive and a negative
     document, and the indetifier which says the one on the first is pos or
     neg"""
 
     target: int
 
     def __init__(
-        self, query: Query, positive: Document, negative: Document, target: int
+        self, query: TopicRecord, positive: Document, negative: Document, target: int
     ):
         super().__init__(query, positive, negative)
         self.target = target
 
 
 class PairwiseRecords(BaseRecords):
     """Pairwise records of queries associated with (positive, negative) pairs"""
 
     # The queries
-    _queries: List[Query]
+    _topics: List[TopicRecord]
 
     # The document IDs (positive)
-    positives: List[Document]
+    positives: List[DocumentRecord]
 
     # The scores of the retriever
-    negatives: List[Document]
+    negatives: List[DocumentRecord]
 
     def __init__(self):
-        self._queries = []
+        self._topics = []
         self.positives = []
         self.negatives = []
 
     def add(self, record: PairwiseRecord):
-        self._queries.append(record.query)
+        self._topics.append(record.query)
         self.positives.append(record.positive)
         self.negatives.append(record.negative)
 
     @property
-    def queries(self):
-        return itertools.chain(self._queries, self._queries)
+    def topics(self):
+        return itertools.chain(self._topics, self._topics)
+
+    queries = topics
 
     @property
-    def unique_queries(self):
-        return self._queries
+    def unique_topics(self):
+        return self._topics
+
+    unique_queries = unique_topics
 
     @property
     def documents(self):
         return itertools.chain(self.positives, self.negatives)
 
     def pairs(self):
-        indices = list(range(len(self._queries)))
+        indices = list(range(len(self._topics)))
         return indices * 2, list(range(2 * len(self.positives)))
 
     def __len__(self):
-        return len(self._queries)
+        return len(self._topics)
 
     def __getitem__(self, ix: Union[slice, int]):
         if isinstance(ix, slice):
             records = PairwiseRecords()
-            for i in range(ix.start, min(ix.stop, len(self._queries)), ix.step or 1):
+            for i in range(ix.start, min(ix.stop, len(self._topics)), ix.step or 1):
                 records.add(
                     PairwiseRecord(
-                        self._queries[i], self.positives[i], self.negatives[i]
+                        self._topics[i], self.positives[i], self.negatives[i]
                     )
                 )
             return records
 
-        return PairwiseRecord(self._queries[ix], self.positives[ix], self.negatives[ix])
+        return PairwiseRecord(self._topics[ix], self.positives[ix], self.negatives[ix])
 
 
 class PairwiseRecordsWithTarget(PairwiseRecords):
     """Pairwise records associated with a label (saying which document is better)"""
 
     target: List[int]
 
     def __init__(self):
         super().__init__()
         self.target = []
 
     def add(self, record: PairwiseRecordWithTarget):
-        self._queries.append(record.query)
+        self._topics.append(record.query)
         self.positives.append(record.positive)
         self.negatives.append(record.negative)
         self.target.append(record.target)
 
     def get_target(self):
         return self.target
 
     def __getitem__(self, ix: Union[slice, int]):
         if isinstance(ix, slice):
             records = PairwiseRecordsWithTarget()
-            for i in range(ix.start, min(ix.stop, len(self._queries)), ix.step or 1):
+            for i in range(ix.start, min(ix.stop, len(self._topics)), ix.step or 1):
                 records.add(
                     PairwiseRecordWithTarget(
-                        self._queries[i],
+                        self._topics[i],
                         self.positives[i],
                         self.negatives[i],
                         self.target[i],
                     )
                 )
             return records
 
         return PairwiseRecordWithTarget(
-            self._queries[ix], self.positives[ix], self.negatives[ix], self.target[ix]
+            self._topics[ix], self.positives[ix], self.negatives[ix], self.target[ix]
         )
 
 
 class BatchwiseRecords(BaseRecords):
     """Several documents (with associated [pseudo]relevance) per query
 
     Assumes that the number of documents per query is always the same (even
@@ -282,76 +326,80 @@
 class ProductRecords(BatchwiseRecords):
     """Computes the score for all the documents and queries
 
     The relevance matrix
 
     Attributes:
 
-        _queries: The list of queries
+        _topics: The list of queries
         _documents: The list of documents
         _relevances: (query x document) matrix with relevance score (between 0 and 1)
     """
 
-    _queries: List[Query]
+    _topics: List[TopicRecord]
     """The list of queries to score"""
 
-    _documents: List[Document]
+    _documents: List[DocumentRecord]
     """The list of documents to score"""
 
     relevances: torch.Tensor
     """A 2D tensor (query x document) indicating the relevance of the each
     query/document pair"""
 
     is_product = True
 
     def __init__(self):
-        self._queries = []
+        self._topics = []
         self._documents = []
 
-    def addQueries(self, *queries: Query):
-        self._queries.extend(queries)
+    def add_topics(self, *topics: TopicRecord):
+        self._topics.extend(topics)
 
-    def addDocuments(self, *documents: Document):
+    def add_documents(self, *documents: Document):
         self._documents.extend(documents)
 
-    def setRelevances(self, relevances: torch.Tensor):
-        assert relevances.shape[0] == len(self._queries), (
-            f"The number of queries {len(self._queries)} "
+    def set_relevances(self, relevances: torch.Tensor):
+        assert relevances.shape[0] == len(self._topics), (
+            f"The number of queries {len(self._topics)} "
             + "does not match the number of rows {relevances.shape[0]}"
         )
         assert relevances.shape[1] == len(self._documents), (
             f"The number of documents {len(self._documents)} "
             + "does not match the number of columns {relevances.shape[1]}"
         )
         self.relevances = relevances
 
     def __len__(self):
-        return len(self._queries)
+        return len(self._topics)
 
     @property
-    def queries(self):
-        for q in self._queries:
+    def topics(self):
+        for q in self._topics:
             for _ in self._documents:
                 yield q
 
+    queries = topics
+
     @property
-    def unique_queries(self):
-        return self._queries
+    def unique_topics(self):
+        return self._topics
+
+    unique_queries = unique_topics
 
     @property
     def documents(self):
-        for _ in self._queries:
+        for _ in self._topics:
             for d in self._documents:
                 yield d
 
     @property
     def unique_documents(self):
         return self._documents
 
     def pairs(self) -> Tuple[Iterable[int], Iterable[int]]:
-        queries = []
+        topics = []
         documents = []
-        for q in range(len(self._queries)):
+        for q in range(len(self._topics)):
             for d in range(len(self._documents)):
-                queries.append(q)
+                topics.append(q)
                 documents.append(d)
-        return queries, documents
+        return topics, documents
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/devices.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/devices.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/samplers.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/samplers.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,58 +1,50 @@
 from pathlib import Path
-from typing import (
-    Iterator,
-    List,
-    Optional,
-    Tuple,
-)
+from typing import Iterator, List, Tuple, Dict
 import numpy as np
 from datamaestro_text.data.ir import (
     Adhoc,
     TrainingTriplets,
     PairwiseSampleDataset,
     PairwiseSample,
-    AdhocDocumentStore,
+    DocumentStore,
 )
-from experimaestro import Config, Param, tqdm, Task, Annotated, pathgenerator
+from datamaestro_text.data.ir.base import IDDocument, IDTopic, TextTopic
+from experimaestro import Param, tqdm, Task, Annotated, pathgenerator
 from experimaestro.annotations import cache
 import torch
+from xpmir.rankers import ScoredDocument
 from xpmir.datasets.adapters import TextStore
 from xpmir.letor.records import (
     BatchwiseRecords,
     PairwiseRecords,
     ProductRecords,
-    Document,
     PairwiseRecord,
     PointwiseRecord,
-    Query,
+    TopicRecord,
+    DocumentRecord,
+    ScoredDocumentRecord,
 )
 from xpmir.rankers import Retriever, Scorer
-from xpmir.utils.utils import EasyLogger, easylog
+from xpmir.learning import Sampler
+from xpmir.utils.utils import easylog
 from xpmir.utils.iter import (
     RandomSerializableIterator,
     SerializableIterator,
     SerializableIteratorAdapter,
     SkippingIterator,
 )
 from datamaestro_text.interfaces.plaintext import read_tsv
 
 logger = easylog()
 
 
 # --- Base classes for samplers
 
 
-class Sampler(Config, EasyLogger):
-    """Abstract data sampler"""
-
-    def initialize(self, random: Optional[np.random.RandomState]):
-        self.random = random or np.random.RandomState(random.randint(0, 2**31))
-
-
 class PointwiseSampler(Sampler):
     def pointwise_iter(self) -> SerializableIterator[PointwiseRecord]:
         """Iterable over pointwise records"""
         raise NotImplementedError(f"{self.__class__} should implement PointwiseRecord")
 
 
 class PairwiseSampler(Sampler):
@@ -65,15 +57,16 @@
         Args:
             batch_size: Number of queries per batch
         """
         raise NotImplementedError(f"{self.__class__} should implement __iter__")
 
 
 class BatchwiseSampler(Sampler):
-    """Batchwise samplers provide for each question a set of documents"""
+    """Base class for batchwise samplers, that provide for each question a list
+    of documents"""
 
     def batchwise_iter(self, batch_size: int) -> SerializableIterator[BatchwiseRecords]:
         """Iterate over batches of size (# of queries) batch_size
 
         Args:
             batch_size: Number of queries per batch
         """
@@ -90,33 +83,30 @@
         dataset: The topics and assessments
         retriever: The document retriever
     """
 
     dataset: Param[Adhoc]
     retriever: Param[Retriever]
 
-    _store: AdhocDocumentStore
+    _store: DocumentStore
 
     def __validate__(self) -> None:
         super().__validate__()
 
         assert (
             self.retriever.get_store() is not None
         ), "The retriever has no associated document store"
 
     def initialize(self, random):
         super().initialize(random)
         self._store = self.retriever.get_store()
 
-    def document_text(self, doc_id):
+    def document(self, doc_id):
         """Returns the document textual content"""
-        text = self._store.document_text(doc_id)
-        if text is None:
-            logger.warning(f"Document {doc_id} has no content")
-        return text
+        return self._store.document_ext(doc_id)
 
     @cache("run")
     def _itertopics(
         self, runpath: Path
     ) -> Iterator[
         Tuple[str, List[Tuple[str, int, float]], List[Tuple[str, int, float]]]
     ]:
@@ -127,20 +117,20 @@
         if not runpath.is_file():
             tmprunpath = runpath.with_suffix(".tmp")
 
             with tmprunpath.open("wt") as fp:
 
                 # Read the assessments
                 self.logger.info("Reading assessments")
-                assessments = {}  # type: Dict[str, Dict[str, float]]
+                assessments: Dict[str, Dict[str, float]] = {}
                 for qrels in self.dataset.assessments.iter():
                     doc2rel = {}
                     assessments[qrels.qid] = doc2rel
                     for qrel in qrels.assessments:
-                        doc2rel[qrel.docno] = qrel.rel
+                        doc2rel[qrel.docid] = qrel.rel
                 self.logger.info("Read assessments for %d topics", len(assessments))
 
                 self.logger.info("Retrieving documents for each topic")
                 queries = []
                 for query in self.dataset.topics.iter():
                     queries.append(query)
 
@@ -156,42 +146,42 @@
                         continue
 
                     # Write all the positive documents
                     positives = []
                     for docno, rel in qassessments.items():
                         if rel > 0:
                             fp.write(
-                                f"{query.text if not positives else ''}"
+                                f"{query.get_text() if not positives else ''}"
                                 f"\t{docno}\t0.\t{rel}\n"
                             )
                             positives.append((docno, rel, 0))
 
                     if not positives:
                         self.logger.debug(
                             "Skipping topic %s (no relevant documents)", query.qid
                         )
                         skipped += 1
                         continue
 
-                    scoreddocuments = self.retriever.retrieve(
-                        query.text
-                    )  # type: List[ScoredDocument]
+                    scoreddocuments: List[ScoredDocument] = self.retriever.retrieve(
+                        query.get_text()
+                    )
 
                     negatives = []
                     for rank, sd in enumerate(scoreddocuments):
                         # Get the assessment (assumes not relevant)
                         rel = qassessments.get(sd.docid, 0)
                         if rel > 0:
                             continue
 
                         negatives.append((sd.docid, rel, sd.score))
                         fp.write(f"\t{sd.docid}\t{sd.score}\t{rel}\n")
 
                     assert len(positives) > 0 and len(negatives) > 0
-                    yield query.text, positives, negatives
+                    yield query.get_text(), positives, negatives
 
                 # Finally, move the cache file in place...
                 self.logger.info(
                     "Processed %d topics (%d skipped)", len(queries), skipped
                 )
                 tmprunpath.rename(runpath)
         else:
@@ -275,24 +265,26 @@
         return topics
 
     def sample(self, samples: List[Tuple[str, int, float]]):
         text = None
         while text is None:
             docid, rel, score = samples[self.random.randint(0, len(samples))]
             text = self.document_text(docid)
-        return Document(docid, text, score)
+        return DocumentRecord(docid, text, score)
 
     def pairwise_iter(self) -> SerializableIterator[PairwiseRecord]:
         def iter(random):
             while True:
                 title, positives, negatives = self.topics[
                     random.randint(0, len(self.topics))
                 ]
                 yield PairwiseRecord(
-                    Query(None, title), self.sample(positives), self.sample(negatives)
+                    TopicRecord(TextTopic(title)),
+                    self.sample(positives),
+                    self.sample(negatives),
                 )
 
         return RandomSerializableIterator(self.random, iter)
 
 
 class PairwiseInBatchNegativesSampler(BatchwiseSampler):
     """An in-batch negative sampler constructured from a pairwise one"""
@@ -312,57 +304,40 @@
             )
 
             while True:
                 batch = ProductRecords()
                 positives = []
                 negatives = []
                 for _, record in zip(range(batch_size), pair_iter):
-                    batch.addQueries(record.query)
+                    batch.add_topics(record.query)
                     positives.append(record.positive)
                     negatives.append(record.negative)
-                batch.addDocuments(*positives)
-                batch.addDocuments(*negatives)
-                batch.setRelevances(relevances)
+                batch.add_documents(*positives)
+                batch.add_documents(*negatives)
+                batch.set_relevances(relevances)
                 yield batch
 
         return SerializableIteratorAdapter(self.sampler.pairwise_iter(), iter)
 
 
-class TripletBasedSampler(PairwiseSampler):
-    """Sampler based on a triplet file
+def always_none(*args, **kwargs):
+    """Just returns None to whatever"""
+    return None
 
-    Attributes:
 
-    source: the source of the triplets
-    index: the index (if the source is only)
-    """
+class TripletBasedSampler(PairwiseSampler):
+    """Sampler based on a triplet source"""
 
     source: Param[TrainingTriplets]
-    index: Param[Optional[AdhocDocumentStore]] = None
-
-    def __validate__(self):
-        assert (
-            not self.source.ids or self.index is not None
-        ), "An index should be provided if source is IDs only"
-
-    def _fromid(self, docid: str):
-        assert self.index is not None
-        return Document(docid, self.index.document_text(docid), None)
-
-    @staticmethod
-    def _fromtext(text: str):
-        return Document(None, text, None)
+    """Triplets"""
 
     def pairwise_iter(self) -> SerializableIterator[PairwiseRecord]:
-        getdoc = self._fromid if self.source.ids else self._fromtext
-        source = self.source
-
         iterator = (
-            PairwiseRecord(Query(None, query), getdoc(pos), getdoc(neg))
-            for query, pos, neg in source.iter()
+            PairwiseRecord(TopicRecord(topic), DocumentRecord(pos), DocumentRecord(neg))
+            for topic, pos, neg in self.source.iter()
         )
 
         return SkippingIterator(iterator)
 
 
 class PairwiseDatasetTripletBasedSampler(PairwiseSampler):
     """Sampler based on a dataset where each query is associated
@@ -431,17 +406,17 @@
     """The path which stores the existing triplets"""
 
     def pairwise_iter(self) -> SerializableIterator[PairwiseRecord]:
         def iter() -> Iterator[PairwiseSample]:
             for triplet in read_tsv(self.pairwise_samples_path):
                 q_id, pos_id, pos_score, neg_id, neg_score = triplet
                 yield PairwiseRecord(
-                    Query(q_id, None),
-                    Document(pos_id, None, pos_score),
-                    Document(neg_id, None, neg_score),
+                    TopicRecord(IDTopic(q_id)),
+                    ScoredDocumentRecord(IDDocument(pos_id), pos_score),
+                    ScoredDocumentRecord(IDDocument(neg_id), neg_score),
                 )
 
         return SkippingIterator(iter)
 
 
 # --- Tasks for hard negatives
 
@@ -454,18 +429,21 @@
 
     retriever: Param[Retriever]
     """The retriever to score of the document wrt the query"""
 
     hard_negative_samples: Annotated[Path, pathgenerator("hard_negatives.tsv")]
     """Path to store the generated hard negatives"""
 
-    def config(self) -> PairwiseSampleDataset:
+    def task_outputs(self, dep) -> PairwiseSampleDataset:
         """return a iterator of PairwiseSample"""
-        return PairwiseSampleDatasetFromTSV(
-            ids=self.dataset.id, hard_negative_samples_path=self.hard_negative_samples
+        return dep(
+            PairwiseSampleDatasetFromTSV(
+                ids=self.dataset.id,
+                hard_negative_samples_path=self.hard_negative_samples,
+            )
         )
 
     def execute(self):
         """Retrieve over the dataset and select the positive and negative
         according to the relevance score and their rank
         """
         self.logger.info("Reading topics and retrieving documents")
@@ -476,15 +454,15 @@
         # Read the assessments
         self.logger.info("Reading assessments")
         assessments = {}  # type: Dict[str, Dict[str, float]]
         for qrels in self.dataset.assessments.iter():
             doc2rel = {}
             assessments[qrels.qid] = doc2rel
             for qrel in qrels.assessments:
-                doc2rel[qrel.docno] = qrel.rel
+                doc2rel[qrel.docid] = qrel.rel
         self.logger.info("Assessment loaded")
         self.logger.info("Read assessments for %d topics", len(assessments))
 
         self.logger.info("Retrieving documents for each topic")
         queries = []
         for query in self.dataset.topics.iter():
             queries.append(query)
@@ -501,17 +479,17 @@
                     skipped += 1
                     self.logger.warning("Skipping topic %s (no assessments)", query.qid)
                     continue
 
                 # Write all the positive documents
                 positives = []
                 negatives = []
-                scoreddocuments = self.retriever.retrieve(
-                    query.text
-                )  # type: List[ScoredDocument]
+                scoreddocuments: List[ScoredDocument] = self.retriever.retrieve(
+                    query.get_text()
+                )
 
                 for rank, sd in enumerate(scoreddocuments):
                     if qassessments.get(sd.docid, 0) > 0:
                         # It is a positive document:
                         positives.append(sd.docid)
                     else:
                         # It is a negative document or
@@ -545,36 +523,38 @@
 class TeacherModelBasedHardNegativesTripletSampler(Task, Sampler):
     """For a given set of triplet, assign the score
     for the documents according to the teacher model"""
 
     sampler: Param[PairwiseSampler]
     """The list of exsting hard negatives which we can sample from"""
 
-    document_store: Param[AdhocDocumentStore]
+    document_store: Param[DocumentStore]
     """The document store"""
 
-    query_store: Param[TextStore]
+    topic_store: Param[TextStore]
     """The query_document store"""
 
     teacher_model: Param[Scorer]
     """The teacher model which scores the positive and negative document"""
 
     hard_negative_triplet: Annotated[Path, pathgenerator("triplet.tsv")]
     """The path to store the generated triplets"""
 
     batch_size: int
     """How many pairs of documents are been calculate in a batch"""
 
-    def config(self) -> PairwiseSampler:
-        return PairwiseSamplerFromTSV(pairwise_samples_path=self.hard_negative_triplet)
+    def task_outputs(self, dep) -> PairwiseSampler:
+        return dep(
+            PairwiseSamplerFromTSV(pairwise_samples_path=self.hard_negative_triplet)
+        )
 
     def iter_pairs_with_text(self) -> Iterator[PairwiseRecord]:
         """Add the information of the text back to the records"""
         for record in self.sampler.pairwise_iter():
-            record.query.text = self.query_store[record.query.id]
+            record.query.text = self.topic_store[record.query.id]
             record.positive.text = self.document_store.document_text(
                 record.positive.docid
             )
             record.negative.text = self.document_store.document_text(
                 record.negative.docid
             )
             yield record
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/learner.py` & `experimaestro-ir-1.0.0/src/xpmir/learning/learner.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,42 +1,36 @@
 from enum import Enum
 import logging
 import torch
-import json
 from pathlib import Path
-from typing import Any, Dict, Iterator, List, NamedTuple
-from datamaestro_text.data.ir import Adhoc
+from typing import Dict, Iterator, List, NamedTuple, Any
 from experimaestro import (
     Task,
     Config,
     Param,
-    copyconfig,
     pathgenerator,
     Annotated,
     tqdm,
     Meta,
 )
 import numpy as np
 from xpmir.context import Hook, InitializationHook
-from xpmir.letor.batchers import RecoverableOOMError
 from xpmir.utils.utils import EasyLogger, easylog, foreach
-from xpmir.evaluation import evaluate
-from xpmir.letor import DEFAULT_DEVICE, Device, DeviceInformation, Random
-from xpmir.letor.trainers import Trainer
-from xpmir.letor.context import (
+from xpmir.learning.devices import DEFAULT_DEVICE, Device, DeviceInformation
+from xpmir.learning import Random
+from xpmir.learning.trainers import Trainer
+from xpmir.learning.context import (
     StepTrainingHook,
     TrainState,
     TrainerContext,
 )
-from xpmir.letor.metrics import Metrics
-from xpmir.rankers import (
-    AbstractLearnableScorer,
-    Retriever,
-)
-from xpmir.letor.optim import ParameterOptimizer, ScheduledOptimizer
+from xpmir.learning.metrics import Metrics
+
+from .optim import Module, ModuleLoader, ParameterOptimizer, ScheduledOptimizer
+from .batchers import RecoverableOOMError
 
 logger = easylog()
 
 
 class LearnerListenerStatus(Enum):
     NO_DECISION = 0
     STOP = 1
@@ -47,208 +41,62 @@
 
 
 class LearnerListener(Config):
     """Hook for learner
 
     Performs some operations after a learning epoch"""
 
-    id: Param[str]
-    """Unique ID to identify the listener"""
+    id: Meta[str]
+    """Unique ID to identify the listener (ignored for signature)"""
 
     def initialize(self, learner: "Learner", context: TrainerContext):
         self.learner = learner
         self.context = context
 
     def __call__(self, state: TrainState) -> LearnerListenerStatus:
         """Process and returns whether the training process should stop"""
         return LearnerListenerStatus.NO_DECISION
 
     def update_metrics(self, metrics: Dict[str, float]):
         """Add metrics"""
         pass
 
-    def taskoutputs(self, learner: "Learner"):
+    def task_outputs(self, learner: "Learner", dep):
         """Outputs from this listeners"""
         return None
 
 
-class ValidationListener(LearnerListener):
-    """Learning validation early-stopping
-
-    Computes a validation metric and stores the best result. If early_stop is
-    set (> 0), then it signals to the learner that the learning process can
-    stop.
-    """
-
-    metrics: Param[Dict[str, bool]] = {"map": True}
-    """Dictionary whose keys are the metrics to record, and boolean
-            values whether the best performance checkpoint should be kept for
-            the associated metric ([parseable by ir-measures](https://ir-measur.es/))"""
-
-    dataset: Param[Adhoc]
-    """The dataset to use"""
-
-    retriever: Param[Retriever]
-    """The retriever for validation"""
-
-    warmup: Param[int] = -1
-    """How many epochs before actually computing the metric"""
-
-    bestpath: Annotated[Path, pathgenerator("best")]
-    """Path to the best checkpoints"""
-
-    last_checkpoint_path: Annotated[Path, pathgenerator("last_checkpoint")]
-    """Path to the last checkpoints"""
-
-    store_last_checkpoint: Param[bool] = False
-    """Besides the model with the best performance, whether store the last
-    checkpoint of the model or not"""
-
-    info: Annotated[Path, pathgenerator("info.json")]
-    """Path to the JSON file that contains the metric values at each epoch"""
-
-    validation_interval: Param[int] = 1
-    """Epochs between each validation"""
-
-    early_stop: Param[int] = 0
-    """Number of epochs without improvement after which we stop learning.
-    Should be a multiple of validation_interval or 0 (no early stopping)"""
-
-    def __validate__(self):
-        assert (
-            self.early_stop % self.validation_interval == 0
-        ), "Early stop should be a multiple of the validation interval"
-
-    def initialize(self, learner: "Learner", context: TrainerContext):
-        super().initialize(learner, context)
-
-        self.retriever.initialize()
-        self.bestpath.mkdir(exist_ok=True, parents=True)
-        if self.store_last_checkpoint:
-            self.last_checkpoint_path.mkdir(exist_ok=True, parents=True)
-
-        # Checkpoint start
-        try:
-            with self.info.open("rt") as fp:
-                self.top = json.load(fp)  # type: Dict[str, Dict[str, float]]
-        except Exception:
-            self.top = {}
-
-    def update_metrics(self, metrics: Dict[str, float]):
-        if self.top:
-            # Just use another key
-            for metric in self.metrics.keys():
-                metrics[f"{self.id}/final/{metric}"] = self.top[metric]["value"]
-
-    def monitored(self) -> Iterator[str]:
-        return [key for key, monitored in self.metrics.items() if monitored]
-
-    def taskoutputs(self, learner: "Learner"):
-        """Experimaestro outputs: returns the best checkpoints for each
-        metric"""
-        res = {
-            key: copyconfig(learner.scorer, checkpoint=str(self.bestpath / key))
-            for key, store in self.metrics.items()
-            if store
-        }
-        if self.store_last_checkpoint:
-            res["last_checkpoint"] = copyconfig(
-                learner.scorer, checkpoint=str(self.last_checkpoint_path)
-            )
-
-        return res
-
-    def should_stop(self, epoch=0):
-        if self.early_stop > 0 and self.top:
-            epochs_since_imp = (epoch or self.context.epoch) - max(
-                info["epoch"] for key, info in self.top.items() if self.metrics[key]
-            )
-            if epochs_since_imp >= self.early_stop:
-                return LearnerListenerStatus.STOP
-
-        return LearnerListenerStatus.DONT_STOP
-
-    def __call__(self, state: TrainState):
-        # Check that we did not stop earlier (when loading from checkpoint / if other
-        # listeners have not stopped yet)
-        if self.should_stop(state.epoch - 1) == LearnerListenerStatus.STOP:
-            return LearnerListenerStatus.STOP
-
-        if state.epoch % self.validation_interval == 0:
-            # Compute validation metrics
-            means, details = evaluate(
-                self.retriever, self.dataset, list(self.metrics.keys()), True
-            )
-
-            for metric, keep in self.metrics.items():
-                value = means[metric]
-
-                self.context.writer.add_scalar(
-                    f"{self.id}/{metric}/mean", value, state.step
-                )
-
-                self.context.writer.add_histogram(
-                    f"{self.id}/{metric}",
-                    np.array(list(details[metric].values()), dtype=np.float32),
-                    state.step,
-                )
-
-                # Update the top validation
-                if state.epoch >= self.warmup:
-                    topstate = self.top.get(metric, None)
-                    if topstate is None or value > topstate["value"]:
-                        # Save the new top JSON
-                        self.top[metric] = {"value": value, "epoch": self.context.epoch}
-
-                        # Copy in corresponding directory
-                        if keep:
-                            logging.info(
-                                f"Saving the checkpoint {state.epoch}"
-                                f" for metric {metric}"
-                            )
-                            self.context.copy(self.bestpath / metric)
-
-            if self.store_last_checkpoint:
-                logging.info(f"Saving the last checkpoint {state.epoch}")
-                self.context.copy(self.last_checkpoint_path)
-
-            # Update information
-            with self.info.open("wt") as fp:
-                json.dump(self.top, fp)
-
-        # Early stopping?
-        return self.should_stop()
-
-
 class LearnerOutput(NamedTuple):
     """The data structure for the output of a learner. It contains a dictionary
     where the key is the name of the listener and the value is the output of
     that listener"""
 
     listeners: Dict[str, Any]
 
+    learned_model: Module
+
 
 class Learner(Task, EasyLogger):
     """Model Learner
 
-    The learner task is generic, and takes two main arguments: (1) the scorer
+    The learner task is generic, and takes two main arguments: (1) the model
     defines the model (e.g. DRMM), and (2) the trainer defines how the model
     should be trained (e.g. pointwise, pairwise, etc.)
 
     When submitted, it returns a dictionary based on the `listeners`
     """
 
     # Training
     random: Param[Random]
     """The random generator"""
 
     trainer: Param[Trainer]
     """Specifies how to train the model"""
 
-    scorer: Param[AbstractLearnableScorer]
+    model: Param[Module]
     """Defines the model that scores documents"""
 
     max_epochs: Param[int] = 1000
     """Maximum number of epochs"""
 
     steps_per_epoch: Param[int] = 128
     """Number of steps for one epoch (after each epoch results are logged)"""
@@ -286,22 +134,30 @@
     def __validate__(self):
         assert self.optimizers, "At least one optimizer should be defined"
         assert len(set(listener.id for listener in self.listeners)) == len(
             self.listeners
         ), "IDs of listeners should be unique"
         return super().__validate__()
 
-    def taskoutputs(self) -> LearnerOutput:
+    def task_outputs(self, dep) -> LearnerOutput:
         """Object returned when submitting the task"""
         return LearnerOutput(
             listeners={
-                listener.id: listener.taskoutputs(self) for listener in self.listeners
-            }
+                listener.id: listener.task_outputs(self, dep)
+                for listener in self.listeners
+            },
+            learned_model=ModuleLoader.construct(
+                self.model, self.last_checkpoint_path / TrainState.MODEL_PATH, dep
+            ),
         )
 
+    @property
+    def last_checkpoint_path(self):
+        return self.checkpointspath / "last"
+
     def execute(self):
         self.device.execute(self.device_execute)
 
     def device_execute(self, device_information: DeviceInformation):
         logger = logging.getLogger()
         logger.setLevel(logging.INFO)
         for handler in logger.handlers:
@@ -312,15 +168,15 @@
         self.context = TrainerContext(
             device_information,
             self.logpath,
             self.checkpointspath,
             self.max_epochs,
             self.steps_per_epoch,
             self.trainer,
-            self.scorer,
+            self.model,
             self.optimizer,
         )
 
         for hook in self.hooks:
             self.context.add_hook(hook)
 
         # Call hooks
@@ -333,27 +189,27 @@
         seed = self.random.state.randint((2**32) - 1)
         np.random.seed(seed)
         torch.manual_seed(seed)
         torch.cuda.manual_seed_all(seed)
 
         # Initialize the scorer and trainer
         self.logger.info("Scorer initialization")
-        self.scorer.initialize(self.random.state)
+        self.model.initialize(self.random.state)
 
         # Initialize the context and the listeners
         self.trainer.initialize(self.random.state, self.context)
         for listener in self.listeners:
             listener.initialize(self, self.context)
 
         self.logger.info("Moving to device %s", device_information.device)
-        self.scorer.to(device_information.device)
+        self.model.to(device_information.device)
         self.trainer.to(device_information.device)
         num_training_steps = self.max_epochs * self.steps_per_epoch
         self.optimizer.initialize(
-            self.optimizers, num_training_steps, self.scorer, self.use_fp16
+            self.optimizers, num_training_steps, self.model, self.use_fp16
         )
 
         foreach(
             self.context.hooks(InitializationHook),
             lambda hook: hook.after(self.context),
         )
 
@@ -379,14 +235,15 @@
 
                 if state.epoch == -1:
                     continue
 
                 if not state.cached and state.epoch % self.checkpoint_interval == 0:
                     # Save checkpoint if needed
                     self.context.save_checkpoint()
+                    self.context.copy(self.last_checkpoint_path)
 
                 # Call listeners
                 decision = LearnerListenerStatus.NO_DECISION
                 for listener in self.listeners:
                     # listener.__call__ returns True if we should stop
                     decision = decision.update(listener(state))
 
@@ -408,15 +265,15 @@
 
             # End of the learning process
             if state is not None and not state.cached:
                 # Set the hyper-parameters
                 metrics = {}
                 for listener in self.listeners:
                     listener.update_metrics(metrics)
-                self.context.writer.add_hparams(self.__tags__, metrics)
+                self.context.writer.add_hparams(getattr(self, "__tags__", {}), metrics)
 
     def iter_train(self, device_information) -> Iterator[TrainState]:
         """Train iteration"""
         # Try to load a checkpoint
 
         if self.context.load_bestcheckpoint(self.max_epochs):
             yield self.context.state
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pointwise.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pointwise.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from experimaestro import Param, Config
 from xpmir.letor.records import PointwiseRecords
 from xpmir.letor.trainers import LossTrainer
-from xpmir.letor.context import TrainerContext
+from xpmir.learning.context import Loss, TrainerContext
 from xpmir.rankers import LearnableScorer, ScorerOutputType
 
 
 class PointwiseLoss(Config):
     NAME = "?"
     weight: Param[float] = 1.0
 
@@ -51,15 +51,15 @@
         return self.loss(rel_scores, (target_relscores > 0).float())
 
 
 class PointwiseTrainer(LossTrainer):
     """Pointwise trainer"""
 
     lossfn: Param[PointwiseLoss] = MSELoss()
-    """Loss function to use (mse, mse-nil, l1, l1pos, smoothl1, cross_entropy, cross_entropy_logits, softmax, mean)"""
+    """Loss function to use"""
 
     def initialize(self, random: np.random.RandomState, ranker, context):
         super().initialize(random, context)
 
         self.sampler.initialize(self.random)
         self.lossfn.initialize(ranker)
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/trainers/batchwise.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/trainers/batchwise.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import sys
 from typing import Iterator
 import torch
 import torch.nn.functional as F
 from experimaestro import Config, Param, initializer
 from xpmir.letor.samplers import BatchwiseSampler, BatchwiseRecords
-from xpmir.letor.context import Loss, TrainerContext, TrainerContext
-from xpmir.rankers import LearnableScorer, ScorerOutputType
+from xpmir.learning.context import Loss, TrainerContext
+from xpmir.rankers import ScorerOutputType
 from xpmir.letor.trainers import LossTrainer
 import numpy as np
 
 
 class BatchwiseLoss(Config):
     NAME = "?"
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/trainers/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/trainers/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,64 +1,24 @@
-from typing import Dict, Iterator, List
-from experimaestro import Config, Param
-import torch.nn as nn
+from typing import Dict
+from experimaestro import Param
 import numpy as np
-from xpmir.letor.metrics import ScalarMetric
+from xpmir.learning.metrics import ScalarMetric
 from xpmir.letor.samplers import Sampler, SerializableIterator
 from xpmir.letor.records import BaseRecords
-from xpmir.utils.utils import EasyLogger, easylog
-from xpmir.letor.batchers import Batcher
-from xpmir.letor.context import (
-    TrainingHook,
+from xpmir.utils.utils import easylog
+from xpmir.learning.batchers import Batcher
+from xpmir.learning.context import (
     TrainerContext,
 )
+from xpmir.learning.trainers import Trainer
 
-from xpmir.utils.utils import foreach
 
 logger = easylog()
 
 
-class Trainer(Config, EasyLogger):
-    """Generic trainer"""
-
-    hooks: Param[List[TrainingHook]] = []
-    """Hooks for this trainer: this includes the losses, but can be adapted for
-        other uses
-
-        The specific list of hooks depends on the specific trainer
-    """
-
-    def initialize(
-        self,
-        random: np.random.RandomState,
-        context: TrainerContext,
-    ):
-        self.random = random
-        self.ranker = context.state.model
-        self.context = context
-
-        foreach(self.hooks, self.context.add_hook)
-
-    def to(self, device):
-        """Change the computing device (if this is needed)"""
-        foreach(self.context.hooks(nn.Module), lambda hook: hook.to(device))
-
-    def iter_batches(self) -> Iterator:
-        raise NotImplementedError
-
-    def process_batch(self, batch):
-        raise NotImplementedError()
-
-    def load_state_dict(self, state: Dict):
-        raise NotImplementedError()
-
-    def state_dict(self):
-        raise NotImplementedError()
-
-
 class LossTrainer(Trainer):
     """Trainer based on a loss function"""
 
     batcher: Param[Batcher] = Batcher()
     """How to batch samples together"""
 
     sampler: Param[Sampler]
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/trainers/multiple.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/trainers/multiple.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Dict, Iterator
 from experimaestro import Param
 import numpy as np
-from xpmir.letor.context import (
+from xpmir.learning.context import (
     TrainerContext,
 )
 from . import Trainer
 
 
 class MultipleTrainer(Trainer):
     """This trainer can be used to combine various trainers"""
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/trainers/pairwise.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/trainers/pairwise.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,95 +3,92 @@
 import sys
 from typing import Iterator
 import torch
 from torch import nn
 from torch.functional import Tensor
 import torch.nn.functional as F
 from experimaestro import Config, Param
-from xpmir.letor.context import Loss
-from xpmir.letor.metrics import ScalarMetric
+from xpmir.learning.context import Loss
+from xpmir.learning.metrics import ScalarMetric
 from xpmir.letor.records import (
     PairwiseRecord,
     PairwiseRecordWithTarget,
     PairwiseRecords,
     PairwiseRecordsWithTarget,
 )
 from xpmir.letor.samplers import PairwiseSampler, SerializableIterator
 from xpmir.letor.trainers import TrainerContext, LossTrainer
 import numpy as np
 from xpmir.rankers import LearnableScorer, ScorerOutputType
 from xpmir.utils.utils import foreach
 
 
 class PairwiseLoss(Config, nn.Module):
+    """Base class for any pairwise loss"""
+
     NAME = "?"
+
     weight: Param[float] = 1.0
+    """The weight :math:`w` with which the loss is multiplied (useful when
+    combining with other ones)"""
 
     def initialize(self, ranker: LearnableScorer):
         pass
 
     def process(self, scores: Tensor, context: TrainerContext):
         value = self.compute(scores, context)
         context.add_loss(Loss(f"pair-{self.NAME}", value, self.weight))
 
     def compute(self, scores: Tensor, info: TrainerContext) -> Tensor:
-        """
-        Compute the loss
-        Arguments:
-        - scores: A (batch x 2) tensor (positive/negative)
+
+        """Computes the loss
+
+        :param scores: A (batch x 2) tensor (positive/negative)
+        :param info: the trainer context
+        :return: a torch scalar
         """
         raise NotImplementedError()
 
 
 class CrossEntropyLoss(PairwiseLoss):
+    r"""Cross-Entropy Loss
+
+    Computes the cross-entropy loss
+
+    Classification loss (relevant vs non-relevant) where the logit
+    is equal to the difference between the relevant and the non relevant
+    document (or equivalently, softmax then mean log probability of relevant documents)
+    Reference: C. Burges et al., Learning to rank using gradient descent, 2005.
+
+    *warning*: this loss assumes the score returned by the scorer is a logit
+
+    .. math::
+
+        \frac{w}{N} \sum_{(s^+,s-)} \log \frac{\exp(s^+)}{\exp(s^+)+\exp(s^-)}
+    """
     NAME = "cross-entropy"
 
     def compute(self, rel_scores_by_record, info: TrainerContext):
         target = (
             torch.zeros(rel_scores_by_record.shape[0])
             .long()
             .to(rel_scores_by_record.device)
         )
         return F.cross_entropy(rel_scores_by_record, target, reduction="mean")
 
 
-class SoftmaxLoss(PairwiseLoss):
-    """Contrastive loss"""
-
-    NAME = "softmax"
+class HingeLoss(PairwiseLoss):
+    r"""Hinge (or max-margin) loss
 
-    def compute(self, rel_scores_by_record, info: TrainerContext):
-        return torch.mean(1.0 - F.softmax(rel_scores_by_record, dim=1)[:, 0])
+    .. math::
 
+       \frac{w}{N} \sum_{(s^+,s-)} \max(0, m - (s^+ - s^-))
 
-class LogSoftmaxLoss(PairwiseLoss):
-    """RankNet loss or log-softmax loss
-    Classification loss (relevant vs non-relevant) where the logit
-    is equal to the difference between the relevant and the non relevant
-    document (or equivalently, softmax then mean log probability of relevant documents)
-    Reference: C. Burges et al., Learning to rank using gradient descent, 2005.
     """
 
-    NAME = "ranknet"
-
-    def __initialize__(self, ranker: LearnableScorer):
-        assert (
-            ranker.outputType != ScorerOutputType.PROBABILITY
-        ), "Probability outputs are not handled"
-
-    def compute(self, scores: torch.Tensor, info: TrainerContext):
-        return -F.logsigmoid(scores[:, 0] - scores[:, 1]).mean()
-
-
-RanknetLoss = LogSoftmaxLoss
-
-
-class HingeLoss(PairwiseLoss):
-    """Hinge loss"""
-
     NAME = "hinge"
 
     margin: Param[float] = 1.0
     """The margin for the Hinge loss"""
 
     def compute(self, rel_scores_by_record, info: TrainerContext):
         return F.relu(
@@ -109,20 +106,29 @@
             -log_probs[targets > 0].sum() + (1.0 - log_probs[targets == 0].exp()).sum()
         )
 
         return loss / log_probs.numel()
 
 
 class PointwiseCrossEntropyLoss(PairwiseLoss):
-    """Point-wise cross-entropy loss
+    r"""Point-wise cross-entropy loss
+
+    This is a point-wise loss adapted as a pairwise one.
 
     This loss adapts to the ranker output type:
+
     - If real, uses a BCELossWithLogits (sigmoid transformation)
     - If probability, uses the BCELoss
     - If log probability, uses a BCEWithLogLoss
+
+    .. math::
+
+        \frac{w}{2N} \sum_{(s^+,s-)} \log \frac{\exp(s^+)}{\exp(s^+)+\exp(s^-)}
+        + \log \frac{\exp(s^-)}{\exp(s^+)+\exp(s^-)}
+
     """
 
     NAME = "pointwise-cross-entropy"
 
     def initialize(self, ranker: LearnableScorer):
         super().initialize(ranker)
         self.rankerOutputType = ranker.outputType
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/distillation/pairwise.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/distillation/pairwise.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import sys
 from typing import Iterator, List
 import torch
 from torch import nn
 from torch.functional import Tensor
 from experimaestro import Config, Param
 from xpmir.letor.records import Document, PairwiseRecord, PairwiseRecords
-from xpmir.letor.context import Loss
+from xpmir.learning.context import Loss
 from xpmir.letor.trainers import TrainerContext, LossTrainer
 from xpmir.utils.utils import batchiter, foreach
 from .samplers import DistillationPairwiseSampler, PairwiseDistillationSample
 import numpy as np
 from xpmir.rankers import LearnableScorer
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/letor/distillation/samplers.py` & `experimaestro-ir-1.0.0/src/xpmir/letor/distillation/samplers.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 from typing import Iterable, Iterator, NamedTuple, Optional, Tuple
 from datamaestro.data import File
 from experimaestro import Config, Meta, Param
+from ir_datasets.formats import GenericDoc
 from xpmir.letor.records import Query
-from datamaestro_text.data.ir import AdhocDocumentStore
-from xpmir.letor.samplers import Sampler
+from datamaestro_text.data.ir import DocumentStore
 from xpmir.rankers import ScoredDocument
 from xpmir.datasets.adapters import TextStore
+from xpmir.learning import Sampler
 import numpy as np
 
 from xpmir.utils.iter import SerializableIterator, SkippingIterator
 
 
 class PairwiseDistillationSample(NamedTuple):
     query: Query
@@ -28,15 +29,15 @@
 
 class PairwiseHydrator(PairwiseDistillationSamples):
     """Hydrate ID-based samples with document and/or query content"""
 
     samples: Param[PairwiseDistillationSamples]
     """The distillation samples without texts for query and documents"""
 
-    documentstore: Param[Optional[AdhocDocumentStore]]
+    documentstore: Param[Optional[DocumentStore]]
     """The store for document texts if needed"""
 
     querystore: Param[Optional[TextStore]]
     """The store for query texts if needed"""
 
     def __iter__(self) -> Iterator[PairwiseDistillationSample]:
         for sample in self.samples:
@@ -66,21 +67,21 @@
                 if self.with_queryid:
                     query = Query(row[2], None)
                 else:
                     query = Query(None, row[2])
 
                 if self.with_docid:
                     documents = (
-                        ScoredDocument(row[3], float(row[0]), None),
-                        ScoredDocument(row[4], float(row[1]), None),
+                        ScoredDocument(GenericDoc(row[3], None), float(row[0])),
+                        ScoredDocument(GenericDoc(row[4], None), float(row[1])),
                     )
                 else:
                     documents = (
-                        ScoredDocument(None, float(row[0]), row[3]),
-                        ScoredDocument(None, float(row[1]), row[4]),
+                        ScoredDocument(GenericDoc(None, row[3]), float(row[0])),
+                        ScoredDocument(GenericDoc(None, row[4]), float(row[1])),
                     )
 
                 yield PairwiseDistillationSample(query, documents)
 
 
 class DistillationPairwiseSampler(Sampler):
     """Just loops over samples"""
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py` & `experimaestro-ir-1.0.0/src/xpmir/dm/config/co/huggingface/datasets/sentence-transformers/msmarco-hard-negatives.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py` & `experimaestro-ir-1.0.0/src/xpmir/dm/config/ca/uwaterloo/jimmylin/anserini.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py` & `experimaestro-ir-1.0.0/src/xpmir/dm/config/com/github/sebastian-hofstaetter/neural-ranking-kd.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/interfaces/anserini.py` & `experimaestro-ir-1.0.0/src/xpmir/interfaces/anserini.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 import asyncio
 from pathlib import Path
+import attrs
 import contextlib
 import json
 import logging
 import os
 import re
 import subprocess
 import sys
 from typing import List, Optional
 from experimaestro import tqdm as xpmtqdm, Task, Meta
 
-from datamaestro_text.data.ir import AdhocDocumentStore
+from datamaestro_text.data.ir import DocumentStore
+from datamaestro_text.data.ir.base import IDDocument
 import datamaestro_text.data.ir.csv as ir_csv
 from datamaestro_text.data.ir.trec import (
-    AdhocDocuments,
-    AdhocTopics,
+    Documents,
+    Topics,
     TipsterCollection,
-    TrecAdhocTopics,
+    TrecTopics,
 )
-from experimaestro import Param, param, pathoption, progress, task
+from experimaestro import Param, param, pathoption, progress
 from tqdm import tqdm
 from xpmir.index.anserini import Index
-from xpmir.rankers import Retriever, ScoredDocument, RetrieverHydrator, document_cache
+from xpmir.rankers import Retriever, ScoredDocument, document_cache
 from xpmir.rankers.standard import BM25, QLDirichlet, Model
 from xpmir.utils.utils import Handler, StreamGenerator, needs_java
 
 pyserini_java = needs_java(11)
 
 
 def anserini_classpath():
@@ -48,23 +50,23 @@
     command = ["{}/bin/java".format(os.environ["JAVA_HOME"]), "-cp"]
     command.append(":".join(get_classpath() + [str(anserini_classpath())]))
 
     return command
 
 
 @pyserini_java
-@param("documents", type=AdhocDocuments)
+@param("documents", type=Documents)
 @param("threads", default=8, ignored=True)
 @pathoption("path", "index")
 class IndexCollection(Index, Task):
     """An [Anserini](https://github.com/castorini/anserini) index"""
 
     CLASSPATH = "io.anserini.index.IndexCollection"
 
-    documents: Param[AdhocDocuments]
+    documents: Param[Documents]
     """The documents to index"""
 
     thread: Meta[int] = 8
     """Number of threads when indexing"""
 
     id: Param[str] = ""
     """Use an empty ID since identifier is determined by documents"""
@@ -82,23 +84,23 @@
                 "-collection",
                 "TrecCollection",
                 "-input",
                 documents.path,
             ]
 
         @chandler()
-        def csv_collection(documents: ir_csv.AdhocDocuments):
+        def csv_collection(documents: ir_csv.Documents):
             def _generator(out):
                 counter = 0
                 size = os.path.getsize(documents.path)
                 with documents.path.open("rt", encoding="utf-8") as fp, tqdm(
                     total=size, unit="B", unit_scale=True
                 ) as pb:
                     for ix, line in enumerate(fp):
-                        # Update progress (TODO: cleanup/factorize the progress code)
+                        # Update progress
                         ll = len(line)
                         pb.update(ll)
                         counter += ll
                         progress(counter / size)
 
                         # Generate document
                         docid, text = line.strip().split(documents.separator, 1)
@@ -111,27 +113,29 @@
                 "-collection",
                 "JsonCollection",
                 "-input",
                 generator.filepath.parent,
             ]
 
         @chandler.default()
-        def generic_collection(documents: AdhocDocuments):
+        def generic_collection(documents: Documents):
             """Generic collection handler, supposes that we can iterate documents"""
 
             def _generator(out):
                 logging.info(
                     "Starting the iterator over the documents (%d documents)",
                     documents.documentcount,
                 )
                 for document in xpmtqdm(
                     documents.iter(), unit="documents", total=documents.documentcount
                 ):
                     # Generate document
-                    json.dump({"id": document.docid, "contents": document.text}, out)
+                    json.dump(
+                        {"id": document.get_id(), "contents": document.get_text()}, out
+                    )
                     out.write("\n")
 
             generator = StreamGenerator(_generator, mode="wt")
 
             return generator, [
                 "-collection",
                 "JsonCollection",
@@ -202,34 +206,33 @@
                 sys.exit(proc.returncode)
 
         asyncio.run(run([str(s) for s in command]))
 
 
 @pyserini_java
 @param("index", Index)
-@param("topics", AdhocTopics)
+@param("topics", Topics)
 @param("model", Model)
 @pathoption("path", "results.trec")
-@task()
-class SearchCollection:
+class SearchCollection(Task):
     def execute(self):
         command = javacommand()
         command.append("io.anserini.search.SearchCollection")
         command.extend(("-index", self.index.path, "-output", self.path))
 
         # Topics
 
         topicshandler = Handler()
 
         @topicshandler()
-        def trectopics(topics: TrecAdhocTopics):
+        def trectopics(topics: TrecTopics):
             return ("-topicreader", "Trec", "-topics", topics.path)
 
         @topicshandler()
-        def tsvtopics(topics: ir_csv.AdhocTopics):
+        def tsvtopics(topics: ir_csv.Topics):
             return ("-topicreader", "TsvInt", "-topics", topics.path)
 
         command.extend(topicshandler[self.topics])
 
         # Model
 
         modelhandler = Handler()
@@ -242,14 +245,28 @@
 
         # Start
         logging.info("Starting command %s", command)
         p = subprocess.run(command)
         sys.exit(p.returncode)
 
 
+@attrs.define()
+class AnseriniDocument(IDDocument):
+    """The hit returned by Anserini"""
+
+    lucene_docid: int
+    """Internal document ID"""
+
+    contents: Optional[str] = None
+    """Processed content"""
+
+    raw: Optional[str] = None
+    """Raw document"""
+
+
 @pyserini_java
 class AnseriniRetriever(Retriever):
     """An Anserini-based retriever
 
     Attributes:
         index: The Anserini index
         model: the model used to search. Only suupports BM25 so far.
@@ -278,53 +295,47 @@
         modelhandler[self.model]
 
     def _get_store(self) -> Optional[Index]:
         """Returns the associated index (if any)"""
         if self.index.storeContents:
             return self.index
 
-    def retrieve(self, query: str, content=False) -> List[ScoredDocument]:
+    def retrieve(self, query: str) -> List[ScoredDocument]:
+        # see
+        # https://github.com/castorini/anserini/blob/master/src/main/java/io/anserini/search/SimpleSearcher.java
         hits = self.searcher.search(query, k=self.k)
         store = self.get_store()
+
         return [
             ScoredDocument(
-                hit.docid,
+                AnseriniDocument(hit.docid, hit.lucene_docid, hit.contents, hit.raw)
+                if store is None
+                else store.document_ext(hit.docid),
                 hit.score,
-                hit.contents or store.document_text(hit.docid) if content else None,
             )
             for hit in hits
         ]
 
 
 @document_cache
 def index_builder(
-    documents: AdhocDocuments, *, launcher=None, **index_params
+    documents: Documents, *, launcher=None, **index_params
 ) -> IndexCollection:
     return IndexCollection(documents=documents, **index_params).submit(
         launcher=launcher
     )
 
 
 def retriever(
     index_builder: IndexCollection,
-    documents: AdhocDocuments,
+    documents: Documents,
     *,
-    content=True,
     k: int = None,
     model: Model = None,
+    store: DocumentStore = None,
 ):
+    """Function to construct an Anserini retriever"""
     index = index_builder(documents)
 
-    index_retriever = AnseriniRetriever(
-        index=index, k=k or AnseriniRetriever.k, model=model
+    return AnseriniRetriever(
+        index=index, k=k or AnseriniRetriever.k, model=model, store=store
     )
-
-    # Use hydrator
-    if content:
-        if isinstance(documents, AdhocDocumentStore):
-            return RetrieverHydrator(store=documents, retriever=index_retriever)
-
-        assert (
-            index.storeRaw or index.storeContents
-        ), "Index does not store content, and no store"
-
-    return index_retriever
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/interfaces/trec.py` & `experimaestro-ir-1.0.0/src/xpmir/interfaces/trec.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/interfaces/apex.py` & `experimaestro-ir-1.0.0/src/xpmir/interfaces/apex.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/interfaces/plaintext.py` & `experimaestro-ir-1.0.0/src/xpmir/interfaces/plaintext.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/utils/iter.py` & `experimaestro-ir-1.0.0/src/xpmir/utils/iter.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/src/xpmir/utils/utils.py` & `experimaestro-ir-1.0.0/src/xpmir/utils/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import inspect
 import logging
 import os
 from pathlib import Path
 import re
 from subprocess import run
 import tempfile
-from experimaestro import SubmitHook, Job, Launcher, submit_hook_decorator
+from experimaestro import SubmitHook, Job, Launcher
 from threading import Thread
 from typing import (
     BinaryIO,
     Callable,
     Iterator,
     List,
     TextIO,
@@ -66,19 +66,19 @@
     """Returns a result that depends on the type of the argument
 
     Example:
     ```
     handler = Handler()
 
     @handler()
-    def trectopics(topics: TrecAdhocTopics):
+    def trectopics(topics: TrecTopics):
         return ("-topicreader", "Trec", "-topics", topics.path)
 
     @handler()
-    def tsvtopics(topics: ir_csv.AdhocTopics):
+    def tsvtopics(topics: ir_csv.Topics):
         return ("-topicreader", "TsvInt", "-topics", topics.path)
 
     command.extend(handler[topics])
 
     ```
     """
 
@@ -200,29 +200,42 @@
         except Exception:
             # silently ignore
             pass
 
     raise FileNotFoundError(f"Java (version >= {min_version}) not found")
 
 
-class NeedsJava(SubmitHook):
+class needs_java(SubmitHook):
     """Experimaestro hook that ensures that JAVA_HOME is set"""
 
     def __init__(self, version: int):
         self.version = version
 
-    def __spec__(self):
+    def spec(self):
         return self.version
 
-    def __call__(self, job: Job, launcher: Launcher):
+    def process(self, job: Job, launcher: Launcher):
         job.environ["JAVA_HOME"] = find_java_home(self.version)
 
 
-@cache
-def needs_java(version: int):
-    """Decorator for tasks requiring java
+class Initializable:
+    """Base class for all initializable (but just once)"""
 
-    This decorator adds an experimaestro task hook that sets the java version
+    def initialize(self, *args, **kwargs):
+        """Main initialization
 
-    :param version: required major version
-    """
-    return submit_hook_decorator(NeedsJava(version))
+        Calls :py:meth:`__initialize__` once (using :py:meth:`__initialize__`)
+        """
+        if not self._initialized:
+            self._initialized = True
+            self.__initialize__(*args, **kwargs)
+        self._initialized = True
+
+    def __init__(self):
+        self._initialized = False
+
+    def __initialize__(self, *args, **kwargs):
+        """Initialize the object
+
+        Parameters depend on the actual class
+        """
+        pass
```

## Comparing `experimaestro-ir-0.6.2/src/xpmir/configuration/__init__.py` & `experimaestro-ir-1.0.0/src/xpmir/configuration/__init__.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/snippets/irds/irds-qrels.py` & `experimaestro-ir-1.0.0/snippets/irds/irds-qrels.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/examples/bm25.py` & `experimaestro-ir-1.0.0/examples/bm25.py`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/.github/workflows/pytest.yml` & `experimaestro-ir-1.0.0/.github/workflows/pytest.yml`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/.github/workflows/python-publish.yml` & `experimaestro-ir-1.0.0/.github/workflows/python-publish.yml`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/Makefile` & `experimaestro-ir-1.0.0/docs/Makefile`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/make.bat` & `experimaestro-ir-1.0.0/docs/make.bat`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/retrieval.rst` & `experimaestro-ir-1.0.0/docs/source/retrieval.rst`

 * *Files 16% similar despite different names*

```diff
@@ -39,25 +39,18 @@
 retrieval, by using a fully fledge retriever first, and then
 re-ranking the results.
 
 .. autoxpmconfig:: xpmir.rankers.TwoStageRetriever
 .. autoxpmconfig:: xpmir.rankers.DuoTwoStageRetriever
 
 
-Factories
----------
+Collection dependendant
+-----------------------
 
-For evaluation purposes, it might useful to generate retrievers
-that depend on a document collection. For instance, when
-using a two-stage retriever based on Anserini, this can be used
-to construct an index on a specific document collection before re-scoring
-the returned documents with a :py:class:`xpmir.rankers.Scorer`.
 
-.. autoclass:: xpmir.rankers.CollectionBasedRetrievers
-.. autofunction:: xpmir.rankers.collection_based_retrievers
 
 
 Anserini
 --------
 
 .. autoxpmconfig:: xpmir.interfaces.anserini.Index
 .. autoxpmconfig:: xpmir.interfaces.anserini.AnseriniRetriever
```

## Comparing `experimaestro-ir-0.6.2/docs/source/index.rst` & `experimaestro-ir-1.0.0/docs/source/index.rst`

 * *Files 24% similar despite different names*

```diff
@@ -1,23 +1,16 @@
 Welcome to Experimaestro IR documentation!
 ==========================================
 
-.. toctree::
-   :maxdepth: 2
-
-   data/index
-   retrieval
-   evaluation
-   letor/index
-   neural
-   hooks
-   text/index
-   papers/index
-   pretrained
-   misc
+experimaestro-IR (XPMIR) is a library for learning IR (neural) models.
+XPMIR defines a large set of components that can be composed arbitrarily,
+allowing to re-use easily components to build your own experiments.
+XPMIR is built upon `experimaestro <https://experimaestro-python.readthedocs.io/en/latest/>`_,
+a library which allows to build complex experimental plans while tracking parameters
+and to execute them locally or on a cluster.
 
 
 Install
 =======
 
 Base experimaestro-IR can be installed with `pip install xpmir`.
 Functionalities can be added by installing optional dependencies:
@@ -42,14 +35,31 @@
 with `TIPSTER_PATH` the path containg the TIPSTER collection (i.e. the folders `Disk1`, `Disk2`, etc.)
 
 You can then execute the following file:
 
 .. literalinclude:: ../../examples/bm25.py
 
 
+Table of Contents
+=================
+
+.. toctree::
+   :maxdepth: 2
+
+   data/index
+   retrieval
+   evaluation
+   letor/index
+   neural
+   hooks
+   text/index
+   papers/index
+   pretrained
+   misc
+
 
 Indices and tables
 ==================
 
 * :ref:`genindex`
 * :ref:`modindex`
 * :ref:`search`
```

## Comparing `experimaestro-ir-0.6.2/docs/source/hooks.rst` & `experimaestro-ir-1.0.0/docs/source/hooks.rst`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 
 .. autoxpmconfig:: InitializationHook
     :members: before, after
 
 Learning
 --------
 
-.. currentmodule:: xpmir.letor.context
+.. currentmodule:: xpmir.learning.context
 
 Hooks can be used to modify the learning process
 
 .. autoxpmconfig:: TrainingHook
 .. autoxpmconfig:: InitializationTrainingHook
     :members: before, after
```

## Comparing `experimaestro-ir-0.6.2/docs/source/evaluation.rst` & `experimaestro-ir-1.0.0/docs/source/evaluation.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/neural.rst` & `experimaestro-ir-1.0.0/docs/source/neural.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/pretrained.rst` & `experimaestro-ir-1.0.0/docs/source/pretrained.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/conf.py` & `experimaestro-ir-1.0.0/docs/source/conf.py`

 * *Files 11% similar despite different names*

```diff
@@ -84,56 +84,69 @@
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 html_static_path = ["_static"]
 
 
 intersphinx_mapping = {
-    "datamaestro_text": "https://datamaestro-text.readthedocs.io/en/latest/objects.inv",
+    "datamaestro_text": (
+        "https://datamaestro-text.readthedocs.io/en/latest",
+        None,
+    )  # /objects.inv",
 }
 
 # Autodoc options
 
 autodoc_default_options = {
     "show-inheritance": True,
 }
 
 autodoc_mock_imports = [
     # "torch",
     "faiss",
     "pandas",
     "bs4",
     "pytorch_transformers",
-    "transformers",
     "pytrec_eval",
     "apex",
     "pytorch_lightning",
     "ir_measures",
 ]
 
 
 def side_effect(*args, **kwargs):
     logging.error("Side effect %s / %s", args, kwargs)
 
 
+class DocMock(mock.Mock):
+    def __repr__(self):
+        names = []
+        mock = self
+        while mock is not None:
+            names.append(mock._mock_name)
+            mock = mock._mock_parent
+
+        return ".".join(names[::-1])
+
+
 for name in [
     "torch",
     "torch.nn",
     "torch.distributed",
     "torch.optim",
     "torch.nn.functional",
     "torch.functional",
     "torch.multiprocessing",
     "torch.utils",
     "torch.utils.tensorboard",
     "torch.utils.tensorboard.writer",
     "torch.optim.lr_scheduler",
     "transformers",
 ]:
-    sys.modules[name] = mock.Mock(side_effect=side_effect)
+    sys.modules[name] = DocMock(side_effect=side_effect, name=name)
 
 import torch.nn as nn  # noqa: E402
 
 
 class TorchModule:
     to = None
```

## Comparing `experimaestro-ir-0.6.2/docs/source/text/index.rst` & `experimaestro-ir-1.0.0/docs/source/text/index.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/papers/index.rst` & `experimaestro-ir-1.0.0/docs/source/papers/index.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/papers/helpers/msmarco.rst` & `experimaestro-ir-1.0.0/docs/source/papers/helpers/msmarco.rst`

 * *Files identical despite different names*

## Comparing `experimaestro-ir-0.6.2/docs/source/letor/optimization.rst` & `experimaestro-ir-1.0.0/docs/source/letor/optimization.rst`

 * *Files 13% similar despite different names*

```diff
@@ -1,41 +1,52 @@
 Optimization
 ============
 
 
+Modules
+-------
+
+
+.. autoxpmconfig:: xpmir.learning.optim.Module
+    :members:
+
+
+The module loader can be used to load a checkpoint
+
+.. autoxpmconfig:: xpmir.learning.optim.ModuleLoader
+
+
 Optimizers
 ----------
 
 
-.. autoxpmconfig:: xpmir.letor.optim.Optimizer
-.. autoxpmconfig:: xpmir.letor.optim.Adam
-.. autoxpmconfig:: xpmir.letor.optim.AdamW
-
-.. autoxpmconfig:: xpmir.letor.optim.ParameterOptimizer
-.. autoxpmconfig:: xpmir.letor.optim.ParameterFilter
-.. autoxpmconfig:: xpmir.letor.optim.Module
+.. autoxpmconfig:: xpmir.learning.optim.Optimizer
+.. autoxpmconfig:: xpmir.learning.optim.Adam
+.. autoxpmconfig:: xpmir.learning.optim.AdamW
 
+.. autoxpmconfig:: xpmir.learning.optim.ParameterOptimizer
+.. autoxpmconfig:: xpmir.learning.optim.ParameterFilter
 
 
 
 Batching
 --------
 
-.. autoxpmconfig:: xpmir.letor.batchers.Batcher
-.. autoxpmconfig:: xpmir.letor.batchers.PowerAdaptativeBatcher
+.. autoxpmconfig:: xpmir.learning.batchers.Batcher
+.. autoxpmconfig:: xpmir.learning.batchers.PowerAdaptativeBatcher
 
 Devices
 -------
 
 The devices configuration allow to select both the device to use for computation and
 the way to use it (i.e. multi-gpu settings).
 
-.. autoxpmconfig:: xpmir.letor.devices.Device
+.. autoxpmconfig:: xpmir.learning.devices.Device
 
-.. autoxpmconfig:: xpmir.letor.devices.CudaDevice
+.. autoxpmconfig:: xpmir.learning.devices.CudaDevice
 
 
 Schedulers
 ----------
 
-.. automodule:: xpmir.letor.schedulers
+.. automodule:: xpmir.learning.schedulers
     :members:
```

## Comparing `experimaestro-ir-0.6.2/docs/source/letor/index.rst` & `experimaestro-ir-1.0.0/docs/source/letor/index.rst`

 * *Files 16% similar despite different names*

```diff
@@ -13,24 +13,24 @@
 
 - the learner is the main class that runs the full process
 - learner listeners are used for validation
 - trainers that iterate over batches of data
 
 The main class is the Learner task.
 
-.. autoxpmconfig:: xpmir.letor.learner.Learner
-.. autonamedtuple:: xpmir.letor.learner.LearnerOutput
+.. autoxpmconfig:: xpmir.learning.learner.Learner
+.. autonamedtuple:: xpmir.learning.learner.LearnerOutput
 
 
 Listeners
 =========
 
 Listeners can be used to monitor the learning process
 
-.. autoxpmconfig:: xpmir.letor.learner.LearnerListener
+.. autoxpmconfig:: xpmir.learning.learner.LearnerListener
    :members: __call__
 
 .. autoxpmconfig:: xpmir.letor.learner.ValidationListener
 
 Scorers
 =======
 
@@ -38,13 +38,13 @@
 scorers, some are have learnable parameters.
 
 .. autoxpmconfig:: xpmir.rankers.Scorer
    :members: initialize, rsv, to, eval, getRetriever
 .. autoxpmconfig:: xpmir.rankers.RandomScorer
 .. autoxpmconfig:: xpmir.rankers.LearnableScorer
 
-.. autofunction:: scorer_retriever
+.. autofunction:: xpmir.rankers.scorer_retriever
 
 Retrievers
 ==========
 
 Scores can be used as retrievers through a :py:class:`xpmir.rankers.TwoStageRetriever`
```

## Comparing `experimaestro-ir-0.6.2/docs/source/letor/trainers.rst` & `experimaestro-ir-1.0.0/docs/source/letor/trainers.rst`

 * *Files 6% similar despite different names*

```diff
@@ -39,31 +39,27 @@
 
 Losses
 ------
 
 .. autoxpmconfig:: xpmir.letor.trainers.pairwise.PairwiseLoss
    :members: compute
 
+
 .. autoxpmconfig:: xpmir.letor.trainers.pairwise.CrossEntropyLoss
-.. autoxpmconfig:: xpmir.letor.trainers.pairwise.SoftmaxLoss
-.. autoxpmconfig:: xpmir.letor.trainers.pairwise.LogSoftmaxLoss
 .. autoxpmconfig:: xpmir.letor.trainers.pairwise.HingeLoss
+.. autoxpmconfig:: xpmir.letor.trainers.pairwise.PointwiseCrossEntropyLoss
+
 
 Other
 *****
 
 .. autoxpmconfig:: xpmir.letor.trainers.multiple.MultipleTrainer
 
-
-Distillation Trainers
-
-=====================
-
-Pairwise
-********
+Distillation: Pairwise
+**********************
 
 Trainer
 -------
 
 .. autoxpmconfig:: xpmir.letor.distillation.pairwise.DistillationPairwiseTrainer
 
 Losses
```

